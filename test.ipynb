{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee46079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Retrieval augumented generation with llamaindex and openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11949124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('data').load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3f539d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='c0b37b7c-3d3f-4469-83f3-adca7efd53f9', embedding=None, metadata={'page_label': 'Cover', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='James Serra\\nDeciphering \\nData   \\n Architectures\\nChoosing Between a Modern Data Warehouse,  \\nData Fabric, Data Lakehouse, and Data Mesh', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1cd21fc9-04f7-4c7e-856c-9f5432ac9fa6', embedding=None, metadata={'page_label': 'BackCover', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='    \\nDATA\\n“His ability to transform \\ncomplex technical \\nconcepts into clear, \\neasy-to-grasp \\nexplanations is  \\ntruly remarkable.”\\n—Annie Xu\\nSenior Data Customer Engineer, Google\\n“Put it on your desk—\\nyou’ll reference it often.”\\n—Sawyer Nyquist\\nOwner, Writer, and Consultant,  \\nThe Data Shop\\nDeciphering Data  \\nArchitectures\\nTwitter: @oreillymedia\\nlinkedin.com/company/oreilly-media\\nyoutube.com/oreillymedia \\nData fabric, data lakehouse, and data mesh have recently \\nappeared as viable alternatives to the modern data warehouse. \\nThese new architectures have solid benefits, but they’re also \\nsurrounded by a lot of hyperbole and confusion. This practical \\nbook provides a guided tour of these architectures to help data \\nprofessionals understand the pros and cons of each.\\nJames Serra, big data and data warehousing solution architect \\nat Microsoft, examines common data architecture concepts, \\nincluding how data warehouses have had to evolve to work \\nwith data lake features. You’ll learn what data lakehouses  \\ncan help you achieve, as well as how to distinguish data mesh \\nhype from reality. Best of all, you’ll be able to determine  \\nthe most appropriate data architecture for your needs. \\nWith this book, you’ll :\\n• Gain a working understanding of several data architectures\\n• Learn the strengths and weaknesses of each approach\\n• Distinguish data architecture theory from reality\\n• Pick the best architecture for your use case\\n• Understand the differences between data warehouses  \\nand data lakes\\n• Learn common data architecture concepts to help  \\nyou build better solutions\\n• Explore the historical evolution and characteristics  \\nof data architectures\\n• Learn essentials of running an architecture design session, \\nteam organization, and project success factors\\nJames Serra has worked at Microsoft \\nas a big data and data warehousing \\nsolution architect over the past  \\nnine years. He’s become a thought \\nleader in the use and application  \\nof big data and advanced analytics, \\nincluding data architectures such  \\nas the modern data warehouse,  \\ndata lakehouse, data fabric, and  \\ndata mesh.\\n9 781098 150761\\n57999\\nUS $79.99  CAN $99.99\\nISBN: 978-1-098-15076-1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='48c38810-b276-4c39-a8ad-1683879b6cac', embedding=None, metadata={'page_label': 'i', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Praise for Deciphering Data Architectures\\nIn Deciphering Data Architectures, James Serra does a wonderful job explaining the\\nevolution of leading data architectures and the trade-offs between them. This book should\\nbe required reading for current and aspiring data architects.\\n—Bill Anton, Data Geek, Opifex Solutions\\nJames has condensed over 30 years of data architecture knowledge and wisdom into this\\ncomprehensive and very readable book. For those who must do the hard work of\\ndelivering analytics rather than singing its praises, this is a must-read.\\n—Dr. Barry Devlin, Founder and Principal, 9sight Consulting\\nThis reference should be on every data architect’s bookshelf. With clear and insightful\\ndescriptions of the current and planned technologies, readers will gain a good sense of\\nhow to steer their companies to meet the challenges of the emerging data landscape. This\\nis an invaluable reference for new starters and veteran data architects alike.\\n—Mike Fung, Master Principal Cloud Solution Architect, Oracle\\nMarketing buzz and industry thought-leader chatter have sown much confusion about\\ndata architecture patterns. With his depth of experience and skill as a communicator,\\nJames Serra cuts through the noise and provides clarity on both long-established data\\narchitecture patterns and cutting-edge industry methods. that will aid data practitioners\\nand data leaders alike. Put it on your desk—you’ll reference it often.\\n—Sawyer Nyquist, Owner, Writer, and Consultant,\\nThe Data Shop', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='caa06273-d827-4b88-888c-dfe54009056d', embedding=None, metadata={'page_label': 'ii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The world of data architectures is complex and full of noise. This book provides a fresh,\\npractical perspective born of decades of experience. Whether you’re a beginner or an\\nexpert, everyone with an interest in data must read this book!\\n—Piethein Strengholt, author of Data Management at Scale\\nAn educational gem! Deciphering Data Architectures strikes a perfect balance between\\nsimplicity and depth, ensuring that technology professionals at all levels can grasp key\\ndata concepts and understand the essential trade-off decisions that really matter when\\nplanning a data journey.\\n—Ben Reyes, Cofounder and Managing Partner,\\nZetaMinusOne LLC\\nI recommend Deciphering Data Architectures as a resource that provides the knowledge to\\nunderstand and navigate the available options when developing a data architecture.\\n—Mike Shelton, Cloud Solution Architect, Microsoft\\nData management is critical to the success of every business. Deciphering Data\\nArchitectures breaks down the buzzwords into simple and understandable concepts and\\npractical solutions to help you get to the right architecture for your dataset.\\n—Matt Usher, Director, Pure Storage\\nAs a consultant and community leader, I often direct people to James Serra’s blog for up-\\nto-date and in-depth coverage of modern data architectures. This book is a great\\ncollection, condensing Serra’s wealth of vendor-neutral knowledge. My favorite is Part III,\\nwhere James discusses the pros and cons of each architecture design. I believe this book\\nwill immensely benefit any organization that plans to modernize its data estate.\\n—Teo Lachev, Consultant, Prologika\\nJames’s blog has been my go-to resource for demystifying architectural concepts,\\nunderstanding technical terminology, and navigating the life of a solution architect or\\ndata engineer. His ability to transform complex technical concepts into clear, easy-to-\\ngrasp explanations is truly remarkable. This book is an invaluable collection of his work,\\nserving as a comprehensive reference guide for designing and\\ncomprehending architectures.\\n—Annie Xu, Senior Data Customer Engineer, Google', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b441386-f849-4808-9f83-14b4598c44a0', embedding=None, metadata={'page_label': 'iii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='James’s superpower has always been taking complex subjects and explaining them in a\\nsimple way. In this book, he hits all the key points to help you choose the right data\\narchitecture and avoid common (and costly!) mistakes.\\n—Rod Colledge, Senior Technical Specialist (Data and AI),\\nMicrosoft\\nThis book represents a great milestone in the evolution of how we handle data in the\\ntechnology industry, and how we have handled it over several decades, or what is easily\\nthe equivalent of a career for most. The content offers great insights for the next\\ngeneration of data professionals in terms of what they need to think about when\\ndesigning future solutions. ‘deciphering’ is certainly an excellent choice of wording for\\nthis, as deciphering is exactly when it is needed when turning requirements\\ninto data products.\\n—Paul Andrew, CTO, Cloud Formations Consulting\\nA fantastic guide for data architects, this book is packed with experience and insights. Its\\ncomprehensive coverage of evolving trends and diverse approaches makes it an essential\\nreference for anyone looking to broaden their understanding of the field.\\n—Simon Whiteley, CTO, Advancing Analytics Limited\\nThere is no one whose knowledge of data architectures and data processes I trust more\\nthan James Serra. This book not only provides a comprehensive and clear description of\\nkey architectural principles, approaches, and pitfalls, it also addresses the all-important\\npeople, cultural, and organizational issues that too often imperil data projects before they\\nget going. This book is destined to become an industry primer studied by college students\\nand business professionals alike who encounter data for the first time (and maybe the\\nsecond and third time as well)!\\n—Wayne Eckerson, President of Eckerson Group\\nDeciphering Data Architectures is an indispensable vendor-neutral guide for today’s data\\nprofessionals. It insightfully compares historical and modern architectures, emphasizing\\nkey trade-offs and decision-making nuances in choosing an appropriate architecture for\\nthe evolving data-driven landscape.\\n—Stacia Varga, author and Data Analytics Consultant,\\nData Inspirations', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b3fe99a-0b76-4f1c-a6e9-304f7e1094a3', embedding=None, metadata={'page_label': 'iv', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Deep, practitioner wisdom within, the latest scenarios in the market today have vendor\\nspecific skew, latest terminology, and sales options. James takes his many years of\\nexpertise to give agnostic, cross cloud, vendor, vertical approaches from small to large.\\n—Jordan Martz, Senior Sales Engineer, Fivetran\\nData Lake, Data Lakehouse, Data Fabric, Data Mesh … It isn’t easy sorting the nuggets\\nfrom the noise. James Serra’s knowledge and experience is a great resource for everyone\\nwith data architecture responsibilities.\\n—Dave Wells, Industry Analyst, eLearningcurve\\nToo often books are “how-to” with no background or logic – this book solves that. With a\\ncomprehensive view of why data is arranged in a certain way, you’ll learn more about the\\nright way to implement the “how. ”\\n—Buck Woody, Principal Data Scientist, Microsoft\\nDeciphering Data Architectures is not only thorough and detailed, but it also provides a\\ncritical perspective on what works, and perhaps more importantly, what may not work\\nwell. Whether discussing older data approaches or newer ones such as Data Mesh, the\\nbook offers words of wisdom and lessons learned that will help any data practitioner\\naccelerate their data journey.\\n—Eric Broda, Entrepreneur, Data Consultant, author of\\nImplementing Data Mesh (O’Reilly)\\nNo other book I know explains so comprehensively about data lake, warehouse, mesh,\\nfabric and lakehouse! It is a must have book for all data architects and engineers.\\n—Vincent Rainardi, Data Architect and author', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca8735e9-2792-4073-8b9d-7aca8ffec8b1', embedding=None, metadata={'page_label': 'v', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='James Serra\\nDeciphering Data Architectures\\nChoosing Between a Modern Data Warehouse,\\nData Fabric, Data Lakehouse, and Data Mesh\\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d51a228b-694c-42ec-a872-4833c1431c0f', embedding=None, metadata={'page_label': 'vi', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='978-1-098-15076-1\\n[LSI]\\nDeciphering Data Architectures\\nby James Serra\\nCopyright © 2024 James Serra. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisitions Editor: Aaron Black\\nDevelopment Editor: Sarah Grey\\nProduction Editor: Katherine Tozer\\nCopyeditor: Paula L. Fleming\\nProofreader: Tove Innis\\nIndexer: WordCo Indexing Services, Inc.\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nFebruary 2024:  First Edition\\nRevision History for the First Release\\n2024-02-06: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098150761 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Deciphering Data Architectures, the\\ncover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the author and do not represent the publisher’s views. While\\nthe publisher and the author have used good faith efforts to ensure that the information and instructions\\ncontained in this work are accurate, the publisher and the author disclaim all responsibility for errors or\\nomissions, including without limitation responsibility for damages resulting from the use of or reliance\\non this work. Use of the information and instructions contained in this work is at your own risk. If any\\ncode samples or other technology this work contains or describes is subject to open source licenses or the\\nintellectual property rights of others, it is your responsibility to ensure that your use thereof complies\\nwith such licenses and/or rights.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dcf6c6e8-c6ba-4ca0-9c42-ea21af64acf4', embedding=None, metadata={'page_label': 'vii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To the loving memory of my grandparents—Dolly, Bill, Martha, and Bert', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3e4c67c-0fc5-4814-aca1-a1b0ce389820', embedding=None, metadata={'page_label': 'viii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xvii\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xix\\nPart I. Foundation\\n1. Big Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Big Data, and How Can It Help Y ou?                                                             4\\nData Maturity                                                                                                                   7\\nStage 1: Reactive                                                                                                           8\\nStage 2: Informative                                                                                                     8\\nStage 3: Predictive                                                                                                        9\\nStage 4: Transformative                                                                                               9\\nSelf-Service Business Intelligence                                                                                  9\\nSummary                                                                                                                         10\\n2. Types of Data Architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  13\\nEvolution of Data Architectures                                                                                  14\\nRelational Data Warehouse                                                                                          16\\nData Lake                                                                                                                        18\\nModern Data Warehouse                                                                                              20\\nData Fabric                                                                                                                      21\\nData Lakehouse                                                                                                              21\\nData Mesh                                                                                                                       22\\nSummary                                                                                                                         23\\nix', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3c6519c1-6fc4-49b5-bdff-848101f3762e', embedding=None, metadata={'page_label': 'ix', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3. The Architecture Design Session. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\\nWhat Is an ADS?                                                                                                            25\\nWhy Hold an ADS?                                                                                                       26\\nBefore the ADS                                                                                                               27\\nPreparing                                                                                                                     27\\nInviting Participants                                                                                                  29\\nConducting the ADS                                                                                                     31\\nIntroductions                                                                                                              31\\nDiscovery                                                                                                                     31\\nWhiteboarding                                                                                                           36\\nAfter the ADS                                                                                                                 37\\nTips for Conducting an ADS                                                                                        38\\nSummary                                                                                                                         40\\nPart II. Common Data Architecture Concepts\\n4. The Relational Data Warehouse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  43\\nWhat Is a Relational Data Warehouse?                                                                       43\\nWhat a Data Warehouse Is Not                                                                                    46\\nThe Top-Down Approach                                                                                            47\\nWhy Use a Relational Data Warehouse?                                                                     49\\nDrawbacks to Using a Relational Data Warehouse                                                   52\\nPopulating a Data Warehouse                                                                                      53\\nHow Often to Extract the Data                                                                                 53\\nExtraction Methods                                                                                                   54\\nHow to Determine What Data Has Changed Since the Last Extraction            54\\nThe Death of the Relational Data Warehouse Has Been Greatly Exaggerated     56\\nSummary                                                                                                                         57\\n5. Data Lake. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59\\nWhat Is a Data Lake?                                                                                                     60\\nWhy Use a Data Lake?                                                                                                   60\\nBottom-Up Approach                                                                                                   62\\nBest Practices for Data Lake Design                                                                            63\\nMultiple Data Lakes                                                                                                       69\\nAdvantages                                                                                                                  69\\nDisadvantages                                                                                                             72\\nSummary                                                                                                                         72\\nx | Table of Contents', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b6e09f12-9ad2-4967-a274-3fc2ea2f75aa', embedding=None, metadata={'page_label': 'x', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6. Data Storage Solutions and Processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75\\nData Storage Solutions                                                                                                  76\\nData Marts                                                                                                                   76\\nOperational Data Stores                                                                                            77\\nData Hubs                                                                                                                    79\\nData Processes                                                                                                                81\\nMaster Data Management                                                                                         81\\nData Virtualization and Data Federation                                                                82\\nData Catalogs                                                                                                              87\\nData Marketplaces                                                                                                      87\\nSummary                                                                                                                         89\\n7. Approaches to Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  91\\nOnline Transaction Processing Versus Online Analytical Processing                   92\\nOperational and Analytical Data                                                                                 94\\nSymmetric Multiprocessing and Massively Parallel Processing                             94\\nLambda Architecture                                                                                                    96\\nKappa Architecture                                                                                                       98\\nPolyglot Persistence and Polyglot Data Stores                                                         100\\nSummary                                                                                                                       101\\n8. Approaches to Data Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  103\\nRelational Modeling                                                                                                    103\\nKeys                                                                                                                            103\\nEntity–Relationship Diagrams                                                                               104\\nNormalization Rules and Forms                                                                            104\\nTracking Changes                                                                                                     106\\nDimensional Modeling                                                                                               107\\nFacts, Dimensions, and Keys                                                                                  107\\nTracking Changes                                                                                                     108\\nDenormalization                                                                                                      109\\nCommon Data Model                                                                                                 111\\nData Vault                                                                                                                     111\\nThe Kimball and Inmon Data Warehousing Methodologies                                113\\nInmon’s Top-Down Methodology                                                                         114\\nKimball’s Bottom-Up Methodology                                                                      115\\nChoosing a Methodology                                                                                        116\\nHybrid Models                                                                                                          118\\nMethodology Myths                                                                                                    120\\nSummary                                                                                                                       123\\nTable of Contents | xi', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e8fc61c3-1a1e-4425-8d6e-6323e81a0b92', embedding=None, metadata={'page_label': 'xi', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9. Approaches to Data Ingestion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  125\\nETL Versus ELT                                                                                                           125\\nReverse ETL                                                                                                                  127\\nBatch Processing Versus Real-Time Processing                                                      129\\nBatch Processing Pros and Cons                                                                            130\\nReal-Time Processing Pros and Cons                                                                   130\\nData Governance                                                                                                         131\\nSummary                                                                                                                       132\\nPart III. Data Architectures\\n10. The Modern Data Warehouse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  135\\nThe MDW Architecture                                                                                             135\\nPros and Cons of the MDW Architecture                                                               140\\nCombining the RDW and Data Lake                                                                        142\\nData Lake                                                                                                                   142\\nRelational Data Warehouse                                                                                     142\\nStepping Stones to the MDW                                                                                    143\\nEDW Augmentation                                                                                                143\\nTemporary Data Lake Plus EDW                                                                           145\\nAll-in-One                                                                                                                 146\\nCase Study: Wilson & Gunkerk’s Strategic Shift to an MDW                               147\\nChallenge                                                                                                                   147\\nSolution                                                                                                                     147\\nOutcome                                                                                                                    148\\nSummary                                                                                                                       148\\n11. Data Fabric. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  151\\nThe Data Fabric Architecture                                                                                    152\\nData Access Policies                                                                                                 154\\nMetadata Catalog                                                                                                      154\\nMaster Data Management                                                                                       155\\nData Virtualization                                                                                                   155\\nReal-Time Processing                                                                                              155\\nAPIs                                                                                                                            155\\nServices                                                                                                                      156\\nProducts                                                                                                                     156\\nWhy Transition from an MDW to a Data Fabric Architecture?                           156\\nPotential Drawbacks                                                                                                    157\\nSummary                                                                                                                       157\\nxii | Table of Contents', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5580ce4d-27d1-4f69-b834-ecc72fb797ab', embedding=None, metadata={'page_label': 'xii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12. Data Lakehouse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  159\\nDelta Lake Features                                                                                                     160\\nPerformance Improvements                                                                                      162\\nThe Data Lakehouse Architecture                                                                             163\\nWhat If Y ou Skip the Relational Data Warehouse?                                                 165\\nRelational Serving Layer                                                                                             167\\nSummary                                                                                                                       167\\n13. Data Mesh Foundation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  169\\nA Decentralized Data Architecture                                                                           170\\nData Mesh Hype                                                                                                           171\\nDehghani’s Four Principles of Data Mesh                                                                172\\nPrinciple #1: Domain Ownership                                                                          172\\nPrinciple #2: Data as a Product                                                                              173\\nPrinciple #3: Self-Serve Data Infrastructure as a Platform                                175\\nPrinciple #4: Federated Computational Governance                                          176\\nThe “Pure” Data Mesh                                                                                                177\\nData Domains                                                                                                              178\\nData Mesh Logical Architecture                                                                                179\\nDifferent Topologies                                                                                                    181\\nData Mesh Versus Data Fabric                                                                                   182\\nUse Cases                                                                                                                       183\\nSummary                                                                                                                       185\\n14. Should You Adopt Data Mesh? Myths, Concerns, and the Future. . . . . . . . . . . . . . . . .  187\\nMyths                                                                                                                             187\\nMyth: Using Data Mesh Is a Silver Bullet That\\nSolves All Data Challenges Quickly                                                                   187\\nMyth: A Data Mesh Will Replace Y our Data Lake and Data Warehouse         188\\nMyth: Data Warehouse Projects Are All Failing,\\nand a Data Mesh Will Solve That Problem                                                       188\\nMyth: Building a Data Mesh Means Decentralizing Absolutely Everything   188\\nMyth: Y ou Can Use Data Virtualization to Create a Data Mesh                       189\\nConcerns                                                                                                                       190\\nPhilosophical and Conceptual Matters                                                                 190\\nCombining Data in a Decentralized Environment                                             191\\nOther Issues of Decentralization                                                                            192\\nComplexity                                                                                                                193\\nDuplication                                                                                                               193\\nFeasibility                                                                                                                  194\\nPeople                                                                                                                         196\\nDomain-Level Barriers                                                                                            197\\nTable of Contents | xiii', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ed29111d-424d-45a0-b1d1-1b8e5ec25148', embedding=None, metadata={'page_label': 'xiii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Organizational Assessment: Should Y ou Adopt a Data Mesh?                             198\\nRecommendations for Implementing a Successful Data Mesh                             199\\nThe Future of Data Mesh                                                                                            201\\nZooming Out: Understanding Data Architectures and Their Applications       202\\nSummary                                                                                                                       203\\nPart IV. People, Processes, and Technology\\n15. People and Processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  207\\nTeam Organization: Roles and Responsibilities                                                      208\\nRoles for MDW , Data Fabric, or Data Lakehouse                                               208\\nRoles for Data Mesh                                                                                                 210\\nWhy Projects Fail: Pitfalls and Prevention                                                               213\\nPitfall: Allowing Executives to Think That BI Is “Easy”                                     213\\nPitfall: Using the Wrong Technologies                                                                  213\\nPitfall: Gathering Too Many Business Requirements                                         213\\nPitfall: Gathering Too Few Business Requirements                                            214\\nPitfall: Presenting Reports Without Validating Their Contents First               214\\nPitfall: Hiring an Inexperienced Consulting Company                                      214\\nPitfall: Hiring a Consulting Company That Outsources\\nDevelopment to Offshore Workers                                                                    215\\nPitfall: Passing Project Ownership Off to Consultants                                       215\\nPitfall: Neglecting the Need to Transfer Knowledge\\nBack into the Organization                                                                                 215\\nPitfall: Slashing the Budget Midway Through the Project                                 215\\nPitfall: Starting with an End Date and Working Backward                               216\\nPitfall: Structuring the Data Warehouse to Reflect the\\nSource Data Rather Than the Business’s Needs                                               216\\nPitfall: Presenting End Users with a Solution with Slow Response Times or\\nOther Performance Issues                                                                                   216\\nPitfall: Overdesigning (or Underdesigning) Y our Data Architecture              217\\nPitfall: Poor Communication Between IT and the Business Domains            217\\nTips for Success                                                                                                            217\\nDon’t Skimp on Y our Investment                                                                          217\\nInvolve Users, Show Them Results, and Get Them Excited                              218\\nAdd Value to New Reports and Dashboards                                                        219\\nAsk End Users to Build a Prototype                                                                      219\\nFind a Project Champion/Sponsor                                                                        219\\nMake a Project Plan That Aims for 80% Efficiency                                            220\\nSummary                                                                                                                       220\\nxiv | Table of Contents', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b2498c43-e4ef-400e-966a-c8ef0a38f1ca', embedding=None, metadata={'page_label': 'xiv', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='16. Technologies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  223\\nChoosing a Platform                                                                                                   223\\nOpen Source Solutions                                                                                            223\\nOn-Premises Solutions                                                                                            226\\nCloud Provider Solutions                                                                                       227\\nCloud Service Models                                                                                                 230\\nMajor Cloud Providers                                                                                            232\\nMulti-Cloud Solutions                                                                                             232\\nSoftware Frameworks                                                                                                  235\\nHadoop                                                                                                                      235\\nDatabricks                                                                                                                 238\\nSnowflake                                                                                                                  240\\nSummary                                                                                                                       241\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  243\\nTable of Contents | xv', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='44a6b735-ee71-4bd5-b89b-5e56b4bedefd', embedding=None, metadata={'page_label': 'xv', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5c953a69-d95b-4192-90dc-4544f40d2290', embedding=None, metadata={'page_label': 'xvi', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Foreword\\nNever in the history of the modern, technology-enabled enterprise has the data land‐\\nscape evolved so quickly. As the pace of change continues to accelerate, the data eco‐\\nsystem becomes far more complex—especially the value chains that connect suppliers\\nand customers to organizations. Data seems to flow everywhere. It has become one of\\nany organization’s most strategic assets, fundamentally underpinning digital transfor‐\\nmation, automation, artificial intelligence, innovation, and more.\\nThis increasing tempo of change intensifies the importance of optimizing your\\norganization’s data architecture to ensure ongoing adaptability, interoperability and\\nmaintainability. In this book, James Serra lays out a clear set of choices for the data\\narchitect, whether you are seeking to create a resilient design or simply reduce techni‐\\ncal debt.\\nIt seems every knowledge worker has a story of a meeting with data engineers and\\narchitects that felt like a Dilbert cartoon, where nobody seemed to speak the same\\nlanguage and the decisions were too complicated and murky. By defining concepts,\\naddressing concerns, dispelling myths, and proposing workarounds for pitfalls, James\\ngives the reader a working knowledge of data architectures and the confidence to\\nmake informed decisions. As a leader, I have been pleased with how well the book’s\\ncontents help to strengthen my data teams’ alignment by providing them with a com‐\\nmon vocabulary and references.\\nWhen I met James around 15 years ago, he was considering expanding his expertise\\nbeyond database administration to business intelligence and analytics. His insatiable\\ndesire to learn new things and share what he was learning to serve the greater good\\nleft a strong impression on me. It still drives him today. The countless blog posts, pre‐\\nsentations, and speaking engagements through which he shares the depth of his expe‐\\nrience now culminate in this expansive resource. This book will benefit all of us who\\nhandle data as we navigate an uncertain future.\\nxvii', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f6e6278f-c348-41fb-a30c-aa527b75babd', embedding=None, metadata={'page_label': 'xvii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Understanding the core principles of data architecture allows you to ride the waves of\\nchange as new data platforms, technology providers, and innovations emerge. Thank‐\\nfully, James has created a foundational resource for all of us. This book will help you\\nto see the bigger picture and design a bright future where your data creates the com‐\\npetitive advantage that you seek.\\n— Sean McCall\\nChief Data Officer, Oceaneering International\\nHouston, December 2023\\nxviii | Foreword', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9afd155a-e094-40a0-8298-4f68fbe62e6a', embedding=None, metadata={'page_label': 'xviii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preface\\nI’ve been in information technology (IT) for nearly 40 years. I’ve worked at compa‐\\nnies of all different sizes, I’ve worked as a consultant, and I’ve owned my own com‐\\npany. For the last 9 years, I have been at Microsoft as a data architect, and for the last\\n15 years, I have been involved with data warehousing. I’ve spoken about data thou‐\\nsands of times, to customers and groups.\\nDuring my career, I have seen many data architectures come and go. I’ve seen too\\nmany companies argue over the best approach and end up building the wrong data\\narchitecture—a mistake that can cost them millions of dollars and months of time,\\nputting them well behind their competitors.\\nWhat’s more, data architectures are complex. I’ve seen firsthand that most people are\\nunclear on the concepts involved, if they’re aware of them at all. Everyone seems to be\\nthrowing around terms like data mesh, data warehouse, and data lakehouse—but if\\nyou ask 10 people what a data mesh is, you will get 11 different answers.\\nWhere do you even start? Are these just buzzwords with a lot of hype but little sub‐\\nstance, or are they viable approaches? They may sound great in theory, but how prac‐\\ntical are they? What are the pros and cons of each architecture?\\nNone of the architectures discussed in this book is “wrong. ” They all have a place, but\\nonly in certain use cases. No one architecture applies to every situation, so this book\\nis not about convincing you to choose one architecture over the others. Instead, you\\nwill get honest opinions on the pros and cons of each architecture. Everything has\\ntrade-offs, and it’s important to understand what those are and not just go with an\\narchitecture that is hyped more than the others. And there is much to learn from each\\narchitecture, even if you don’t use it. For example, understanding how a data mesh\\nworks will get you thinking about data ownership, a concept that can apply to any\\narchitecture.\\nxix', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3db510da-f3b8-447e-9a0c-891b2db30ec9', embedding=None, metadata={'page_label': 'xix', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This book provides a basic grounding in common data architecture concepts. There\\nare so many concepts out there, and figuring out which to choose and how to imple‐\\nment them can be intimidating. I’m here to help you to understand all these concepts\\nand architectures at a high level so you get a sense of the options and can see which\\none is the most appropriate for your situation. The goal of the book is to allow you to\\ntalk intelligently about data concepts and architectures, then dig deeper into any that\\nare relevant to the solution you are building.\\nThere are no standard definitions of data concepts and architectures. If there were,\\nthis book would not be needed. My hope is to provide standard definitions that help\\neveryone get onto the same page, to make discussions easier. I’m under no illusion\\nthat my definitions will be universally accepted, but I’ d like to give us all a starting\\npoint for conversations about how to adjust those definitions.\\nI have written this book for anyone with an interest in getting value out of data,\\nwhether you’re a database developer or administrator, a data architect, a CTO or CIO,\\nor even someone in a role outside of IT. Y ou could be early in your career or a seas‐\\noned veteran. The only skills you need are a little familiarity with data from your\\nwork and a sense of curiosity.\\nFor readers with less experience with these topics, I provide an overview of big data\\n(Chapter 1 ) and data architectures ( Chapter 2 ), as well as basic data concepts\\n(Part II ). If you’ve been in the data game for a while but need to understand new\\narchitectures, you might find a lot of value in Part III, which dives into the details of\\nparticular data architectures, as well as in reviewing some of the basics. For you, this\\nwill be a quick cover-to-cover read; feel free to skip over the sections with material\\nthat you already know well. Also note that although the focus is on big data, the con‐\\ncepts and architectures apply even if you have “small” data.\\nThis is a vendor-neutral book. Y ou should be able to apply the architectures and con‐\\ncepts you learn here with any cloud provider. I’ll also note here that I am employed\\nby Microsoft. However, the opinions expressed here are mine alone and do not reflect\\nthe views of my employer.\\nI wrote this book because I have an innate curiosity that drives me to comprehend\\nand then share things in a way that everyone can understand. This book is the culmi‐\\nnation of my life’s work. I hope you find it valuable.\\nxx | Preface', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='938fb9f7-5dbd-4294-a338-3340f6115dac', embedding=None, metadata={'page_label': 'xx', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Conventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com.\\nPreface | xxi', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8f1fae4e-7fd9-458b-ad74-3479bfecdda8', embedding=None, metadata={'page_label': 'xxi', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-889-8969 (in the United States or Canada)\\n707-827-7019 (international or local)\\n707-829-0104 (fax)\\nsupport@oreilly.com\\nhttps://www.oreilly.com/about/contact.html\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at https://oreil.ly/deciphering-data-architectures.\\nFor news and information about our books and courses, visit https://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\\nFollow us on Twitter: https://twitter.com/oreillymedia.\\nWatch us on Y ouTube: https://youtube.com/oreillymedia.\\nAcknowledgments\\nThis book would not have been possible without the unwavering support and\\npatience of my wife, Mary. Her encouragement was instrumental during those long\\nnights spent writing, even when it meant missing out on card games with family and\\nfriends. Her presence was a constant source of comfort and motivation.\\nMy journey has been enriched by the support of my family: my parents, Jim and Lor‐\\nraine; my sisters, Denise, Michele, and Nicole; and my now-adult children, Lauren,\\nRaeAnn, and James, who have been a source of inspiration despite having no idea\\nwhat the book is about!\\nA heartfelt thank you goes to my mentors and colleagues from my years at Microsoft.\\nIndividuals like Steve Busby, Eduardo Kassner, and Martin Lee helped shape my\\ncareer, offering wisdom that often found its way into these pages.\\nGratitude is due to those who lent their critical eye and constructive feedback, nota‐\\nbly Piethein Strengholt, Barry Devlin, Bill Inmon, and Mike Shelton. Y our insights\\nwere invaluable.\\nI am particularly grateful to Sean McCall, not only for introducing me to the world of\\ndata warehousing many years ago but also for being a steadfast friend and agreeing to\\npen the forward for this book.\\nxxii | Preface', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='14f635d6-361d-4daf-aa13-7da1f8d9f347', embedding=None, metadata={'page_label': 'xxii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Finally, I want to thank all the amazing people at O’Reilly who made this book possi‐\\nble: Sarah Grey, whose amazing editing and suggestions make this book so much bet‐\\nter than if I went at it alone; Aaron Black, for helping me to create and the book\\nabstract and get it approved; Paula Fleming, for her exceptional copyediting; Katie\\nTozer, for managing the production of the book; Kristen Brown, for keeping every‐\\nthing running smoothly; and Suzanne Huston, for her wonderful marketing of the\\nbook.\\nI’ d like to express my deep appreciation to you, the reader, whose interest and engage‐\\nment in this work makes the countless hours of writing not just worthwhile but\\ndeeply fulfilling.\\nAs I close this chapter of my life and look forward to the new horizons ahead, I am\\nprofoundly grateful for the journey this book has taken me on and the incredible peo‐\\nple who have been a part of it.\\nPreface | xxiii', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56415f67-4653-49f0-bcef-dc95d9335e4c', embedding=None, metadata={'page_label': 'xxiii', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='98265ceb-f4da-4f51-b314-3f9f28f009c4', embedding=None, metadata={'page_label': 'xxiv', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='PART I\\nFoundation\\nIn Part I  of this book, I will lay the foundation for deciphering data architectures.\\nChapter 1 starts with a description of big data, while Chapter 2 provides an overview\\nof the types of data architectures and their evolution. Chapter 3 shows how you can\\nconduct an architecture design session to help determine the best data architecture to\\nuse for your project.\\nThis part of the book will give you a good starting point to understand the value of\\nbig data and the history of the architectures that captured that data. The later chap‐\\nters will then dig into the details.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02a95cc4-c312-4b8c-aef3-42645821bde2', embedding=None, metadata={'page_label': '1', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c716f331-5e50-4f69-bceb-e3e52f7ce4f3', embedding=None, metadata={'page_label': '2', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 1\\nBig Data\\nThe number of companies building data architectures has exploded in the 2020s.\\nThat growth is unlikely to slow down anytime soon, in large part because more data is\\navailable than ever before: from social media, Internet of Things (IoT) devices, home‐\\ngrown applications, and third-party software, to name just a few sources. According\\nto a 2023 BCG study , “the volume of data generated approximately doubled from\\n2018 to 2021 to about 84 ZB, a rate of growth that is expected to continue. ” The\\nresearchers “estimate that the volume of data generated will rise at a compound\\nannual growth rate (CAGR) of 21% from 2021 to 2024, reaching 149 ZB. ” Companies\\nknow that they can save millions of dollars and increase revenue by gathering this\\ndata and using it to analyze the past and present and make predictions about the\\nfuture—but to do that, they need a way to store all that data.\\nThroughout the business world, the rush is on to build data architectures as quickly\\nas possible. Those architectures need to be ready to handle any future data—no mat‐\\nter its size, speed, or type—and to maintain its accuracy. And those of us who work\\nwith data architectures need a clear understanding of how they work and what the\\noptions are. That’s where this book comes in. I have seen firsthand the result of not\\nproperly understanding data architecture concepts. One company I know of built a\\ndata architecture at the cost of $100 million over two years, only to discover that the\\narchitecture used the wrong technology, was too difficult to use, and was not flexible\\nenough to handle certain types of data. It had to be scraped and restarted from\\nscratch. Don’t let this happen to you!\\nIt’s all about getting the right information to the right people at the right time in the\\nright format. To do that, you need a data architecture to ingest, store, transform, and\\nmodel the data (big data processing) so it can be accurately and easily used. Y ou need\\nan architecture that allows any end user, even one with very little technical\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='36bd2cbc-9e3e-464a-8186-1797c35fa4f1', embedding=None, metadata={'page_label': '3', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='knowledge, to analyze the data and generate reports and dashboards, instead of rely‐\\ning on people in IT with deep technical knowledge to do it for them.\\nChapter 1 begins by introducing big data and some of its fundamental ideas. I then\\ndiscuss how companies are using their data, with an emphasis on business intelli‐\\ngence and how this usage grows as a company’s data architecture matures.\\nWhat Is Big Data, and How Can It Help You?\\nEven though the term big is used in big data, it’s not just about the size of the data. It’s\\nalso about all the data, big or small, within your company and all the data outside\\nyour company that would be helpful to you. The data can be in any format and can be\\ncollected with any degree of regularity. So the best way to define big data is to think of\\nit as all data, no matter its size (volume), speed (velocity), or type (variety). In addi‐\\ntion to those criteria, there are three more factors you can use to describe data: verac‐\\nity, variability, and value. Together, they’re commonly known as the “six Vs” of big\\ndata, as shown in Figure 1-1.\\nFigure 1-1. The six Vs of big data (source: The Cloud Data Lake by Rukmani Gopalan\\n[O’Reilly, 2023]).\\nLet’s take a closer look at each one:\\nVolume\\nVolume is the sheer amount of data generated and stored. This can be anywhere\\nfrom terabytes to petabytes of data, and it can come from a wide range of sources\\n4 | Chapter 1: Big Data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fe267038-faac-4fca-97df-db39e43ce6e1', embedding=None, metadata={'page_label': '4', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='including social media, ecommerce transactions, scientific experiments, sensor\\ndata from IoT devices, and much more. For example, data from an order entry\\nsystem might amount to a couple of terabytes a day, while IoT devices can stream\\nmillions of events per minute and generate hundreds of terabytes of data a day.\\nVariety\\nVariety refers to the wide range of data sources and formats. These can be further\\nbroken down into structured data  (from relational databases), semi-structured\\ndata (such as logs and CSV , XML, and JSON formats), unstructured data (like\\nemails, documents, and PDFs), and binary data (images, audio, video). For exam‐\\nple, data from an order entry system would be structured data because it comes\\nfrom a relational database, while data from an IoT device would likely be in\\nJSON format.\\nVelocity\\nVelocity refers to the speed at which data is generated and processed. Collecting\\ndata infrequently is often called batch processing; for example, each night the\\norders for the day are collected and processed. Data can also be collected very\\nfrequently or even in real time, especially if it’s generated at a high velocity such\\nas data from social media, IoT devices, and mobile applications.\\nVeracity\\nVeracity is about the accuracy and reliability of data. Big data comes from a huge\\nvariety of sources. Unreliable or incomplete sources can damage the quality of\\nthe data. For example, if data is coming from an IoT device, such as an outdoor\\nsecurity camera located at the front of your house that is pointing to the drive‐\\nway, and it sends you a text message when it detects a person, it’s possible that\\nenvironmental factors, such as weather, have made the device falsely detect a per‐\\nson, corrupting the data. Thus, the data needs to be validated when received.\\nVariability\\nVariability refers to the consistency (or inconsistency) of data in terms of its for‐\\nmat, quality, and meaning. Processing and analyzing structured, semi-structured,\\nand unstructured data formats require different tools and techniques. For exam‐\\nple, the type, frequency, and quality of sensor data from IoT devices can vary\\ngreatly. Temperature and humidity sensors might generate data points at regular\\nintervals, while motion sensors might generate data only when they detect\\nmotion.\\nValue\\nValue, the most important V, relates to the usefulness and relevance of data.\\nCompanies use big data to gain insights and make decisions that can lead to busi‐\\nness value, such as increased efficiency, cost savings, or new revenue streams. For\\nexample, by analyzing customer data, organizations can better understand their\\nWhat Is Big Data, and How Can It Help You? | 5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c56f1db9-57fd-4b43-a373-cc1a0bd73f80', embedding=None, metadata={'page_label': '5', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='customers’ behaviors, preferences, and needs. They can use this information to\\ndevelop better targeted marketing campaigns, improve customer experiences,\\nand drive sales.\\nCollecting big data allows companies to gain insights that help them make better\\nbusiness decisions. Predictive analysis is a type of data analysis that involves using\\nstatistical algorithms and machine learning to analyze historical data and make pre‐\\ndictions about future events and trends. This allows businesses to be proactive, not\\njust reactive.\\nY ou’ll hear many companies calling data “the new oil, ” because it has become an\\nincredibly valuable resource in today’s digital economy, much like oil was in the\\nindustrial economy. Data is like oil in a number of ways:\\n• It’s a raw material that needs to be extracted, refined, and processed in order to be\\nuseful. In the case of data, that involves collecting, storing, and analyzing it in\\norder to gain insights that can drive business decisions.\\n• It’s incredibly valuable. Companies that collect and analyze large amounts of data\\ncan use it to improve their products and services, make better business decisions,\\nand gain a competitive advantage.\\n• It can be used in a variety of ways. For example, if you use data to train machine\\nlearning algorithms, you can then use those algorithms to automate tasks, iden‐\\ntify patterns, and make predictions.\\n• It’s a powerful resource with a transformative effect on society. The widespread\\nuse of oil powered the growth of industries and enabled new technologies, while\\ndata has led to advances in fields like artificial intelligence, machine learning, and\\npredictive analytics.\\n• It can be a source of power and influence, thanks to all of the preceding factors.\\nFor example, you can use big data to generate reports and dashboards that tell you\\nwhere sales are lagging and take steps “after the fact” to improve those sales. Y ou can\\nalso use machine learning to predict where sales will drop in the future and take pro‐\\nactive steps to prevent that drop. This is called business intelligence (BI): the process of\\ncollecting, analyzing, and using data to help businesses make more informed\\ndecisions.\\nAs Figure 1-2 shows, I can collect data from new sources, such as IoT devices, web\\nlogs, and social media, as well as older sources, such as line-of-business, enterprise\\nresource planning (ERP), and customer relationship management (CRM) applica‐\\ntions. This data can be in multiple formats, such as CSV files, JSON files, and Parquet\\nfiles. It can come over in batches, say once an hour, or it can be streamed in multiple\\ntimes a second (this is called real-time streaming).\\n6 | Chapter 1: Big Data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3608c4cf-8b0b-4ebc-b64e-4d3f2d3022e3', embedding=None, metadata={'page_label': '6', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1-2. Big data processing (source: The Cloud Data Lake by Rukmani Gopalan\\n[O’Reilly, 2023])\\nIt’s important for companies to understand where they are in their journey to use data\\ncompared to other companies. This is called data maturity , and the next section\\nshows the stages of the data maturity journey so you can understand where your\\ncompany is.\\nData Maturity\\nY ou may have heard many in the IT industry use the term digital transformation,\\nwhich refers to how companies embed technologies across their business to drive\\nfundamental change in the way they get value out of data and in how they operate\\nand deliver value to customers. The process involves shifting away from traditional,\\nmanual, or paper-based processes to digital ones, leveraging the power of technology\\nto improve efficiency, productivity, and innovation. A big part of this transformation\\nis usually using data to improve a company’s business, which could mean creating a\\ncustomer 360 profile  to improve customer experience or using machine learning to\\nimprove the speed and accuracy of manufacturing lines.\\nThis digital transformation can be broken into four stages, called the enterprise data\\nmaturity stages, illustrated in Figure 1-3 . While this term is used widely in the IT\\nData Maturity | 7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='44b815e4-44c6-49b8-8f77-201428edf779', embedding=None, metadata={'page_label': '7', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='industry, I have my own take on what those stages look like. They describe the level of\\ndevelopment and sophistication an organization has reached in managing, utilizing,\\nand deriving value from its data. This model is a way to assess an organization’s data\\nmanagement capabilities and readiness for advanced analytics, artificial intelligence,\\nand other data-driven initiatives. Each stage represents a step forward in leveraging\\ndata for business value and decision making. The remainder of this section describes\\neach stage.\\nFigure 1-3. Enterprise data maturity stages\\nStage 1: Reactive\\nIn, the first stage, a company has data scattered all over, likely in a bunch of Excel\\nspreadsheets and/or desktop databases on many different filesystems, being emailed\\nall over the place. Data architects call this a spreadmart (short for “spreadsheet data\\nmart”): an informal, decentralized collection of data often found within an organiza‐\\ntion that uses spreadsheets to store, manage, and analyze data. Individuals or teams\\ntypically create and maintain spreadmarts independently of the organization’s cen‐\\ntralized data management system or official data warehouse. Spreadmarts suffer from\\ndata inconsistency, lack of governance, limited scalability, and inefficiency (since they\\noften result in a lot of duplicated effort).\\nStage 2: Informative\\nCompanies reach the second maturity stage when they start to centralize their data,\\nmaking analysis and reporting much easier. Stages 1 and 2 are for historical reporting,\\nor seeing trends and patterns from the past, so Figure 1-3 calls them the “rearview\\nmirror. ” In these stages, you are reacting to what’s already happened.\\n8 | Chapter 1: Big Data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0a443327-e399-4a0b-aa0f-bad760b42db7', embedding=None, metadata={'page_label': '8', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 Being on-prem, short for on-premises, refers to an organization’s hosting and managing its IT infrastructure—\\nsuch as servers, storage, and networking equipment—within its own physical facilities, usually called data\\ncenters. This contrasts with cloud-based services, where these resources are hosted and managed by third-\\nparty providers such as Azure, Amazon Web Services (AWS), or Google Cloud Platform (GCP) in remote\\ndata centers. I’ll discuss the benefits of moving from on-prem to the cloud in Chapter 16, but for now, know\\nthat transitioning from on-prem servers to cloud is a huge part of most enterprises’ digital transformations.\\nAt stage 2, the solution built to gather the data is usually not very scalable. Generally,\\nthe size and types of data it can handle are limited, and it can ingest data only infre‐\\nquently (every night, for example). Most companies are at stage 2, especially if their\\ninfrastructure is still on-prem.1\\nStage 3: Predictive\\nBy stage 3, companies have moved to the cloud and have built a system that can han‐\\ndle larger quantities of data, different types of data, and data that is ingested more fre‐\\nquently (hourly or streaming). They have also improved their decision making by\\nincorporating machine learning (advanced analytics) to make decisions in real time.\\nFor example, while a user is in an online bookstore, the system might recommend\\nadditional books on the checkout page based on the user’s prior purchases.\\nStage 4: Transformative\\nFinally, at stage 4, the company has built a solution that can handle any data, no mat‐\\nter its size, speed, or type. It is easy to onboard new data with a shortened lead time\\nbecause the architecture can handle it and has the infrastructure capacity to support\\nit. This is a solution that lets nontechnical end users easily create reports and dash‐\\nboards with the tools of their choice.\\nStages 3 and 4 are the focus of this book. In particular, when end users are doing their\\nown reporting, this activity is called self-service business intelligence, which is the sub‐\\nject of the next section.\\nSelf-Service Business Intelligence\\nFor many years, if an end user within an organization needed a report or dashboard,\\nthey had to gather all their requirements (the source data needed, plus a description\\nof what the report or dashboard should look like), fill out an IT request form, and\\nwait. IT then built the report, which involved extracting the data, loading it into the\\ndata warehouse, building a data model, and then finally creating the report or dash‐\\nboard. The end user would review it and either approve it or request changes. This\\noften resulted in a long queue of IT requests so that IT ended up becoming a huge\\nbottleneck. It took days, weeks, or even months for end users to get value out of the\\nSelf-Service Business Intelligence | 9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fbef2da8-9a00-486b-bde0-5fc2bc250157', embedding=None, metadata={'page_label': '9', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data. This process is now called “traditional BI, ” because in recent years something\\nbetter has developed: self-service BI.\\nThe goal of any data architecture solution you build should be to make it quick and\\neasy for any end user, no matter what their technical skills are, to query the data and\\nto create reports and dashboards. They should not have to get IT involved to perform\\nany of those tasks—they should be able to do it all on their own.\\nThis goal requires more up-front work; IT will have to contact all the end users to\\nfind out what data they need, then build the data architecture with their needs in\\nmind. But it will be well worth it for the time savings in creating the reports. This\\napproach eliminates the queue and the back-and-forth with IT, whose team members\\ngenerally have little understanding of the data. Instead, the end user, who knows the\\ndata best, accesses the data directly, prepares it, builds the data model, creates the\\nreports, and validates that the reports are correct. This workflow is much more\\nproductive.\\nCreating that easy-to-consume data solution results in self-service BI. Creating a\\nreport should be as easy as dragging fields around in a workspace. End users\\nshouldn’t have to understand how to join data from different tables or worry about a\\nreport running too slowly. When you are creating a data solution, always be asking:\\nHow easy will it be for people to build their own reports?\\nSummary\\nIn this chapter, you learned what big data is and how it can help you and your organi‐\\nzation make better business decisions, especially when combined with machine learn‐\\ning. Y ou saw how to describe big data using the six Vs, and you learned what data\\nmaturity means and how to identify its stages. Finally, you learned the difference\\nbetween traditional and self-service BI, where the goal is for everyone to be able to\\nuse the data to create reports and identify insights quickly and easily.\\nLet me now give you an idea of what to expect in the following chapters. In Chap‐\\nter 2, I will go into what a data architecture is and provide a high-level overview of\\nhow the types of data architectures have changed over the years. Chapter 3 is where I\\nshow you how to conduct an architecture design session to help determine the best\\ndata architecture to use.\\nPart II, “Common Data Architecture Concepts, ” gets into more detail about various\\narchitectures. In Chapter 4, I cover what a data warehouse is and what it is not, as\\nwell as why you would want to use one. I’ll discuss the “top-down approach, ” ask if\\nthe relational data warehouse is dead, and cover ways to populate a data warehouse.\\nChapter 5 describes what a data lake is and why you would want to use one. It also\\ndiscusses the bottom-up approach and then dives into data lake design and when to\\nuse multiple data lakes.\\n10 | Chapter 1: Big Data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='257d2809-63ff-40ac-baed-d2c24099dc17', embedding=None, metadata={'page_label': '10', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 6 is about common data architecture concepts related to data stores, includ‐\\ning data marts, operational data stores, master data management, and data virtualiza‐\\ntion. Chapter 7  covers common data architecture concepts related to design,\\nincluding OLTP versus OLAP , operational versus analytical data, SMP versus MPP ,\\nLambda architecture, Kappa architecture, and polyglot persistence. Chapter 8 is all\\nabout data modeling, including relational and dimensional modeling, the Kimball\\nversus Inmon debate, the common data model, and data vaults. And in Chapter 9,\\nyou will read about data ingestion, with sections on ELT versus ELT, reverse ELT,\\nbatch versus real-time processing, and data governance.\\nPart III focuses on specific data architectures. Chapter 10 describes the modern data\\nwarehouse and the five stages of building one. Chapter 11  covers the data fabric\\narchitecture and its use cases. Chapter 12 goes over the data lakehouse architecture\\nand the trade-offs of not using a relational data warehouse.\\nChapters 13 and 14 are both about data mesh architectures—there’s a lot to talk\\nabout! Chapter 13  focuses on the data mesh’s decentralized approach and the four\\nprinciples of a data mesh, and it describes what data domains and data products are.\\nChapter 14 gets into the concerns and challenges of building a data mesh and tackles\\nsome common myths of data mesh. It’ll help you check if you are ready to adopt a\\ndata mesh. It finishes with what the future of the data mesh might look like.\\nChapter 15 looks at why projects succeed and why they fail, and it describes the team\\norganization you’ll need for building a data architecture. Finally, Chapter 16 is a dis‐\\ncussion of open source, the benefits of the cloud, the major cloud providers, being\\nmulti-cloud, and software frameworks.\\nNow I’m about to revolutionize your data world. Are you ready?\\nSummary | 11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e1097d20-f1c8-4a97-a2df-87d4545da55b', embedding=None, metadata={'page_label': '11', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5bcb2f19-59ae-4130-87d6-1c6829c7bcf4', embedding=None, metadata={'page_label': '12', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 2\\nTypes of Data Architectures\\nIt’s absolutely vital to invest time up front designing and building the right data archi‐\\ntecture. I found this out the hard way early in my career. I was so excited to start\\nbuilding my solution that I breezed over important decisions about the design of the\\narchitecture and what products to use. Three months into the project, I realized the\\narchitecture would not support some of the required data sources. We essentially had\\nto restart the project from scratch and come up with another architecture and differ‐\\nent products, wasting a ton of money and time. Without the right planning, end users\\nwon’t get value out of your solution, they’ll be angry about the missed deadlines, and\\nyour company risks falling farther and farther behind its competitors.\\nWhen building a data solution, you need a well-thought-out blueprint to follow. That\\nis where a data architecture comes into play. A data architecture defines a high-level\\narchitectural approach and concept to follow, outlines a set of technologies to use,\\nand states the flow of data that will be used to build your data solution to capture big\\ndata. Deciding on a data architecture can be very challenging, as there is no one-size-\\nfits-all architecture. Y ou can’t flip through a book to find a stock list of architecture\\napproaches with corresponding products to use. There’s no simple flowchart to follow\\nwith decision trees that will lead you to the perfect architecture. Y our architectural\\napproach and the technologies you use will vary greatly from customer to customer,\\nuse case to use case.\\nThe major types of high-level architectural approaches and concepts are exactly what\\nthis book is about. It’s not a stock list, but it will give you a sense of what they’re all\\nabout. Although it’s useful to separate data architectures into types based on their\\ncharacteristics, which I will do in this book, that’s not the same thing as choosing\\nfrom a bunch of predefined one-size-fits-all templates. Each data architecture is\\nunique and requires a customized approach to meet specific business needs.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='294a2153-a5fc-488f-b332-119fe7207854', embedding=None, metadata={'page_label': '13', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 E. F . Codd, “Derivability, Redundancy, and Consistency of Relations Stored in Large Data Banks, ” Communi‐\\ncations of the ACM, 13, no. 6 (1970): 377–87, available at A Relational Model of Data for Large Shared Data\\nBanks.\\nData architecture refers to the overall design and organization of data within an infor‐\\nmation system. Predefined templates for data architectures can seem like an easy way\\nto set up a new system quickly. However, data architecture templates often fail to\\naccount for the specific requirements and constraints of the system to which they are\\nbeing applied, which can lead to issues with data quality, system performance, and\\nmaintenance. Additionally, the organization’s needs and data systems are likely to\\nchange over time, requiring updates and adjustments to the data architecture. A stan‐\\ndardized template may not be flexible enough to accommodate these changes, which\\ncan introduce inefficiencies and limitations in the system.\\nThis chapter gives a brief guided tour of the major types the book will cover: rela‐\\ntional data warehouse, data lake, modern data warehouse, data fabric, data lakehouse,\\nand data mesh. Each type will get its own chapter with plenty of detail later in the\\nbook.\\nEvolution of Data Architectures\\nA relational database stores data in a structured manner, with relationships between\\nthe data elements defined by keys. The data is typically organized into tables, with\\neach table consisting of rows and columns. Each row represents a single instance of\\ndata, while each column represents a specific attribute of the data.\\nRelational databases are designed to handle structured data, and they provide a\\nframework for creating, modifying, and querying data using a standardized language\\nknown as Structured Query Language, or SQL. The relational model was first pro‐\\nposed by Edgar F . Codd in 1970,1 and since the mid-’70s it has become the dominant\\nmodel for database management systems. Most operational applications need to per‐\\nmanently store data, and a relational database is the tool of choice for a large majority.\\nIn relational databases, where consistency and data integrity are of primary impor‐\\ntance, data is usually organized with an approach called schema-on-write. Schema\\nrefers to the formal structure that defines the organization of—and relationships\\nbetween—tables, fields, data types, and constraints. It serves as a blueprint for storing\\nand managing data and ensures consistency, integrity, and efficient organization\\nwithin the database. Relational databases (and relational data warehouses) require a\\nbit of up-front work before data can land in them. Y ou must create the database and\\nits tables, fields, and schema, then write the code to transfer the data into the data‐\\nbase. With a schema-on-write approach, the data schema is defined and enforced\\nwhen data is written or ingested into the database. Data must adhere to the\\n14 | Chapter 2: Types of Data Architectures', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fda1316a-f728-4e7c-9269-eb23ee346519', embedding=None, metadata={'page_label': '14', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='predefined schema, including data types, constraints, and relationships, before it can\\nbe stored.\\nBy contrast, in a schema-on-read approach, the schema is applied when data is read or\\naccessed, rather than when it is written. Data can be ingested into the storage system\\nwithout conforming to a strict schema, and the structure is defined only when data is\\nqueried or consumed. This approach offers more flexibility in storing unstructured or\\nsemi-structured data, and it is commonly used in data lakes, discussed later in this\\nchapter.\\nAt a high level, data architectures provide a framework for organizing and managing\\ndata in a way that supports the needs of an organization. This involves defining how\\ndata is collected, stored, processed, and accessed, as well as maintaining data quality,\\nsecurity, and privacy. While data architectures can take many different forms, some\\ncommon elements include:\\nData storage\\nAll data architectures need to specify how data is stored, including the physical\\nstorage medium (such as hard drives or cloud storage) and the data structures\\nused to organize the data.\\nData processing\\nData architectures need to define how data is processed, including any transfor‐\\nmations or calculations that are performed on the data before it is stored or\\nanalyzed.\\nData access\\nData architectures need to provide mechanisms for accessing data, including user\\ninterfaces and application program interfaces (APIs) that enable data to be quer‐\\nied and analyzed.\\nData security and privacy\\nData architectures need to incorporate mechanisms for ensuring the security and\\nprivacy of data, such as access controls, encryption, and data masking.\\nData governance\\nData architectures need to provide frameworks for managing data, including\\nquality standards, lineage tracking, and retention policies.\\nOverall, the main goal of a data architecture is to enable an organization to manage\\nand leverage its data assets effectively in order to support its business objectives and\\ndecision-making processes.\\nTable 2-1, which provides a high-level comparison of the characteristics of the data\\narchitectures I cover in this book, should give you a starting point to determine\\nwhich architecture may be the best fit for your use case.\\nEvolution of Data Architectures | 15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='52914bf9-a63a-4843-9f61-595b9f1dd246', embedding=None, metadata={'page_label': '15', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 2-1. Comparison of data architectures\\nCharacteristic Relational\\ndata\\nwarehouse\\nData lake Modern data\\nwarehouse\\nData fabric Data\\nlakehouse\\nData mesh\\nYear introduced 1984 2010 2011 2016 2020 2019\\nCentralized/\\ndecentralized\\nCentralized Centralized Centralized Centralized Centralized Decentralized\\nStorage type Relational Object Relational and\\nobject\\nRelational and\\nobject\\nObject Domain-\\nspecific\\nSchema type Schema-on-\\nwrite\\nSchema-on-\\nread\\nSchema-on-\\nread and\\nschema-on-\\nwrite\\nSchema-on-\\nread and\\nschema-on-\\nwrite\\nSchema-on-\\nread\\nDomain-\\nspecific\\nData security High Low to\\nmedium\\nMedium to high High Medium Domain-\\nspecific\\nData latency Low High Low to high Low to high Medium to\\nhigh\\nDomain-\\nspecific\\nTime to value Medium Low Low Low Low High\\nTotal cost of\\nsolution\\nHigh Low Medium Medium to high Low to\\nmedium\\nHigh\\nSupported use\\ncases\\nLow Low to\\nmedium\\nMedium Medium to high High High\\nDifficulty of\\ndevelopment\\nLow Medium Medium Medium Medium to\\nhigh\\nHigh\\nMaturity of\\ntechnology\\nHigh Medium Medium to high Medium to high Medium to\\nhigh\\nLow\\nCompany skill set\\nneeded\\nLow Low to\\nmedium\\nMedium Medium to high Medium to\\nhigh\\nHigh\\nRelational Data Warehouse\\nRelational databases were the mainstay of data storage for several decades. The first\\nrelational data warehouse used in production was the Teradata system, developed at\\nStanford University by Dr. Jack E. Shemer, who founded the Teradata Corporation in\\n1979. Wells Fargo Bank installed the first Teradata system in 1983 and used it to ana‐\\nlyze financial data.\\nAs organizations began generating ever more vast amounts of data, it was increas‐\\ningly challenging to process and analyze that data without a long delay. The\\nlimitations of relational databases led to the development of relational data\\n16 | Chapter 2: Types of Data Architectures', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aefc8b31-33ab-470b-9945-e84580b0aefb', embedding=None, metadata={'page_label': '16', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 A note on language: Y ou might see people refer to the relational data warehouse architecture as a traditional\\ndata warehouse. In this book, I mostly use relational data warehouse (RDW), sometimes shortening it to data\\nwarehouse. These terms all refer to the same thing.\\n3 RDW’s popularity increased thanks largely to both Barry Devlin and Bill Inmon, which I discuss in Chapter 8.\\nwarehouses,2 which became more widely popular in the late 1980s, 3 about 15 years\\nafter relational databases had come along. A relational data warehouse (RDW) is a\\nspecific type of relational database that is designed for data warehousing and business\\nintelligence applications, with optimized query performance and support for large-\\nscale data analysis. While both relational data warehouses and transactional process‐\\ning use the relational model to organize data, a relational data warehouse is typically\\nlarger in scale and is optimized for analytical queries.\\nRDWs have both a compute engine and storage. The compute engine is the process‐\\ning power used to query the data. The storage is relational storage, which holds data\\nthat is structured via tables, rows, and columns. The RDW’s compute power can be\\nused only on its relational storage—they are tied together.\\nSome of the most important features of RDWs include transaction support (ensuring\\nthat data is processed reliably and consistently), audit trails (keeping a record of all\\nactivity performed on the data in the system), and schema enforcement (ensuring\\nthat data is organized and structured in a predefined way).\\nIn the 1970s and 1980s, organizations were using relational databases for operational\\napplications like order entry and inventory management. These applications are\\ncalled online transaction processing (OLTP) systems. OLTP systems can make create,\\nread, update, and delete changes to data in a database, or CRUD operations. CRUD\\noperations form the foundation of data manipulation and management in data archi‐\\ntectures. They’re essential to designing and implementing data storage systems and\\nuser interfaces that interact with data.\\nCRUD operations require fast response times from the application so that end users\\ndon’t get frustrated at how long it takes to update data. Y ou can run queries and gen‐\\nerate reports on a relational database that’s in use by an operational application, but\\ndoing so uses a lot of resources and can conflict with other CRUD operations run‐\\nning at the same time. That can slow everything down.\\nRDWs were invented in part to solve this problem. The data from the relational data‐\\nbase is copied into a data warehouse, and users can run queries and reports against\\nthe data warehouse instead of the relational database. This way, they’re not taxing the\\nsystem that houses the relational database and slowing the application for end users.\\nRDWs also centralize data from multiple applications in order to improve reporting,\\nas pictured in Figure 2-1.\\nChapter 4 will discuss RDWs in more detail.\\nRelational Data Warehouse | 17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5892ce1f-a057-4a62-9f01-dcbb35eb3c89', embedding=None, metadata={'page_label': '17', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4 Open source refers to software whose source code is made available for free to the public for use, modification,\\nand distribution.\\nFigure 2-1. Data warehousing\\nData Lake\\nThe data lake is a more recent concept, first appearing around 2010. Y ou can think of\\na data lake as a glorified filesystem, not very different from the filesystem on your lap‐\\ntop. A data lake is simply storage—unlike a relational data warehouse, there is no\\ncompute engine associated with it. Fortunately, there are many compute engines that\\ncan work with data lakes, so compute power is usually cheaper for a data lake than for\\na relational data warehouse. Another difference is that while RDWs use relational\\nstorage, data lakes use object storage, which does not need the data to be structured\\ninto rows and columns.\\nData lake storage technology started with the Apache Hadoop Distributed File Sys‐\\ntem (HDFS), a free open source technology hosted almost exclusively on-prem that\\nwas very popular in the early 2010s. 4 HDFS is a scalable, fault-tolerant distributed-\\nstorage system designed to run on commodity hardware. It is a core component of\\nthe Apache Hadoop ecosystem, which I discuss more in Chapter 16. As cloud com‐\\nputing continued to grow in importance, data lakes were built in the cloud using a\\ndifferent type of storage, and most data lakes now exist in the cloud.\\nIn contrast to a relational data warehouse, a data lake is schema-on-read, meaning\\nthat no up-front work is needed to put data in the data lake: it can be as simple as\\ncopying files into it, like you would with folders on your laptop. The data in a data\\nlake is stored in its natural (or raw) format, meaning it can go from its source system\\ninto the data lake without being transformed into another format. For example, if you\\nexport data from a relational database into a file in raw CSV format, you could store it\\nunaltered in a data lake. If you wanted to store it in a relational data warehouse, how‐\\never, you’ d have to transform it to fit into the rows and columns of a table.\\n18 | Chapter 2: Types of Data Architectures', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c1a3ee8-267d-49a7-8510-7837e667359d', embedding=None, metadata={'page_label': '18', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='When you copy a data file into a data lake, its schema might not be copied along with\\nit or might be in a different file. So you must define the schema by creating it or pull‐\\ning it from the separate file—hence the term schema-on-read. As Figure 2-2 shows,\\ndata from source systems such as operational application databases, sensor data, and\\nsocial media data can all land in the data lake. These files could hold data that is\\nstructured (like data from relational databases), semi-structured (like CSV , logs,\\nXML, or JSON files), or unstructured (such as from emails, documents, and PDFs).\\nThey can even hold binary data (like images, audio, and video).\\nFigure 2-2. Data lake\\nData lakes started out as the solution to all the problems with relational data ware‐\\nhouses, including high cost, limited scalability, poor performance, data preparation\\noverhead, and limited support for complex data types. Companies selling Hadoop\\nand data lakes, such as Cloudera, Hortonworks, and MapR, hyped them as if they\\nwere filled with unicorns and rainbows that would copy and clean data and make it\\navailable to end users with magical ease. They claimed that data lakes could replace\\nrelational data warehouses entirely, in a “one technology to do everything” approach.\\nMore than a few companies decided to save money by using free open source tools\\nfor all their technology.\\nThe problem was that querying data lakes isn’t actually that easy: it requires some\\nfairly advanced skill sets. IT would tell end users, “Hey, we copied all the data you\\nneed into this data lake. Just go and open a Jupyter notebook and use Hive and\\nPython to build your reports with the files in these folders. ” This failed miserably,\\nsince most end users did not have anywhere near the skills needed to do all that.\\nCompanies found out the hard way that these complex, difficult-to-use solutions\\nwound up actually being more expensive because of hardware and support costs, pro‐\\nduction delays, and lost productivity. In addition, data lakes did not have some of the\\nData Lake | 19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7fd09ebf-367e-40ac-b8d0-2040101bf55f', embedding=None, metadata={'page_label': '19', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='features people liked about data warehouses, like transaction support, schema\\nenforcement, and audit trails. This resulted in two of the three top data lake suppliers,\\nHortonworks and MapR, going out of business.\\nBut the data lake did not go away. Instead, its purpose morphed into a different, but\\nvery useful, one: staging and preparing data. Chapter 5  will discuss data lakes in\\ndetail.\\nModern Data Warehouse\\nRelational data warehouses and data lakes, on their own, are simplistic architectures.\\nThey use only one technology to centralize the data, with few to no supporting prod‐\\nucts. When you use more technologies and products to support relational data ware‐\\nhouses or data lakes, they evolve into the architectures discussed in this and the\\nfollowing chapters. Data lakes failed to replace relational data warehouses but still\\noffered benefits for staging and preparing data. Why not have the advantages of both?\\nAround 2011, many companies started building architectures that place data lakes\\nside by side with relational data warehouses to form the data architecture we now call\\nthe modern data warehouse  (MDW), shown in Figure 2-3 . The modern in modern\\ndata warehouse  refers to the use of newer technologies and approaches to data\\nwarehousing.\\nFigure 2-3. Modern data warehouse (MDW)\\nIt’s a “best of both worlds” approach: the data lake is for staging and preparing data,\\nand data scientists use it to build machine learning models; the data warehouse is for\\nserving, security, and compliance, and business users do their querying and reporting\\nwith it. Chapter 10  will provide much more detail on modern data warehouse\\narchitectures.\\n20 | Chapter 2: Types of Data Architectures', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ee28d347-cdd1-4e0e-abc3-478331204bf5', embedding=None, metadata={'page_label': '20', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Fabric\\nData fabrics started to appear around 2016. Y ou could think of the data fabric archi‐\\ntecture as an evolution of the modern data warehouse architecture, with more tech‐\\nnology added to source more data, secure it, and make it available. Also,\\nimprovements have been made to how the system ingests data, transforms, queries,\\nsearches, and access data, as Figure 2-4 shows. With all those additions, the system\\nbecomes a “fabric”—a large framework that can ingest any sort of data. Chapter 11\\nwill go into this in more detail.\\nFigure 2-4. Data fabric\\nData Lakehouse\\nThe term data lakehouse is a portmanteau (blend) of data lake and data warehouse.\\nData lakehouse architectures became popular around 2020, when the company Data‐\\nbricks started using the term. The concept of a lakehouse is to get rid of the relational\\ndata warehouse and use just one repository, a data lake, in your data architecture. All\\ntypes of data—structured, semi-structured, and unstructured—are ingested into the\\ndata lake, and all queries and reports are done from the data lake.\\nI know what you’re thinking: “Wait a minute. Y ou said that data lakes took this same\\napproach when they first appeared, and it failed miserably! What changed?” The\\nanswer, as shown in Figure 2-5, is a transactional storage software layer that runs on\\ntop of an existing data lake and makes it work more like a relational database. The\\ncompeting open source options for this layer include Delta Lake, Apache Iceberg, and\\nApache Hudi. All of this will be covered in more detail in Chapter 12.\\nData Lakehouse | 21', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aaea8642-dca3-4385-a982-71c00960d90d', embedding=None, metadata={'page_label': '21', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 2-5. Data lakehouse\\nData Mesh\\nThe term data mesh was first introduced in a May 2019 blog post by Zhamak Deh‐\\nghani, founder and CEO of Nextdata and author of Data Mesh: Delivering Data-\\nDriven Value at Scale (O’Reilly, 2022). In December 2020, Dehghani further clarified\\nwhat a data mesh is and set out four underpinning principles. Data mesh architec‐\\ntures have been an extremely hot topic ever since, getting talked about in tons of\\nblogs, presentations, conferences, and media coverage and even appearing in the\\nGartner Hype Cycle for data management . There is a lot to like about data mesh\\narchitecture, but despite the hype, it is only a fit for a small number of use cases.\\nThe modern data warehouse, data fabric, and data lakehouse architectures all involve\\ncentralizing data: copying operational data into a central location owned by IT under\\nan architecture that IT controls, where IT then creates analytical data (the left side of\\nFigure 2-6). This centralized approach brings three main challenges: data ownership,\\ndata quality, and organizational/technical scaling. The aim of the data mesh is to solve\\nthese challenges.\\nIn a data mesh, data is kept within several domains within a company, such as manu‐\\nfacturing, sales, and suppliers (the right side of Figure 2-6). Each domain has its own\\nmini IT team that owns its data, cleans it, creates the analytical data, and makes it\\navailable. Each domain also has its own compute and storage infrastructure. This\\nresults in a decentralized architecture where data, people, and infrastructure are\\nscaled out—the more domains you have, the more people and infrastructure you get.\\nThe system can handle more data, and IT is no longer a bottleneck.\\n22 | Chapter 2: Types of Data Architectures', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2898b3cb-bfa7-4ff8-90cd-7df28894e8bd', embedding=None, metadata={'page_label': '22', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 2-6. Data mesh\\nIt is important to understand that data mesh is a concept, not a technology. There is\\nno “data mesh in a box” that you can buy. Implementing data mesh involves a very\\nbig organizational and cultural shift that very few companies are ready for. (Indeed,\\nmost companies aren’t even large enough to be considered for a data mesh architec‐\\nture: this is very much an enterprise approach.) Building it requires determining\\nwhich pieces of existing technology you can repurpose for it and which pieces you\\nwill have to create. Each domain gets to determine what technologies it will use to\\nbuild out its part of the data mesh, which could include building a modern data ware‐\\nhouse, data fabric, or data lakehouse. There is much to discuss regarding data mesh\\narchitectures, and I’ll do that in Chapters 13 and 14.\\nSummary\\nNow that you have a high-level understanding of the types of data architectures, the\\nnext chapter will talk about how to determine the best data architecture to use: a pro‐\\ncess called an architecture design session.\\nSummary | 23', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5fe2e660-b9f0-4842-a6ff-1d686beca1b5', embedding=None, metadata={'page_label': '23', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='017c137f-de32-4066-9d6a-423e6e885875', embedding=None, metadata={'page_label': '24', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 3\\nThe Architecture Design Session\\nI have conducted hundreds of architecture design sessions and have found them to be\\nvital to building a successful data solution. This chapter walks you through how to\\nhold a successful architecture design session that puts your data solution on the right\\npath.\\nWhat Is an ADS?\\nAn architecture design session  (ADS) is a structured discussion with business and\\ntechnical stakeholders that’s driven by technical experts and focused on defining and\\nplanning the high-level design of a solution to collect data for specific business\\nopportunities. The first ADS is the start of the architecture process and will lead to\\nmany more discussions (including, quite possibly, other ADSs) to support the data\\nsolution project. The ADS should produce two deliverables:\\n• An architecture (or “blueprint”) that can serve as a starting point for the data\\nsolution\\n• A high-level plan of action, which may include follow-on demonstrations, proofs\\nof concept, and product discussions\\nAn ADS is not a technical workshop, a technical training, a tech demonstration, nor a\\nlow-level requirements session.\\n25', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf67a766-4110-48df-b046-06be2af91644', embedding=None, metadata={'page_label': '25', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Why Hold an ADS?\\nThere are many reasons to hold an ADS. First, it often brings together multiple stake‐\\nholders from the same organization, including some who have never met face-to-\\nface. Even just the act of getting together in person or virtually can foster creativity\\nand problem solving. Further, it may include stakeholders who understand their busi‐\\nness very well and who can describe in great detail the specific use cases they are\\nlooking to solve.\\nThe ADS also differentiates projects with a solid business case from “science projects”\\nmeant to test some technology or product. It flushes out the reasons for the project\\nand, if it is indeed a science project, limits the resources devoted to it.\\nWhat’s more, an ADS provides a structured framework for “thinking big, starting\\nsmall. ” As noted, this involves coming up with a high-level architecture, but should\\nalso involve discussing how you can start small to get quick wins. For example, the\\narchitecture may ultimately involve ingesting data from dozens of sources, but the\\nADS might choose a low-risk, high-reward approach, such as starting with two or\\nthree high-value data sources that require fewer estimated work hours to complete.\\nY ou can also include your organization’s customers and/or partner organizations in\\nan ADS. When I was about to hold my first ADS, another Microsoft architect\\ndescribed the process as “business therapy. ” I quickly found out what he meant. We\\nhad about a dozen attendees from a customer company, most of whom had never\\nmet. We started discussing their current environment, and a few brought up some\\npain points that were blocking progress. To my surprise, another attendee from that\\norganization chimed in with advice on how to solve the blocker. Then they solved\\ntwo more blockers while I sat back and listened. (Of course, my coworkers and I\\noften help with removing blockers, too, but it was enlightening to be quiet and allow\\nthe others to work it out among themselves.)\\nY ou’ll learn from your customer in other ways, too, especially if your account team\\nhasn’t been as thorough as they might have been. Y ou can dig into any questions you\\nhave about their current environment, their pain points, their goals, and other infor‐\\nmation you’ll need.\\nA customer ADS also gives your account team many ways to follow up, such as\\ndemos, proofs of concept, and answers to technical questions. It’s a great way to kick‐\\nstart an account that hasn’t been getting much traction and accelerate the sales cycle.\\nAnd if you’re working with a partner, such as a consulting firm, consider bringing\\nthem into the ADS alongside the customer in order to educate them both on how to\\nuse the technology. Make sure to clear this with the customer first, especially if you’ll\\nbe discussing sensitive information.\\n26 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd98f4ac-ff55-4538-9176-676257b3d883', embedding=None, metadata={'page_label': '26', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Before the ADS\\nIn this section, I’ll assume that you are the architect who will be leading the ADS.\\nEven if you’re not, I recommend you read this section before participating in an ADS.\\nPreparing\\nSet aside at least a day to prepare for an ADS workshop. Y ou’ll need to prepare for the\\nlogistics of the meeting. Is it physical or virtual? If physical, do you have a big enough\\nroom? If virtual, what platform will you use? Does it have the necessary equipment,\\nsuch as a whiteboard, audio/video, and other supplies? Who will take care of lunch?\\nIf the ADS will be in person, and especially if a large group is traveling to get there, I\\nusually set aside a full seven- to eight-hour day, since logistics may make it difficult to\\nget everyone in the same room again. If much of the technology is new to the cus‐\\ntomer, however, six or seven hours is often enough. If you notice that participants are\\nfeeling overwhelmed, end the ADS early, because at that point anything else you\\ncover will be forgotten.\\nFor remote ADSs, two four-hour days is usually better than one full day. I try to hold\\nthe two four-hour sessions on back-to-back days or, at least, within a week of each\\nother.\\nMake sure you find out the project budget and timeline and identify the decision\\nmaker. This is important whether the meeting is internal or not, but if you have a\\nlong list of customers who want to hold an ADS, this information can help you focus\\non your best prospects. If the ADS is for a customer, read their latest corporate report\\nfor insight on their wider interests and concerns.\\nI also recommend you hold a pre-call to speak with the account team in your com‐\\npany that is supporting the customer. Y ou’ll likely want to discuss the following:\\n• Details about the customer and the project’s background and context\\n• The customer’s problems with their current architecture\\n• How you can help the customer\\n• The account team’s goals and the outcome they want from the meeting (for\\nexample, do they want the customer to request a proof of concept?)\\n• How well the customer knows your products\\n• How you will run the ADS and what you expect from the account team\\nThe deliverables from this meeting will be an email to the account team that recaps\\nthe call, a draft agenda that they can send to the customer, a seating chart (if the ADS\\nwill be held in person), and a reminder for the account team to set up a pre-call with\\nthe customer.\\nBefore the ADS | 27', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85bed2c8-5a9a-47e8-97dd-1d7ccc9b0f31', embedding=None, metadata={'page_label': '27', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Make sure to tell the customer the main reason for the pre-call with them is to help\\nyou all prepare as much as possible for the ADS. The information surfaced will allow\\nyou to brush up on topics as needed or bring in subject matter experts (SMEs) to help\\nwith areas that you are not familiar with. Y ou’ll also share the agenda, and together\\nyou can make sure it does not include any off-topic items. For example, if the focus of\\nthe ADS is on data architectures, discussions of security or DevOps are best taken\\ncare of outside the ADS.\\nThe customer pre-call should include a few things:\\nHere’s what I understand so far…\\nFirst, recap your understanding of the problem or opportunity so the customer\\nhas a chance to correct any misperceptions.\\nTell me about the attendees…\\nDetermine who will be in attendance and what roles they play in the organiza‐\\ntion. This will help you determine how deep or high-level to make the technol‐\\nogy discussion.\\nHere’s what will happen…\\nProvide an agenda and go over it with the customer, but assure them that the dis‐\\ncussion can pivot if needed. Explain the approach you will take in running the\\nADS and set their expectations accordingly. For example, it’s a good idea to tell\\nthem to be 100% focused—no phones or emails for anyone involved, including\\nyou.\\nWhat do you have for us to look at?\\nThis is the discovery phase. Encourage them to send you any architecture dia‐\\ngrams for their current solution, any architectures they may have drafted for\\ntheir future solution, and any documents describing their current solution and\\nfuture plans.\\nWhat would a successful ADS look like to you?\\nAsk, “Is there anything else that I should be aware of?” This question can surface\\nquestions or goals that might otherwise be missed.\\nFigure 3-1 shows a sample ADS agenda.\\nMake sure to include scheduled breaks in the agenda—and don’t skip them. It may be\\ntempting to bypass a break if you are in a groove with the discussion, but that’s usu‐\\nally a mistake. Ask someone on your account team to remind you to take a break in\\ncase you forget.\\n28 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d52d3825-c790-461b-ad75-96666604a9f0', embedding=None, metadata={'page_label': '28', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3-1. Sample agenda for an ADS about a modern data warehouse project\\nSet aside some time beforehand to get to know the ins and outs of any tools you will\\nuse to whiteboard (for example, a Surface Hub) so you aren’t figuring things out dur‐\\ning the ADS. Pretend you are in an ADS and use the whiteboard to write down goals,\\npain points, and follow-ups and to draw the architecture, based on your guesses as to\\nhow the customer will answer your questions. Y ou can even ask a coworker to role-\\nplay as the customer to help you out.\\nInviting Participants\\nThe people who should participate in the ADS will vary a bit depending on whether\\nthe customer is internal (a group within your company) or external (an outside com‐\\npany you do business with).\\nBefore the ADS | 29', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='034c35fe-05b9-4cac-bc54-29f0f2d4577b', embedding=None, metadata={'page_label': '29', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='From the customer side (or if it’s an internal ADS, from the group for which you are\\ndoing the ADS), attendees should include:\\n• The sponsors (at least one from business and one from IT)\\n• Business representatives\\n• Technical representatives\\n• Project manager\\n• Advisors, architects, developers, and infrastructure or operations people, as\\nnecessary\\nFrom your team, include:\\n• An architect to facilitate the session and make sure the ADS meets its objectives\\n• From the account team, an account executive, account specialist, and/or cloud\\nsolution architect\\n• Subject matter experts (SMEs) to provide in-depth knowledge on specific topics\\n• At least one person to take notes (possibly a member of the account team)\\nIf the customer wants to talk about a subject area that you are not familiar with, in\\nmost cases it’s better to bring in a SME instead of trying to learn the topic yourself,\\nespecially if your time is limited. Make sure to thank the SME and send their manager\\na nice email afterward. When a SME attends the ADS, everyone wins: the customer is\\nhappy to get expert advice, the account team is happy because the customer is satis‐\\nfied, and you can learn from the SME.\\nIf you’re new to facilitating, you might also want to ask a mentor to participate, to\\nback you up during the ADS (by answering questions that you can’t answer), and to\\ngive you feedback afterward on what you did well and what you could do better.\\nSometimes I arrange the agenda to allow people to attend only part of the day. For\\nexample, I might plan for the morning to be more about business and discovery, sav‐\\ning the technical side for the afternoon when we’ll delve into the proposed architec‐\\nture and technologies. In this example, the C-level executives would likely attend the\\nmorning session, and the technical people would attend in the afternoon. However,\\nanyone who wanted to could attend for the whole day—maybe some C-level people\\nwould be interested in the technical discussion, while some technical people would be\\ninterested in discovery.\\n30 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ee3c44e0-6027-4924-9c1c-0fce9dc44f65', embedding=None, metadata={'page_label': '30', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Conducting the ADS\\nRemember, you’re in charge, so it’s up to you to set the tone and keep the meeting on\\ncourse. If someone brings up an off-topic point, say that you can discuss it offline;\\nthen write it on the whiteboard for follow-up. Similarly, if you or someone else agrees\\nto follow up on a specific item, add it to the whiteboard. Let the account team take\\nnotes—you need to focus on talking with the customer and whiteboarding.\\nAt the halfway point, check the agenda and goals. If you are behind, mention to the\\ncustomer that the meeting will need to stay on track to get through all the goals. If it\\nseems you will not have time to cover all the goals, ask the customer to reprioritize\\nthe goals so that any goals that don’t get discussed are the lowest priority.\\nIntroductions\\nAt the start of the ADS, have everyone introduce themselves, stating their name, role,\\nand what (if anything) they know about the technology you will discuss. Then, per‐\\nhaps most important, ask what they want to get out of the ADS (their learning goals).\\nWrite the goals on the whiteboard and prioritize them as a group. If some attendees\\nwere not on the pre-call, make clear what the ADS will cover.\\nUse this time to explain how and why the ADS has been arranged and set ground\\nrules for the rest of the day. For instance, you might tell them that you’ll send them a\\ncopy of the final whiteboard so they don’t need to take pictures of it, that it will be an\\ninteractive session and you encourage lots of questions, that the agenda is just a rec‐\\nommendation and the group can cover other topics they feel would be valuable, and\\nthat you can have follow-up meetings to cover additional topics if you run out of time\\n(there’s no need to rush through important conversations).\\nDiscovery\\nDiscovery is where you spend an hour or two at the beginning of the ADS asking\\nquestions about things like:\\n• The customer’s current pain points\\n• Their current technology and architecture\\n• Their future architecture (if any has been discussed)\\n• Any decisions they’ve already made about technologies, products, or tools they\\nuse or plan to use\\n• Current and future use cases\\n• Details on their business\\nConducting the ADS | 31', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5ef09071-d148-447b-a527-6ae18cd19c16', embedding=None, metadata={'page_label': '31', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The customer should be doing most of the talking, especially early on in the ADS.\\nY ou won’t learn anything about them if you do all the talking. I always start out ask‐\\ning a ton of questions and getting answers before I go into “education mode, ” where I\\nstart talking for long periods. Even then, I always make sure to pause for questions.\\nY ou may want to let them know early on that you will stop periodically for questions.\\nA good architect asks lots of questions. As the saying goes, “Y ou don’t know what you\\ndon’t know. ” Experienced architects are aware of all the available architectures, tech‐\\nniques, and tools and keep up with the constantly changing landscape of technologies\\nand products. They have the expertise to ask questions that will help them choose the\\nright technologies and products. Discovery is the best way to narrow down the prod‐\\nuct options to a workable number that you can consider. Then you can come up with\\nthe best architecture and products for the particular use case.\\nBy way of analogy, say you’re selling medical instruments out of a van filled with a\\nthousand instruments. (It doesn’t usually work that way, but bear with me.) Y ou\\nwouldn’t just walk up to a doctor, point to your van, and have them look at all the\\ninstruments to find the ones they like. Instead, you would ask the doctor a bunch of\\nquestions—about their practice area, instruments, and budget—and based on their\\nanswers, you would pull out only a small number of instruments for the doctor to\\nlook at. Architects do the same thing, and the discovery phase of the ADS is a great\\nopportunity to ask your questions while the appropriate people are in the room.\\nI’ll pass on another great bit of advice I received early in my career: as you start ask‐\\ning questions, make sure to stay away from the solution. That is, don’t talk about\\narchitectures or products until you are done asking all the discovery questions. When\\nI first started running ADSs, as soon as the customer mentioned they wanted to build\\na data warehouse, I would jump right into demonstrating a Microsoft product. I\\nshould have waited and asked all of my questions to make sure I was clear on what\\nthe customer was trying to accomplish. Instead, I sometimes discovered that the cus‐\\ntomer really didn’t need a data warehouse at all, which meant my efforts to educate\\nthem about the data warehouse product were wasted.\\nADS Questionnaire\\nAs you plan your ADS, take a look at this list of questions, which I developed over the\\ncourse of numerous ADSs I have conducted. I spend an hour or two at the beginning\\nof the meeting doing “discovery. ” Y ou’ll notice that none of the questions is about any\\nspecific product. This is so you can begin creating an architecture in a vendor-neutral\\nway. Later on in the ADS, you can ask what products the customer currently uses and\\nwhat cloud provider they prefer and then apply products to the architecture.\\nIs your business using the cloud?\\nNowadays, the answer is almost always yes. If not, evaluate why and see if you\\ncan overcome whatever the source of resistance is. Many such customers just\\n32 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f380806a-7fff-4b7f-b6de-8c2f04c2917c', embedding=None, metadata={'page_label': '32', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='need to better understand the benefits of the cloud (see Chapter 16). Others have\\noutdated reasons for not using the cloud, like “not enough security, ” that have\\nlong been taken care of.\\nIs the data architecture you’re considering a new solution or a migration?\\nIf this is a migration, then part of the conversation will need to be about how to\\nmigrate the existing solution to the new solution.\\nWhat are the skill sets of the engineers?\\nKnowing what products and technologies the data engineers are familiar with\\nheavily influences the products I recommend. If they are already familiar with\\nthe tool, they’ll need less training time, which will likely save costs.\\nWill you use nonrelational data?\\nIn other words, what kind of variety will the data have? If nonrelational data is\\ncoming from any of the sources, the customer will definitely need a data lake, so\\nthis is a good time to introduce them to data lake technologies and design.\\nHow much data do you need to store?\\nWhat volume of storage is needed? The size of the data will heavily influence per‐\\nformance, so if there’s a large volume of data, you’ll need to discuss how to design\\nthe architecture (for example, the folder structure of the data lake) to avoid per‐\\nformance issues such as slow queries and reports.\\nWill you have streaming data?\\nWhat kind of velocity is needed? Streaming data sources, such IoT devices, influ‐\\nence the types of products needed to support that kind of data.\\nWill you use dashboards and/or ad hoc queries?\\nKnowing how the data will be used will influence not only the types of products\\nyou recommend but also the system’s performance needs. For example, with\\ndashboards, you need millisecond response time so end users can slice and dice\\nthe data without noticeable delays.\\nWill you use batch and/or interactive queries?\\nMake sure you understand what types of queries will go against the data. This\\nwill influence the type of storage used and the compute needed to ensure accept‐\\nable performance.\\nHow fast do the reports need to run? Are there service level agreements (SLAs) in place\\nwith specific requirements?\\nWhether the reports need to run in milliseconds or minutes will affect the archi‐\\ntecture, the products used, and how many copies of the data will be needed to get\\nto an acceptable level of performance. For example, the data may need to be\\naggregated.\\nConducting the ADS | 33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ec896d79-5390-48f4-887c-3c5c93f71bfd', embedding=None, metadata={'page_label': '33', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Will you use this data in predictive analytics or machine learning?\\nIf so, discuss the machine learning (ML) or analytics products that will be used\\nand ways to gather the data that will facilitate training the ML models.\\nWhat are your high-availability and/or disaster recovery requirements (such as recovery\\ntime objectives and recovery point objectives)?\\nMost cloud providers build in all the high availability the average customer\\nneeds, but supporting any specific high-level requirements could require a\\nchange in the architecture. If disaster recovery is required, depending on the\\nrecovery point objectives (RPOs), you might need to make major additions to the\\narchitecture. Asking this question is also a good way to start the customer think‐\\ning about disaster recovery, if they have none built into their current solution.\\nDo you need to master the data?\\nMaster data management (MDM) involves creating a single master record for\\neach person, place, or thing in a business, gathered from across internal and\\nexternal data sources and applications. These master records can then be used to\\nbuild more accurate reports, dashboards, queries, and machine learning models.\\nI explain MDM in detail in Chapter 6. If MDM is needed, you will have to show\\nhow it fits into the architecture and recommend products.\\nAre there any security limitations with storing data in the cloud (for example, defined in\\nyour customer contracts)?\\nThis is a very important question: contracts can limit what the organization can\\ndo with its customers’ data. For example, if customers keep data in certain coun‐\\ntries and that data can’t leave the country, you will need to architect a solution\\nwith multiple data lakes. Similarly, if they cannot store any personally identifiable\\ninformation in the cloud, you will need to talk about how and when to anonym‐\\nize that data.\\nDoes this solution require 24/7 client access?\\nIf so, the architecture needs to minimize or eliminate downtime for each of the\\ncomponents. For example, if the customer is using a relational data warehouse,\\nthe solution needs to make sure there is no need for a maintenance window to\\nload and clean data.\\nHow many concurrent users will be accessing the solution at peak times? And how many\\non average?\\nThis is important to know because some products limit how many concurrent\\nusers they support. If the number of concurrent users is over or close to such a\\nlimit, your design should avoid those products or combine them with other\\nproducts to minimize the chance of exceeding a limit.\\nWhat is the skill level of the end users?\\nThis will determine what products you recommend. If the skill level is low, you\\nmight recommend no-code/low-code products. For example, if I mention Spark\\nand get a blank stare from the customer, I won’t even talk about Spark-type\\n34 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ce4db6ec-f9dd-4f2c-a7a1-88716561281e', embedding=None, metadata={'page_label': '34', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='products but will instead focus on easier-to-use products. If the skill level is high,\\nyou can consider higher-code options.\\nWhat is your budget?\\nThe project budget, of course, has a huge impact on the architecture and the\\nproducts you choose. If you have a large budget, there are no constraints on the\\narchitecture—you can design whatever you wish. However, you will often run\\ninto customers with low budgets. This forces you to make trade-offs: for exam‐\\nple, you may reduce financial costs by using less compute but at the cost of a hit\\nto performance (as data loading and reporting will take longer).\\nWhat is your planned timeline?\\nIf the timeline is long, which is usually the case when building a data warehouse\\nsolution, meet with the cloud provider to find out what products are in develop‐\\nment but haven’t been announced yet. Then you can educate the customer about\\nanything worth waiting for. The last thing you want is to start building a solution\\nwith one product only to find out 4 or 5 months into development that the cloud\\nprovider is about to release a better product.\\nIs the source data in the cloud and/or on-prem?\\nIf some of the data sources are on premises, you’ll need a pipeline from the on-\\nprem source to the cloud. Depending on the volume of the data, this could force\\narchitecture changes, such as using a product that allows for a bigger pipeline.\\nHow much data needs to be imported into the solution every day?\\nIf a lot of data needs to be uploaded each night, transferring it could take too\\nlong. Instead of uploading many large files every night, the system may need to\\nupload data on a more frequent basis, such as hourly. Also, look into architec‐\\ntural changes that can speed uploading, such as doing parallel data uploads, com‐\\npressing the data before uploading, or using products that perform faster file\\ntransfers.\\nWhat are your current pain points or obstacles with regard to performance? Scale? Stor‐\\nage? Concurrency? Query times?\\nY ou need to know these pain points so you can design an architecture that will\\naddress and overcome them. No one wants to pour time and money into build‐\\ning a new solution that leaves them with the same problems, so you’ll have to\\nconvince the ADS attendees that the solution you’re whiteboarding will solve\\ntheir problems.\\nDo you want to use third-party and/or open source tools?\\nVery few customers will be completely committed to a specific cloud provider,\\nusing only that cloud provider’s products. So, when you finish exploring the\\nhigh-level architecture and get to the point of talking about products, you might\\nrecommend using third-party and open source tools that the engineers are\\nalready familiar with.\\nConducting the ADS | 35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3a70e831-680b-4338-ba0b-3e308976ebc3', embedding=None, metadata={'page_label': '35', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Are you OK with using products that are in public or private preview?\\nBefore a product is made generally available (or GA ’ d), it starts out in private pre‐\\nview (available only to a few invited customers) and then moves to public pre‐\\nview (anyone can use it). This is done to make sure the product is as stable and\\nbug-free as possible. The negatives of using a product before it is GA ’ d include\\nlimited functionality, potential bugs, lack of support, unpredictable changes, and\\nuncertainty around pricing. If the company has some appetite for using private\\nor public preview products, it can get a jump on using a new product or feature.\\nSome companies have strict policies forbidding the use of products before they\\nare GA ’ d. However, this policy may apply only to products that the company has\\nput into production, which means it is OK to use preview products that will be\\nGA ’ d by the time the solution is put into production. (It takes a long time to build\\na data architecture, so this happens a lot.)\\nWhat are your security requirements? Do you need data sovereignty?\\nThe answers to these questions can change the architecture in many ways. Y ou\\nmight need to build in encryption or extra security features. The requirement to\\nhave multiple data lakes because data can’t leave a country might apply here, too.\\nIs data movement a challenge?\\nData movement is the process of extracting data from source systems and bring‐\\ning it into the data warehouse or lake. If the data is coming from many different\\nsources or if there’s a mixture of on-prem and cloud sources, your architecture\\ndecisions will have to accommodate these various sources. For example, the sys‐\\ntem might need to use replication software to pull data from a source system into\\nthe data lake more quickly.\\nHow much self-service BI would you like?\\nIf the end users have limited or no technical skills but want to be able to create\\ntheir own reports, then the architecture will need to go to extra lengths to put the\\ndata in an easily consumable format. This might mean a relational data ware‐\\nhouse (see Chapter 4) and/or a star schema (see Chapter 8). If the end users are\\nall very technical, then a data lakehouse (see Chapter 12 ) may be the best\\narchitecture.\\nWhiteboarding\\nUse a whiteboard, not presentation slides. An ADS is all about discovery; there’s lots\\nof talking, and then you’ll move on to whiteboarding. Too many slides and the ADS\\nbecomes just another presentation. Y our whiteboard should include a rough diagram\\nof the architecture, as well as places for goals, pain points, and items for follow-up.\\nFigure 3-2 shows what a final whiteboard might look like for an ADS focused on a\\nmodern data warehouse.\\n36 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f8bb85cb-a2dd-4c7c-a240-aa253b5d2e72', embedding=None, metadata={'page_label': '36', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3-2. A typical whiteboard after the end of an ADS\\nThe whiteboard contains not only the architecture but prioritized goals, pain points,\\nand “parking lot” items and other things to follow up on.\\nList all the pain points on the whiteboard and make sure to check them off as you\\nsolve them. By the end of the ADS, all of the pain points should be addressed in the\\narchitecture you have laid out. If not, schedule a follow-up meeting with the cus‐\\ntomer to address missed pain points. The same applies to goals: if you can’t address\\nthem all, schedule a follow-up meeting.\\nReserve the last 30 minutes of the ADS to discuss follow-up items.\\nAfter the ADS\\nDebrief with the account team after the ADS. Ask them what went well and what you\\ncan do better. This is part of putting a growth mindset into action, and it is a great\\nway to get feedback and improve your ADS sessions. Y ou should also communicate\\nthe outcomes of the ADS to all stakeholders. This might include a high-level sum‐\\nmary for upper management and a more detailed report for those directly involved in\\nthe project. Last, collect all the materials you used for the ADS into a digital folder\\nthat you can use as a reference for future ADSs. For example, I store all my previous\\nwhiteboards; I’ve found that they’re great to review when you’re planning an ADS\\nsimilar to one you have done before.\\nShortly after the ADS, email the customer with the following information:\\nSummary document\\nBriefly summarize the major points discussed during the ADS.\\nAfter the ADS | 37', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6f8f1007-4aa3-47cb-8029-16cd322f5e29', embedding=None, metadata={'page_label': '37', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Physical architecture\\nIf you used a digital whiteboard, export the end result to a file that you can send\\nto stakeholders, including the customer.\\nAction items\\nInclude any next steps that you agreed on, such as “Meet next Tuesday to discuss\\na proof of concept” or “Customer will email a diagram of their current\\narchitecture. ”\\nParking lot items and follow-ups\\nY ou tracked these on the whiteboard. Now in the email you can go into more\\ndetail and list who is responsible for following up on each item. This provides the\\ncustomer with an opportunity to clarify anything you didn’t get quite right.\\nSurvey\\nAt an in-person ADS, it’s best to hand a survey to each participant at the end.\\nThat way, most if not all attendees will fill it out—especially if you joke with them\\nthat they can’t leave until they do! Consider making this request during the last\\nbreak of the day, instead of at the very end of the ADS. If it’s a remote ADS, you\\ncan post the link to the survey in the meeting chat shortly before the end of the\\nsession with encouragement to “copy the link before you leave the meeting so\\nyou can provide your feedback. ” Tell them it takes just two minutes to complete\\n(and make sure that’s the truth). Y ou can also include a link to the survey in your\\nfollow-up email.\\nTips for Conducting an ADS\\nWhile it’s rare, I’ve had my fair share of difficult people decide to challenge me during\\nan ADS. Sometimes a person in the room is afraid of change, afraid a new architec‐\\nture will put them out of a job (especially when the ADS is about a cloud migration),\\nor simply believes they already have all the answers. If you find yourself dealing with\\nsomeone like this, don’t get into an argument—even if you are right. Just reply with\\nsomething that defuses the argument and gets the ADS back on track, for instance,\\n“Y ou have an excellent point and I’ d like to dive into that further, but we’ll need to do\\nthat offline at another time so that we can stay on topic. ”\\nUse humor, too. Y ou don’t need to tell jokes, but if you’re comfortable, try making\\nfunny comments throughout the day to lighten the mood and keep everyone’s atten‐\\ntion engaged. An ADS can make for a long day, and laughing is the best way to boost\\npeople’s energy levels.\\nBe humble—don’t come off as a know-it-all. People don’t like that. They much prefer\\npeople who admit they don’t know everything. If you are asked a question you can’t\\nanswer, just say you don’t know but will find out and get back to them quickly. Y ou\\ncan research the answer during the lunch break and respond that afternoon. Or find a\\n38 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e08944e5-6b79-4382-96f4-4d19d31ec8cd', embedding=None, metadata={'page_label': '38', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='coworker who knows the answer and invite them in to answer the question directly\\nand to take follow-up questions. Never try to make up or fudge an answer. Y ou might\\nget caught, and even if you don’t, that’s just not the right way to treat a customer or a\\ncolleague.\\nOf course, customers aren’t the only people who can throw things off. It’s not unusual\\nfor a member of the account team to talk at length in the hope of looking good in\\nfront of the customer, without adding any value to the ADS. If that happens, pull\\nthem aside during a break for a friendly word and politely ask them to refrain from\\ntalking too much. Tell them you’ll make sure to ask them pertinent questions that\\nthey can answer during the ADS so they can establish their expertise with the\\ncustomer.\\nRead the room, too. Y ou know who the decision makers are, so keep them focused\\nand engaged. Pay attention to their body language: if they go from looking at you to\\nreading their emails, they’re bored. If so, switch to another topic or take a break.\\nLearn to adjust on the fly. It’s rare that I follow an agenda completely; other topics\\noften come up that are much more interesting and useful to the customer. The chal‐\\nlenge will be when those topics are unfamiliar to you! Over time, you will gain the\\nknowledge to be able to pivot and cover just about any topic. It took me about two\\nyears of doing ADSs nearly daily to get to that point, so be patient!\\nFinally, I recommend building up your stamina. When I first started doing full-day\\nADSs, I found it challenging to keep up my energy—not to mention my voice—for\\nsix to eight hours and would collapse when I got home. I talked so much more in\\nADSs than I was used to, and it took me a while to stop going hoarse! Expect it to\\ntake a few months before you can keep up your energy and voice the whole day. My\\npersonal favorite tips to keep up stamina and to keep your voice sharp are to get at\\nleast eight hours of sleep the day before, stay hydrated during the day by sipping\\nwater every few minutes, eat healthy snacks, and exercise regularly during the week.\\nWhen you are facilitating an ADS in a remote or hybrid environment, here are some\\nadditional tips to keep in mind:\\n• Turn on live captions in your communication software. Not only does this make\\nthe session more accessible for everyone, it can also help you follow the conver‐\\nsation and prevent you from having to ask people to repeat themselves.\\n• Sign into the call using two different devices: one for screensharing and white‐\\nboarding, the other to communicate (most communication software supports\\nthis). Ideally, your communication device will be a computer with two or three\\nlarge monitors. This way, you don’t have to constantly minimize windows or\\nmove them around, and you can always see the customer as you are whiteboard‐\\ning and speaking.\\nTips for Conducting an ADS | 39', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a5e0327e-4290-4325-a8c8-581e0abeef04', embedding=None, metadata={'page_label': '39', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='• I recommend having a back channel going with the account team (and only the\\naccount team) in the chat tool of whatever communication software you are\\nusing. This is a great way to communicate with your account team without the\\ncustomer hearing—for instance, to ask them to research a question you can’t\\nanswer, to jump in and talk when needed, or to provide details on the person\\nwho is speaking.\\n• I also recommend these tools:\\n— If you are using Microsoft Teams, turn on Speaker Coach, a feature that listens\\nto your audio while you present and provides private real-time feedback and\\nsuggestions for improvement, as well as a summary afterward. Only you can\\nsee its live insights, and they are not saved in recorded meetings’ transcripts.\\n— I also like ZoomIt, a screen zoom, annotation, and recording tool for technical\\npresentations and application demonstrations. It runs unobtrusively in the\\ntray and activates with customizable hotkeys to let you zoom in on an area of\\nthe screen, move around while zoomed, and draw on the zoomed image. It is\\nan excellent way to call attention to specific portions of your presentation—\\nmuch better than using your cursor, which can be hard to see.\\nSummary\\nAn ADS is a vital part of building a data architecture. It helps you align design deci‐\\nsions with business goals, address potential risks and challenges, optimize costs and\\nresources, and foster collaboration among stakeholders.\\nThe end result of an ADS is to build a good data architecture, or even a great one. A\\ngood data architecture is robust and scalable and can effectively support data-driven\\ninitiatives. Taking a data architecture from good to great means meeting these needs\\nend to end, with feedback from all users in the data value chain, so that the overall\\ndata strategy can meet the organization’s goals. Building a data solution is a user-\\nfocused journey into design and feedback, and it requires the kind of strategy and\\nplanning that only an ADS can provide.\\n40 | Chapter 3: The Architecture Design Session', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='004ee308-22b9-4c70-9dca-d5713adefa9e', embedding=None, metadata={'page_label': '40', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='PART II\\nCommon Data Architecture Concepts\\nBefore diving into data architectures, it’s important to make sure you understand all\\nthe data architecture concepts that could be used within an architecture. I find there\\ntends to be a lot of confusion about many of these concepts, which I hope to clear up.\\nTherefore, you will find discussion of over 20 such concepts in the upcoming chap‐\\nters. At the very least, these chapters will be a refresher for those who may have not\\nused these concepts in a while. I don’t claim that all my definitions of these concepts\\nare universally agreed upon by everyone, but at least these chapters will help get\\neveryone on the same page to make it easier to discuss architectures.\\nI have included the relational data warehouse and the data lake under concepts\\ninstead of architectures. At one time, when they were the only products used in a sol‐\\nution, they could have been considered data architectures. Now, however, they are\\nalmost always combined with other products to form the solution. For example,\\nmany years ago there were relational data warehouse products that included the rela‐\\ntional storage, compute, ETL (extract, transform, and load) software, and reporting\\nsoftware—basically everything you needed bundled together from one vendor.\\nNowadays, you will stitch together multiple products from possibly multiple vendors,\\nwith each product having a particular focus (e.g., ETL), to complete your data\\narchitecture.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9c294c6f-ec54-4958-a811-c4cc953fd352', embedding=None, metadata={'page_label': '41', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0eb790d9-0b3a-4d61-a5e6-4720e4bfdc8f', embedding=None, metadata={'page_label': '42', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 4\\nThe Relational Data Warehouse\\nBy the mid-2000s, I had used relational databases for years, but I had never been\\nexposed to relational data warehouses. I was working as a database administrator\\n(DBA) for a company that used an accounting software package to manage its finan‐\\ncial transactions. The reporting from the package was limited and slow, so the com‐\\npany was looking to improve performance, create dashboards to slice and dice the\\ndata, and combine its financial data with data from a homegrown application to get a\\nbetter understanding of the business.\\nMy employer hired a consulting company to build this thing called a “relational data\\nwarehouse”—and, in a decision that changed the course of my career, asked me to\\nhelp. We generated dashboards that saved the end users a ton of time and added busi‐\\nness insights they’ d never had before. When I saw the excitement on their faces, I\\nknew I’ d found my new passion. I changed my career to focus on data warehousing\\nand never looked back.\\nWhat Is a Relational Data Warehouse?\\nA relational data warehouse (RDW) is where you centrally store and manage large\\nvolumes of structured data copied from multiple data sources to be used for historical\\nand trend analysis reporting so that your company can make better business deci‐\\nsions. It is called relational because it is based on the relational model, a widely used\\napproach to data representation and organization for databases. In the relational\\nmodel, data is organized into tables (also known as relations, hence the name). These\\ntables consist of rows and columns, where each row represents an entity (such as a\\ncustomer or product) and each column represents an attribute of that entity (like\\nname, price, or quantity). It is called a data warehouse because it collects, stores, and\\nmanages massive volumes of structured data from various sources, such as transac‐\\ntional databases, application systems, and external data feeds.\\n43', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='46f46b18-197e-4277-bfa6-e2586631412f', embedding=None, metadata={'page_label': '43', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Not all data warehouses are based on the relational model. Non-relational data ware‐\\nhouses include types like columnar, NoSQL, and graph data warehouses. However,\\nrelational data warehouses are much more popular and widely adopted, primarily\\nbecause relational databases have been the dominant data management paradigm for\\ndecades. The relational model is well suited for structured data, which is commonly\\nfound in business applications. It is also popular due to the widespread use of SQL,\\nwhich has been the standard language for relational data warehouses for many years.\\nAn RDW acts as a central repository for many subject areas and contains the single\\nversion of truth (SVOT). A critical concept in data warehousing, the SVOT refers to\\nthe practice of creating a unified, consistent view of an organization’s data. It means\\nthat all the data within the data warehouse is stored in a standardized, structured for‐\\nmat and represents a single, accurate version of the information. This ensures that all\\nusers have access to the same information, eliminating any discrepancies or inconsis‐\\ntencies and eliminating data silos. This improves decision making, collaboration, and\\nefficiency across the organization. It also reduces the risk of errors and misunder‐\\nstandings that can arise from working with disparate, inconsistent data sources.\\nImagine you don’t have a data warehouse and are generating reports directly from\\nmultiple source systems, and maybe even some Excel files. If a report viewer ques‐\\ntions the accuracy of the data, what can you tell them? The “truth” can be spread out\\nover so many source systems that it’s difficult to trace where the data came from. In\\naddition, some reports will give different results for the same data—for example, if\\ntwo reports use complex logic to pull the data from multiple sources and the logic is\\nupdated incorrectly (or not at all). Having all the data in one central location means\\nthat the data warehouse is the single source of truth; any questions about the report\\ndata can be answered by the data warehouse. Maintaining a SVOT is essential for\\norganizations looking to harness the full potential of their data.\\nIf a data warehouse (DW) is used by the entire company, it’s often called an enterprise\\ndata warehouse (EDW). This is a more comprehensive and robust version of a data\\nwarehouse, designed to support the needs of the entire organization. While a stan‐\\ndard DW might support a few business units, with many DWs throughout the orga‐\\nnization, the EDW uses a wider range of data sources and data types to support all\\nbusiness units. The EDW provides a single, unified view of all of the organization’s\\ndata.\\nFigure 4-1 illustrates a major reason to have a data warehouse. The diagram on the\\nleft shows how challenging it is to run a report using data from multiple applications\\nwhen you do not have a data warehouse. Each department runs a report that collects\\ndata from all the databases associated with each application. So many queries are\\nbeing run that you’re bound to have performance problems and incorrect data. It’s a\\nmess. The diagram on the right shows that, with all application data copied into the\\n44 | Chapter 4: The Relational Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ac6d7985-3acc-4028-9f09-fe95091a4e44', embedding=None, metadata={'page_label': '44', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='EDW , it becomes very easy for each department to run a report without compromis‐\\ning performance.\\nFigure 4-1. Before and after an enterprise data warehouse\\nTypically, to build a data warehouse, you will create data pipelines that perform three\\nsteps, called extract, transform, and load (ETL):\\n1. The pipeline extracts data from the source systems, such as databases and flat\\nfiles.\\n2. The extracted data is then transformed or manipulated to fit the target systems’\\nrequirements (in this case, to fit a data warehouse). This can involve cleaning, fil‐\\ntering, aggregating, or combining data from multiple sources.\\n3. The transformed data is loaded into the data warehouse. A DBA can make the\\ndatabase and field names more meaningful, making it easier and faster for end\\nusers to create reports.\\nWhat Is a Relational Data Warehouse? | 45', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2e7b7b65-4256-4d94-8750-93ae7dcc887e', embedding=None, metadata={'page_label': '45', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What a Data Warehouse Is Not\\nNow that you know what a data warehouse is, let’s clarify its purpose by looking at\\nsolutions that should not be considered a data warehouse (though I have seen people\\ndo so many times):\\nDW prefix\\nA data warehouse is not a just copy of a source database from an operational sys‐\\ntem with DW added to the filename. For example, say you were to copy a data‐\\nbase called Finance containing 50 operational tables and call the copy\\nDW_Finance, then use those 50 tables to build your reports. This would result in\\na data warehouse designed for operational data when instead you need it\\ndesigned for analytical data. With analytical data, you have better read perfor‐\\nmance and can create data models to make it easier for end users to build reports.\\n(I’ll explain more in the next section.)\\nViews with unions\\nA data warehouse is not a copy of multiple tables from various source systems\\nunioned together in a SQL view. ( Unioning is done via the SQL UNION statement,\\nwhich combines the results of two or more SELECT statements into a single result\\nset.) For example, if you copied data from three source systems that each contain\\ncustomers, you’ d end up with three tables in the data warehouse called Customer\\nSource1, CustomerSource2, and CustomerSource3. So you’ d need to create a view\\ncalled CustomerView that is a SELECT statement unioning the tables Customer\\nSource1, CustomerSource2, and CustomerSource3. Y ou’ d repeat this process for\\nother tables, such as products and orders.\\nInstead, the data from the three tables should be copied into one table in the data\\nwarehouse, which requires the extra work of creating a data model that fits all\\nthree tables. Y ou would likely want to use master data management (MDM),\\nexplained in Chapter 6, at this point to prevent duplicates and improve accessi‐\\nbility and performance.\\nDumping ground\\nA data warehouse is not a dumping ground for tables. Many times, this practice\\narises when a company does not have a DW and an end user wants to create a\\nreport from a subset of data from a couple of source systems. To help them out\\nquickly, a person from IT creates a DW without much thought, copying the data\\nfrom those two source systems into the DW . Then other end users see the benefit\\nthe first end user got, and they want additional data from those same source sys‐\\ntems and a few others to create their own reports. So once again the IT person\\nquickly copies the requested data into the DW . This process repeats over and over\\nuntil the DW becomes a jumbled mess of databases and tables.\\n46 | Chapter 4: The Relational Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c4b60c7d-1699-419f-8222-0df40e5aad97', embedding=None, metadata={'page_label': '46', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='So many DWs start out as one-off solutions for a couple of users, then morph into\\nfull-blown but poorly designed DWs for the entire company. There is a better way.\\nInstead, when that first end-user request comes in, assess your company’s reporting\\nneeds. Find out if the request is really a one-off or if it should be the start of building\\nan EDW . If it should, this is your chance to show senior leaders why your company\\nneeds a DW . If so, be adamant that you need enough up-front time to design a DW\\nthat can support many data sources and end users. (Use “Why Use a Relational Data\\nWarehouse?” on page 49 to support your case.)\\nThe Top-Down Approach\\nIn an RDW , you will do a lot of work up front to get the data to where you can use it\\nto create reports. Doing all this work beforehand is a design and implementation\\nmethodology referred to as a top-down approach . This approach works well for\\nhistorical-type reporting, in which you’re trying to determine what happened\\n(descriptive analytics) and why it happened ( diagnostic analytics). In the top-down\\napproach, you first establish the overall planning, design, and architecture of the data\\nwarehouse first and then develop specific components. This method emphasizes the\\nimportance of defining an enterprise-wide vision and understanding the organiza‐\\ntion’s strategic goals and information requirements before diving into the develop‐\\nment of the data warehouse.\\nDescriptive analytics and diagnostic analytics are two important types of data analysis\\nthat are commonly used in business. Descriptive analytics involves analyzing data to\\ndescribe past or current events, often through the use of summary statistics or data\\nvisualizations. This type of analysis is used to understand what has happened in the\\npast and to identify patterns or trends in the data that can help with decision making.\\nDiagnostic analytics is used to investigate the causes of past events, typically by exam‐\\nining relationships between different variables or factors. This type of analysis can\\nidentify the root causes of problems or diagnose issues that may be affecting business\\nperformance.\\nSuppose a company wants to analyze sales data from the past year. Descriptive analyt‐\\nics would involve calculating summary statistics such as total sales revenue, average\\nsales per day, and sales by product category to understand what happened. Diagnostic\\nanalytics, in contrast, would examine relationships between factors (such as sales and\\nmarketing spend, or seasonality and customer demographics) to understand why\\nsales fluctuated throughout the year. By combining both approaches, companies can\\ngain a deeper understanding of their data and make more informed decisions.\\nFigure 4-2 shows the architecture of a typical RDW . ETL is used to ingest data from\\nmultiple sources into the RDW , where reporting and other analytics can be\\nperformed.\\nThe Top-Down Approach | 47', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b1f43ccc-b6db-462b-be2a-3692ae208a2a', embedding=None, metadata={'page_label': '47', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 4-2. The architecture of a data warehouse\\nThe top-down approach typically involves the following steps:\\n1. Formulate some hypotheses up front.\\nStart with a clear understanding of corporate strategy. Then make sure you know\\nwhat questions you want to ask of the data.\\n2. Define the business requirements.\\nIdentify the organization’s goals, objectives, and key performance indicators\\n(KPIs). Gather and analyze the information needs of various departments and\\nusers. Y ou can also think of this step as defining your reporting requirements.\\n3. Design the data warehouse architecture.\\nBased on the business requirements, create a high-level architecture for the data\\nwarehouse, including its structure, data models, and data integration processes.\\nThese will be your technical requirements.\\n4. Develop the data model.\\nDesign a detailed data model for the data warehouse, taking into account the\\nrelationships between various data entities and the granularity of the data.\\n5. Build the architecture.\\nDevelop the appropriate databases, schemas, tables, and fields for the data ware‐\\nhouse. This is the previously described approach called schema-on-write.\\n6. Develop ETL.\\nDevelop the ETL processes to extract data from various source systems, trans‐\\nform it into the desired format, and load it into the data warehouse.\\n7. Develop and deploy BI tools and applications.\\nImplement BI tools and applications that allow users to access, analyze, and\\nreport on the data stored in the data warehouse.\\n48 | Chapter 4: The Relational Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='78ae2697-1943-4f6b-a20e-2c049abd2de2', embedding=None, metadata={'page_label': '48', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8. Test and refine the data warehouse.\\nPerform testing to ensure data quality, performance, and reliability. Make any\\nnecessary adjustments to optimize the system.\\n9. Maintain and expand the data warehouse.\\nAs the organization’s needs evolve, update and expand the data warehouse\\naccordingly.\\nThe top-down approach has some advantages, such as a comprehensive view of the\\norganization’s data needs, better data consistency, and improved governance. How‐\\never, it can also be time-consuming and resource intensive, taking longer to deliver\\nvalue than to the bottom-up approach used by the data lake, described in Chapter 5.\\nThe modern data warehouse architecture, described in Chapter 10 , combines both\\nthe top-down and bottom-up approaches.\\nWhy Use a Relational Data Warehouse?\\nHaving an RDW makes it so much easier to build any kind of BI solution, since BI\\nsolutions can pull data just from the RDW without having to create complex logic to\\npull data from multiple source systems. Also, they won’t have to clean or join the data\\nbecause the RDW will have already done that. The BI solution that is built from the\\nRDW could be a data mart (which contains a subset of the RDW data for a specific\\ngroup of people, as explained in Chapter 6 ), aggregate data to make queries and\\nreports faster, and even be usable within Microsoft Excel. The bottom line is that with\\na RDW , you already have a solid foundation to build upon.\\nLet’s look in detail at some of the major benefits you can get from using an RDW:\\nReduce stress on the production system\\nY ou may have seen this problem before: an angry call from an end user com‐\\nplaining that inserting orders via the order entry application is taking forever.\\nY ou look into it, and it turns out that another end user is running a report via the\\norder entry application that is hogging all the resources on the server where the\\napplication resides. This situation is especially common when end users are\\nallowed to create ad hoc queries and they come up with poorly written SQL.\\nBy copying the order entry application database to a DW and optimizing it, you\\ncan have all reports and ad hoc queries go against the DW and avoid this prob‐\\nlem altogether, especially if the end user needs to run a report that goes against\\nmultiple application databases.\\nOptimize for read access\\nApplication databases are going to be optimized to support all the CRUD opera‐\\ntions equally, so the reading of data will not be as fast as it could be. The data\\nwarehouse, on the other hand, is a write-once, read-many type of system,\\nWhy Use a Relational Data Warehouse? | 49', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='178cfb88-a5ef-4ef6-9cc1-cf53ce2bb23e', embedding=None, metadata={'page_label': '49', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='meaning it will be used mainly for the reading of data. Therefore, it can be opti‐\\nmized for read access, especially for the time-consuming sequential disk scans\\nthat frequently occur when reports or queries are run. There are many database\\ntechniques that can be used to speed up read access in a DW , some to the detri‐\\nment of write access, which we are not concerned about.\\nIntegrate multiple sources of data\\nThe ability to integrate many sources of data in order to create more useful\\nreports is one of the more important reasons to build a DW . Locating all the data\\nin one spot instead of having it spread out over various databases not only makes\\nreport building easier but greatly improves reporting performance.\\nRun accurate historical reports\\nWithout a DW , end users of applications usually run all their reports on a partic‐\\nular day each month (usually the last). They then save them to disk so they have\\ncopies that they can refer to in the future. For example, the user wants to look at a\\nreport from a few months ago that lists customer sales by state. However, one\\ncustomer has recently moved to a different state. If the user runs a current report,\\nit would incorrectly show that customer’s sales in their new state instead of their\\nold state (since their record in the database has been updated to their new state).\\nHence, the user must look back at a saved older report instead of running a cur‐\\nrent report.\\nA DW can take care of this by keeping track of when a customer moves (via\\ntracking customer location history with start and end dates) as well as any other\\nfields that need to be tracked (for example, employer or income). Now the user\\ncan run a report today but ask it to pull data as of some date in the past, and the\\nreport will be accurate. Moreover, saving report files each month is no longer\\nrequired.\\nRestructure and rename tables\\nMany application databases have table and field names that are very difficult to\\nunderstand, especially older ERP and CRM products (think table names such as\\nT116 and field names like RAP16). In the data warehouse, you can copy the data\\nfrom those source tables into something much easier to understand (for example,\\nCustomer instead of T116). Y ou can also likely come up with a better data model\\nfor all the tables. End users will be able to create reports much more easily when\\nthey don’t have to translate cryptic table and field names.\\nProtect against application upgrades\\nImagine you don’t have a DW and users instead create reports against an applica‐\\ntion database. Everything is running fine, and then all of a sudden, many reports\\nstart giving errors. It turns out the application went through an upgrade, instal‐\\nling a new version that renamed a bunch of tables and fields. So now you must go\\n50 | Chapter 4: The Relational Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='91287936-bf70-4893-9f5b-8c1c311e690c', embedding=None, metadata={'page_label': '50', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='through each and every report, out of hundreds, and rename the changed tables\\nand fields. That could take months, resulting in a lot of upset end users. Even\\nafter that, any reports that got missed might still give errors.\\nA DW can protect you against this. After an application upgrade, only the ETL\\nthat copies data from the application databases to the DW needs to be updated—\\na quick task. The reports do not have to be changed. End users won’t see any new\\ndata until the ETL is fixed, but their reports don’t have errors.\\nReduce security concerns\\nWithout a DW , your team would need to give each end user security access to\\neach application database they needed to use for reporting purposes. There could\\nbe dozens; the process of providing access could take weeks, and sometimes they\\nstill might not have access to everything they need. With a DW , each end user\\nneeds access only to the appropriate tables, and providing this is much faster and\\neasier.\\nKeep historical data\\nMany production systems limit the amount of historical data they keep (for\\nexample, data from the last three years). They do this to save space and improve\\nperformance and, in some cases, to comply with regulations. Older data is usually\\npurged yearly or monthly. On the other hand, a DW can hold all of the history, so\\nyou never have to worry about running a report for older years and not finding\\nany data.\\nMaster data management (MDM)\\nAs you collect data from multiple source systems, many times you will need to\\nuse MDM to remove duplicate records for such things as customers, products,\\nand assets. (See Chapter 6 for a more detailed explanation of MDM.) The DW is\\nthe perfect place to perform MDM. Also, many of the MDM tools allow you to\\ncreate hierarchies (for example, Company → Department → Employee), adding\\nmore value to mastering the data.\\nImprove data quality by plugging holes in source systems\\nY ou will find that a lot of the data you get from the various source systems needs\\nto be cleaned, despite what the owners of the applications say (I have heard them\\nsay “Our data is clean” many times, only to be proved wrong). For example, an\\norder entry application may require a customer’s birth date, and if the person\\nentering the data does not know the customer’s birth date, they might enter a\\ndate in the future or a date more than 100 years old just to be able to complete\\nthe order. Or maybe the application does not check the accuracy of the two digits\\nentered for a state code. There are always dozens of “holes” in the source system.\\nY ou can not only clean the data in the DW , but also notify the people who main‐\\ntain the applications of the holes in their systems so they can fix them. In this\\nway, you help prevent the entry of bad data in the future.\\nWhy Use a Relational Data Warehouse? | 51', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9ba58c99-5a8d-4316-8bf2-ff2638bbdc58', embedding=None, metadata={'page_label': '51', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Eliminate IT involvement in report creation\\nThis goes back to the self-service BI mentioned in Chapter 3: building a proper\\nDW will remove the need to get IT involved with building reports and leave this\\ntask in the hands of the end user. Without the bottleneck of limited IT resources,\\nreports and dashboards can be built sooner. And IT will be thankful they can\\nwork on more interesting projects than creating reports!\\nDrawbacks to Using a Relational Data Warehouse\\nThere are always trade-offs, and here are the drawbacks to consider when building an\\nRDW:\\nComplexity\\nDWs can be complex and time-consuming to design, build, and maintain. The\\nspecialized skills and resources required can increase costs.\\nHigh costs\\nImplementing a DW can be expensive, requiring significant investments in hard‐\\nware, software, and personnel. Ongoing maintenance and upgrades can also add\\nto the cost.\\nData integration challenges\\nIntegrating data from various sources can be challenging, as it may involve deal‐\\ning with different data formats, structures, and quality issues. This can result in\\nspending time and effort on data cleaning and preprocessing. In addition, certain\\ndata, such as streaming data from IoT devices, is too challenging to ingest into an\\nRDW and so the potential insights from this information are lost.\\nTime-consuming data transformation\\nFor data to be loaded into a DW , it may need to be transformed to conform to the\\nwarehouse’s data model. This process can be time-consuming, and errors in data\\ntransformation can lead to inaccurate analysis.\\nData latency\\nBecause DWs are designed to handle large volumes of data, they can be slower to\\nprocess than other types of databases. This can result in data latency, where the\\ndata in the warehouse is not up-to-date with the most recent changes to the\\nsource databases.\\nMaintenance window\\nWith an RDW , you usually need a maintenance window. Loading and cleaning\\ndata is very resource intensive, and if users are trying to run reports at the same\\ntime, they will experience very slow performance. So users must be locked out of\\nthe warehouse while maintenance is going on, preventing 24/7 access. If any\\nproblems occur during the maintenance window, such as a failed ETL job, you\\n52 | Chapter 4: The Relational Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a60566c2-0fdb-4bfe-a160-833cb3856c65', embedding=None, metadata={'page_label': '52', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='may have to extend the maintenance window. If users try to run reports and are\\nstill locked out, you’ll have upset users who can’t perform their jobs.\\nLimited flexibility\\nDWs are designed to support specific types of analysis, which can limit their flex‐\\nibility for other types of data processing or analysis. Additional tools or systems\\nmay need to be integrated with the warehouse to meet specific needs.\\nSecurity and privacy concerns\\nStoring large amounts of sensitive data in a centralized location can increase the\\nrisk of data breaches and privacy violations, necessitating strong security\\nmeasures.\\nPopulating a Data Warehouse\\nBecause the source tables that are fed into a data warehouse change over time, the\\nDW needs to reflect those changes. This sounds simple enough, but there are many\\ndecisions to make: how often to extract (or pull) the data, what extract method to use,\\nhow to physically extract the data, and how to determine which data has changed\\nsince the last extraction. I’ll briefly discuss each of these.\\nHow Often to Extract the Data\\nHow often you need to update the DW depends largely on how often the source sys‐\\ntems are updated and how timely the end user needs the reporting to be. Often end\\nusers don’t want to see data for the current day, preferring to get all data through the\\nend of the prior day. In this case, you can run your jobs to extract the data from the\\nsource systems via ETL tools each night after the source system databases have fin‐\\nished updating, creating a nightly maintenance window to do all the data transfer. If\\nthe end users require updates during the day, then a more frequent extract, say\\nhourly, will be required.\\nOne thing to consider is the size of the data for each extract. If it is very large, updat‐\\ning the DW may take too long, so you might want to split the update into smaller\\nchunks and do more frequent extracts and updates (for example, hourly instead of\\ndaily). Also, it may take too long to transfer large amounts of data from the source\\nsystems to the data warehouse, especially if the source data is on-prem and you don’t\\nhave a large pipeline from the source system to the internet. This is another reason\\nwhy you may want to go from a large nightly transfer to smaller hourly transfers dur‐\\ning the day.\\nPopulating a Data Warehouse | 53', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='36268f4c-a860-439b-90e5-e1ce136d0a17', embedding=None, metadata={'page_label': '53', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Extraction Methods\\nThere are two methods for extracting data from source systems. Let’s look at each:\\nFull extraction\\nIn a full extraction, all the data is extracted completely from one or more tables in\\nthe source system. This works best for smaller tables. Because this extraction\\nreflects all the data currently available on the source system, there is no need to\\nkeep track of changes, making this method very easy to build. The source data is\\nprovided as is, and you don’t need any additional information (for example,\\ntimestamps).\\nIncremental extraction\\nIn incremental extraction, you’re pulling only the data that has changed since a\\nspecified time (such as the last extraction or the end of a fiscal period) instead of\\nthe whole table. This works best for large tables, and it works only when it’s pos‐\\nsible to identify all the changed information (discussed below).\\nWith most source systems, you’ll use a combination of these two methods.\\nWhether you’re doing a full or an incremental extraction, there are two ways to\\nextract the data: online and offline.\\nIn online extraction, the extraction process can connect directly to the source system\\nto access the source tables, or it may connect to an intermediate system that stores\\nchanges to the data in a preconfigured manner (for example, in transaction logs or\\nchange tables).\\nHowever, direct access to the source system is not always available. In such cases, the\\ndata is staged outside the original source system and is created by an extraction rou‐\\ntine originating from the source system (for example, a mainframe performs an\\nextract routine on a table and deposits the data in a folder in a filesystem). The\\nextracted data is usually placed in a flat file that is in a defined, generic format (for\\nexample, CSV or JSON).\\nHow to Determine What Data Has Changed Since the Last Extraction\\nUnfortunately, it can be difficult for many source systems to identify the recently\\nmodified data and do an incremental extract. Following are several techniques for\\nidentifying recently modified data and implementing incremental extraction from\\nsource systems. These techniques can work in conjunction with the data extraction\\nmethods discussed. Some techniques are based on the characteristics of the source\\nsystems; others may require modifications to the source systems. The source system’s\\nowners should carefully evaluate any technique prior to implementation:\\n54 | Chapter 4: The Relational Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='83221a95-f576-43fe-a4fd-19225938020d', embedding=None, metadata={'page_label': '54', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Timestamps\\nTimestamps are the most preferable option and the easiest to implement. The\\ntables in some operational systems have timestamp columns with the time and\\ndate that a given row was last modified, making it easy to identify the latest data.\\nIn relational databases, the timestamp column is often given the data type time\\nstamp or datetime, along with a column name like Timestamp or Last Modified.\\nThe source application will then populate this column. If not, you can set up the\\nrelational database to default to the current date when the record is saved, or you\\ncan add database triggers to populate the column.\\nChange data capture\\nMost relational databases support change data capture (CDC), which records the\\nINSERTs, UPDATEs, and DELETEs applied to database tables and makes a table\\nrecord available of what changed, where, and when based on the relational data‐\\nbase’s transaction log. If you need near-real-time data warehousing, where you\\ncan see changes to the source system reflected in the data warehouse within a few\\nseconds, CDC can be the key enabling technology.\\nPartitioning\\nSome source systems use range partitioning, in which the source tables are parti‐\\ntioned along a date key, which makes it easy to identify new data. For example, if\\nyou are extracting from an orders table partitioned by day, it is easy to identify\\nthe current or previous day’s data.\\nDatabase triggers\\nY ou can add a trigger for INSERT, UPDATE, and DELETE on a single table and have\\nthose triggers write the information about the record change to a “change table. ”\\nThis is similar to change data capture, so use CDC if your database product sup‐\\nports it; otherwise, use triggers.\\nMERGE statement\\nThe least preferable option is to do a full extraction from the source system to a\\nstaging area in the DW , then compare this table with a previous full extract from\\nthe source system using a MERGE statement to identify the changed data. Y ou will\\nneed to compare all source fields with all destination fields (or use a hash func‐\\ntion). This approach likely won’t have a significant impact on the source system,\\nbut it can place a considerable burden on the DW , particularly if the data vol‐\\numes are large. This option is usually the last resort if no other options are\\npossible.\\nPopulating a Data Warehouse | 55', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fc5d17bf-ee1a-4936-973d-84c65991c7bf', embedding=None, metadata={'page_label': '55', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The Death of the Relational Data Warehouse\\nHas Been Greatly Exaggerated\\nAround the early 2010s, people in IT started questioning whether the relational data\\nwarehouse was needed anymore, asking, “Is the relational data warehouse dead?”\\nMany people understood this as asking if businesses still need DWs. They do, as this\\nchapter points out. But the question is really about the data warehouse architecture—\\ncan you just use a data lake, or should you use both a data lake and an RDW?\\nWhen data lakes first appeared, they were built on Apache Hadoop technology, and it\\nwas largely Hadoop vendors pronouncing the RDW dead. “Just put all your data in\\nthe data lake and get rid of your RDW , ” they advised. As mentioned in Chapter 2, the\\nprojects that attempted to do that all failed.\\nFor many years, I had felt that RDWs would always be needed because data lakes\\nwere all Hadoop based and there were just too many limitations. But once solutions\\nlike Delta lake (see Chapter 12) had become available and data lakes began using bet‐\\nter, easier-to-use products than Hadoop (see Chapter 16), I started to see some use\\ncases where a solution could work without an RDW . That type of solution is a data\\nlakehouse architecture, which will be covered in Chapter 12.\\nHowever, there are still plenty of use cases where an RDW is needed. And while data\\nlake technologies will continue to improve, thereby reducing or eliminating the con‐\\ncerns about bypassing an RDW (see Chapter 12), we will never completely do away\\nwith RDWs. I think there are three reasons for this. First, it’s still harder to report off\\na data lake than from a DW . Second, RDWs continue to meet the information needs\\nof users and provide value. Third, many people use, depend on, and trust DWs and\\ndon’t want to replace them with data lakes.\\nData lakes offer a rich source of data for data scientists and self-service data consum‐\\ners (“power users”), and they serve the needs of analytics and big data well. But not all\\ndata and information workers want to become power users. The majority continue to\\nneed well-integrated, systematically cleansed, easy-to-access relational data that\\nincludes a historical log that captures how things have evolved or progressed over a\\nperiod of time. These people are best served with a data warehouse.\\n56 | Chapter 4: The Relational Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='357defcc-42ef-4e14-a857-3efaa944a8ca', embedding=None, metadata={'page_label': '56', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Summary\\nThis chapter covered the first widely used technology solution to centralize data from\\nmultiple sources and report on it: the relational data warehouse. The RDW revolu‐\\ntionized the way businesses and organizations manage their data by providing a cen‐\\ntralized repository for data storage and retrieval, enabling more efficient data\\nmanagement and analysis. With the ability to store and organize data in a structured\\nmanner, RDWs allow users to generate complex queries and reports quickly and\\neasily, providing valuable insights and supporting critical decision making.\\nToday, the relational data warehouse remains a fundamental component of many data\\narchitectures and can be seen in a wide range of industries, from finance and health‐\\ncare to retail and manufacturing. The following chapter discusses the next technology\\nto become a major factor in centralizing and reporting on data: the data lake.\\nSummary | 57', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='32f003a4-9d5b-49ca-b147-c3e37b1bc0b0', embedding=None, metadata={'page_label': '57', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d29e4fcd-ed07-4253-960f-ea951ca0fb1b', embedding=None, metadata={'page_label': '58', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 5\\nData Lake\\nBig data started appearing in unprecedented volumes in the early 2010s due to an\\nincrease in sources that output semistructured and unstructured data, such as sen‐\\nsors, videos, and social media. Semi-structured and unstructured data hold a phe‐\\nnomenal amount of value—think of the insights contained in years’ worth of\\ncustomer emails! However, relational data warehouses at that time could only handle\\nstructured data. They also had trouble handling large amounts of data or data that\\nneeded to be ingested often, so they were not an option for storing these types of\\ndata. This forced the industry to come up with a solution: data lakes. Data lakes can\\neasily handle semi-structured and unstructured data and manage data that is ingested\\noften.\\nY ears ago, I spoke with analysts from a large retail chain who wanted to ingest data\\nfrom Twitter to see what customers thought about their stores. They knew customers\\nwould hesitate to bring up complaints to store employees but would be quick to put\\nthem on Twitter. I helped them to ingest the Twitter data into a data lake and assess\\nthe sentiment of the customer comments, categorizing them as positive, neutral, or\\nnegative. When they read the negative comments, they found an unusually large\\nnumber of complaints about dressing rooms—they were too small, too crowded, and\\nnot private enough. As an experiment, the company decided to remodel the dressing\\nrooms in one store. A month after the remodel, the analysts found an overwhelming\\nnumber of positive comments about the dressing rooms in that store, along with a 7%\\nincrease in sales. That prompted the company to remodel the dressing rooms in all of\\nits stores, resulting in a 6% increase in sales nationwide and millions more in profit.\\nAll thanks to a data lake!\\n59', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c70a492b-e5ee-4067-b2cc-548d754f9ca6', embedding=None, metadata={'page_label': '59', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What Is a Data Lake?\\nThe term data lake is a metaphor to describe the concept of storing vast amounts of\\nraw data in its natural format. Just as a lake holds water without changing the water’s\\nnature, a data lake holds data without the need to structure or process it first. Also,\\njust as lakes hold various wildlife and plant species, a data lake holds different types of\\ndata (structured, semi-structured, and unstructured). Contrast that with the data\\nwarehouse, where data is more structured and processed, like bottled or packaged\\ngoods in a warehouse.\\nOnce the data is in a data lake, it must be cleaned, joined, and possibly aggregated in\\norder to make it useful. This is where some type of compute (that is, the processing\\npower required to manage, manipulate, and analyze data) must connect to the data\\nlake, take and transform the data, then put it back into the data lake.\\nWhy Use a Data Lake?\\nThere are many reasons for using a data lake, especially alongside an RDW . As men‐\\ntioned, you can quickly store data in a data lake with no up-front work—an approach\\ncalled schema-on-read (see Chapter 2 ). This enables you to access the data much\\nfaster, which can allow power users to run reports quickly (for faster return on invest‐\\nment, or ROI) and give data scientists quick access to data to train a machine learning\\nmodel. It’s also useful for investigating data. If an end user asks you to copy source\\ndata into a DW , you can quickly copy the source data to the data lake and investigate\\nit to make sure it has value before you put forth the effort of creating the schema in\\nthe data warehouse and writing the ETL.\\nI mentioned in Chapter 4 that typically, with a DW , you have a maintenance window\\nat night where you kick the end users off so you can load the source data into DW\\nstaging tables. Y ou then clean the data and copy it into the DW production tables.\\nThe problem is that if you run into problems and maintenance takes longer than\\nexpected, when end users try to access the data in the morning, they’re locked out. A\\nbetter option is to transform the data in the data lake instead of in the DW . The bene‐\\nfits of this include:\\n• Cost savings, since the DW compute is usually much more expensive than other\\ntypes of compute that you can use on the data in the data lake.\\n• Extreme performance for transformations (if needed), by having multiple com‐\\npute options where each compute can access different folders containing data\\nand run in parallel (or run in parallel on the same data).\\n• Ability to refine the data in many more ways than with the DW because of the\\nflexibility to use many different types of compute options on the data in the data\\nlake. For example, you can use code libraries with prebuilt routines to do\\n60 | Chapter 5: Data Lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='69a0fbc4-cb53-4eed-8077-803e0d3225ef', embedding=None, metadata={'page_label': '60', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='complex transformations that are very difficult or impossible to do using SQL in\\nthe DW .\\n• No need for a DW maintenance window, allowing 24/7 use of the DW for quer‐\\nies. Y ou also avoid users competing for the same compute resources if you try to\\nrun reports while data transformations are going on. Since data transformations\\nare resource hogs, you can really slow each other down.\\nA data lake is an inexpensive way to store an unlimited amount of data. Unlike on-\\nprem storage, cloud providers have unlimited storage, so you never have to worry\\nabout running out of space in your data lake. Also, cloud storage is relatively cheap,\\nand most cloud providers have multiple storage tiers that allow their customers to\\nsave even more money.\\nThe data lake lets you collect any data “just in case” you might need it in the future.\\nThis data stockpiling is rarely done with a DW since storage in a DW is usually much\\nmore expensive than in a data lake and holding more data in the DW can affect per‐\\nformance. And because storage is so cheap in a data lake, data is rarely deleted unless\\nfor regulatory reasons.\\nBecause it acts as a centralized place for all subjects of data, a data lake can be the\\n“single version of the truth. ” If any and all data lands in the data lake before it is\\ncopied elsewhere, the data lake can be the place to go back to if there are questions\\nabout the accuracy of the data in any of the other places it is used, such as queries,\\nreports, and dashboards. A data lake also makes it easy for any end user to access any\\ndata from any location and use it many times for any analytic needs and use cases\\nthey wish. When you use a data lake alongside a DW , the data lake becomes the single\\nversion of the truth instead of the DW . Furthermore, a data lake is a place to land\\nstreaming data, such as from IoT devices, which is nearly impossible to land in a DW .\\nThe data lake also acts as an online archive. For example, you could keep the last\\nthree years of order data in the DW and order data older than three years in the data\\nlake. Most of the technology used to build a DW will allow you to query data in a data\\nlake using SQL, so users can still use the data in the lake in reports (with the trade-off\\nof slower performance). This can save costs and avoid running out of storage space in\\nthe DW .\\nThe data lake is also a place where you can back up data from the data warehouse in\\ncase you need to restore data to the DW due to accidental deletion, corruption, an\\nincorrect WHERE clause on an UPDATE query, and so on.\\nIn addition, you can integrate differently structured data into a data lake: everything\\nfrom CSV and JSON files to Word documents and media files. Data can be extracted\\nfrom those files to give you more value.\\nWhy Use a Data Lake? | 61', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d9fce0e-31e3-4a97-b5c5-91500d99602d', embedding=None, metadata={'page_label': '61', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='With a DW , the raw data that is copied from a source system into staging tables in the\\nDW , then transformed and loaded into the production tables is usually deleted after a\\nday or two to save space in the limited relational storage. A problem arises if an ETL\\nerror is discovered from an ETL run many days ago; if the ETL needs to be rerun but\\nthe raw data has been deleted, you have to go back to the source system and possibly\\naffect its performance. By doing the transformation in the data lake, where storage is\\ncheaper, you can keep a long history of the raw data and never have to go back to the\\nsource system.\\nBottom-Up Approach\\nBecause a data lake is schema-on-read, little up-front work needs to be done to start\\nusing the data. This works really well if you don’t know what questions to ask of the\\ndata yet—you can quickly explore the data to surface the relevant questions. This\\nresults in what is referred to as a bottom-up approach, as seen in Figure 5-2, where\\nyou collect data up front before generating any theories or hypotheses. This is very\\ndifferent from the top-down approach of a relational data warehouse, as seen in\\nFigure 5-1.\\nFigure 5-1. Top-down approach\\nFigure 5-2. Bottom-up approach\\nThis allows data scientists, who typically use software that prefers data in files, to use\\nmachine learning on the data in the data lake to determine what will happen (predic‐\\ntive analytics) and how can people make it happen (prescriptive analytics).\\nPredictive analytics makes use of data, statistical algorithms, and machine learning\\ntechniques to predict future outcomes based on historical data. It includes a variety of\\nstatistical techniques, such as data mining, predictive modeling, and machine learn‐\\ning, to analyze current and historical facts to make predictions about future or other‐\\nwise unknown events. This allows you to be proactive, not just reactive. For instance,\\n62 | Chapter 5: Data Lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='606d5e63-2404-4d93-91e6-6d6711d6c551', embedding=None, metadata={'page_label': '62', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='predictive analytics can be used in healthcare to forecast patient readmission rates, in\\nretail to predict future sales, or in banking to predict loan defaults.\\nPrescriptive analytics goes a step further than predictive analytics. It utilizes optimiza‐\\ntion and simulation algorithms to advise on possible outcomes. Prescriptive analytics\\nnot only predicts what will happen in the future but also suggests actions to take to\\naffect those outcomes. The goal is to provide advice based on predicted future scenar‐\\nios to optimize decision making. Prescriptive analytics can suggest decision options\\non how to take advantage of a future opportunity or mitigate a future risk, and it can\\nillustrate the implications of each decision option. For example, in logistics, prescrip‐\\ntive analytics can be used to find the best routes for delivery and even suggest alter‐\\nnate routes in case of unexpected road closures.\\nData lakes were used mainly for predictive and prescriptive analytics at first to avoid\\nthe difficulties of trying to perform advanced analytics with the traditional data ware‐\\nhouse. But now data lakes are used for much more, as indicated in “Why Use a Data\\nLake?” on page 60.\\nIf you find valuable data in the data lake when exploring and you want to make it\\neasily accessible to end users, you can always model it later by copying it to an RDW . \\nData modeling is like creating a blueprint for organizing and understanding data. It\\nhelps define what data is important, how it’s related, and how it should be stored and\\nused in an RDW . Data lakes have limited tools for data modeling, while DWs have\\nhad modeling tools for years.\\nBest Practices for Data Lake Design\\nDesigning a data lake should be a time-consuming process. I often find that compa‐\\nnies don’t spend enough time thinking through all their use cases when designing\\ntheir data lakes and later have to redesign and rebuild them. So make sure you think\\nthrough all the sources of data you use now and will use in the future, and that you\\nunderstand the size, type, and speed of the data. Then absorb all the information you\\ncan find on data lake design and choose the appropriate design for your situation.\\nA data lake generally doesn’t enforce a specific structure on the data it ingests; in fact,\\nthis is one of its key characteristics. This is different from a traditional database or\\ndata warehouse, which requires data to be structured or modeled beforehand. How‐\\never, to make data usable and prevent the data lake from becoming a “data swamp”\\n(an unorganized and unmanageable collection of data), it’s important to apply some\\norganization and governance practices. This section introduces some best practices to\\nget you started.\\nThe first best practice is to logically divide the data lake into multiple layers (also\\ncalled zones) corresponding to increasing levels of data quality, as shown in\\nBest Practices for Data Lake Design | 63', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7d743673-03e5-491b-ba46-948b43eaca73', embedding=None, metadata={'page_label': '63', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 5-3 . Otherwise, you would have all your files in one folder, greatly affecting\\nperformance, manageability, and security.\\nFigure 5-3. Data lake layers\\nTo better understand the structure and function of a data lake, let’s delve into its vari‐\\nous layers, each representing a step in the enhancement of data quality and usability.\\nThese layers are arranged in a hierarchy, progressing from raw, unprocessed data to\\nhighly refined and business-ready information. Here’s a closer look at each layer:\\nRaw layer\\nRaw events, stored for historical reference, are usually kept forever (immutable).\\nThink of the raw layer as a reservoir that holds data in its natural and original\\nstate (no transformations applied). It’s unfiltered and unpurified. This layer is\\nalso called the bronze layer, staging layer, or landing area.\\nConformed layer\\nMany times, the data in the raw layer will be stored as different types, such as\\nCSV , JSON, and Parquet (a common storage file format optimized for efficient\\nbig data processing). The conformed layer is where all the file types are converted\\nto one format, usually Parquet. Think of the lake as having two reservoirs, one of\\nsaltwater and one of freshwater, and using desalination to turn the saltwater into\\nfreshwater. This layer is also called the base or standardized layer.\\nCleansed layer\\nHere, raw events are transformed (data is cleaned, integrated, and consolidated)\\ninto directly consumable datasets. Think of the cleansed layer as a filtration layer.\\nIt removes impurities and may also enrich the data. The aim is to make the stored\\nfiles uniform in terms of encoding, schema, format, data types, and content (such\\nas strings and integers). This layer is also called the silver, transformed, refined,\\nintegrated, processed, or enriched layer.\\n64 | Chapter 5: Data Lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6fa4e702-af2c-4732-88be-eaf40f84a48f', embedding=None, metadata={'page_label': '64', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Presentation layer\\nBusiness logic is applied to the cleansed data to produce data ready to be con‐\\nsumed by end users or applications, typically in a format that’s easy to understand\\nand use. Transformation might involve aggregations or summaries, or it may\\nmean putting files into a specific layout for use in reporting tools (such as a star\\nschema, discussed in Chapter 8 ), often including information about the data\\n(metadata) within each file. This layer is also called the application, workspace,\\ntrusted, gold, secure, production ready, governed, curated, serving, analytics, or con‐\\nsumption layer.\\nSandbox layer\\nThis optional layer is used to “play” in. Generally the sandbox is used by data sci‐\\nentists. It is usually a copy of the raw layer where data can not only be read, but\\nalso modified. Y ou might create multiple sandbox layers. This layer is also called\\nthe exploration layer, development layer, or data science workspace.\\nThis is just one way of setting up the layers in your data lake. I have seen many other\\nexcellent designs that include additional layers or combine some layers, such as the\\nconformed and cleansed layers.\\nFor an example using the layers in Figure 5-3, let’s consider a retail company that has\\ndifferent data sources like point-of-sale (POS) systems, online sales, customer feed‐\\nback, and inventory systems. The four layers would contain data of different quality\\nas follows:\\n• Raw layer\\n— Raw logs from the POS system, including every transaction detail (such as\\ntimestamp, items purchased, quantities, prices, total amount, cashier ID, and\\nstore location)\\n— Online sales data from the website or app, such as user ID, items purchased,\\nquantities, prices, total amount, and timestamps\\n— Inventory data from various warehouses and stores, including items, quanti‐\\nties, and restock dates\\n— Raw customer feedback collected through surveys, reviews, and ratings\\n• Conformed layer\\n— All POS, online sales, inventory, and customer feedback files are converted to\\nParquet format if they are not already in this format.\\n• Cleaned layer\\n— POS and online sales data with any errors removed or corrected (such as\\ninconsistencies in how items are named, errors in quantities, and missing\\ntimestamps). Data has also been transformed into a common schema (for\\nBest Practices for Data Lake Design | 65', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3c7b2c0b-f03e-4d72-8fb4-a6923892d3e1', embedding=None, metadata={'page_label': '65', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='example, common naming conventions and formats for items, common time\\nformat, or common store ID system).\\n— Inventory data with standardization in item naming and with errors or incon‐\\nsistencies corrected. Data has also been transformed to align with the com‐\\nmon item names and store IDs used in the sales data.\\n— Customer feedback data that has been cleaned to remove irrelevant or errone‐\\nous responses, given standardized formats, and so forth. Data has also been\\ntransformed into a common format, aligning with the common store ID sys‐\\ntem and perhaps with common elements extracted from the feedback for\\nanalysis.\\n• Presentation layer\\n— A consolidated sales report showing total sales per store, per region, or per\\nday/month/year, and perhaps sales broken down by item or category\\n— An inventory report showing current stock levels for each item in each store\\nor warehouse, as well as restocking schedules\\n— A customer feedback report summarizing the feedback, maybe with a senti‐\\nment analysis score for each store or product\\nAnother best practice is to create a folder structure, usually a different one for each\\nlayer that can be divided up in many different ways for different reasons. Here are\\nsome examples:\\nData segregation\\nOrganizing data based on source, business unit, or data type makes it easier for\\ndata scientists and analysts to locate and use the relevant data.\\nAccess control\\nDifferent teams or individuals within an organization may have different levels of\\naccess to data. By structuring folders based on user roles or departments, organi‐\\nzations can implement fine-grained access control policies.\\nPerformance optimization\\nOrganizing data in a specific way can lead to improved performance. For\\ninstance, certain data processing or querying operations may be faster if the data\\nis grouped based on specific characteristics.\\nData lifecycle management\\nData often has a lifecycle, from ingestion to archival or deletion. Different folders\\nmight be used to segregate data based on its stage in the lifecycle.\\nMetadata management\\nFolders can be used to manage and segregate metadata from the raw data. This\\nsegregation can simplify metadata management and speed up data discovery.\\n66 | Chapter 5: Data Lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='054a714b-48a0-4eb8-8457-f95093bc4dbc', embedding=None, metadata={'page_label': '66', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Compliance requirements\\nIn many industries, compliance requirements dictate that certain types of data be\\nstored and managed in specific ways. Different folder structures can help organi‐\\nzations meet these requirements.\\nBackup and disaster recovery\\nHaving different folder structures can assist in creating a strategic backup and\\ndisaster recovery plan. Certain folders might be backed up more frequently or\\nretained for longer based on their importance.\\nData versioning\\nDifferent folders might be used to manage different versions of the same dataset.\\nData partitioning\\nData can be partitioned by key attributes for quicker query performance.\\nIngestion and processing needs\\nBased on the source, data might need different processing pipelines. Different\\nfolders can help manage and streamline these processes.\\nTo satisfy one or more of the reasons above, you may divide the folders up in one or\\nmore of these ways:\\n• Time partitioning (year/month/day/hour/minute), such as when the data was\\ncopied into the data lake\\n• Subject area\\n• Source of the data\\n• Object, such as a source table\\n• Security boundaries, such as department or business area\\n• Downstream app or purpose\\n• Type of data, such as detail or summary\\n• Data retention policy, such as temporary and permanent data, applicable period,\\nor project lifetime\\n• Business impact or criticality, such as high, medium, or low\\n• Owner/steward/SME\\n• Probability of data access, such as for recent or current data versus historical data\\n• Confidential classification, such as public information, internal use only, sup‐\\nplier/partner confidential, personally identifiable information, or sensitive\\ninformation\\nFigure 5-4 shows an example folder structure for the raw and cleaned zones.\\nBest Practices for Data Lake Design | 67', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d280b469-c322-4217-932f-3b1c6e9000de', embedding=None, metadata={'page_label': '67', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 5-4. Folder structure in two data lake zones\\nMost times, all these layers are under one cloud subscription. There are some excep‐\\ntions, such as if you have specific requirements for billing, if you’ll hit some subscrip‐\\ntion limit, or if you want separate subscriptions for development, testing, and\\nproduction. (See the next section for more exceptions.)\\nMost customers create a storage account for each layer (so three layers means three\\nstorage accounts), all within a single resource group (a container that holds related\\nresources for a solution). This isolates the layers to help make performance more pre‐\\ndictable and allows for different features and functionality at the storage account\\nlevel.\\nTypical features that are set at each storage account level are available through most\\ncloud providers (I’m being specific to Azure in this section) include lifecycle manage‐\\nment, which allows you to reduce costs by automating data management tasks (like\\ntransitioning data across different storage tiers or deleting data when it’s no longer\\nneeded). Y ou can also set firewall rules to only allow access to certain trusted groups\\nor individuals, or to prevent hitting an account’s storage or throughput limit.\\nMost data lakes use cloud storage access tiers, with the raw and conformed layers\\nusing the archive tier, the cleansed layer using the cold tier, and the presentation and\\nsandbox layers using the hot tier. Each tier offers a different balance of storage cost\\nand access costs, with the hot tier being the most expensive for storage and the\\narchive tier having the highest retrieval cost.\\nI recommend putting auditing or integrity checks in place to make sure the data is\\naccurate as it moves through the layers. For example, if working with finance data,\\nyou might create a query that sums the day’s orders (row count and sales total) and\\ncompares the values to the source data to make sure that those values are equal in all\\nthe data layers. If they are not equal, that indicates a problem with the pipelines you\\ncreated to move and transform the data. Y ou’ll need to fix the problem and rerun the\\npipelines.\\n68 | Chapter 5: Data Lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f20dad20-e161-4f78-998a-830bb8a18096', embedding=None, metadata={'page_label': '68', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Multiple Data Lakes\\nIdeally, you’ d just create one large data lake and use it for all your data. This approach\\nwould simplify finding and combining data for queries or reports. But there are many\\nreasons you might need to create multiple physically separate data lakes (as shown in\\nFigure 5-5) instead of just dividing one data lake into different sections. Most of the\\nuse cases I discuss next aren’t possible (or are at least much less convenient) unless\\nthe data lakes are physically separate.\\nFigure 5-5. Separate data lakes across the world\\nAdvantages\\nI’ve grouped the reasons for creating physically separate data lakes into five sections,\\nafter which I’ll discuss the disadvantages of having multiple data lakes.\\nOrganizational structure and ownership\\nMaintaining multiple data lakes can be advantageous for many reasons, most of\\nwhich relate to organization structure and ownership. For example, the company’s\\norganizational structure may encourage or require each organizational unit to retain\\nownership of its own data, a feature typically associated with a data mesh (see\\nChapter 13).\\nMultiple Data Lakes | 69', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d64cc0b-2e07-4c6f-a5d2-4e5a91bb4d0a', embedding=None, metadata={'page_label': '69', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Another reason is that different teams or departments may require their own distinct\\ndata lakes for specific use cases or projects.\\nFurther, a company may benefit from having a source-aligned data lake as well as a\\nconsumer-aligned data lake. Data from a particular source can be collected into a data\\nlake with minimal transformations. This works well for users who understand the\\nsource data, such as those in manufacturing when the data is sourced from a manu‐\\nfacturing process; however, those outside of manufacturing may find the data difficult\\nto understand. In such cases, you might copy the data to a consumer-aligned data\\nlake and transform it to make it more understandable. Thus, having separate data\\nlakes for source-aligned and consumer-aligned data instead of one data lake can sim‐\\nplify the data-handling process.\\nCompliance, governance, and security\\nThe decision to have multiple data lakes instead of just one is significantly influenced\\nby a variety of factors falling under the broad categories of compliance, governance,\\nand security. For instance, multi-regional deployments can necessitate multiple data\\nlakes when data residency or sovereignty requirements vary across regions. For\\nexample, data originating from China cannot be exported outside the country, so it\\nrequires a unique data lake within the region.\\nAnother crucial consideration is that multiple data lakes allow you to separate sensi‐\\ntive or confidential data from less sensitive data. More restrictive and stringent secu‐\\nrity controls can be applied specifically to sensitive data lakes, enhancing overall data\\nsecurity.\\nIf some individuals have elevated access privileges, separate data lakes allow you to\\nlimit the scope of those privileges, confining them to the particular data lake in which\\nthe individual is working. This promotes a more secure data environment.\\nFinally, diverse governance and compliance requirements can be a driving force for\\nhaving multiple data lakes. Regulations such as the General Data Protection Regula‐\\ntion (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA)\\nnecessitate unique data management standards and practices. With multiple data\\nlakes, organizations can manage their data according to the specific governance and\\ncompliance requirements that apply to each dataset, a practice that is particularly vital\\nin highly regulated sectors.\\nCloud subscription, service limits, and policies\\nHaving multiple data lakes can also be advantageous for reasons primarily associated\\nwith cloud subscription, service limits, and policies. For example, having multiple\\nlakes can help circumvent cloud providers’ subscription or service limits, quotas, and\\nconstraints, such as a cap on the maximum number of storage accounts per\\n70 | Chapter 5: Data Lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c7ebd8dc-4725-46ee-8e87-5324c5be1b85', embedding=None, metadata={'page_label': '70', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='subscription. In such scenarios, if you were to operate with only a single data lake,\\nyour needs could exceed those limits.\\nMoreover, multiple data lakes allow you to implement distinct cloud policies. Cloud\\nproviders typically offer hundreds of different policy options, which can be tailored\\nindividually for each data lake. This flexibility helps ensure compliance with company\\npolicies. An example might be a specification that some storage accounts have infra‐\\nstructure encryption, a rule that varies from one data lake to another within your\\noperation.\\nFinally, it can be more straightforward to track costs for billing purposes when you\\nmaintain separate data lakes, each with its own cloud subscription. This level of gran‐\\nularity in cost tracking offers a clear advantage over alternative methods provided by\\ncloud services, such as using tags for each resource.\\nPerformance, availability, and disaster recovery\\nPerformance, availability, and disaster recovery also provide numerous reasons to\\nconsider employing multiple data lakes. One such reason is improved latency. If a set\\nof global end users are accessing a single data lake, those who are far away may find\\nthe service quite slow. By situating a data lake in the same region as the end user or\\napplication querying the data, you can significantly decrease the time it takes to\\naccess data.\\nMaintaining multiple data lakes with copies of the data in different regions also sig‐\\nnificantly enhances disaster recovery capabilities. If a data lake in one region becomes\\ninaccessible, end users can be redirected to an alternate region holding identical data.\\nHaving multiple data lakes also provides the flexibility to implement different data\\nrecovery and disaster recovery strategies for different types of data. Critical data that\\ndemands constant availability could be hosted in a data lake whose data is replicated\\nacross multiple lakes in different regions, while less critical data that can withstand\\nperiods of inaccessibility could be stored in a single data lake for significant cost\\nsavings.\\nFinally, with multiple data lakes, you can implement different service levels for dis‐\\ntinct types of data. For instance, you could dedicate one data lake to storing and pro‐\\ncessing high-priority data, which requires low-latency access and high availability,\\nand dedicate another data lake to lower-priority data where higher latencies and\\nlower availability are tolerable. This strategy optimizes the cost and performance of\\nyour data management infrastructure by allowing you to use less expensive storage\\nand processing resources for lower-priority data.\\nMultiple Data Lakes | 71', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9f23d31-d182-4a9d-be10-792c16ef8461', embedding=None, metadata={'page_label': '71', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data retention and environment management\\nMultiple data lakes can make data retention and environment management more effi‐\\ncient. For instance, by segregating the data lakes dedicated to development, testing,\\nand production environments, organizations can ensure that each environment has\\nits own isolated space for data storage and processing. This minimizes the risk of\\ninterference or conflicts between different stages of the data lifecycle.\\nAnother advantage of having multiple data lakes is that you can implement distinct\\ndata retention policies. Legal or regulatory requirements often dictate the need to\\nretain data for specific periods. If you have separate data lakes for different types of\\ndata, you can easily enforce diverse retention policies tailored to those categories.\\nThis approach allows for efficient management of data retention, ensuring compli‐\\nance with various regulations while optimizing storage resources.\\nDisadvantages\\nUsing multiple data lakes can increase the complexity and cost of your data manage‐\\nment infrastructure and require more resources and more expertise to maintain, so if\\nyou have a choice, it’s important to weigh the benefits against the costs. However, in\\nsome cases, you will have no choice; for example, if you’re designing around a need\\nfor data sovereignty, you’ll have to have multiple data lakes.\\nProperly transferring data between multiple data lakes while maintaining its consis‐\\ntency may require additional integration and management tools (such as Azure Data\\nFactory, Informatica, or Apache NiFi). Finally, having multiple data lakes adds the\\nperformance challenge of combining the data when a query or report needs data\\nfrom multiple lakes. If the lakes are physically located very far apart, possibly even in\\ndifferent parts of the world, it can be time consuming to copy all the data into one\\nlocation.\\nSummary\\nThis chapter provided a comprehensive exploration of the data lake, a large-scale data\\nstorage and management solution capable of holding raw data in its native format. I\\ndiscussed the data lake’s role and design principles, as well as why you might consider\\nmaintaining more than one data lake in certain contexts. Unlike relational data ware‐\\nhouses, data lakes allow quick data storage without any up-front preparation or trans‐\\nformation. This ensures a rapid and effortless data ingestion process, particularly\\nuseful in today’s era of big data.\\n72 | Chapter 5: Data Lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ccdc2940-a91c-4be5-a492-34b90863aaf3', embedding=None, metadata={'page_label': '72', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='I discussed the benefits of using a data lake, emphasizing the flexibility and speed it\\noffers for data storage. This aspect, along with its ability to accommodate a diverse\\nrange of data types (structured, semi-structured, and unstructured) is a key advan‐\\ntage, especially in situations where quick, scalable, and diverse data collection is\\ncrucial.\\nThe chapter also introduced the bottom-up approach, an important methodology in\\nwhich you collect data collection before generating any theories or hypotheses. Con‐\\ntrary to the traditional top-down strategy, this approach fosters a more agile, data-\\ncentric decision-making process.\\nA substantial part of the chapter focused on the design of a data lake, introducing the\\nconcept of logically dividing a data lake into multiple layers. This layering structure\\nhelps to manage data in an organized manner, paving the way for efficient data\\nretrieval and analytics, and it can also aid in security, access control, and data lifecycle\\nmanagement.\\nThe chapter concluded by exploring reasons why an organization might choose to\\nhave multiple data lakes. The potential benefits include increased security, improved\\nregulatory compliance, and better data management across distinct business units or\\nspecific use cases.\\nIn Chapter 6, we’ll move on to looking at data stores.\\nSummary | 73', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0be88a43-14ab-4e68-8d99-57a96f86db6e', embedding=None, metadata={'page_label': '73', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1747b9ae-0e47-404c-92d5-336c805876fc', embedding=None, metadata={'page_label': '74', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 6\\nData Storage Solutions and Processes\\nIn the digital age, data has become the lifeblood of organizations. But, as any seas‐\\noned data professional knows, simply having data isn’t enough. The real value lies in\\nhow effectively this data is managed, stored, and processed. That’s why this chapter is\\na comprehensive guide to navigating the intricate world of data management. I’ll take\\nan in-depth look at how data storage solutions and processes operate within diverse\\ndata architectures.\\nThe components of data management are like gears in a complex machine; each ful‐\\nfills a unique role, yet they all work synchronously to achieve a common goal. I’ll\\nstart with some of the most practical storage solutions: data marts, operational data\\nstores, and data hubs. The second half of the chapter, “Data Processes”, reveals the\\nbroad spectrum of processes involved in managing and leveraging data effectively,\\nexamining concepts like master data management, data virtualization, data catalogs,\\nand data marketplaces.\\nFor instance, while data marts provide department-specific views of data, operational\\ndata stores offer a broader, real-time view of business operations. On the other hand,\\nmaster data management ensures the consistency and reliability of data, with data\\nvirtualization providing a unified, abstracted view of data from various sources. The\\ndata catalog serves as the reference guide, making data discovery a breeze, while\\nthe data marketplace opens doors for data sharing or even monetizing. Meanwhile,\\nthe data hub acts as the system’s data epicenter, organizing and managing data from\\ndifferent sources for unified access. Each of these components can stand alone in its\\nfunction; however, when integrated, they can form a powerful, holistic data strategy,\\ndriving your business toward informed decision making, enhanced operational effi‐\\nciency, and improved competitive advantage.\\n75', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='25beadfb-c32e-47fa-b9b9-b10fe9bbe34e', embedding=None, metadata={'page_label': '75', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whether you’re a seasoned data scientist, an aspiring data professional, or a strategic\\ndecision maker, understanding these key components and learning to orchestrate\\nthem in harmony can unlock the true potential of your organization’s data.\\nData Storage Solutions\\nLet’s dive in, shall we? This isn’t just about storing and processing data—it’s about set‐\\nting the stage for data-driven transformation.\\nData Marts\\nA data mart, as shown in Figure 6-1, is a subset of the data warehouse. It is usually\\ndesigned as a focused repository of data that is optimized to meet the specific needs\\nof a department or a business line within an organization (like finance, human\\nresources, or marketing). Because of its narrower scope, a data mart can provide\\nusers with a more streamlined and accessible view of information than a DW can.\\nIt can be challenging for users to navigate large and complex data warehouses that\\nhouse vast amounts of data. Data marts address this challenge by extracting and con‐\\nsolidating relevant data from the DW and presenting it in a more user-friendly man‐\\nner. This way, users within a department can easily locate and retrieve the data they\\nneed.\\nFigure 6-1. Data marts\\nOne of the key advantages of data marts is that they can be created and maintained by\\nindividual departments, rather than relying solely on the IT department. Also, each\\ndata mart can be structured differently to accommodate the needs of the department\\nit serves. This allows business users more control over the data; they can tailor it to\\ntheir unique requirements and use the data structures that are most relevant to their\\ndomain.\\nDepartment-specific data marts provide users quicker, easier access to the data they\\nneed for their day-to-day operations and decision-making processes. They can\\n76 | Chapter 6: Data Storage Solutions and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7e1169c0-5db7-4c61-844c-13baca67b871', embedding=None, metadata={'page_label': '76', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='perform analytical queries, generate reports, and gain insights into their department’s\\nperformance without having to navigate the entire DW .\\nMoreover, data marts promote data governance and security. Each department can\\ndefine and enforce access policies for its data mart, ensuring that only authorized\\nindividuals have access to specific datasets. This helps to protect sensitive informa‐\\ntion and ensure compliance with data privacy regulations.\\nHaving delved into data marts, with their targeted, department-specific insights, we\\nnow shift our attention to operational data stores.\\nOperational Data Stores\\nThe operational data store (ODS) has a broader scope than a data mart, providing an\\nenterprise-wide, real-time view of data. While data marts offer valuable, in-depth\\ninsights into specific areas, the ODS presents a comprehensive look at the organiza‐\\ntion’s operational data. Each tool offers a different perspective; if a data mart lets you\\nlook at your data through a microscope, the ODS lets you view it through a wide-\\nangle lens.\\nThe purpose of an ODS is to integrate corporate data from different sources to enable\\noperational reporting in (or near) real time. It is not a data warehouse. As its name\\nimplies, it stores operational data, not analytical data. It provides data much faster\\nthan a DW can: every few minutes, instead of daily.\\nAn ODS might be a good fit for your organization if the data you’re currently using\\nfor reporting in the source systems is too limited or if you want a better and more\\npowerful reporting tool than what the source systems offer. If only a few people have\\nthe security clearance needed to access the source systems but you want to allow oth‐\\ners to generate reports, an ODS may be a good solution.\\nUsually, data in an ODS is structured similarly to its structure in the source systems.\\nDuring integration, however, the ODS solution can clean, normalize, and apply busi‐\\nness rules to the data to ensure data integrity. This integration happens quite fre‐\\nquently throughout the day, at the lowest level of granularity. Normally, you won’t\\nneed to optimize an ODS for historical and trend analysis, since this is done in the\\ndata warehouse.\\nThe data in the ODS is closer to real time than the data required by the data ware‐\\nhouse. Y ou can use an ODS as one of the staging areas for the DW , at least for the data\\nthat it maintains (see Figure 6-2). I recommend that you first reconcile and cleanse\\nthat data, populating the ODS in near real time to provide value to the operational\\nand tactical decision makers who need it. Y ou can then also use it to populate the data\\nwarehouse, which typically has less demanding load cycles. Y ou will still need the\\nstaging area for DW-required data that is not hosted in the ODS, but in following this\\nData Storage Solutions | 77', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6cb72119-6394-4330-9cf5-cd60dad1203a', embedding=None, metadata={'page_label': '77', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='recommendation, you reduce the number of ETL flows and staging-area sizes. That\\nhelps improve performance without sacrificing value or function.\\nFigure 6-2. An operational data store\\nTable 6-1 summarizes the most important differences between an ODS and a data\\nwarehouse.\\nTable 6-1. Differences between an ODS and a data warehouse\\nOperational data store Data warehouse\\nBest suited for Granular, low-level queries against detailed\\noperational data\\nComplex queries against summary or aggregated\\nanalytical data\\nPurpose Operational reporting; current or near real-time\\nreporting\\nHistorical and trend analysis reporting on a large\\nvolume of data\\nData duration Contains a short window of data Contains the entire history of the organization’s data\\nDecision making Supports operational and tactical decisions on\\ncurrent or near real-time data\\nProvides feedback on strategic decisions, leading to\\noverall system improvements\\nData load\\nfrequency\\nMight load data every few minutes or hourly Might load data daily, weekly, monthly, or quarterly\\nUse case\\nLet’s say you’re running inventory reports for a fictional retail chain called ShoesFor‐\\nLess. If you had only one retail store with one database to track orders, you could just\\nrun the inventory report against that database. But since you have many retail stores,\\neach of which tracks orders in its own database, the only way to get up-to-the-minute\\naccurate inventory reports is to combine the order data from those databases. This is\\nimportant for letting customers know which items are out of stock, as well as restock‐\\ning items quickly when supply is low. A data warehouse would not be real-time\\nenough for that situation.\\n78 | Chapter 6: Data Storage Solutions and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='31ad24e4-8bfc-41da-a099-a179500d40c7', embedding=None, metadata={'page_label': '78', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Now suppose a customer service representative gets a call from a customer asking\\nwhether a particular style and size of shoe is available. The representative queries the\\nODS to retrieve real-time data on availability for that type of shoe (for example,\\n“men’s running shoe, size 10”) and return the current stock level.\\nOn the other hand, if the store’s marketing manager wants to understand sales trends\\nfor that shoe style over the past year, they can use the DW . They run an aggregated\\nquery that sums the sales of “men’s running shoe” for each month within the past\\nyear. This provides a high-level view of trends that can inform strategic marketing\\nand inventory decisions.\\nData Hubs\\nA data hub is a centralized data storage and management system that helps an organi‐\\nzation collect, integrate, store, organize, and share data from various sources and pro‐\\nvides easy access to analytics, reporting, and decision-making tools. Most\\nsignificantly, data hubs eliminate the need to use hundreds or thousands of point-to-\\npoint interfaces to exchange data between systems. While data warehouses and data\\nlakes have the primary (and often exclusive) goal of serving data for analytics, data\\nhubs are also valuable as a means of communication and data exchange among\\noperational systems (Figure 6-3).\\nFigure 6-3. Data hub\\nData hubs are often used in conjunction with data lakes and warehouses to create a\\ncomprehensive data management infrastructure. As you learned in Chapters 4 and 5,\\ndata lakes store large volumes of raw, unprocessed data, while data warehouses store\\nstructured, processed data optimized for analytical processing. A data hub comple‐\\nments these systems by acting as an intermediary for data exchange, collaboration,\\nand governance. To clarify, let’s look at how the data hub interacts with and compares\\nto the other approaches in this chapter. (Table 6-2 summarizes this comparison.)\\nData Storage Solutions | 79', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ca4b823-d0b1-4b27-ab18-de243811472f', embedding=None, metadata={'page_label': '79', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 6-2. Comparison of data solution features\\nFeature Data hub Data lake Relational data\\nwarehouse\\nData catalog Data marketplace\\nPrimary\\nfocus\\nData storage,\\nintegration,\\ndistribution\\nData storage,\\nprocessing,\\nanalytics\\nData storage,\\nprocessing,\\nanalytics\\nMetadata\\nmanagement, data\\ndiscovery\\nData buying,\\nselling, sharing\\nData storage Raw or lightly\\nprocessed data\\nRaw and processed\\ndata\\nHighly structured,\\nprocessed data\\nMetadata only Various formats,\\nprovider-\\ndependent\\nData\\nstructure\\nStructured, semi-\\nstructured,\\nunstructured\\nStructured, semi-\\nstructured,\\nunstructured\\nStructured Metadata only Structured, semi-\\nstructured,\\nunstructured\\nData\\nprocessing\\nLimited processing,\\nmainly integration\\nand distribution\\nOn-demand\\nprocessing,\\nanalytics\\nExtensive ETL,\\ncomplex queries\\nNone Provider\\ndependent\\nData sharing Built-in Third party Third party None Built-in\\nData\\nownership\\nWithin organization\\nor related groups\\nWithin organization\\nor related groups\\nWithin\\norganization or\\nrelated groups\\nWithin organization\\nor related groups\\nMultiple\\norganizations\\nUse cases Data ingestion,\\nintegration,\\ndistribution\\nBig data analytics,\\nmachine learning,\\nartificial\\nintelligence\\nComplex analytics,\\nreporting, decision\\nmaking\\nData discovery,\\ndata lineage,\\nenforcing data\\ngovernance policies\\nMonetizing data,\\nacquiring new\\ndatasets, data\\ndiscovery\\nA data hub stores copies of source data and is focused on managing and distributing,\\nwhereas a data catalog does not store actual data and is primarily concerned with\\nmetadata management, data lineage, enforcement of data governance policies, and\\nfacilitation of data discovery.\\nA data hub can serve as a source for a data marketplace, which does not store data.\\nThe hub is focused on managing and distributing data within an organization, not\\nthe commercial transactions of a data marketplace. It is a central platform that uses\\nmore storage systems than a data lake or an RDW . Data lakes typically use object stor‐\\nage to store data, while RDWs use relational storage; data hubs can combine storage\\nsystems like relational databases, NoSQL databases, and data lakes.\\nLike a data lake, a data hub supports structured, semi-structured, and unstructured\\ndata, as shown in Figure 6-3, whereas an RDW supports only structured data. Data\\nhubs are more likely to provide built-in mechanisms for data cataloging, lineage, and\\naccess controls, where a data lake often integrates third-party tools or custom solu‐\\ntions instead.\\nIn terms of processing, a data lake uses compute to clean data and land it back in the\\ndata lake. RDWs do extensive data processing, including ETL operations, to clean,\\nnormalize, and structure data for analytical purposes. With a data hub, however, data\\n80 | Chapter 6: Data Storage Solutions and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='07e19555-d075-4270-a88c-33d316ca7f7a', embedding=None, metadata={'page_label': '80', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='is usually transformed and analyzed outside the hub using external tools or\\napplications.\\nIn summary, a data hub is a flexible and versatile solution for ingesting, managing,\\nintegrating, and distributing data from a wide variety of types and sources. By con‐\\ntrast, a data lake is focused on storing and processing large volumes of raw data for\\nuse in analytics and machine learning, and a data warehouse is optimized for struc‐\\ntured data storage for use in complex analytics, reporting, and decision making.\\nData Processes\\nThis chapter has explored the landscape of data storage solutions and how they pro‐\\nvide versatile, efficient ways to organize and access data. However, storing data is only\\none piece of the puzzle. To truly leverage the potential of data, we must also under‐\\nstand how to effectively manage it. So, we now transition from storage solutions to\\nthe intricacies of data processes, where we will discover the strategies and techniques\\nthat help us manipulate, govern, and capitalize on our data assets.\\nMaster Data Management\\nMaster data management (MDM) is a set of technology, tools, and processes that ena‐\\nbles users to create and maintain consistent and accurate lists of master data (such as\\ncustomer, product, and supplier lists). It involves creating a single master record for\\neach person, place, or thing in a business from across internal and external data sour‐\\nces and applications. Y ou can then use these master records to build more accurate\\nreports, dashboards, queries, and machine learning models.\\nMost MDM products have very sophisticated tools to clean, transform, merge, and\\nvalidate the data to enforce data standards. They also allow you to build hierarchies\\n(for example, Company → Department → Employee), which you can use to improve\\nreporting and dashboards. The first step is that you copy the data you want to master\\ninto the MDM product. The product then cleans and standardizes the data and cre‐\\nates a master record for each entity (say, a customer) that you are trying to master. It\\nremoves most duplicates automatically, but some of the work will have to be done\\nmanually, with someone reviewing the records that might be duplicates. The master\\nrecords are then copied back into the data lake or DW . This master record is also\\nknown as a “golden source” or “best version of the truth. ”\\nIf you are creating a star schema (see Chapter 8), the mastered records would be your\\ndimension tables, which would then be joined to your fact tables (which are not\\nmastered).\\nData Processes | 81', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='920bb061-3d33-4835-b200-44dd7a406949', embedding=None, metadata={'page_label': '81', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Use case\\nLet’s go back to ShoesForLess, the retail chain where each store has its own customer\\ndatabase. The company collects the customer data from all of those databases into a\\ndata warehouse. Because some customers have purchased items from more than one\\nretail location, the DW will include duplicates of some customer records.\\nIf the customer’s name is spelled exactly the same in both records, you can easily filter\\nout duplicates. But if the name is misspelled or entered differently in one record (for\\nexample, if it contains a middle initial, a hyphen, or a suffix like Junior or Senior in\\none record but not the other), the filter will not recognize the names as duplicates.\\n(These are called near-duplicates.)\\nIf you don’t implement MDM, the first time the end user receives a report from the\\nDW , they will see the same customers showing up multiple times and will likely ques‐\\ntion the report’s accuracy. Y ou will have already lost their trust—and once you do, it is\\nvery difficult to win it back. MDM is the solution!\\nData Virtualization and Data Federation\\nData virtualization goes by several names; you might see it referred to as a logical data\\nwarehouse, virtual data warehouse, or decentralized data warehouse. It is a technology\\nthat allows you to access and combine data from various sources and formats by cre‐\\nating a logical view of the data without replicating or moving the actual data. It pro‐\\nvides a single access point for all data, enabling real-time data integration and\\nreducing the time and costs associated with traditional data integration approaches\\nsuch as ETL or ELT (see Chapter 9).\\nData federation and data virtualization are very similar terms that are often used\\ninterchangeably, and you are unlikely to get into trouble if you mix them up. How‐\\never, there are subtle differences between the two. Data federation involves an\\numbrella organization consisting of smaller, fully or partially autonomous subset\\norganizations that control all or part of their operations. (The US system of federal\\nand state governments is a good example of a federation.) Like data virtualization,\\ndata federation creates a united view of data from multiple data sources. Data virtuali‐\\nzation, however, is a broader concept that encompasses data federation as well as data\\ntransformation, caching, security, and governance.\\nThe better data virtualization tools provide features that help with performance, such\\nas query optimization, query pushdown, and caching. Y ou may see tools with these\\nfeatures called data virtualization and tools without them called data federation.\\nData virtualization can replace a data warehouse (see Chapter 4) or ETL/data move‐\\nment. Let’s look at how.\\n82 | Chapter 6: Data Storage Solutions and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='249d443c-56bc-46fb-a4e6-f073a28c77bc', embedding=None, metadata={'page_label': '82', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Virtualization as a replacement for the data warehouse\\nIn some cases, especially when there are many data sources or they are changing con‐\\nstantly, data virtualization can be an alternative to a data warehouse. When you use\\nthis approach, there is no copying and storing of the data in a central location.\\nFigure 6-4 shows how most data virtualization products work. The grid (step 1) rep‐\\nresents an end user’s query results: a list of addresses. When the end user scrolls\\nthrough the grid to see more data, the grid requests items that are currently not\\nloaded from the data virtualization engine (step 2). If the engine has a cache of data\\navailable, it returns the cache immediately, and the grid simply updates to display the\\ndata. If you request data from outside the cache, the data virtualization engine passes\\nthe request to the remote data source (step 3), where it is processed. The newly\\nfetched data is returned to the engine (step 4) and then passed to the grid. In both\\nsituations, the grid always remains completely responsive.\\nFigure 6-4. Data virtualization\\nThe main advantage of a data virtualization solution is that it optimizes for speed to\\nmarket. It can be built in a fraction of the time it takes to build a data warehouse,\\nbecause you don’t need to design and build a DW and ETL to copy the data into it\\nand you don’t need to spend as much time testing it. Copying the data (as with a DW)\\nmeans higher storage and governance costs, more ETL flows to build and maintain,\\nand more data inconsistencies, so using virtualization can save you a lot of money.\\nData Processes | 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a4e040fb-a0cd-4200-8e77-5f319d8530ad', embedding=None, metadata={'page_label': '83', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 The difference between data federation and federated queries is that data federation is more about the system’s\\narchitecture, while federated queries are the mechanism used to extract information from this system. In\\npractice, if you have a data federation system in place, you are using federated queries to retrieve your data.\\n2 In data virtualization, the term push-down query refers to the capability to push certain parts of a query, such\\nas filtering or aggregation, to the data sources themselves. This allows the data virtualization layer to offload\\nprocessing tasks and retrieve only the relevant results. The benefits include enhancing performance and mini‐\\nmizing data transfer; the trade-off is that it uses compute resources on the server where the data source\\nresides.\\n3 Database indexes are a type of data structure that improves the speed of data retrieval operations on a database\\ntable. They work much like book indexes, providing users a quick way to access the data they’re seeking.\\n4 Database statistics are a set of information that a database management system (DBMS) maintains about the\\ndata stored in a database. These statistics help the DBMS optimize query performance by informing how it\\ndetermines the most efficient way to execute a given query.\\nData virtualization supports federated queries, in which you can issue a single query\\nthat retrieves data from multiple sources and formats and combines the results into a\\nsingle result set.1\\nHowever, there are some major drawbacks to data virtualization and federated quer‐\\nies. If you’re considering a data virtualization solution to replace your DW , I recom‐\\nmend you ask the following questions:\\n• What level of performance do I need? Is this a solution I could use for a dash‐\\nboard that needs sub-second response times, or is it more for operational\\nreporting?\\n• How much will the data virtualization solution affect the performance of the\\nsource system? Will my query run slowly if another end user runs a query against\\nthe same data source that consumes all the resources of the server? Does it push\\ndown the query?2\\n• Will I need to install something on every server that contains a data source I want\\nto use?\\n• Does the solution use the index of each technology on the data source, or does it\\ncreate its own indexes? 3 Does it use the database statistics from each data source\\nfor queries?4\\n• Can I restrict which users can access each data source for security purposes?\\n• How does the solution handle near-duplicates?\\n• Where and how will the data be cleaned?\\n• Will reports break if the source system is changed?\\n• What is the cost of the virtualization software, and how complex is it to use?\\n84 | Chapter 6: Data Storage Solutions and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b33bf5c9-0f5c-4ae4-8a92-e07300383e6d', embedding=None, metadata={'page_label': '84', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In addition to reasons uncovered with these questions, there are other valid rationales\\nfor using a physical data warehouse, as discussed in Chapter 4, such as a need to run\\naccurate historical reports, rename tables, and keep historical data.\\nVirtualization as a replacement for ETL or data movement\\nIf you’re building a data warehouse, should you move all the source data into the data\\nwarehouse, or is it better to keep the source data where it is and create a virtualization\\nlayer on top of some of it?\\nThe most common reason to move data is if you plan to aggregate or transform it\\nonce and then query the results many times. Another is if you will frequently be join‐\\ning datasets from multiple sources and need super-fast performance. In these scenar‐\\nios, I generally recommend data warehouse solutions.\\nHowever, at times, you might have ad hoc queries that don’t need to be super fast.\\nAnd you could certainly have a DW that uses data movement for some tables and\\ndata virtualization for others. Table 6-3 compares data movement and virtualization.\\nTable 6-3. Comparison of data movement and data virtualization\\nDevelopment and\\noperations costs\\nTime to\\nsolution\\nSecurity Data freshness\\nand quality\\nCompliance\\nData\\nmovement\\nCostly to build and\\nmaintain ELT jobs;\\nduplicate data\\nstorage costs\\nTakes time to\\nbuild and run\\njobs\\nCreating copies of\\nthe data makes it\\nmore vulnerable\\nto hackers\\nETL pipelines\\nmake the data\\n“stale”; ETL can\\nintroduce data\\nerrors\\nMoving data in and\\nout of compliance\\nboundaries can cause\\ndata governance\\nissues\\nData\\nvirtualization\\nReduces ongoing\\nmaintenance and\\nchange management;\\nminimizes storage\\ncosts\\nAllows for rapid\\niterations and\\nprototyping\\nData is kept in one\\nsecure place,\\nminimizing the\\nattack surface\\nData being\\nqueried is always\\n“fresh” and\\naccurate from the\\nsource\\nHaving few copies of\\ndata and less\\nmovement helps\\nmeet compliance\\nrequirements\\nData virtualization offers several benefits. First, it provides a comprehensive data lin‐\\neage, ensuring you can clearly understand the data’s path from its source to the pre‐\\nsentation layer. Second, including additional data sources becomes seamless,\\neliminating the need to modify transformation packages or staging tables. Last, data\\nvirtualization software presents all data through a unified SQL interface. This allows\\nusers easy access regardless of the source, which could be flat files, spreadsheets,\\nmainframes, relational databases, or other types of data repositories.\\nWhile Table 6-3 shows the benefits of data virtualization over data movement, those\\nbenefits may not be enough to overcome the sacrifice in performance and other\\ndrawbacks of not using a relational data warehouse, listed in Chapter 4. Also, keep in\\nmind that the virtualization tool you choose may not support all of your data sources.\\nData Processes | 85', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21f25f22-7beb-4774-ab8a-cb72af05ee52', embedding=None, metadata={'page_label': '85', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Here are some reasons you might choose data virtualization:\\nThere are regulatory constraints on moving data\\nAs you learned in “Compliance, governance, and security” in Chapter 5, Chinese\\ndata sovereignty regulations hold that data located in China has to stay in China\\nand can only be queried by a person currently located there. But say you’re in the\\nUnited States and try to query customer data in China from your application.\\nThe application can retrieve metadata about the customer data in China, but it\\ncan’t ingest that data. Y ou could use virtualization software to keep the data\\nwithin the country by letting only a person inside the country query it. Anyone\\noutside China trying to query the data would either get an error message or just\\nnot see any results.\\nAn end user outside the company wants to access customer data\\nInstead of you sending the user a copy of the data, they can use virtualization\\nsoftware to query it. This lets you control who accesses what data and know how\\nmuch they’ve accessed.\\nThere is reference data in a database outside the DW that changes frequently\\nInstead of copying that data into the DW again and again, you could use virtuali‐\\nzation to query it when you’re joining the external database’s data with the DW\\ndata.\\nYou want to query data from different data stores and join it together\\nVirtualization software usually supports many data sources (say, a data lake, rela‐\\ntional database, and NoSQL database) and can handle this.\\nYou want your users to do self-service analytics via a virtual sandbox\\nA virtual sandbox is a secure and isolated environment where users can explore,\\nanalyze, and manipulate data without affecting the underlying production data\\nor systems. For example, say you’re using a virtual sandbox to analyze both cus‐\\ntomer data and third-party data. Instead of copying that data into the virtual\\nsandbox, you could use virtualization to query it within the virtual sandbox. This\\nminimizes the number of copies made of the data, and you avoid having to create\\nETL.\\nYou want to build a solution quickly to move source data\\nInstead of using a traditional ETL tool to copy the source data to a central loca‐\\ntion and then creating reports from that, you can just create reports by using the\\nvirtualization software. It will run queries against the source data and do the cen‐\\ntralization automatically. This means you can build a solution a lot faster, since\\nyou don’t have to use an ETL tool (which can be very time-consuming).\\n86 | Chapter 6: Data Storage Solutions and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='68e15e7f-8790-4b55-902a-c77177eb46b6', embedding=None, metadata={'page_label': '86', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5 Sometimes, however, a data marketplace is internal to a company and is simply used to help its users find and\\nexchange data.\\nData Catalogs\\nA data catalog is a centralized repository, typically housed in the cloud, that stores\\nand organizes metadata about all of an organization’s data sources, tables, schemas,\\ncolumns, and other data assets such as reports and dashboards, ETL processes, and\\nSQL scripts. It functions as a single source of truth for users to discover, understand,\\nand manage data that is located in application databases, data lakes, relational data\\nwarehouses, operational data stores, data marts, and any other data storage form.\\nA data catalog typically includes the following information about each data asset:\\n• Information about the source, such as location, type, and connection details\\n• If within an RDW , table and schema information (such as structure, relation‐\\nships, and organization) and column information (such as data types, formats,\\ndescriptions, and relationships between columns)\\n• If within an object store such as a data lake, file properties (storage, folder name,\\nfilename)\\n• Data lineage (how the data arrived from its origin), including information on any\\ntransformation, aggregation, and integration the data has undergone\\n• Data governance and compliance details, such as data quality, ownership, and\\npolicies\\n• Search and discovery tools for users to search, filter, and find relevant data\\nA data catalog helps you manage data more effectively by providing visibility into the\\ndata landscape. This, in turn, facilitates better collaboration between teams and more\\ninformed decision making. It also helps ensure data quality, security, and compliance\\nby making it easier to track data lineage, enforce data policies, and maintain a clear\\nunderstanding of data assets.\\nThere are many products on the market for creating data catalogs, some of which\\nhave been around for nearly a decade. The most popular products are Informatica’s\\nEnterprise Data Catalog, Collibra Data Catalog, and Microsoft Purview. These prod‐\\nucts have additional functionalities such as data governance and compliance, business\\nglossaries, and data quality metrics.\\nData Marketplaces\\nA data marketplace, sometimes called a data exchange, is an online platform where\\ndata providers and data consumers come together to buy, sell, and exchange datasets.5\\nData Processes | 87', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a2480ee-8d9b-4584-bb67-f4ee905cdf2f', embedding=None, metadata={'page_label': '87', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='These marketplaces are designed to help data consumers (like businesses, researchers,\\nand developers) discover, evaluate, and purchase data for purposes such as analysis,\\nmachine learning, business intelligence, and decision making. Providers usually\\ninclude organizations, governments, and individuals who have collected or generated\\nvaluable datasets.\\nA data marketplace typically includes a data catalog. Many marketplaces assess and\\nimprove the quality of datasets to ensure they are accurate, complete, consistent, and\\neasy to use. Some provide tools and services for cleaning, transforming, and enrich‐\\ning data, which can save users time and effort in preparing it for analysis. (This is par‐\\nticularly valuable when you’re integrating data from multiple sources or dealing with\\nincomplete or inconsistent data.) Once a consumer makes a purchase, the market‐\\nplace may offer tools to help them access the data and integrate it into their work‐\\nflows, applications, or systems. Some even allow users to customize datasets by\\ncombining data from multiple sources or filtering it based on specific criteria so they\\ncan access exactly the data they need.\\nData marketplaces should have robust security measures in place to protect user data,\\nensure compliance with data protection regulations, and maintain the confidentiality\\nof sensitive information. Some offer ways to easily obfuscate private or sensitive data.\\nData marketplaces’ pricing structures and licensing agreements clearly define the\\nterms of use and ensure legal compliance for both data providers and consumers.\\nMany data marketplaces also offer built-in analytics and data visualization tools that\\nallow you to analyze and visualize (or simply preview) data directly within the plat‐\\nform. Other common features and tools include:\\n• Ratings and reviews of datasets\\n• Recommendations for other datasets you might like\\n• Shopper profiles for easy reordering, favorites lists, and so on\\n• Collaboration and community\\n• Training and support\\nFrom the providers’ perspective, data marketplaces offer a way to monetize data\\nassets, generating new revenue streams. This can be particularly beneficial for organi‐\\nzations that collect large amounts of data but don’t have the resources or expertise to\\nanalyze and monetize it themselves.\\nData marketplaces have increased in popularity as the demand for data-driven\\ninsights grows and more diverse datasets continue to become available. There are not\\nnearly as many products available for creating data marketplaces as there are for data\\ncatalogs, since marketplaces just started appearing in the early 2020s. The most popu‐\\nlar are Snowflake Marketplace, Datarade, and Informatica’s Cloud Data Marketplace.\\nBoth providers and consumers can benefit from a more efficient and transparent\\n88 | Chapter 6: Data Storage Solutions and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b70dd6b9-40cd-44d9-8399-9f7fe6666cd6', embedding=None, metadata={'page_label': '88', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='exchange of data, which can foster innovation of new data-driven products and\\nservices.\\nSummary\\nThis chapter explored various approaches to data stores, all of which play pivotal\\nroles in today’s data-driven business environment.\\nI discussed data storage, including data marts, operational data stores, and data hubs,\\nand then shifted the focus to processes. Master data management (MDM) is a crucial\\napproach that focuses on managing core business entities to ensure data accuracy,\\nuniformity, and consistency across multiple systems and processes. I then moved on\\nto discuss data virtualization, data catalogs, and data marketplaces.\\nBy understanding these different strategies and their functionalities, you can better\\nalign your data architecture to your organization’s needs and drive more significant\\nbusiness value from your data.\\nSummary | 89', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='639921d3-5dd3-4752-a248-fad17a3f6a32', embedding=None, metadata={'page_label': '89', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='73338a87-0f13-4753-a808-01309ef236d8', embedding=None, metadata={'page_label': '90', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 7\\nApproaches to Design\\nThis chapter explores design approaches to handling and organizing data and how\\nthese methods help build powerful, adaptable, and reliable systems for data manage‐\\nment. In simple terms, we’ll learn about the strategies that help us decide how data\\nshould be stored, processed, and accessed to best enhance the speed and dependabil‐\\nity of our data systems.\\nTo clarify, let’s distinguish between data design and data modeling, which is the sub‐\\nject of Chapter 8. Think of data design as like building a city. It’s about deciding where\\nthe buildings go, which roads connect different parts of the city, and how traffic\\nflows. On the other hand, data modeling is more like designing individual buildings—\\nit’s about arranging rooms, deciding how they connect, and defining the purpose of\\neach room.\\nIn this chapter, we’ll look at different types of data designs. We’ll compare and con‐\\ntrast methods like OLTP and OLAP , which are essentially different ways of processing\\nand analyzing data. We’ll also explore concepts like SMP and MPP , which are strate‐\\ngies to process data more efficiently. Then, we’ll learn about Lambda and Kappa\\narchitectures, which are blueprints for handling large amounts of data in real time.\\nLastly, we’ll talk about an approach called polyglot persistence, which allows us to use\\ndifferent types of data storage technologies within the same application.\\nMy aim here isn’t to push one approach as the best solution for every situation but to\\nhelp you understand the strengths and weaknesses of each. This way, you can choose\\nor combine the right methods based on your specific needs. By the end of this chap‐\\nter, you’ll have a stronger grasp of how to create efficient data systems that can adapt\\nto changing needs and technological advancements.\\n91', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c75af44-18e6-4a21-9219-89f9f07ca02e', embedding=None, metadata={'page_label': '91', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Online Transaction Processing Versus\\nOnline Analytical Processing\\nOnline transaction processing (OLTP) is a type of informational system or application\\nthat processes online create, read, update, and delete (CRUD) transactions (see Chap‐\\nter 2) in a real-time environment. OLTP systems are designed to support high levels\\nof concurrency, which means they can handle a large number of transactions at the\\nsame time. They typically use a relational model (see Chapter 8) and are optimized\\nfor low latency, which means they can process transactions very quickly. Examples\\ninclude point-of-sale applications, ecommerce websites, and online banking\\nsolutions.\\nIn an OLTP system, transactions are typically processed quickly and in a very specific\\nway. For example, a customer’s purchase at a store would be considered a transaction.\\nThe transaction would involve the customer’s account being debited for the purchase\\namount, the store’s inventory count being reduced, and the store’s financial records\\nbeing updated to reflect the sale. OLTP systems use a variety of DBMSs to store and\\nmanage the data, such as Microsoft SQL Server and Oracle.\\nOLTP systems are often contrasted with online analytical processing (OLAP) systems,\\nwhich are used for data analysis and reporting to support business intelligence and\\ndecision making. OLAP systems are optimized for fast query performance, allowing\\nend users to easily and quickly analyze data from multiple perspectives by slicing and\\ndicing the data in reports and dashboards much faster than with an OLTP system.\\nThink of an OLAP system as “write-once, read-many” (as opposed to CRUD). Often,\\nmultiple OLTP databases are used as the sources that feed into a data warehouse,\\nwhich then feeds into an OLAP database, as shown in Figure 7-1. (However, some‐\\ntimes you can also build an OLAP database directly from OLTP sources.)\\nFigure 7-1. OLAP database architecture\\nAn OLAP database is typically composed of one or more OLAP cubes. An OLAP\\ncube is where data is pre-aggregated by the cube, meaning that it has already been\\nsummarized and grouped by certain dimensions; end users can quickly access it from\\nmultiple dimensions and levels of detail without having to wait for long-running\\n92 | Chapter 7: Approaches to Design', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fe3b5f44-a9fd-4dd3-9232-cb5a1bae2e11', embedding=None, metadata={'page_label': '92', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='queries to complete. Creating an OLAP cube generally involves using a multidimen‐\\nsional model, which uses a star or snowflake schema to represent the data. ( Chapter 8\\nwill discuss these schemas in more detail.)\\nFor example, a retail corporation might use an OLAP database with OLAP cubes to\\nquickly analyze sales data. This pre-aggregated data is organized by time, allowing for\\nthe analysis of sales trends, whether yearly or daily or anywhere in between. The data\\nis also sorted by location, which facilitates geographic comparisons to look at, for\\nexample, product sales in different cities. Furthermore, the data is categorized by\\nproduct, making it easier to track the performance of individual items. If a regional\\nmanager needs to quickly review how a particular product category performed in\\ntheir region during the last holiday season, the OLAP cube has already prepared the\\ndata. Instead of waiting for a lengthy query to sort through each individual sale, the\\nmanager finds the relevant data—sorted by product, region, and time—is readily\\navailable, accelerating the decision-making process.\\nY ou can use another, more recent model, called a tabular data model, that gives simi‐\\nlarly fast query performance as an OLAP cube. Instead of a multidimensional model,\\na tabular data model uses a tabular structure, similar to a relational database table, to\\nrepresent the data. This makes it more flexible and simpler than a multidimensional\\nmodel. This book refers to the multidimensional model as an “OLAP cube” and the\\ntabular model as a “tabular cube, ” but please note that the term cube is used inter‐\\nchangeably in the industry to denote either of these models.\\nTable 7-1 compares the OLTP and OLAP design styles.\\nTable 7-1. Comparison of OLTP and OLAP systems\\nOLTP OLAP/Tabular\\nProcessing type Transactional Analytical\\nData nature Operational data Consolidation data\\nOrientation Application oriented Subject oriented\\nPurpose Works on ongoing business tasks Helps in decision making\\nTransaction frequency Frequent transactions Occasional transactions\\nOperation types Short online transactions: Insert, Update, Delete Lots of read scans\\nData design Normalized database (3NF) Denormalized database\\nCommon usage Used in retail sales and other financial\\ntransactions systems\\nOften used in data mining, sales, and\\nmarketing\\nResponse time Response time is instant Response time varies from seconds to hours\\nQuery complexity Simple and instant queries Complex queries\\nUsage pattern Repetitive usage Ad hoc usage\\nTransaction nature Short, simple transactions Complex queries\\nDatabase size Gigabyte database size Terabyte database size\\nOnline Transaction Processing Versus Online Analytical Processing | 93', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3cd5e743-d936-4e14-b6c7-89ce426cab60', embedding=None, metadata={'page_label': '93', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='OLAP databases and data warehouses (see Chapter 4) are related but distinct con‐\\ncepts and are often used in conjunction. Through a multidimensional or tabular\\nmodel, OLAP databases provide a way to analyze the data stored in the DW in a way\\nthat is more flexible and interactive than executing traditional SQL-based queries on\\na DW that contains large amounts of data. Multidimensional models and tabular\\nmodels are often considered semantic layers or models, which means that they pro‐\\nvide a level of abstraction over the schemas in the DW .\\nOperational and Analytical Data\\nOperational data is real-time data used to manage day-to-day operations and pro‐\\ncesses. It is captured, stored, and processed by OLTP systems. Y ou can use it to take a\\n“snapshot” of the current state of the business to ensure that operations are running\\nsmoothly and efficiently. Operational data tends to be high volume and helps in mak‐\\ning decisions quickly.\\nAnalytical data comes from collecting and transforming operational data. It is a his‐\\ntorical view of the data maintained and used by OLAP/Tabular systems and DWs.\\nWith data analytics tools, you can perform analysis to understand trends, patterns,\\nand relationships over time. Analytical data is optimized for creating reports and vis‐\\nualizations and for training machine learning models. It usually provides a view of\\ndata over a longer period than you get with operational data, often has a lower vol‐\\nume, and is generally consolidated and aggregated. Data is usually ingested in batches\\nand requires more processing time than operational data.\\nIn summary, operational data is used to monitor and control business processes in\\nreal time, while analytical data is used to gain insights and inform decision making\\nover a longer period. Both types of data are essential for effective business manage‐\\nment, and they complement each other to provide a complete view of an organiza‐\\ntion’s operations and performance.\\nThink of OLTP as the technology used to implement operational data, and of OLAP/\\nTabular and DWs as the technology used to implement analytical data.\\nSymmetric Multiprocessing and Massively\\nParallel Processing\\nSome of the first relational databases used a symmetric multiprocessing (SMP) design,\\nwhere computer processing is done by multiple processors that share disk and mem‐\\nory, all in one server—think SQL Server and Oracle. (This is pictured on the left side\\nof Figure 7-2 .) To get more processing power for these systems, you “scale up” by\\nincreasing the processors and memory in the server. This works well for OLTP data‐\\nbases but not so well for the write-once, read-many environment of a DW .\\n94 | Chapter 7: Approaches to Design', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='86d6538c-c5bf-41c6-9c71-431d7139f84b', embedding=None, metadata={'page_label': '94', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As data warehouses grew in popularity in the 1990s and started to ingest huge\\namounts of data, performance became a big problem. To help with that, along came a\\nnew kind of database design. In a massively parallel processing (MPP) design, the data‐\\nbase has multiple servers, each with multiple processors, and (unlike in SMP) each\\nprocessor has its own memory and its own disk. This allows you to “scale out” (rather\\nthan up) by adding more servers.\\nMPP servers distribute a portion of the data from the database to the disk on each\\nserver (whereas SMP databases keep all the data on one disk). Queries are then sent\\nto a control node (also called a name node) that splits each query into multiple subqu‐\\neries that are sent to each server (called a compute node or worker node), as shown on\\nthe right in Figure 7-2 . There, the subquery is executed and the results from each\\ncompute node are sent back to the control node, mashed together, and sent back to\\nthe user. This is how solutions such as Teradata and Netezza work.\\nFigure 7-2. SMP and MPP database designs\\nBy way of analogy, imagine your friend Fiona has a deck of 52 cards and is looking\\nfor the ace of hearts. It takes Fiona about 15 seconds, on average, to find the card. Y ou\\ncan “scale up” by replacing Fiona with another friend, Max, who is faster. Using Max\\nbrings the average time down to 10 seconds—a limited improvement. This is how\\nSMP databases work.\\nNow imagine you “scale out” instead of up, by replacing Fiona with 26 people, each of\\nwhom has only two cards. Now the average time to find the card is just 1 second.\\nThat’s how MPP databases work.\\nSMP and MPP databases started as on-prem solutions, and these are still prevalent\\ntoday, but there are now many equivalent solutions in the cloud.\\nSymmetric Multiprocessing and Massively Parallel Processing | 95', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4c3db746-10ae-4212-b65a-42c828d21c1a', embedding=None, metadata={'page_label': '95', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Lambda Architecture\\nLambda architecture is a data-processing architecture designed to handle massive\\nquantities of data by using both batch and real-time stream processing methods. The\\nidea is to get comprehensive and accurate views of the batch data and to balance\\nlatency, throughput, scaling, and fault tolerance by using batch processing, while\\nsimultaneously using real-time stream processing to provide views of online data\\n(such as IoT devices, Twitter feeds, or computer log files). Y ou can join the two view\\noutputs before the presentation/serving layer.\\nLambda architecture bridges the gap between the historical “single source of truth”\\nand the highly sought after “I want it now” real-time solution by combining tradi‐\\ntional batch processing systems with stream consumption tools to meet both needs.\\nThe Lambda architecture has three key principles:\\nDual data model\\nThe Lambda architecture uses one model for batch processing (batch layer) and\\nanother model for real-time processing (stream layer). This allows the system to\\nhandle both batch and real-time data and to perform both types of processing in\\nscalable and fault-tolerant ways.\\nSingle unified view\\nThe Lambda architecture uses a single unified view (called the presentation layer)\\nto present the results of both batch and real-time processing to end users. This\\nallows the user to see a complete and up-to-date view of the data, even though it’s\\nbeing processed by two different systems.\\nDecoupled processing layers\\nThe Lambda architecture decouples the batch and real-time processing layers so\\nthat they can be scaled independently and developed and maintained separately,\\nallowing for flexibility and ease of development.\\nFigure 7-3 depicts a high-level overview of the Lambda architecture.\\nOn the left of Figure 7-3 is the data consumption layer. This is where you import the\\ndata from all source systems. Some sources may be streaming the data, while others\\nonly provide data daily or hourly.\\nIn the top middle, you see the stream layer, also called the speed layer. It provides for\\nincremental updating, making it the more complex of the two middle layers. It trades\\naccuracy for low latency, looking at only recent data. The data in here may be only\\nseconds behind, but the trade-off is that it might not be clean. Data in this layer is\\nusually stored in a data lake.\\n96 | Chapter 7: Approaches to Design', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90656021-f98a-45c2-82a4-805fd9787fe9', embedding=None, metadata={'page_label': '96', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 7-3. Overview of Lambda architecture\\nBeneath that is the batch layer, which looks at all the data at once and eventually cor‐\\nrects the data that comes into the stream layer. It is the single source of truth, the trus‐\\nted layer. Here there’s usually lots of ETL, and data is stored in a traditional data\\nwarehouse or data lake. This layer is built using a predefined schedule, usually daily\\nor hourly, and including importing the data currently stored in the stream layer.\\nAt the right of Figure 7-3 is the presentation layer, also called the serving layer. Think\\nof it as a mediator; when it accepts queries, it decides when to use the batch layer and\\nwhen to use the speed layer. It generally defaults to the batch layer, since that has the\\ntrusted data, but if you ask it for up-to-the-second data (perhaps by setting alerts for\\ncertain log messages that indicate a server is down), it will pull from the stream layer.\\nThis layer has to balance retrieving the data you can trust with retrieving the data you\\nwant right now.\\nThe Lambda architecture is an excellent choice for building distributed systems that\\nneed to handle both batch and real-time data, like recommendation engines and\\nfraud detection systems. However, that doesn’t mean it’s the best choice for every sit‐\\nuation. Some potential drawbacks of the Lambda architecture include:\\nComplexity\\nThe Lambda architecture includes a dual data model and a single unified view.\\nThat can be more complex to implement and maintain than other architectures.\\nLimited real-time processing\\nThe Lambda architecture is designed for both batch and real-time processing,\\nbut it may not be as efficient at handling high volumes of real-time data as the\\nKappa architecture (discussed in the next section), which is specifically designed\\nfor real-time processing.\\nLambda Architecture | 97', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='525d86bb-0401-40ab-a482-0c0aa94a37f3', embedding=None, metadata={'page_label': '97', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Limited support for stateful processing\\nThe Lambda architecture is designed for stateless processing and may not be well\\nsuited for applications that require maintaining state across multiple events. For\\nexample, consider a retail store with a recommendation system that suggests\\nproducts based on customers’ browsing and purchasing patterns. If this system\\nused a Lambda architecture, which processes each event separately without\\nmaintaining state, it could miss the customer’s shopping journey and intent. If\\nthe customer browses for shoes, then socks, and then shoe polish, a stateless sys‐\\ntem might not correctly recommend related items like shoelaces or shoe storage,\\nbecause it doesn’t consider the sequence of events. It might also recommend\\nitems that are already in the customer’s cart.\\nOverall, you should consider the Lambda architecture if you need to build a dis‐\\ntributed system that can handle both batch and real-time data but needs to provide a\\nsingle unified view of the data. If you need stateful processing or to handle high vol‐\\numes of real-time data, you may want to consider the Kappa architecture.\\nKappa Architecture\\nAs opposed to the Lambda architecture, which is designed to handle both real-time\\nand batch data, Kappa is designed to handle just real-time data. And like the Lambda\\narchitecture, Kappa architecture is also designed to handle high levels of concurrency\\nand high volumes of data. Figure 7-4  provides a high-level overview of the Kappa\\narchitecture.\\nThe three key principles of the Kappa architecture are:\\nReal-time processing\\nThe Kappa architecture is designed for real-time processing, which means that\\nevents are processed as soon as they are received rather than being batch pro‐\\ncessed later. This decreases latency and enables the system to respond quickly to\\nchanging conditions.\\nSingle event stream\\nThe Kappa architecture uses a single event stream to store all data that flows\\nthrough the system. This allows for easy scalability and fault tolerance, since the\\ndata can be distributed easily across multiple nodes.\\nStateless processing\\nIn the Kappa architecture, all processing is stateless. This means that each event\\nis processed independently, without relying on the state of previous events. This\\nmakes it easier to scale the system, because there is no need to maintain state\\nacross multiple nodes.\\n98 | Chapter 7: Approaches to Design', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4615dc2e-e12f-450e-b71f-2e1e4d298a69', embedding=None, metadata={'page_label': '98', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 7-4. Overview of Kappa architecture\\nThe layers in the Kappa architecture are exactly the same as in the Lamba architec‐\\nture, except that the Kappa architecture does not have a batch layer.\\nSome potential drawbacks of the Kappa architecture include:\\nComplexity\\nThe Kappa architecture involves a single event stream and stateless processing,\\nwhich can be more complex to implement and maintain than other architectures.\\nLimited batch processing\\nThe Kappa architecture is designed for real-time processing and does not easily\\nsupport batch processing of historical data. If you need to perform batch process‐\\ning, you may want to consider the Lambda architecture instead.\\nLimited support for ad-hoc queries\\nBecause the Kappa architecture is designed for real-time processing, it may not\\nbe well suited for ad hoc queries that need to process large amounts of historical\\ndata.\\nOverall, the Kappa architecture is an excellent choice for building distributed systems\\nthat need to handle large amounts of data in real time and that need to be scalable,\\nfault tolerant, and have low latency. Examples include streaming platforms and finan‐\\ncial trading systems. However, if you need to perform batch processing or support ad\\nhoc queries, then the Lambda architecture may be a better choice.\\nNote that the Lambda and Kappa architectures are high-level design patterns that can\\nbe implemented within any of the data architectures described in Part III of this book.\\nIf you use one of those architectures to build a solution that supports both batch and\\nreal-time data, then that architecture supports the Lambda architecture; if you use\\none of those architectures to build a solution that supports only real-time data, that\\narchitecture supports the Kappa architecture.\\nKappa Architecture | 99', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e7e11b11-2ef3-4594-a04d-841703006014', embedding=None, metadata={'page_label': '99', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Polyglot Persistence and Polyglot Data Stores\\nPolyglot persistence is a fancy term that means using multiple data storage technolo‐\\ngies to store different types of data within a single application or system , based upon\\nhow the data will be used. Different kinds of data are best kept in different data stores.\\nIn short, polyglot persistence means picking the right tool for the right use case. It’s\\nthe same idea as the one behind polyglot programming , in which applications are\\nwritten in a mix of languages to take advantage of different languages’ strengths in\\ntackling different problems.\\nBy contrast, a polyglot data store means using multiple data stores across an organiza‐\\ntion or enterprise. Each data store is optimized for a specific type of data or use case.\\nThis approach allows organizations to use different data stores for different projects\\nor business units, rather than a one-size-fits-all approach for the entire organization.\\nFor example, say you’re building an ecommerce platform that will deal with many\\ntypes of data (shopping carts, inventory, completed orders, and so forth). Instead of\\ntrying to store all the different types of data in one database, which would require a\\nlot of conversion, you could take a polyglot persistence approach and store each kind\\nof data in the database best suited for it. So, an ecommerce platform might look like\\nthe diagram in Figure 7-5.\\nFigure 7-5. An ecommerce platform with a polyglot persistence design\\nThis results in the best tool being used for each type of data. In Figure 7-5, you can\\nsee that the database uses a key-value store for shopping cart and session data (giving\\nvery fast retrieval), a document store for completed orders (making storing and\\nretrieving order data fast and easy), an RDBMS for inventory and item prices (since\\nthose are best stored in a relational database due to the structured nature of the data),\\nand a graph store for customer social graphs (since it’s very difficult to store graph\\ndata in a non-graph store).\\n100 | Chapter 7: Approaches to Design', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90e4a45b-d3b4-4ba5-b868-180444a7d2e6', embedding=None, metadata={'page_label': '100', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This will come at a cost in complexity, since each data storage solution means learn‐\\ning a new technology. But the benefits will be worth it. For instance, if you try to use\\nrelational databases for non-relational data, the design can significantly slow applica‐\\ntion development and performance; using the appropriate storage type pays off in\\nspeed.\\nSummary\\nThis chapter explored the architectural concepts and design philosophies that form\\nthe basis of effective data systems.\\nFirst, you learned about the two primary types of data processing systems: online\\ntransaction processing (OLTP) and online analytical processing (OLAP). OLTP sys‐\\ntems are designed for fast, reliable, short transactions, typically in the operational\\ndatabases that power daily business operations. In contrast, OLAP systems support\\ncomplex analytical queries, aggregations, and computations used for strategic deci‐\\nsion making, typically in a data warehouse. Y ou then learned about the differences\\nbetween operational and analytical data.\\nY ou also learned the differences between symmetric multiprocessing (SMP) and mas‐\\nsively parallel processing (MPP) architectures. We then delved into two modern big\\ndata–processing architectures: Lambda and Kappa. Last, we explored the concepts of\\npolyglot persistence and polyglot data stores, which promote using the best-suited\\ndatabase technology for the specific needs and workload characteristics of the given\\ndata.\\nStarting with the next chapter, our focus will shift from data storage and processing to\\nthe principles and practices of data modeling: the crucial bridge between raw data\\nand meaningful insights. The approaches to data modeling, such as relational and\\ndimensional approaches and the common data model, serve as an underpinning\\nstructure that allows you to use and interpret data efficiently across diverse\\napplications.\\nAs we delve into these topics, you’ll see how data modeling lets you use the storage\\nand processing solutions we’ve studied efficiently, serving as a blueprint for trans‐\\nforming raw data into actionable insights.\\nSummary | 101', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e6fab4d1-dfb4-4be9-9970-6c7dc6e4f0cd', embedding=None, metadata={'page_label': '101', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85b2fab2-3363-4a23-b76b-146130f52829', embedding=None, metadata={'page_label': '102', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 8\\nApproaches to Data Modeling\\nData modeling is a high-level conceptual technique used to design a database. There\\nare several different types of data modeling, including relational modeling and\\ndimensional modeling. The process involves identifying the data that needs to be\\nstored, then creating a structured representation of that data and the relationships\\namong the data, organized into tables and columns. Think of the tables and columns\\nas the logical representation of the database, where the physical data stored in those\\ntables and columns can be in a relational database product or a data lake.\\nY ou can use data modeling with any type of database: relational, dimensional,\\nNoSQL, and so on. I recommend dedicating a lot of time to data modeling to ensure\\nthat your database is logical, efficient, and easy to use. This ensures maximum perfor‐\\nmance and makes it easier to retrieve and analyze data.\\nRelational Modeling\\nRelational modeling, developed by Edgar F . Codd in 1970 (as mentioned in Chap‐\\nter 2), is a detailed modeling technique used to design a database. It involves organiz‐\\ning the data into tables and defining the relationships between the tables. In relational\\ndatabases and relational data warehouses, each table consists of rows (also called\\nrecords or tuples) and columns (also called fields or attributes). Each row represents a\\nunique instance of the data, and each column represents a specific piece of informa‐\\ntion about the data.\\nKeys\\nIn relational modeling, you define relationships between tables, rows, and columns\\nusing primary keys and foreign keys. A primary key is a unique identifier for each\\nrecord in a table, ensuring that no two rows of data have the same key value. A foreign\\n103', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0cd131e3-6928-4602-8306-1e0fe165540d', embedding=None, metadata={'page_label': '103', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='key is a column in a table that refers to the primary key in another table. It’s used to\\nestablish a relationship between the two tables in order to ensure the integrity of the\\ndata. For example, you might have a unique key in a product table called ProductKey\\nthat refers to a foreign key in a sales facts table called ProductKey. A natural key is a\\nfield that already exists in a table and is unique to each row, such as a person’s Social\\nSecurity number or a product’s serial number. In relational modeling, a natural key is\\nusually used as the primary key.\\nEntity–Relationship Diagrams\\nUsually, you start relational modeling with an entity–relationship (ER) diagram: a\\nhigh-level visual structure of the database that represents the entities (data) and the\\nrelationships between them. It uses boxes for the entities and connecting lines for the\\nrelationships. Figure 8-1 shows an example of an ER diagram. Once that is complete,\\nyou can build a more detailed relational model that represents the actual tables and\\ncolumns of a database.\\nFigure 8-1. An entity–relationship diagram\\nNormalization Rules and Forms\\nNext, you apply normalization rules, which are a way of decomposing a complex data‐\\nbase into smaller, simpler tables. This minimizes redundancy and dependency,\\nimproves data integrity, and makes the database more efficient and easier to maintain\\nand manage. Y ou do this by applying progressing levels of normalization.\\n104 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b6883b9c-95eb-420c-83f4-97d512620319', embedding=None, metadata={'page_label': '104', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A table is in first normal form (1NF) if it satisfies all of the following conditions:\\n• The table has a primary key. For example, in a Students table, StudentID could\\nbe the primary key.\\n• Each attribute in the table contains a single value, not a list of values. For exam‐\\nple, in the Students table, each column (attribute), like Name, holds a single value.\\n• The table has no repeating groups of columns. For example, in the Students\\ntable, you should not have multiple “Course” columns, such as Course1, Course2,\\nand Course3. Instead, each student–course pairing should have its own row.\\nA table is in second normal form (2NF) if it satisfies all of the above 1NF conditions,\\nplus:\\n• Every detail (non-key attribute) in the database record must rely entirely on its\\nunique identifier (primary key) and not on any other detail. For example, in a\\nStudents table with StudentID, Name, and Major columns, the student’s Major\\n(non-key attribute) must be determined solely by the StudentID (primary key),\\nnot by the Name or any other attribute in the table.\\nA table is in third normal form (3NF) if and only if it satisfies all of the above 1NF and\\n2NF conditions, plus:\\n• Every non-key detail in the table should relate directly to the main identifier (pri‐\\nmary key) and not through another detail. For example, in a Students table with\\nStudentID, Major, and DepartmentHead (where DepartmentHead is the head of\\nthe Major), the DepartmentHead should not depend on the Major, which in turn\\nshould not depend on the StudentID.\\nMost relational models, especially for OLTP databases (see Chapter 7 ), are in 3NF .\\nThat should be your ultimate goal.\\nA relational model uses a normalized database schema, in which data is stored in one\\nplace and organized into multiple tables with strictly defined relationships between\\nthem. This helps to ensure integrity, but it can also make querying more time-\\nconsuming, because the database will likely need to join multiple tables together in\\norder to retrieve the desired data.\\nFigure 8-2 shows what a relational model for sales data might look like.\\nRelational Modeling | 105', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8fc7ef1f-801b-45f6-b52f-f4769d8f8e2a', embedding=None, metadata={'page_label': '105', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 8-2. Example of a relational model for sales data\\nTracking Changes\\nIt’s important to track changes to relationally modeled data over time. To maintain a\\nrecord of past data and changes, history tables are often used. A history table is typi‐\\ncally a copy of the original table with additional columns added to track changes,\\nsuch as a date/time stamp, a user ID, and a “before” and “after” value for each column\\nin the table. History tables can be useful for auditing, reporting, and data recovery,\\nhelping to ensure the integrity of the data and making it easier to understand how it\\nhas changed over time.\\n106 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92422aa0-6574-49bb-ad02-0e39fd02d609', embedding=None, metadata={'page_label': '106', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Dimensional Modeling\\nDimensional modeling started in 1996 as a way to support efficient querying and anal‐\\nysis by organizing data into facts and dimensions. Dimensional models are needed\\nwhen querying and reporting off the relational model becomes too slow or complex.\\nDimensional models typically use a relational model as a data source.\\nFacts, Dimensions, and Keys\\nDimensional models use facts and dimensions:\\nFacts\\nFacts, in this context, are pieces of data, typically numeric values, used to measure\\nsomething, such as sales, orders, or revenue. Facts can be aggregated or summar‐\\nized for performance reasons, such as by calculating averages or counting\\nrecords, so that these values are already calculated and don’t have to be calculated\\nas part of running the query.\\nDimensions\\nDimensions describe the data’s characteristics, such as time, product, customer, or\\nlocation. They are typically represented as hierarchies, with each level of the hier‐\\narchy providing a more detailed description of the data.\\nAnother way to understand dimensional models is to remember that fact tables con‐\\ntain metrics, while dimension tables contain attributes of the metrics in the fact\\ntables.\\nInstead of the natural keys used in relational modeling, dimensional modeling uses\\nsurrogate keys: artificial values created specifically (and usually automatically) to serve\\nas the primary key for a table. A surrogate key is often used when a natural key is not\\navailable or is not suitable for use as the primary key, which happens frequently in\\ndimensional modeling.\\nNatural keys are often more meaningful and easier to understand than surrogate\\nkeys, so they can make the database easier to use and maintain. However, there are\\nalso some drawbacks to using natural keys:\\n• They can be longer and more complex than surrogate keys and thus more chal‐\\nlenging to work with.\\n• They often contain sensitive information, which can raise privacy concerns.\\n• They can create challenges due to potential duplication and non-standardized\\nformats. For example, if you are using customer IDs as the natural key and your\\ncompany purchases another company that uses matching customer IDs or a dif‐\\nferent format (such as characters instead of integers), conflicts can arise. The\\nunderlying principle here is that duplicates can be an issue when merging or\\nDimensional Modeling | 107', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='132038bf-808b-4b76-869f-6b8f5c6a3dc0', embedding=None, metadata={'page_label': '107', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='comparing data across different systems, and inconsistency in formats can fur‐\\nther complicate the integration process.\\nTracking Changes\\nTo track changes made to a table in dimensional modeling, you’ll use slowly changing\\ndimensions (SCDs), of which there are three types:\\nType 1\\nType 1 SCDs overwrite existing data with the new data and discard the old data.\\nThis is the simplest and most common type of SCD, and it is often used when the\\nchanges to the data are not significant or when the old data is no longer needed,\\nsuch as when correcting a customer’s phone number.\\nType 2\\nType 2 SCDs maintain multiple versions of the data: the new data and a record of\\nthe old data. Use this type of SCD when you need to track the changes to the data\\nover time and maintain a record of the old data. For example, let’s say that six\\nmonths ago, a company analyzed its sales data and found New Y ork to be its top-\\nselling state. Now, if some customers have since moved out of New Y ork to New\\nJersey, rerunning the same report without accounting for these changes would\\nshow inaccurately lower sales figures for New Y ork. This would lead to a mis‐\\ntaken perception of historical data showing declining sales in New Y ork, which\\ncould influence strategic decisions. So, if a customer moves from one US state to\\nanother, the company’s Type 2 SCD would store both the old state and the new\\none.\\nType 3\\nType 3 SCDs create a new record for each change so you can maintain a complete\\nhistory of the data. This type of SCD is the most complex but also the most flexi‐\\nble. For example, when a customer moves from one US state to another, a Type 3\\nSCD would store the entire old record and an entire new record, which could\\ninclude dozens of fields and not just the US state field.\\nOne key difference between history tables in relational modeling and SCDs is their\\nlevel of detail. History tables track changes to the data at the level of individual\\nrecords, while SCDs track changes at the dimension level. And while history tables\\nare typically used to track changes to any type of data, SCDs are specifically for track‐\\ning changes to dimension tables in a dimensional model.\\nFigure 8-3 shows a dimensional model for sales, derived from the relational model in\\nFigure 8-2.\\n108 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b0d2fa52-9daf-437f-8cd2-a5036b0716b7', embedding=None, metadata={'page_label': '108', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 8-3. A dimensional model for sales data\\nDenormalization\\nDimensional models use a process called denormalization, where you include redun‐\\ndant copies of the data in multiple tables. This reduces the number of tables. When\\nyou query the database, it does not need to join as many tables together, so querying\\nis much faster. Less joining also reduces complexity for end users creating reports.\\nHowever, it means that the redundant copies of data must be kept in sync to ensure\\nintegrity, and that requires careful maintenance. The redundant data also uses more\\nstorage, which may slightly increase costs.\\nIn Figure 8-3, you can see that CategoryName is included in the list of product keys.\\nAs a result, the category name is repeated in all product records. By contrast, in the\\nrelational model in Figure 8-2 , the product records show the CategoryID. In the\\ndimensional model, if the category name were to change, you would need to change\\nthe name in multiple records in the product table; in the relational model, you would\\njust need to change it once (in the Category table).\\nThe more tables you have in a relational model, the more useful a dimensional model\\nwill be. If you only have a handful of tables in the relational model, you might not\\nneed a dimensional model.\\nA database schema is a logical blueprint that outlines the structure, relationships, con‐\\nstraints, and other elements of how data is organized and related within a database.\\nMany types of schemas are used in dimensional modeling, such as snowflake schemas\\nand schemas with multiple fact tables. A star schema  is a dimensional modeling\\nDimensional Modeling | 109', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5b59c274-1271-4960-8ba1-58651315d9e6', embedding=None, metadata={'page_label': '109', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='technique with a central fact table surrounded by multiple dimension tables. The dia‐\\ngram of the schema looks like a star (Figure 8-4), hence the name.\\nFigure 8-4. Example of a star schema\\nIn summary, a relational model captures a business solution for how part of the busi‐\\nness works, while a dimensional model captures the details the business needs to\\nanswer questions about how well the business is doing. The relational model is easier\\nto build if the source system is already relational, but the dimensional model is easier\\nfor business users to employ and will generally perform better for analytic queries.\\n110 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d22c535-f299-42f3-ac82-ca317c7dc3c0', embedding=None, metadata={'page_label': '110', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Common Data Model\\nA common data model (CDM) is a standardized structure for storing and organizing\\ndata that is typically used when building a data warehouse solution. It provides a con‐\\nsistent way to represent data within tables and relationships between tables, making it\\neasy for any system or application to understand the data.\\nImagine you have many source systems from which you want to import data into\\nyour DW . Some are customer relationship management (CRM) applications from dif‐\\nferent vendors that have different formats (that is, the tables and fields are different).\\nY ou have to decide what table format to use in the data warehouse. Do you pick one\\nof the CRM vendors’ models? No. It’s better to create a new model that can handle all\\nthose vendors’ formats.\\nThis new model ( Figure 8-5) is your common data model. Building it will require a\\nlot of work, but if you do it right, your CDM will be able to support current and\\nfuture data from any CRM vendor. Fortunately, you don’t have to build it from\\nscratch. Many cloud providers and software vendors have very robust CDMs tailored\\nfor industries such as banking, healthcare, and retail. If you can customize a premade\\nmodel, you’ll save yourself many hours of modeling, reduce risk, and enable all the\\napplications in your organization to access data using a common language.\\nFigure 8-5. The common data model architecture\\nData Vault\\nCreated by Daniel Linstedt in 2000, data vault modeling is an entire methodology\\nspecifically designed for use in data warehousing and business intelligence systems.\\nIts main goal is to provide a flexible, scalable, and standardized way to model and\\nmanage historical data.\\nData Vault | 111', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0fe0ea21-2e85-4234-8fe3-716204578c3b', embedding=None, metadata={'page_label': '111', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A data vault model (shown in Figure 8-6) is built around three kinds of entities:\\nHubs\\nHubs are the central business entities in the model, representing key business con‐\\ncepts such as customers or products. Hubs are typically modeled as a single table\\nwith a unique identifier (such as a primary key) and a set of attributes.\\nLinks\\nLinks model the relationships between hubs, typically as a separate table. A link\\ntable contains a set of foreign keys that reference the primary keys of the related\\nhubs.\\nSatellites\\nSatellites store descriptive attributes about a hub or link, such as changes to the\\ndata over time. They are typically modeled as separate tables and linked to a hub\\nor link table through a foreign key.\\nFigure 8-6. Data vault\\nData vault–based models let you track data lineage, audit data, and enforce rules and\\nstandards. They are often used in conjunction with other data-modeling techniques,\\nsuch as dimensional modeling, to provide a comprehensive and flexible data architec‐\\nture that can be integrated easily with other systems and applications. On the spec‐\\ntrum of models we’ve looked at so far, the data vault fits somewhere between 3NF\\ndata and a star schema; it is well suited for organizations with large amounts of data\\nand complex data relationships, while the Kimball model (discussed in “Kimball’s\\nBottom-Up Methodology” on page 115) is better for organizations with simpler data\\nneeds.\\n112 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6420ff9c-acc6-4c9d-b99c-912db70fa68f', embedding=None, metadata={'page_label': '112', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 Inmon likely first coined the term data warehouse around 1985; his Building the Data Warehouse (Wiley, 1992,\\navailable via the Internet Archive) was the first book on the topic. However, Barry Devlin and Paul Murphy\\npublished the first detailed architectural description of the data warehouse as we know it today: “An Architec‐\\nture for a Business and Information System”, IBM Systems Journal, 1988 (available via 9sight Consulting).\\nSome of the main disadvantages of the data vault methodology include:\\nComplexity\\nData vault modeling can be complex to implement and maintain, especially for\\norganizations that are not familiar with the technique. It requires a significant\\ninvestment of resources and expertise, and understanding and navigating the\\ndata model can be difficult without proper documentation.\\nData duplication\\nData vault modeling often leads to duplication, since data is stored at the atomic\\nlevel. This can increase storage costs and make it more difficult to ensure data\\nconsistency.\\nPerformance\\nData vault modeling can result in a large number of tables and relationships,\\nwhich can degrade query performance. Y ou may need to use more complex quer‐\\nies and indexing strategies to achieve acceptable performance.\\nLack of standardization\\nData vault modeling is a relatively new and little-used technique, and it lacks a\\nstandardized approach. Finding engineers to build and maintain it can be\\nchallenging.\\nThe Kimball and Inmon Data Warehousing Methodologies\\nBill Inmon is considered the “father of data warehousing” because of his early pio‐\\nneering work in the data warehousing field in the late 1980s. He wrote the first book\\non data warehousing in 1992 and played a significant role in popularizing and\\nadvancing the concept.1 Inmon’s approach to data warehousing is often characterized\\nas a top-down approach to data marts that uses relational modeling.\\nThe terms top-down and bottom-up here have nothing to do with\\nthe top-down and bottom-up approaches for relational data ware‐\\nhouses and data lakes described in Chapters 4 and 5. Those\\napproaches are related to the work done up front (or not) in build‐\\ning a data architecture that includes an RDW and a data lake. The\\nKimball and Inmon approaches are instead mainly focused on how\\ndata marts are used in building a data architecture that includes\\nonly an RDW and not a data lake (among other details).\\nThe Kimball and Inmon Data Warehousing Methodologies | 113', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4ec3af48-a6ae-4e82-ac86-45067cf3836a', embedding=None, metadata={'page_label': '113', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ralph Kimball, another early pioneer in data warehousing, popularized the star\\nschema in data warehouse design with his first book, The Data Warehouse Toolkit:\\nPractical Techniques for Building Dimensional Data Warehouses  (Wiley, 1996). Kim‐\\nball’s approach is often characterized as a bottom-up approach, and it is also called\\ndimensional modeling or the Kimball methodology.\\nInmon’s and Kimball’s methodologies for building a data warehouse have sparked\\nyears of debates. This section will discuss both in more detail and dispel some com‐\\nmon myths.\\nInmon’s Top-Down Methodology\\nInmon’s methodology emphasizes integrating data from multiple sources and pre‐\\nsenting it in ways that aid in decision making. The top-down approach is a tradi‐\\ntional, rigorous, and well-defined methodology, and it is often preferred in large and\\ncomplex organizations that have a lot of data and value governance, data quality, and\\nregulatory compliance. It is driven mainly by the technology department; end users\\nparticipate passively.\\nThis architecture, pictured in Figure 8-7 , first pulls data from each of the OLTP\\nsource systems (see Chapter 7) into temporary relational staging tables as quickly as\\npossible, without transforming or cleaning it.\\nFigure 8-7. A data warehouse designed using the Inmon methodology\\nTransforming data can greatly increase CPU and bandwidth on source systems, so\\nthis approach avoids slowing down the source systems. Staging tables are also useful\\nfor auditing purposes, and if you need to rerun the transformations, you can pull the\\ndata from them instead of going back to the source system.\\nIn the next step, data is copied into what Inmon calls a corporate information factory\\n(CIF), where all the data is centralized and stored at the atomic level (as granular as\\npossible) in third normal form. Y ou can think of it as an enterprise data warehouse\\n(EDW) that is a single version of the truth. I use the term enterprise data warehouse in\\nthis section over just data warehouse, as most solutions built in the early years of data\\n114 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30fe7e67-0ae6-4af8-a579-d3f823d7b80b', embedding=None, metadata={'page_label': '114', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='warehousing incorporated data from all areas of an organization. Nowadays some\\ncompanies will have multiple data warehouses, but more commonly solutions are\\nbuilt using just one “enterprise” data warehouse.\\nThe CIF is focused on data modeling and design, with a strong emphasis on gover‐\\nnance and data quality. From it, you can create dependent data marts: separate physi‐\\ncal subsets of data, usually created to give each department the specific data it needs.\\nThe data marts are called “dependent” because they depend on data from the CIF and\\ndo not get any data from anywhere else. This is also called a hub-and-spoke architec‐\\nture (for its shape). Users can query the data marts for reports, or query subsets—\\nsmall dimensional models called cubes (see Chapter 7 )—for specific purposes. For\\ninstance, analysts might only need to see sales volumes. The data in a cube is often\\naggregated.\\nInmon’s methodology is called “top-down” because it starts with identifying the busi‐\\nness needs of the end users. Data modeling and design come next, then integrating\\nand populating the data, and finally access and reporting. All data is centralized in a\\nCIF , and then subsets are copied to subject-oriented data marts.\\nThe CIF is off-limits to end users, who access data through the data marts or cubes.\\nOne drawback is that this means the data is permanently duplicated up to three times\\nfor the CIF , the data mart, and the cube. This results in higher storage costs, greater\\nmaintenance complexity, and the challenge of keeping all three copies synchronized.\\nKimball’s Bottom-Up Methodology\\nKimball’s “bottom-up” or “agile” methodology starts similarly to Inmon’s, by pulling\\nraw data from each of the OLTP source systems (see Chapter 7) into temporary rela‐\\ntional staging tables with no transforming or cleaning. The Kimball methodology\\nthen moves into a phase of integrating the data and ensuring its quality, followed by\\nphases for modeling, design, and access. Its architecture is pictured in Figure 8-8.\\nFigure 8-8. A data warehouse designed using the Kimball approach\\nThe Kimball and Inmon Data Warehousing Methodologies | 115', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5f0bf03-44c9-411f-8b59-714d5f0b53ee', embedding=None, metadata={'page_label': '115', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The data from the staging tables is then copied to mission-critical independent data\\nmarts, which serve the analytic needs of departments. The data marts are independ‐\\nent because they focus on a specific subject area or business process, rather than stor‐\\ning and managing all data in a single central repository like Inmon’s CIF . These\\nindependent data marts use a dimensional model and are stored at the atomic or\\nsummary level, depending on users’ needs. The data marts are integrated for data\\nconsistency through a DW bus (sometimes called an information bus), which you can\\nthink of as a conformed data warehouse. All of the data is in a single unified view and\\nappears to be in one database in one central physical location, but it’s actually in mul‐\\ntiple databases (and often in separate locations). These data marts are decentralized,\\nbut they do not have to be in separate physical data stores—they could be multiple\\ndatabases all on the same server, or even in one database but separated by different\\nschema names.\\nTo provide consistency across data sources, the data marts are integrated using con‐\\nformed dimensions, which are dimensions that have been standardized and made con‐\\nsistent across multiple fact tables so the data can be used and compared across fact\\ntables without any issue. For example, if a customer dimension is used in multiple\\nfact tables, the customer names and IDs should be the same across all the fact tables.\\nFrom the data marts, subsets of the data can be copied into cubes. Y ou can even build\\na cube directly from the staging tables using a dimensionalized view if that would\\nprovide all the types of access needed. (A dimensionalized view is a SQL view that\\norganizes the data into a dimensional model.) Reporting can be done from the cube\\nor from the data mart. Kimball’s methodology is business driven, and end users are\\nactive participants. Data is copied either just twice, to a data mart and to a cube, or\\nonly once if using a dimensionalized view.\\nChoosing a Methodology\\nIt really boils down to two differences. First, the Inmon methodology creates a nor‐\\nmalized DW before creating data marts, while the Kimball methodology skips the\\nnormalized DW . Second, the Inmon methodology uses a physical EDW , while the\\nKimball methodology uses a conformed EDW but no physical EDW . Both methodol‐\\nogies predate data lakes, but when data lakes started to become popular, both meth‐\\nodologies incorporated them. Kimball incorporates data lakes alongside data marts in\\nthe DW bus, and both Inmon and Kimball allow data lakes to replace relational stag‐\\ning tables.\\nHowever, there are many reasons to have a physical EDW , such as:\\n• A physical EDW makes it much easier to have a single version of the truth; hav‐\\ning many data marts make up a conformed data warehouse can be difficult and\\n116 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7cec2a9e-3d85-4d14-b2e1-8fea21dba448', embedding=None, metadata={'page_label': '116', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='confusing for users. Having the data physically in the same database is easier to\\nunderstand.\\n• Building from lightly denormalized tables in a physical EDW is easier than build‐\\ning directly from the OLTP source.\\n• A normalized physical EDW provides enterprise-wide consistency. This makes it\\neasier to create data marts, though with the trade-off of duplicating data.\\n• Physical EDWs require fewer ETL refreshes and reconciliations; with many data\\nsources and dimensional data marts in multiple databases, you need many more\\ndaily or hourly refreshes and reconciliations to keep everything in sync.\\n• In a physical EDW , there is only place to control data, so you aren’t duplicating\\nyour efforts or your data.\\nHowever, if you have just a few data sources and need quick reporting, you might be\\nbetter off without a physical EDW .\\nWhich model should you use? In truth, the two models have been modified over the\\nyears and are now fairly similar. In fact, they can complement each other. Y ou could\\nadd a normalized EDW to a Kimball-based approach or add dimensionally struc‐\\ntured data marts to an Inmon-based approach. The bottom line is that you don’t you\\nneed to choose just one approach and stick to it strictly. Y ou should understand both\\napproaches and pick parts from both that work for your situation. That said, no solu‐\\ntion will be effective unless your team has solid skills in leadership, communication,\\nplanning, and interpersonal relationships (see the Chapter 15).\\nThe Kimball and Inmon Data Warehousing Methodologies | 117', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='28cfa4ad-ba44-4b46-b2a5-711958ecaba9', embedding=None, metadata={'page_label': '117', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Hybrid Models\\nLet’s explore the possibilities of a hybrid of the two methodologies, as pictured in\\nFigure 8-9.\\nFigure 8-9. A hybrid of the Kimball and Inmon approaches\\nThis model starts out like the other two: pulling raw data from each of the OLTP\\nsource systems directly into temporary relational staging tables. There is still no\\ntransforming or cleaning the data, but you do add a mirrored OLTP (more on that in\\na moment).\\nNext, you’ll copy the data into a CIF , to be centralized and stored at the atomic level\\nin 3NF , as in the Inmon methodology. The data from the CIF is then copied to inde‐\\npendent, dimensional data marts. Some data marts are stored at the atomic level and\\nothers at a summary level, depending on the analytic needs of each department.\\nFrom the data marts or from the CIF , you can also copy subsets of the data into cubes.\\nY ou can do reporting from a cube or a data mart, but I recommend reporting from\\nthe cube where possible. The cube’s advantages include:\\n• It acts as a semantic layer.\\n• It can handle many concurrent users.\\n• Its data is aggregated for better performance.\\n• Y ou don’t have to deal with joins or relationships.\\n• It can contain hierarchies and KPIs.\\n• It has row-level security, which enhances data privacy by restricting user access to\\nspecific rows in a database.\\n• It offers advanced time calculations, which help you understand patterns and\\ntrends in data over time, like year-over-year sales growth.\\n118 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0f08ec67-fa14-4d33-9aad-d2ea0ab099c0', embedding=None, metadata={'page_label': '118', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whether you use a Kimball, Inmon, or hybrid model, consider using database views.\\nA database view is a virtual table based on the result of a SQL SELECT statement. It\\ndoes not store data, only SQL code; it retrieves data from one or more tables each\\ntime it is queried. Using views in the extract, transform, and load (ETL) process\\nmakes for simpler code inside the ETL, and you don’t have to see the SQL code within\\nthe view to understand what it is reading. A view makes it easier to query a database\\nfor debugging purposes, and ETL can be optimized by updating the view outside of\\nthe ETL code. Anyone can read, modify, or optimize views, not only in the ETL tool\\nbut also in other tools, and you can analyze and track dependencies in a view using\\nthird-party tools (such as the tables and fields the view uses). Views can provide\\ndefault values and perform simple calculations, and you can rename fields to help you\\nunderstand their flow. Views can present a star schema, even if the underlying struc‐\\nture is much more complex.\\nY ou can also use views in a cube. If you do, you can rename database columns to align\\nwith cube attributes, which has the advantage of exposing all the transformations to\\nthe database administrator. Using views simplifies handling fast variations and gives\\nyou full control of any joins sent to the source the cube is pulling from. And, like in\\nthe ETL, views in a cube can expose a star schema, even if the underlying structure is\\nnot a simple star schema.\\nI mentioned that this hybrid involves a mirrored OLTP, which is a duplicate of the\\noriginal OLTP system that runs in parallel with it. Creating an OLTP mirror is a great\\nstrategy for several reasons. First, it means you’re only using the “real” OLTP briefly,\\nduring mirroring. After that, it’s freed up and available for end users or maintenance.\\nY ou can modify the keys, relations, and views of a mirror, leading to simpler ETL pro‐\\ncesses that are cheaper to build and maintain. Y ou can also build particular indexes in\\nthe OLTP mirror, meaning you don’t need to do index maintenance on the original\\nOLTP . This can improve performance on the other steps of the ETL processes.\\nFinally, because you can mirror a subset of columns, you don’t have to load all the\\nOLTP tables. This makes the mirror much smaller and faster than the full OLTP .\\nKeep in mind that there is no one model that will work for every use case. There are\\nmany reasons you might need to modify your model, including security, data size,\\nand performance. For example, you could add another layer, such as an EDW con‐\\ntaining just star schemas, between the CIF and the DW bus architecture. This would\\nimprove performance, because star schemas are highly optimized for queries.\\nThe Kimball and Inmon Data Warehousing Methodologies | 119', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8e8fe727-4b0b-420d-8302-214418d5c606', embedding=None, metadata={'page_label': '119', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 Claudia Imhoff, Nicholas Galemmo, and Jonathan G. Geiger, Mastering Data Warehouse Design (Wiley,\\n2003), 21.\\nMethodology Myths\\nBecause Inmon and Kimball have been so hotly debated over the years, plenty of\\nmyths have arisen about the methodologies. This section finishes the chapter by\\ndebunking some of them:\\nMyth: Kimball is a bottom-up approach without enterprise focus.\\nKimball’s approach actually incorporates both a top-down and a bottom-up\\nmethodology. The top-down aspect is demonstrated in the strategic planning and\\ndesign across the enterprise. Kimball emphasizes spending significant time up\\nfront to design your solution using a tool known as the EDW bus matrix. This\\ntool serves as an architectural blueprint of an organization’s core business pro‐\\ncesses and its related dimensions, including conformed dimensions. The EDW\\nbus matrix provides a top-down strategic perspective to ensure that data in the\\nDW/BI environment can be integrated across the entire enterprise.\\nThe bottom-up aspect comes into play during the execution stage. Here, Kim‐\\nball’s methodology focuses on one business process at a time, ensuring an agile\\ndelivery of solutions. This approach emphasizes practical implementation and\\ndemonstrates the synergy of using a top-down planning strategy along with a\\nbottom-up execution approach.\\nMyth: Inmon requires you to complete a ton of design before you start building the solu‐\\ntion, similar to the big bang or waterfall approaches.\\nInmon has stated from the beginning that the way to build data warehouses is\\niteratively. In fact, he goes so far as to say that the absolute most critical success\\nfactor in building a data warehouse is not using the big bang approach.\\nThis myth is related to the long-standing myth that it takes a long time and lots\\nof resources to build an Inmon-style architecture. Claudia Imhoff, one of the\\nbest-known experts on the Inmon approach, and her coauthors put it this way:\\nNowhere do we recommend that you build an entire data warehouse containing\\nall the strategic enterprise data you will ever need before building the first analyti‐\\ncal capability (data mart). Each successive business program solved by another\\ndata mart implementation will add to the growing set of data serving as the foun‐\\ndation in your data warehouse. Eventually, the amount of data that must be added\\nto the data warehouse to support a new data mart will be negligible because most\\nof it will already be present in the data warehouse.2\\nPart of the confusion comes from people conflating Inmon’s up-front design and\\nplanning approach with the architecture solution itself. Both methodologies\\n120 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7d7be1a6-983d-4ac2-9909-8f20c08689b2', embedding=None, metadata={'page_label': '120', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='reserve time for designing, planning, and gathering requirements before actually\\nbuilding anything. Both follow that with an iterative/agile approach where you\\nbuild some of the solution, then do more designing and planning, then more\\nbuilding, in a repeating cycle.\\nThe sliding scale shown in Figure 8-10 indicates how much design work is done\\nbefore building the solution. On the far left you see the ad hoc model, where no\\ndesign work is done before building the solution; on the far right is the waterfall\\nmodel, where all design work is done first (often called the “big bang” approach).\\nThe Inmon and Kimball methodologies both fall at the mark about 25% from the\\nleft, where a decent amount of design work is done up front before building the\\nsolution.\\nFigure 8-10. Percentage of design work done before building a solution\\nAn analogy would be building a new city. Y ou would design a blueprint for the\\ncity with all the neighborhoods inside it, then you would build each neighbor‐\\nhood and have people move in once it is completed. Throughout that process, the\\noverall architecture of the city could be modified if needed. Y ou wouldn’t build all\\nthe neighborhoods before anyone moved in, nor would you start building neigh‐\\nborhoods without a good idea of what the overall city should look like.\\nMyth: Inmon’s model does not allow star schema data marts.\\nIn later editions of Building the Data Warehouse  and “ A Tale of Two Architec‐\\ntures, ” Inmon argues that star schema data marts are good for giving end users\\ndirect access to data and that star schemas are good for data marts. He is not\\nagainst them.\\nMyth: Very few companies use the Inmon method.\\nRecent surveys show that more companies use the Inmon methodology than use\\nKimball. Many more companies use an EDW without any data marts.\\nMyth: The Kimball and Inmon architectures are incompatible.\\nThe two methods can work together to provide a better solution, as you learned\\nin “Hybrid Models” on page 118.\\nKimball also defines a development lifecycle in his books, covering such topics as\\nproject planning, business requirements definition, and maintenance (see\\nFigure 8-11). Inmon, on the other hand, just covers the data warehouse, not how to\\nuse it.\\nMethodology Myths | 121', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6011b66e-1a49-44b4-912e-afea07705580', embedding=None, metadata={'page_label': '121', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 8-11. Kimball’s methodology incorporates a development lifecycle that goes\\nbeyond building the data warehouse (source: Kimball, The Microsoft Data Warehouse\\nToolkit)\\nWhen people say they are using the Kimball methodology, often what they really\\nmean is dimensional modeling—the two terms are commonly treated as synony‐\\nmous. Kimball didn’t invent the basic concepts of facts and dimensions; however, he\\nestablished an extensive portfolio of dimensional techniques and vocabulary, includ‐\\ning conformed dimensions, slowly changing dimensions, junk dimensions , mini-\\ndimensions, bridge tables, and periodic and accumulating snapshot fact tables.\\n122 | Chapter 8: Approaches to Data Modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='409ce1f7-88dd-49cb-8172-3656e5019142', embedding=None, metadata={'page_label': '122', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Summary\\nThis chapter introduced you to the essential practices and methodologies behind\\norganizing and defining data structures, relationships, and constraints for efficient\\ndata processing, storage, and retrieval. We explored the process of creating a data\\nmodel, which serves as an abstract representation of the organizational data. It’s a\\ncrucial initial step in managing data, since it provides a blueprint for how data is\\nstored, consumed, and integrated within systems.\\nNext, we investigated relational modeling, a classic approach based on the concept of\\nnormalizing data into tables (or relations) where each row represents a record and\\ncolumns represent attributes. This model is key to preventing data redundancy and\\nenhancing data integrity, and it’s at the core of most traditional relational database\\nsystems. I then discussed dimensional modeling, a methodology commonly used in\\ndata warehousing that categorizes data into facts (measurable quantities) and dimen‐\\nsions (contextual information); the common data model (CDM), a shared data lan‐\\nguage enabling interoperability of data across various applications and business\\nprocesses; and the data vault model, a resilient hybrid approach that combines the\\nbest features of the 3NF and the star schema. The rest of the chapter made a vital\\ncomparison between two competing data warehousing philosophies. Kimball’s\\nbottom-up approach suggests building individual data marts first and then combin‐\\ning them into a full data warehouse, whereas Inmon’s top-down approach advocates\\ncreating a comprehensive data warehouse first, then deriving data marts from it.\\nUnderstanding these methodologies can greatly help you design effective data archi‐\\ntectures that cater to your organization’s specific requirements while balancing opera‐\\ntional efficiency with strategic analytics.\\nSummary | 123', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc4ec7f6-50b9-49db-84ac-7894859466c7', embedding=None, metadata={'page_label': '123', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='335b38d2-b9e5-4d4f-93d5-0dfdf96d3aee', embedding=None, metadata={'page_label': '124', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 9\\nApproaches to Data Ingestion\\nIn our ever-growing digital world, handling and making sense of all sorts of data has\\nbecome incredibly important for businesses of all types. This chapter is all about the\\nfirst step in handling data—getting it into the system in the first place.\\nHere, we’ll unravel some of the key ways data can be brought into different systems.\\nI’ll kick things off by explaining two common methods, known as ETL and ELT, in a\\nway that is easy to understand. I’ll also introduce you to a new idea called reverse ETL\\nand explain how it flips the traditional methods on their head.\\nSince not all data needs are the same, we’ll explore different techniques like batch and\\nreal-time processing. This will help you figure out what might work best based on\\nhow much data you’re dealing with and how quickly you need it. Finally, we’ll talk\\nabout the importance of data governance—ensuring your data is accurate, consistent,\\nand accessible.\\nThis chapter aims to simplify these complex ideas and show you how they all connect\\nto the bigger picture of data handling. Whether you’re a data whiz or a complete nov‐\\nice, I’m excited to guide you through this fascinating world of data ingestion. Wel‐\\ncome aboard!\\nETL Versus ELT\\nFor many years, extract-transform-load (ETL) was the most common method for\\ntransferring data from a source system to a relational data warehouse. But recently,\\nextract-load-transform (ELT) has become popular, especially with data lakes.\\nThe ETL process involves extracting data from outside sources, transforming and\\ncleaning it in-flight to fit the format and structure of the destination, and then load‐\\ning it into that destination (usually a data lake or RDW).\\n125', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c8e20c5f-d45a-480f-8089-a4517851be3c', embedding=None, metadata={'page_label': '125', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='How to Avoid Confusing ETL and ELT\\nThe terms ETL and ELT, owing to their similarity, could be easily misinterpreted or\\nmixed up. To help you remember which is which, here’s a useful mnemonic:\\nETL (Extract-Transform-Load) = “Early Transformation Leads”\\nThis emphasizes that transformation happens early in the process, right after extrac‐\\ntion and before loading. This captures the essence of the ETL process, where data is\\ncleaned and transformed before being loaded into the target system.\\nFor ELT, think:\\nELT (Extract-Load-Transform) = “Every Load Transforms”\\nThis indicates that after each batch of raw data is loaded into the system, it undergoes\\ntransformation; that is, the transformation happens after the data is loaded into the\\nsystem.\\nThere are a few major drawbacks with ETL. First, transforming data takes time, and\\nextraction can be a resource hog. The longer an extract is happening on the source\\nsystem, the greater the likelihood that end users on that system experience perfor‐\\nmance issues. Second, if there is a bug in the ETL process and you need to rerun it,\\nyou will have to go back to the source system to extract the data again, likely affecting\\nperformance once more. Third, if there is a very large amount of data being copied,\\ntraditional ETL tools may not be able to handle it due to their limited processing\\npower. Fourth, ELT performs transformations one record at a time, which can also be\\nslow. Finally, some ETL tools are limited in the types of data they support.\\nOn the plus side, ETL is often ideal for smaller datasets, where the transformation\\nprocess isn’t excessively complex. It offers better control over data quality, since clean‐\\ning and transforming data before loading helps minimize errors. ETL can be key for\\ndata security, as only necessary and cleaned data is loaded, reducing potential security\\nrisks. Finally, ETL is preferred when both the source and destination are relational\\ndatabases, since it is typically more efficient.\\nELT is similar to ETL, except the order is different. ELT also copies data from outside\\nsources into a destination system, but the data is not transformed in-flight. Instead, it\\nlands in the destination system in its raw form and is transformed and cleaned to fit\\nthe format and structure of the destination. Y ou may, however, filter unneeded rows\\nand columns as you extract data from the source so that you don’t waste resources on\\nunneeded data.\\nELT is often the preferred approach for data lakes and is particularly useful when\\ndealing with high-volume unstructured or semi-structured data. Its flexibility lies in\\n126 | Chapter 9: Approaches to Data Ingestion', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02998514-6933-4e3f-a38a-3494e9837042', embedding=None, metadata={'page_label': '126', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the ability to load all raw data into the destination system first and then apply trans‐\\nformations as needed. Once the data is in the data lake, ELT can leverage the vast pro‐\\ncessing power of today’s technology, making it efficient for handling very large\\namounts of data. ELT can also be faster at transformations as it processes them in\\nbatches, a notable advantage over ETL ’s record-by-record transformation approach.\\nThis also ensures that transformations can be altered or optimized without needing\\nto re-extract the data. Another advantage of ELT is the wide array of tooling options\\navailable, including those designed specifically for big data platforms. This means that\\nELT processes can support a larger variety of data types and adapt to the evolving\\nneeds of the data environment. Additionally, the ELT approach can minimize poten‐\\ntial performance issues on the source system, since it extracts data only once; future\\ntransformations are handled within the target system.\\nIn conclusion, both ETL and ELT have unique strengths and are suited to different\\ndata-processing scenarios. ETL, being the older of the two methodologies, offers solid\\ndata quality control, security, and efficiency, especially when the source and destina‐\\ntion are relational databases, making it ideal for smaller datasets. ELT, a more recent\\nmethodology, has gained popularity for use with data lakes, offering greater flexibility\\nand scalability, especially when dealing with high-volume and unstructured data.\\nELT’s batch processing of transformations and wide range of tooling options make it\\na versatile and efficient choice for big data platforms.\\nThe choice between ETL and ELT will depend largely on the specific needs and struc‐\\nture of your data environment. With data technologies evolving quickly, the choice\\nisn’t a strict either–or decision; instead, it’s about finding the balance that best\\naddresses your data-processing requirements.\\nReverse ETL\\nReverse ETL  is the process of moving data from a modern data warehouse into a\\nthird-party system or systems to make the data operational. ( Figure 9-1  compares\\nETL, ELT, and reverse ETL.) Traditionally, data stored in a DW is used for analytical\\nworkloads and business intelligence—to identify long-term trends and influence\\nlong-term strategy—and is not copied outside the DW . However, some companies are\\nnow also using this data for operational analytics.\\nOperational analytics helps with day-to-day decisions and has the goal of improving\\nefficiency and effectiveness. In simpler terms, it puts a company’s data to work to help\\neveryone make better and smarter decisions about the business. For example, say you\\ningest customer data into your DW and then clean and master it (ELT). Y ou could\\nthen copy that customer data into multiple software-as-a-service (SaaS) systems, such\\nas Salesforce, to get a consistent view of each customer across all systems. Y ou could\\nalso copy the customer data into a customer support system and/or a sales system.\\nReverse ETL | 127', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c3f4aeac-6d26-4ba3-bfce-da9fde380f47', embedding=None, metadata={'page_label': '127', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Y ou could even identify customers who are at risk of churning (leaving the company’s\\nproducts or services) by surfacing usage data in a CRM.\\nFigure 9-1. Comparison of ETL, ELT, and reverse ETL\\nCompanies are building essential metrics in SQL on top of their data warehouses to\\nbetter understand and engage with customers. These metrics might include lifetime\\nvalue (a prediction of the total net profit a company expects to earn from its future\\nrelationship with a customer), product qualified lead (a potential customer who has\\nengaged with a product and could become a paying customer), and propensity score\\n(the likelihood that a user will buy a product), among many others.\\nWhile you could easily create reports and visualizations using this data in BI tools or\\nSQL, the insights can be much more powerful if they drive the everyday operations of\\nyour teams in the tools they use every day. For example, you could build BI reports\\nfrom the DW and train sales reps to use them— or operationalize their analysis by\\nhaving a data analyst feed lead scores from the DW into a custom field in Salesforce.\\nAs another example, say your data science team calculates a propensity score and\\nstores it in a DW or data lake. Using reverse ETL, you could move that propensity\\nscore to an operational production database to serve customers personalized in-app\\nexperiences in real time. In practice, this means that the company can enhance the\\ncustomer’s experience, providing more relevant and tailored content, recommenda‐\\ntions, or offers based on how likely the customer is to buy a product. The app\\nbecomes more engaging and responsive to the customer’s individual preferences and\\nbehaviors.\\n128 | Chapter 9: Approaches to Data Ingestion', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9290f452-11e5-4d8b-b4cb-6ce7c6ddb600', embedding=None, metadata={'page_label': '128', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Batch Processing Versus Real-Time Processing\\nIn ETL and ELT, you have two choices for when and how often to extract data from\\nthe source system: batch processing and real-time processing. Let’s take a closer look\\nat each:\\nBatch processing\\nBatch processing is an efficient way to process large volumes of data. A set of simi‐\\nlar transactions from a source system are grouped together (“batched”) for a spe‐\\ncific period of time. At regular intervals, the system automatically “runs a job, ”\\nand the whole batch is copied from the source system to the destination (a data\\nlake or warehouse), typically during non-peak business hours. The main function\\nof a batch-processing system is to run batches at regularly scheduled times (for\\ninstance, overnight every night) or as needed.\\nY our electric bill is an example of batch processing. Y our electricity meter collects\\nyour consumption data over a set period (usually a month), and then the electric\\nutility processes the whole batch in the form of your monthly bill.\\nReal-time processing\\nReal-time data processing is a method of processing data continuously as it is col‐\\nlected, within a matter of seconds or milliseconds, providing near-instantaneous\\ninsights. A real-time system reacts quickly to new information in an event-based\\narchitecture. When it detects a new file or record in a source system, an event is\\ntriggered, and it copies the file or record to the destination.\\nIoT devices, social media messages, and financial trading systems all use real-\\ntime processing systems. For example, if there’s a suspicious credit card transac‐\\ntion, the bank needs to alert the cardholder without delay, which is why banks\\nuse real-time processing to detect fraud. Y ou wouldn’t want them to wait until the\\nnext day, after the overnight batch of transactions is executed!\\nTraffic apps are another good example. The navigation app Waze automatically\\nupdates traffic congestion levels in real time, using information acquired from\\nvarious mobile devices and road sensors in the area to suggest the optimal and\\nshortest path to reach your destination. Obviously, if you had to wait until a\\nnightly batch run to get the traffic info, it would be totally useless.\\nReal-time processing updates the target system immediately, so reports or queries\\nare never out-of-date. This allows quick detection of business operations that\\nneed immediate attention and updates to applications or reports that rely on real-\\ntime data.\\nRDWs have traditionally only used batch processing. Real-time data has become very\\npopular in the last few years. Systems that use it mainly store their data in a data lake,\\nBatch Processing Versus Real-Time Processing | 129', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='095edae0-2aed-473b-a47b-c069b4e5be2b', embedding=None, metadata={'page_label': '129', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='which is much more suited for real-time data, since it could include millions of events\\nper second.\\nEach method has its own set of advantages and trade-offs when used in data ware‐\\nhousing.\\nBatch Processing Pros and Cons\\nBatch processing is efficient for large volumes of data, as all data is processed at once\\nrather than individually. This also allows batch-processing tasks to be scheduled dur‐\\ning off-peak times, minimizing disruption to regular system use. Furthermore, batch\\nprocessing poses a lower risk of system failure as failed tasks can be retried without\\nsignificant repercussions.\\nThe drawbacks to batch processing include potential delays in data availability, since\\nit takes time to process large amounts of data at once. This could lead to underutiliza‐\\ntion of resources if not managed properly. Also, batch processing is not suitable for\\napplications that require real-time insights, as updates are larger and less frequent\\nand any changes in data or calculations require a complete rerun of the batch, which\\ncould impact performance.\\nReal-Time Processing Pros and Cons\\nIn contrast to batch processing, real-time processing provides up-to-the-minute\\ninsights, enabling immediate action based on data. It is particularly beneficial for sys‐\\ntems that require continuous updates and can effectively handle streaming data. Also,\\nreal-time processing is more flexible and responsive to changing business needs.\\nReal-time processing also has its drawbacks. It requires more system resources for\\ncontinuous processing and poses a higher risk of system failure. Handling errors and\\nrecovery in real-time systems can be complex and requires robust tools. Ensuring\\ndata consistency might be more challenging due to constant updates. Moreover, the\\ncosts associated with real-time processing can be higher due to the need for continu‐\\nous processing.\\nDeciding between batch processing and real-time processing means weighing factors\\nsuch as the type of data, your processing needs, and your tolerance for latency or\\ndelay. The choice typically involves balancing the demand for instantaneous data with\\nthe system’s capacity to allocate the necessary resources for real-time processing—\\noften referred to as latency tolerance. If a business process or system can afford a\\nslight delay in data access—in other words, if it has a high latency tolerance—batch\\nprocessing might be the appropriate approach. Conversely, if the need for immediate\\ndata is critical and the system is equipped to manage the resources required—indicat‐\\ning a low latency tolerance—real-time processing could be better.\\n130 | Chapter 9: Approaches to Data Ingestion', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='91d6ba4a-2b26-4ff4-8fb4-d15ddb270977', embedding=None, metadata={'page_label': '130', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Governance\\nData governance is the overall management of data in an organization. It involves\\nestablishing policies and procedures for collecting, storing, securing, transforming,\\nand reporting data. In particular, it is used to ensure that the organization is comply‐\\ning with legal and regulatory requirements. It also includes monitoring data quality\\nand accuracy—for instance, making sure data is properly cleaned and transformed.\\nGovernance should include a framework that defines the people and roles responsible\\nfor managing, maintaining, and using the data within an organization. One way to do\\nthat is by creating a data governance center of excellence (CoE). The CoE serves as a\\nhub for developing the organization’s data governance policies, procedures, and\\nstandards, and defining roles, responsibilities, and decision-making processes for\\ndata-related activities.\\nIt’s important to invest time up front in defining a data governance framework and\\nbuilding out your CoE before building any data warehouse solution. Too often,\\nprojects fail because no one has paid enough attention to data governance.\\nFigure 9-2  shows a data governance maturity model I created to help you identify\\nareas your organization should work on.\\nFigure 9-2. Data governance maturity model\\nData Governance | 131', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='28dba520-4d55-4bbe-bc1f-b85354b47c6b', embedding=None, metadata={'page_label': '131', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Summary\\nThis chapter delved into various techniques and strategies used to transport data\\nfrom diverse sources into a centralized storage system, arriving ready for processing\\nand analysis.\\nI compared two fundamental methods: ETL (extract, transform, load) and ELT\\n(extract, load, transform). The ETL approach entails extracting data from sources,\\ntransforming it to fit the operational needs, and then loading it into the end system.\\nConversely, ELT loads data into the system before transforming it. Both methodolo‐\\ngies have their advantages and are suited to different scenarios, depending on data\\nvolumes, complexity, performance needs, and architectural considerations.\\nNext, I introduced the concept of reverse ETL, a technique that allows businesses to\\ntransfer data from their data warehouse or lake back into operational systems. This\\nprocess is essential for feeding insights derived from data analyses back into business\\napplications for actionable outcomes.\\nY ou learned about two contrasting approaches to processing data: batch and real-time\\nprocessing. Then we moved into the critical aspect of data governance, the discipline\\nof managing data quality, privacy, security, and compliance. Data governance frame‐\\nworks help maintain integrity and reliability throughout the data lifecycle, ensuring\\nthat data is accurate, consistent, and secure.\\nChoosing the right data ingestion strategy is a significant business decision that parti‐\\nally determines how well your organization can leverage its data for business decision\\nmaking and operations. The stakes are high; the wrong strategy can lead to poor data\\nquality, performance issues, increased costs, and even regulatory compliance\\nbreaches. So think through your approach to data ingestion carefully, not just as a\\ntechnical necessity but as a business necessity.\\n132 | Chapter 9: Approaches to Data Ingestion', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dff3a13e-afe7-4bfe-a325-eaba04f9f6c1', embedding=None, metadata={'page_label': '132', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='PART III\\nData Architectures\\nNow that you have a good understanding of the data architecture concepts, it’s time\\nto get into the meat of the book. The five chapters of Part III cover four architectures,\\nin order of when they first appeared: the modern data warehouse, data fabric, data\\nlakehouse, and data mesh.\\nThese are my interpretations of the data architectures; others may have different\\nopinions. In the end, your solution may even be a combination of these architectures.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dfc22f8b-0023-454b-b227-b429286f58f1', embedding=None, metadata={'page_label': '133', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9dc6501f-b9ef-4d20-89b5-d87faccd7c8b', embedding=None, metadata={'page_label': '134', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 10\\nThe Modern Data Warehouse\\nIn Part II  of this book, you learned about relational data warehouses (RDWs) and\\ndata lakes, two key components of the data management landscape. Now, let’s con‐\\nsider the bustling world of modern business. Every day, organizations must sift\\nthrough immense amounts of data to gain insights, make decisions, and drive\\ngrowth. Imagine a city supermarket switching from traditional databases to a modern\\ndata warehouse (MDW). Managers can now access real-time inventory data, predict\\nshopping trends, and streamline the shopping experience for their customers. That’s\\nthe power of an MDW . It blends the best of both worlds: the structure of RDWs and\\nthe flexibility of data lakes.\\nWhy should you care about MDWs? Because they are at the heart of our rapidly\\nevolving data ecosystem, enabling organizations to harness the information they need\\nto innovate and compete. In this chapter, I’ll clarify what MDWs are and what you\\ncan achieve with them, and I’ll show you some important considerations to keep in\\nmind. We’ll journey through the architecture, functionality, and common stepping\\nstones to an MDW , concluding with an insightful case study. Let’s dive into the world\\nof modern data warehouses where data is more than just numbers—it’s the fuel for\\nsuccess.\\nThe MDW Architecture\\nFigure 10-1 illustrates the hybrid nature of the MDW , which combines an RDW with\\na data lake to create an environment that allows for flexible data manipulation and\\nrobust analytics.\\n135', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5b353399-519f-4201-a736-7ce5b690d9c6', embedding=None, metadata={'page_label': '135', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As Chapter 4  detailed, an RDW operates on a top-down principle. Building one\\ninvolves significant preparation in building the warehouse before you do any data\\nloading (schema-on-write). This approach helps with historical reporting by enabling\\nanalysts to delve into descriptive analytics (unraveling what happened) and diagnos‐\\ntic analytics (probing why it happened).\\nFigure 10-1. The full range of analytics facilitated by a modern data warehouse\\nIn contrast, a data lake is defined by its bottom-up philosophy, as described in Chap‐\\nter 5 . Minimal up-front work is required to begin utilizing the data (schema-on-\\nread), allowing rapid deployment of machine learning models to explore predictive\\nanalytics (forecasting what will happen) and prescriptive analytics (prescribing solu‐\\ntions to make desired outcomes occur).\\n136 | Chapter 10: The Modern Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='46a8bce8-b42e-41a2-ba45-d4347aff85e5', embedding=None, metadata={'page_label': '136', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Within an MDW architecture, a data lake is not merely a repository for storing vast\\namounts of information and training ML models; it’s a dynamic component, respon‐\\nsible for data transformation and the other purposes explored in Chapter 5. It may\\nact, for instance, as a receiving hub for streaming data, a gateway for users to do quick\\ndata exploration and reporting, and a centralized source to maintain a single version\\nof truth.\\nAn essential distinguishing feature of the MDW architecture is that at least some of its\\ndata must be replicated to an RDW . Without this duplication, it would be a data lake‐\\nhouse architecture—a topic for Chapter 12.\\nIn the mid-2010s, many organizations tried to use the data lake for all data use cases\\nand bypass using an RDW altogether. After that approach failed, the MDW became\\nthe most popular architecture, driven by an exponential increase in data volume,\\nvariety, and velocity. Organizations in many sectors, including healthcare, finance,\\nretail, and technology, recognized the need for a more agile and scalable approach to\\ndata storage and analysis. Their RDWs were no longer sufficient to handle the chal‐\\nlenges of big data, but MDWs offered a flexible integration of RDWs and data lakes.\\nKey players emerged as leading providers—Microsoft with Azure Synapse Analytics,\\nAmazon with Redshift, Google with BigQuery, and Snowflake—revolutionizing the\\nway businesses access and utilize their data.\\nAs streaming data, large data, and semi-structured data became even more popular,\\nlarge enterprises started to use the data fabric and data lakehouse architectures dis‐\\ncussed in Chapters 11 and 12. However, for customers with not much data (say, under\\n10 terabytes), the MDW is still popular and will likely stay that way, at least until data\\nlakes can operate as well as RDWs can.\\nIn fact, there are still companies building cloud solutions that aren’t using data lakes\\nat all, mostly for use cases where the company has small amounts of data and is\\nmigrating from an on-prem solution that did not have a data lake. I still see this as an\\nacceptable approach, with the caveat that the company has to be absolutely sure that\\nthe data warehouse will not grow much (which is very hard to say for certain). Also, if\\nyou have such a small amount of data, it may be feasible to go with a cheaper sym‐\\nmetric multiprocessing (SMP) solution for your RDW than a massively parallel pro‐\\ncessing (MPP) solution (see Chapter 7).\\nNow let’s dig into the details of the MDW . Figure 10-2 shows a typical flow of data\\nthrough the data lake and RDW in an MDW .\\nThe MDW Architecture | 137', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ff03ba0d-5ea3-45f3-a14d-729be39a2de9', embedding=None, metadata={'page_label': '137', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 10-2. Overview of the journey that data takes through a modern data warehouse\\narchitecture\\nAs data travels through the MDW architecture, it goes through five stages, which are\\nnumbered in Figure 10-2. Let’s follow the data through each stage to give you a tour\\nof the MDW architecture:\\nStep 1: Ingestion\\nAn MDW can handle just about any type of data, and it can come from many\\nsources, both on-prem and in the cloud. The data may vary in size, speed, and\\ntype; it can be unstructured, semi-structured, or relational; it can come in batches\\nor via real-time streaming; it can come in small files or massive ones. This variety\\ncan be a challenge during data ingestion.\\nStep 2: Storage\\nOnce the data is ingested, it lands in a data lake that contains all the various lay‐\\ners explained in Chapter 5, allowing end users to access it no matter where they\\nare (provided they have access to the cloud). Every cloud provider offers unlimi‐\\nted data lake storage, and the cost is relatively very cheap. Baked into the data\\n138 | Chapter 10: The Modern Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='700198c6-59de-403a-ab59-88afbc242da6', embedding=None, metadata={'page_label': '138', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='lake are high availability and robust disaster recovery, along with many options\\nfor security and encryption.\\nStep 3: Transformation\\nThe essence of a data lake is that it’s a storage hub. However, to truly make sense\\nof and work with the data, you need some muscle—which is where computing\\npower comes in. A significant advantage of the MDW is its clear distinction\\nbetween storing data (in the data lake) and processing it (using computing\\npower). Keeping the two separate frees you to pick and choose from a variety of\\ncomputing tools, from cloud providers or other sources. These tools, designed to\\nfetch and understand data from the lake, can read data in various formats and\\nthen organize it into a more standardized one, like Parquet. This separation\\noffers flexibility and enhances efficiency.\\nThe computing tool then takes the files from the conformed layer, transforms the\\ndata (enriching and cleaning it), and stores it in the cleaned layer of the data lake.\\nFinally, the computing tool takes the files from the enriched layer and performs\\nmore transformations for performance or ease of use (such as joining data from\\nmultiple files and aggregating it) and then writes it to the presentation layer in\\nthe data lake.\\nStep 4: Data modeling\\nReporting directly from the data in a data lake can be slow, unsecure, and confus‐\\ning for end users. Instead, you can copy all or some of the data from the data lake\\ninto a relational data warehouse. This means you will create a relational model\\nfor the data in the RDW , where the model is usually in third normal form (see\\nChapter 8). Y ou may also want to copy it into a star schema within the RDW for\\nperformance reasons and simplification.\\nStep 5: Visualization\\nOnce the data is in the RDW in an easy-to-understand format, business users can\\nanalyze it using familiar tools, such as reports and dashboards.\\nFor the ingestion stage, you will need to determine how often to extract data and\\nwhether to use an incremental or a full extract (see Chapter 4). Keep in mind the size\\nand bandwidth of the pipeline from the data sources to the cloud where the data lake\\nresides. If you need to transfer large files and your bandwidth is small, you might\\nneed to upload data multiple times a day in smaller chunks instead of in one big\\nchunk once a day. Y our cloud provider might even have options for purchasing more\\nbandwidth to the internet.\\nAt various steps along this journey, data scientists can use machine learning to train\\nand build models from the data in the MDW . Depending on their needs, they can\\ntake data from the raw, cleaned, or presentation layers in the data lake, from a\\nThe MDW Architecture | 139', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ab7822f9-f4ac-4210-b6b1-8ca90ad49ea9', embedding=None, metadata={'page_label': '139', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='sandbox layer in the data lake (a dedicated space to do data experimentation, explora‐\\ntion, and preliminary analysis without affecting the other layers), or from the RDW .\\nThere are some exceptions to the journey I’ve just laid out in which data flows\\nthrough the MDW architecture, depending on the characteristics of the source data.\\nFor example, not all data in the presentation layer of the data lake needs to be copied\\nto the RDW , especially with newer tools making it easier to query and report off data\\nin a data lake. This means that in the visualization stage, the business user doesn’t\\nhave to go to the RDW for all the data—they can access the data in the data lake that\\nhas not been copied to the RDW .\\nAnother exception is that not all source data has to be copied to the data lake. In\\nsome cases, particularly new projects being built in the cloud, it’s faster to copy data\\nfrom the source right to the RDW , bypassing the data lake, especially for structured\\n(relational) source data. For instance, reference tables that are used as dimension\\ntables don’t need to be cleaned, are full extracts, and can easily be retrieved from the\\nsource if an ETL package needed to be rerun. After all, it could be a lot of work to\\nextract data from a relational database, copy it to the data lake (losing valuable meta‐\\ndata for that data, such as data types, constraints, and foreign keys), only to then\\nimport it into another relational database (the data warehouse).\\nThis is particularly true of a typical migration from an on-prem solution that is not\\nusing a data lake and has many ETL packages that copy data from a relational data‐\\nbase to an RDW . To reduce the migration effort, you’ d need to modify the existing\\nETL packages only slightly—changing just the destination source. Over time, you\\ncould modify all the ETL packages to use the data lake. To get up and running\\nquickly, however, you might only modify a few slow-running ETL packages to use the\\ndata lake and do the rest later.\\nSource data that bypasses the data lake would miss out on some of its benefits—in\\nparticular, being backed up, in case you need to rerun the ETL package. Bypassing\\ncan also place undue strain on the RDW , which then becomes responsible for data\\ncleansing. Additionally, the people using the data lake will find that data missing, pre‐\\nventing the data lake from serving as the single source of truth.\\nPros and Cons of the MDW Architecture\\nMDWs offer numerous benefits and some challenges. The pros include:\\nIntegration of multiple data sources\\nMDWs can handle both structured and unstructured data from various sources,\\nincluding RDWs and data lakes, offering a comprehensive view of the\\ninformation.\\n140 | Chapter 10: The Modern Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a15a3283-0d42-4eb8-bd82-e709f81508bd', embedding=None, metadata={'page_label': '140', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Scalability\\nMDWs are designed to grow with the business, easily scaling to handle increased\\ndata loads.\\nReal-time data analysis\\nMDWs facilitate real-time analytics, allowing businesses to make timely deci‐\\nsions based on current data.\\nImproved performance\\nBy leveraging advanced technology and optimized data processing, MDWs can\\noffer faster query performance and insights retrieval.\\nFlexibility\\nMDWs provide flexibility in data modeling, allowing for both traditional struc‐\\ntured queries and big data processing techniques.\\nEnhanced security\\nThe leading MDW providers implement strong security measures to protect sen‐\\nsitive data.\\nSome of the cons of the MDW architecture are:\\nComplexity\\nImplementing and managing an MDW can be complex, especially in hybrid\\narchitectures that combine different types of data storage and processing.\\nCost\\nThe initial setup, ongoing maintenance, and scaling can be expensive, particu‐\\nlarly for small- to medium-sized businesses. Plus, there will be extra costs for\\nstorage and for additional data pipelines to create multiple copies of the data (for\\nboth the RDW and data lake).\\nSkill requirements\\nUtilizing an MDW to its full potential requires specialized knowledge and skills,\\npotentially leading to hiring challenges or additional training costs.\\nPotential data silos\\nWithout proper integration and governance, MDWs can lead to data silos, or\\nplaces where information becomes isolated and difficult to access across the\\norganization.\\nCompliance challenges\\nMeeting regulatory compliance in an environment that handles diverse data\\ntypes and sources can be a challenging aspect of managing an MDW .\\nPros and Cons of the MDW Architecture | 141', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dfecc0c8-fdd6-4c0e-a7c2-d32bc8559e26', embedding=None, metadata={'page_label': '141', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Vendor dependency\\nIf you’re using a cloud-based MDW service, it may mean a dependency on a par‐\\nticular vendor, which could lead to potential lock-in and limit flexibility in the\\nfuture.\\nAs the data is copied within an MDW and moves through the data lake and into the\\nRDW , it changes format and becomes easier and easier to use. This helps provide\\nuser-friendly self-service BI, where end users can build a report simply by dragging\\nfields from a list to a workspace without joining any tables. The IT department must\\ndo some extra up-front work to make the data really easy to use, but that’s often\\nworth it, since it releases that department from being involved in (and likely a bottle‐\\nneck for) all report and dashboard creation.\\nCombining the RDW and Data Lake\\nIn an MDW , the data lake is used for staging and preparing data, while the RDW is for\\nserving, security, and compliance. Let’s look more closely at where the functionality\\nresides when you’re using both an RDW and data lake.\\nData Lake\\nIn a data lake, data scientists and power users—specifically those with higher techni‐\\ncal skills—will have exclusive access, due to the complex folder-file structure and sep‐\\naration of metadata. Data lakes can be difficult to navigate, and access may require\\nmore sophisticated tools. The data lake’s functions extend to batch processing, where\\ndata is transformed in batches, and real-time processing, which serves as the landing\\nspot for streaming data (see Chapter 9). The data lake is also used to refine and clean\\ndata, providing a platform to use as much compute power as needed, with multiple\\ncompute options to select from. It also accommodates ELT workloads.\\nThe data lake, in this framework, serves as a place to store older or backup data,\\ninstead of keeping it in the RDW . It’s also a place to back up data from the data ware‐\\nhouse itself. Users can easily create copies of data in the data lake for sandbox pur‐\\nposes, allowing others to use and “play” in the data. The data lake also provides an\\nopportunity to view and explore data if you’re not sure what questions to ask of it;\\nyou can gauge its value before copying it to the data warehouse. It facilitates quick\\nreporting and access to data, especially since it’s schema-on-read so data can land\\nswiftly.\\nRelational Data Warehouse\\nBusiness people often turn to RDWs as the place for non-technical individuals to\\naccess data, especially if they are accustomed to relational databases. These databases\\noffer low latency, enabling much faster querying, especially if you’re using MPP\\n142 | Chapter 10: The Modern Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='81981069-4dd7-4633-8cfe-2b9643b344db', embedding=None, metadata={'page_label': '142', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='technology (see Chapter 7). They can handle a large number of table joins and com‐\\nplex queries, and they’re great for running interactive ad hoc queries because of the\\nfast performance MPP technology provides. This allows you to fine-tune queries con‐\\ntinuously, without long waits. RDWs can also support many concurrent users run‐\\nning queries and reports simultaneously.\\nThe additional security features of RDWs in the MDW context include options such\\nas row-level and column-level security. RDWs also provide plenty of support for\\ntools—and since they have been around for much longer than data lakes, many more\\ntools are available. Data lakes don’t have the performance to support dashboards, but\\nyou can run dashboards against RDWs, thanks to their superior performance down\\nto the millisecond. A forced metadata layer above the data requires more up-front\\nwork but makes self-service BI much easier. Generally, when you’re building out an\\nRDW , you already know what questions you want to ask of the data and can plan for\\nthem, making the RDW a powerful and versatile tool.\\nStepping Stones to the MDW\\nBuilding an MDW is a critical but long and arduous endeavor that demands consid‐\\nerable investment in technology, human resources, and time. It represents the evolu‐\\ntion of data management, where integration, accessibility, security, and scalability are\\nparamount. While that process is starting, most organizations require interim solu‐\\ntions to address their current data warehousing needs. These solutions, which func‐\\ntion as stepping stones to the fully operational MDW , ensure that the organization\\ncan still derive value from its data in the meantime and remain agile and responsive\\nto business needs. They are not merely temporary fixes but vital components in a\\nstrategic migration.\\nThree common types of stepping-stone architectures are:\\n• EDW augmentation\\n• Temporary data lake plus EDW\\n• All-in-one\\nEach of these approaches offers unique benefits and challenges. Their suitability as\\npathways to an MDW can vary depending on the organization’s needs, existing infra‐\\nstructure, budget, and strategic objectives. Let’s look at each one more closely.\\nEDW Augmentation\\nThis architecture is usually seen when a company that has had a large on-prem EDW\\nfor a long time wants to get value out of its data, but its EDW can’t handle “big data”\\ndue to a lack of storage space, compute power, available time in the maintenance win‐\\ndow for data loading, or general support for semi-structured data.\\nStepping Stones to the MDW | 143', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dfd894cc-0aa2-4d58-8eb7-4351639e8690', embedding=None, metadata={'page_label': '143', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='How it works\\nY ou create a data lake in the cloud and copy the big data to it. Users can query and\\nbuild reports from the data lake, but the primary data remains stored in the EDW , as\\nshown in Figure 10-3.\\nFigure 10-3. EDW augmentation architecture\\nBenefits\\nEDW augmentation architecture enhances data capacity and flexibility by leveraging\\na cloud data lake alongside an existing EDW . This strategy offers scalability and cost\\nefficiency, creating opportunities for innovative analytics while maintaining current\\nstructures. It provides a balanced approach that aligns with business growth and\\ncontinuity.\\nChallenges\\nThis architecture does present some potential difficulties:\\n• If you need to combine data from the EDW with data in the data lake, you have\\nto copy the data from the EDW to the data lake.\\n• Y our existing query tools may not work against a data lake.\\n• Y ou’ll need a new form of compute to clean the data in the data lake, which may\\nbe expensive and require new skill sets.\\n• This architecture does not help you offload any of the EDW’s workload.\\nMigration\\nThis architecture can be the start of a phased approach to migrating the on-prem\\nEDW to the cloud. After the architecture is up and running with the big data, you can\\nbegin migrating the data in the on-prem EDW to the data lake. Data from source\\n144 | Chapter 10: The Modern Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3aecce1c-983b-4d0f-8e9b-8839d1ee87b0', embedding=None, metadata={'page_label': '144', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='systems goes first to the data lake and then, if needed, to a new RDW in the cloud,\\nwhich is part of a true MDW .\\nTemporary Data Lake Plus EDW\\nCompanies usually use a temporary data lake with an EDW when they have an EDW\\nand need to incorporate big data, but transforming it would take too much time.\\nThese companies want to reduce the EDW maintenance window by offloading data\\ntransformations.\\nHow it works\\nThis architecture uses a data lake, but solely as a temporary staging and refining area.\\nIt is not used for querying or reporting; that’s all done from the EDW , as Figure 10-4\\nshows. This limited scope means the data lake can be incorporated much faster.\\nSometimes you can even copy EDW data to the data lake, refine it, and move it back\\nto the EDW .\\nFigure 10-4. Using an EDW with a temporary data lake\\nBenefits\\nThis architecture enables offloading data processing to the data lake, alleviating strain\\non the EDW and enhancing overall performance. Using other types of compute on\\nthe data lake can provide more speed and functionality, allowing for flexible handling\\nof big data. This strategy offers a cost-effective solution for incorporating large data‐\\nsets without disrupting existing EDW operations, making it an agile and scalable\\napproach.\\nChallenges\\nAlthough you’re using a data lake, this architecture does not give you the full benefits\\nof a data lake (discussed in Chapter 5).\\nStepping Stones to the MDW | 145', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf2d0739-a1f6-4e36-9fda-743b55bf5461', embedding=None, metadata={'page_label': '145', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Migration\\nWith just a few modifications, this architecture can evolve into a full-blown MDW , so\\nit’s a good stepping stone.\\nAll-in-One\\nAll-in-one architecture is typically adopted by organizations seeking a streamlined\\napproach to data handling, such as startups or small businesses. It may be favored to\\ndo quick prototyping or to achieve specific short-term goals. It is also a good option\\nwhen the primary users are technical experts who prefer an integrated platform.\\nHow it works\\nAll reporting and querying are done using the data lake, as Figure 10-5  shows. No\\nRDW is involved. (All-in-one is closely tied to the data lakehouse architecture dis‐\\ncussed in Chapter 12.)\\nFigure 10-5. All-in-one architecture\\nBenefits\\nThis approach allows for quick implementation and immediate results, benefits that\\nare particularly attractive for technical teams aiming for rapid progress. By consoli‐\\ndating reporting and querying within the data lake, it simplifies the architecture,\\npotentially reducing maintenance and integration complexities. This approach can be\\nmore agile and adaptable, accommodating various data types and fostering a more\\nstreamlined development process.\\nChallenges\\nHaving no RDW involves trade-offs with respect to performance, security, referential\\nintegrity, or user friendliness.\\n146 | Chapter 10: The Modern Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f492cb1b-1b2e-4c00-a54e-5edaba87218b', embedding=None, metadata={'page_label': '146', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Migration\\nFor some companies and use cases, such as when the data in the data lake will be used\\nonly by data scientists, the data-lake-only approach may be fine. But to make it a step‐\\nping stone to an MDW , you’ll need to add an RDW .\\nCase Study: Wilson & Gunkerk’s Strategic Shift to an MDW\\nWilson & Gunkerk, a fictitious midsized pharmaceutical company located in the US\\nMidwest, had been relying on an on-prem solution to handle its data management\\nrequirements. With data volumes consistently under 1TB, the company never felt the\\nneed to transition to more advanced solutions like a data lake or an MDW .\\nChallenge\\nThe company’s need for actionable insights into market trends and customer behav‐\\niors became more pressing as its business environment became more competitive.\\nThe existing data system started showing its limitations, and the organization faced a\\ndilemma: whether to upgrade to a more comprehensive data lake solution or an\\nMDW . Another critical consideration was uncertainty about future data growth, par‐\\nticularly with new research and development projects in the pipeline.\\nSolution\\nAfter a thorough analysis, Wilson & Gunkerk decided to implement an MDW solu‐\\ntion in the cloud. A few factors that led to this decision were:\\n• The existing data volume was well under 1TB, and projected growth was within\\n10TB.\\n• The migration to a cloud-based MDW offered a smoother transition from the\\nexisting on-prem solution without the need to adapt to an entirely new technol‐\\nogy like a data lake.\\n• An MDW would be cost-effective. Wilson & Gunkerk opted for an SMP solution\\nrather than a more expensive MPP option, since its data needs were modest and\\nmanageable.\\n• The chosen MDW provided robust performance and sufficient scalability to han‐\\ndle anticipated data growth within acceptable limits.\\nCase Study: Wilson & Gunkerk’s Strategic Shift to an MDW | 147', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f39141e4-33a9-4ef3-af3b-5c17cd8b5b01', embedding=None, metadata={'page_label': '147', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Outcome\\nThe transition to the MDW was completed within a few months with minimal dis‐\\nruption to ongoing business processes. Wilson & Gunkerk realized immediate\\nbenefits:\\nEnhanced analytics\\nThe new system facilitated better data analytics, enabling more informed deci‐\\nsion making. The company moved beyond simple historical analysis into predic‐\\ntive analytics. Before the MDW , insights were limited to past trends. The new\\nsystem facilitated the integration of diverse data sources and the use of sophisti‐\\ncated data-mining algorithms. This allowed the company to create predictive\\nmodels, such as forecasting drug efficacy based on patient demographics and\\ngenetic factors. The transition led to more nuanced decision making, offering a\\nmore precise approach to drug development and personalized medical solutions.\\nCost savings\\nThe SMP solution provided excellent performance at a reduced cost compared to\\npotential MPP solutions or a full-fledged data lake.\\nFuture-proofing\\nWhile keeping the door open to future transitions, the MDW allowed for gradual\\ngrowth and adaptability without overinvestment in technology the company\\ndidn’t immediately need.\\nWilson & Gunkerk’s case illustrates a balanced approach to data management solu‐\\ntions. For companies dealing with modest data volumes and seeking a scalable and\\ncost-effective solution, MDWs can still present an appealing option. Each organiza‐\\ntion’s needs and growth trajectory are unique, so a careful analysis of present and\\nfuture requirements is vital to select the most appropriate data architecture.\\nSummary\\nThis chapter started with a detailed description of the MDW and then described the\\nfive key stages of data’s journey through an MDW: ingestion, storage, transformation,\\nmodeling, and visualization. These stages represent the lifecycle of data as it is ini‐\\ntially captured, securely stored, refined for usability, modeled for insights, and finally\\nvisualized for easy interpretation and decision making.\\nI detailed the benefits of MDW , including aspects such as integration, scalability, real-\\ntime analysis, and security, while considering challenges like complexity, cost, and\\nvendor dependency. We then delved into the combination of traditional RDWs with\\ndata lakes, highlighting the unique capabilities this blend provides and the enhanced\\nflexibility it offers in dealing with various data types and structures.\\n148 | Chapter 10: The Modern Data Warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='420eedbd-80da-49c5-a58a-3cb7a96b7f50', embedding=None, metadata={'page_label': '148', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='I then provided a detailed exploration into three transitional architectures that com‐\\npanies often deploy as temporary solutions for their data-warehousing needs: enter‐\\nprise data warehouse augmentation, temporary data lake plus EDW , and all-in-one.\\nEach model serves as an interim measure while organizations work toward imple‐\\nmentation of a full-fledged MDW .\\nI concluded with a case study of Wilson & Gunkerk, a midsized pharmaceutical com‐\\npany, and its strategic shift to an MDW .\\nThe coming chapters will delve deeper into the data fabric and data lakehouse archi‐\\ntectures, unpacking their benefits and potential use cases in the evolving landscape of\\ndata management.\\nSummary | 149', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e726256c-e825-4999-8ed2-0c656bbb24b2', embedding=None, metadata={'page_label': '149', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b8544bf0-504f-4eec-8b6d-721b60566a29', embedding=None, metadata={'page_label': '150', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 11\\nData Fabric\\nThe data fabric architecture is an evolution of the modern data warehouse (MDW)\\narchitecture: an advanced layer built onto the MDW to enhance data accessibility,\\nsecurity, discoverability, and availability. Picture the data fabric weaving its way\\nthroughout your entire company, accumulating all of the data and providing it to\\neveryone who needs it, within your company or even outside it. It’s an architecture\\nthat can consume data no matter the size, speed, or type. The most important aspect\\nof the data fabric philosophy is that a data fabric solution can consume any and all\\ndata within the organization.\\nThat is my definition of a data fabric; others in the industry define it differently. Some\\neven use the term interchangeably with modern data warehouse ! For instance, the\\nconsulting firm Gartner starts with a similar definition, writing that the data fabric is\\na design concept that serves as an integrated layer (fabric) of data and connecting pro‐\\ncesses. A data fabric utilizes continuous analytics over existing, discoverable and infer‐\\nenced metadata assets to support the design, deployment and utilization of integrated\\nand reusable data across all environments, including hybrid and multi-cloud\\nplatforms.\\nWhere Gartner’s view diverges from mine is in the view that data virtualization\\n(which you learned about in Chapter 6) is a major piece of the data fabric technology\\nthat reduces the need to move or copy data from siloed systems. Gartner envisions an\\n“intelligent data fabric” that leverages knowledge graphs and AI and ML for automat‐\\ning data discovery and cataloging, finding data relationships, orchestrating and inte‐\\ngrating data, and auto-discovering metadata. However, much of the needed\\ntechnology does not yet exist, and so far, neither does a data fabric that would satisfy\\nGartner’s definition. I’m hopeful that technology will be available one day, but for\\nnow I recommend focusing on data fabric architectures as they exist today.\\n151', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd7b4ed9-2da0-476f-b283-73e8ee620973', embedding=None, metadata={'page_label': '151', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data fabric is the most widely used architecture for new solutions being built in the\\ncloud, especially for large amounts of data (over 10 terabytes). A lot of companies are\\n“upgrading” their MDW architecture into a data fabric by adding additional technol‐\\nogy, especially real-time processing, data access policies, and a metadata catalog. This\\nchapter outlines eight technology features that take an MDW into data fabric terri‐\\ntory. There’s some ambiguity here; if you add three of those features, do you have an\\nMDW or a data fabric? There is no bright line between the two. Personally, I call it a\\ndata fabric architecture if it uses at least three of those eight technologies. Many peo‐\\nple use none of them but nonetheless call their solution a data fabric because they\\nabide by the data fabric philosophy (I think this is perfectly fine).\\nThis chapter gives you an overview of the data fabric architecture and its eight key\\ncomponents: data access policies, a metadata catalog, master data management\\n(MDM), data virtualization, real-time processing, APIs, services, and products.\\nThe Data Fabric Architecture\\nThe data fabric architecture contains all of the components of the MDW but adds\\nseveral features. Figure 11-1 shows a diagram of this architecture, with the stages of\\nthe data’s journey labeled by number like they were in Figure 10-2. Data in a data fab‐\\nric follows the same process as in an MDW; as you may recall from Chapter 10, those\\nstages are (1) ingestion, (2) storage, (3) transformation, (4) modeling, and (5)\\nvisualization.\\nSo what does a data fabric offer above and beyond the MDW? This section will take a\\nlook at its added features.\\n152 | Chapter 11: Data Fabric', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b417be44-b77b-4f28-9d3c-94a455f6db98', embedding=None, metadata={'page_label': '152', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 11-1. An overview of the journey data takes through a data fabric architecture\\nThe Data Fabric Architecture | 153', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e734dd2e-5dce-4938-9062-edf2464c1bc2', embedding=None, metadata={'page_label': '153', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Access Policies\\nData access policies are the key to data governance. They comprise a set of guidelines,\\nrules, and procedures that control who has access to what information, how that\\ninformation may be used, and when access can be granted or denied within an orga‐\\nnization. They help ensure the security, privacy, and integrity of sensitive data, as well\\nas compliance with laws and regulations, such as the UK’s General Data Protection\\nRegulation (GDPR) and the US’s Health Insurance Portability and Accountability Act\\n(HIPAA). These policies typically cover topics like data classification, user authentica‐\\ntion and authorization, data encryption, data retention, data backup and recovery,\\nand data disposal.\\nAll data requests within an organization must adhere to its established data access\\npolicies and must be processed through specific mechanisms, such as APIs, drivers,\\nor data virtualization, ensuring compliance with regulations such as HIPAA. For\\ninstance, a healthcare organization’s data access policies might require that only\\nauthorized medical staff have access to patient medical records. These policies should\\noutline procedures for verifying staff credentials and what uses of the information are\\nacceptable. To facilitate this controlled access, the organization might implement\\nAPIs that interface with secure authentication systems. With this security in place,\\nwhen a health care provider requests a patient’s record, the API checks the provider’s\\ncredentials against the access policies before granting access.\\nMetadata Catalog\\nData fabrics include a metadata catalog: a repository that stores information about\\ndata assets, including their structure, relationships, and characteristics. It provides a\\ncentralized, organized way to manage and discover data, making it easier for users to\\nfind and understand the data they need. For example, say a user searches the meta‐\\ndata catalog for “customer. ” The results will include any files, database tables, reports,\\nor dashboards that contain customer data. Making it possible to see what ingestion\\nand reporting has already been done helps everyone avoid duplicate efforts.\\nAn important part of the catalog is data lineage, a record of the history of any given\\npiece of data, including where it came from, how it was transformed, and where it is\\nstored. Data lineage is used to comply with regulations such as data privacy laws. It’s\\nalso where you can trace the origin of data and the transformations it has undergone,\\nwhich is an important part of understanding how it was created and how reliable it is.\\nY ou wouldn’t want to base important decisions on data unless you knew you could\\ntrust it! For example, if a user asks about a particular value in a report, you can look\\nup its lineage in the metadata catalog to show how that value came about.\\n154 | Chapter 11: Data Fabric', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6a377ccf-7c8b-4725-a227-20046d49cbc8', embedding=None, metadata={'page_label': '154', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Master Data Management\\nAs you learned in Chapter 6, MDM is the process of collecting, consolidating, and\\nmaintaining consistent and accurate data from various sources within a company to\\ncreate a single authoritative source for master data. This data is typically nontransac‐\\ntional and describes the key entities of a company, such as customers, products, sup‐\\npliers, and employees.\\nMDM helps companies make informed decisions and avoid data-related problems\\nsuch as duplicate records, inconsistencies, and incorrect information.\\nData Virtualization\\nY ou also encountered data virtualization in Chapter 6. To refresh your memory, data\\nvirtualization is a software architecture that allows applications and end users to\\naccess data from multiple sources as if it were stored in a single location. The virtual‐\\nized data is stored in a logical layer that acts as an intermediary between the end users\\nand the data sources. That virtual layer is a single point of access that abstracts the\\ncomplexity of the underlying data sources, making it easy for end users to access and\\ncombine data from multiple sources in real time without needing to copy or physi‐\\ncally integrate it.\\nUsing data virtualization does not mean you need to connect all data sources. In most\\ncases, it’s used only for a small number of situations, with most data still being cen‐\\ntralized. Some in the industry see data virtualization as a definitive feature of a data\\nfabric, arguing that an architecture is not a data fabric without virtualization; I disa‐\\ngree. In my definition of data fabric, virtualization is optional.\\nReal-Time Processing\\nAs you saw in Chapter 9, real-time processing refers to processing data and producing\\nimmediate results as soon as that data becomes available, without any noticeable\\ndelay. This allows you to make decisions based on up-to-date information (for exam‐\\nple, traffic information while driving).\\nAPIs\\nInstead of relying on connection strings, APIs provide data in a standardized way\\nfrom a variety of sources—such as a data lake or the RDW—without sharing the par‐\\nticulars of where the data is located. If the data is moved, you only need to update the\\nAPI’s internal code; none of the applications calling the API are affected. This makes\\nit easier to take advantage of new technologies.\\nThe Data Fabric Architecture | 155', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7acff2c0-7aa1-4505-b536-d4b3664666dc', embedding=None, metadata={'page_label': '155', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='APIs can also incorporate multiple types of security measures. They can also provide\\nflexibility with security filtering, providing fine-grained control over which users can\\naccess which data.\\nServices\\nThe data fabric can be built in “blocks, ” which make it easier to reuse code. For exam‐\\nple, there may be others within your company who do not have a use for the full data\\nfabric, but who do need the code you created to clean the data or to ingest it. Y ou can\\nmake that code generic and encapsulate it in a service that anyone can use.\\nProducts\\nAn entire data fabric can be bundled as a product and sold. This would be particu‐\\nlarly attractive if made specifically for an industry, such as healthcare.\\nWhy Transition from an MDW to a\\nData Fabric Architecture?\\nThere are several reasons organizations decide to transition from an MDW to a data\\nfabric. While MDWs have traditionally been the foundation for many businesses’\\ndata infrastructures, they can be rigid and are sometimes hard to scale as data types\\ncontinue to evolve rapidly. In contrast, a data fabric is designed with scalability in\\nmind. Its inherent flexibility allows it to adapt readily to various data types and sour‐\\nces, making it resilient to current data demands and, to some extent, future-proof.\\nIn addition, the sheer variety of data sources can be overwhelming. A data fabric can\\nweave varied sources together seamlessly, offering a singular, unified view of the data\\nthat makes it much easier and more efficient to handle multifaceted data streams.\\nIn a swiftly changing business environment, the ability to process data in real time\\nand derive immediate insights is crucial. A data fabric meets this need by supporting\\nreal-time data processing, granting businesses instant access to the latest data.\\nFurthermore, as concerns about data breaches increase and the emphasis on adhering\\nto data protection regulations intensifies, data security and governance become even\\nmore critical. A data fabric addresses these issues with advanced access policies, pro‐\\nviding a more secure data environment. It also enhances data governance by restrict‐\\ning access to properly authorized users, protecting sensitive information from\\npotential threats. And for multinational companies that operate across different juris‐\\ndictions, each with distinct data protection regulations, a data fabric’s advanced gov‐\\nernance capabilities help can maintain compliance no matter where the data is.\\n156 | Chapter 11: Data Fabric', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd448146-30be-47d2-a86f-790ee6a8265c', embedding=None, metadata={'page_label': '156', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Lastly, certain industries—like stock trading and ecommerce—are characterized by\\nrapidly shifting market conditions. In these volatile worlds, staying updated in real\\ntime is not just an advantage but a necessity. This immediacy is where a data fabric\\nproves invaluable; its support for real-time data processing keeps such businesses\\nagile, informed, and ready to adapt at a moment’s notice.\\nPotential Drawbacks\\nWhile a data fabric architecture has numerous advantages, it isn’t devoid of chal‐\\nlenges. Transitioning from an MDW to a data fabric can be resource intensive, with\\ninitial hiccups related to cost, training, and integration. Also, not all businesses need\\nthe advanced capabilities of a data fabric—for small businesses with limited data\\nsources and straightforward processing needs, an MDW might suffice.\\nMoreover, the inherent complexity of a data fabric can make troubleshooting more\\nchallenging. It’s crucial to ensure that your organization has the required expertise in-\\nhouse or accessible through partners before making the shift.\\nWhile data fabric offers a cutting-edge solution to modern data management chal‐\\nlenges, it’s essential for every business to evaluate its unique needs, potential return\\non investment, and long-term strategic goals before making the transition.\\nSummary\\nIn this chapter, we delved into the concept of data fabric architecture, an advanced\\nevolution of the MDW designed to enhance data accessibility, security, discoverabil‐\\nity, and availability. I defined data fabric and contrasted my definition with industry\\nperspectives, highlighting differences such as the role of data virtualization. The\\nchapter outlined eight key technologies that transition an MDW into data fabric terri‐\\ntory, without making a strict demarcation between the two.\\nI also explained the core components of the data fabric architecture, including data\\naccess policies, metadata catalog, master data management, real-time processing,\\nAPIs, and optional elements like data virtualization. The chapter emphasizes data fab‐\\nric’s adaptability, especially in handling large data volumes and real-time processing\\ndemands.\\nWe looked at reasons for transitioning from an MDW to a data fabric, as well as\\npotential drawbacks such as complexity and resource-intensive transitions. In short,\\nevery organization should carefully evaluate its unique needs and capabilities before\\nembracing this cutting-edge solution.\\nSummary | 157', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='75cf7642-486e-462e-a865-f6e27cfa0211', embedding=None, metadata={'page_label': '157', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='694d7afa-a38b-4f12-8b19-0677a2b2324e', embedding=None, metadata={'page_label': '158', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 12\\nData Lakehouse\\nI’ve touched briefly on the data lakehouse as a harmonization of the concepts of the\\ndata lake and data warehouse. The idea behind a data lakehouse is to simplify things\\nby using just a data lake to store all your data, instead of also having a separate rela‐\\ntional data warehouse. To do this, the data lake needs more functionality to replace\\nthe features of an RDW . That’s where Databricks’ Delta Lake comes into play.\\nDelta Lake is a transactional storage software layer that runs on top of an existing data\\nlake and adds RDW-like features that improve the lake’s reliability, security, and per‐\\nformance. Delta Lake itself is not storage.  In most cases, it’s easy to turn a data lake\\ninto a Delta Lake; all you need to do is specify, when you are storing data to your data\\nlake, that you want to save it in Delta Lake format (as opposed to other formats, like\\nCSV or JSON).\\nBehind the scenes, when you store a file using Delta Lake format, it is stored in its\\nown specialized way, which consists of Parquet files in folders and a transaction log to\\nkeep track of all changes made to the data. While the actual data sits in your data lake\\nin a format similar to what you’re used to, the added transaction log turns it into a\\nDelta Lake, enhancing its capabilities. But this means that anything that interacts with\\nDelta Lake will need to support Delta Lake format; most products do, since it has\\nbecome very popular.\\nDelta Lake is not the only option to provide additional functionality to a data lake;\\ntwo other popular choices are Apache Iceberg and Apache Hudi, which have very\\nsimilar features. However, in this chapter I’ll focus on Delta Lake.\\nSo far, you’ve learned about the relational data warehouse ( Chapter 4 ), data lake\\n(Chapter 5), modern data warehouse (Chapter 10), and data fabric (Chapter 11). This\\nchapter adds the data lakehouse. Figure 12-1 shows these architectures on a historical\\ntimeline.\\n159', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='53a041a5-b5d3-4ef9-82d5-d36a1daa5882', embedding=None, metadata={'page_label': '159', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 12-1. Historical timeline of data architectures\\nDelta Lake Features\\nDelta Lake adds several RDW-like features to a data lake. This section will walk you\\nthrough some of them.\\nPerhaps the biggest reason companies use Delta Lakes is that they support data\\nmanipulation language  (DML) commands, such as INSERT, DELETE, UPDATE, and\\nMERGE. These commands simplify complex data management tasks, making data han‐\\ndling more flexible and reliable within Delta Lake. Data lakes do not provide native\\nsupport for these operations, because data lakes are optimized for batch processing\\nand storing large amounts of data, not for real-time updates.\\n160 | Chapter 12: Data Lakehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='48bd950b-087d-42ea-a496-1e355680fa42', embedding=None, metadata={'page_label': '160', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Updating data in a data lake typically involves reading the entire file, making the nec‐\\nessary updates, and writing the entire updated file back to the data lake, which can\\ntake a long time, especially for large files. In contrast, when Delta Lake initially works\\nwith a table—essentially a file that is an organized collection of data in rows and col‐\\numns—it breaks that table down into several smaller digital files for easier manage‐\\nment. The result is a Delta Table. Delta Lake then uses a transaction log to track\\nchanges, which makes DML commands work much faster due to using optimized\\nstorage (columnar storage format), in-memory processing, and optimizations for\\nbatch processing. For example, with an UPDATE statement, Delta Lake finds and selects\\nall files containing data that matches the predicate and that therefore need to be\\nupdated. It then reads each matching file into memory, updates the relevant rows, and\\nwrites out the result into new data files. This updates the data efficiently, without hav‐\\ning to rewrite the entire Delta Table. The transaction log maintains a history of\\nchanges to the data and ensures that the data is always in a consistent state, even in\\nthe case of failures or system crashes.\\nA common acronym you will hear when dealing with databases is ACID, which\\nstands for atomicity, consistency, isolation, and durability. These are the four proper‐\\nties that ensure the reliability and integrity of a database transaction. They guarantee\\nthat a transaction can be completed as a single, reliable operation, with its data\\nchanges either fully committed or fully rolled back. While Delta Lake can be said to\\nsupport ACID transactions, that statement requires qualifying. Unlike processing\\nACID transactions in a relational database, such as SQL Server, Delta Lake ACID\\nsupport is constrained to a single Delta Table. Executing DML over more than one\\nDelta Table in a Delta Lake will not guarantee ACID integrity. With a relational data‐\\nbase, ACID integrity works with DML transactions that span multiple tables.\\nDelta Lake also offers “time travel, ” a feature that allows you to query data stored in\\nDelta Tables as it existed at a specific point in time. A history of changes to the data is\\nmaintained in the Delta transaction log, along with metadata, such as when each\\nchange was made and by which user. Users can view and access previous versions of\\nthe data, and even revert it to a previous version if necessary. This can be useful for\\nauditing, debugging, or recovering data in the event of unintended data changes or\\nother issues (such as rollbacks).\\nThe “small files” problem, where a large number of small files can slow down read\\nand write operations and increase storage costs, is a common issue in data lakes.\\nDelta Lakes solve this problem by using optimized compaction algorithms to effi‐\\nciently merge small files into large ones. The compaction process is performed auto‐\\nmatically in the background and can be configured to run on a schedule or triggered\\nmanually.\\nWith Delta Lake, users can perform both batch processing and real-time streaming\\non the same data in a Delta Table, eliminating the need to maintain separate data\\nDelta Lake Features | 161', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='00fb8dd7-00d8-4509-93c8-8f8b50a577b5', embedding=None, metadata={'page_label': '161', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='pipelines and systems for batch and streaming processing. This unified solution\\nmakes it easier to manage and maintain pipelines and simplifies the data-processing\\narchitecture. This also means that Delta Lake supports the Lambda architecture (see\\nChapter 7).\\nSchema enforcement is a Delta Table feature that allows you to specify the expected\\nschema for the data in a Delta Table and enforce rules such as nullable constraints,\\ndata type constraints, and unique constraints. This ensures that data written to the\\nDelta Table conforms to the specified schema, which helps prevent data corruption.\\nWithout schema enforcement, a file with an invalid schema can be added to a folder,\\nwhich can cause ELT jobs to error out. If incoming data does not match the schema,\\nDelta Lake will reject the write operation and raise an error.\\nPerformance Improvements\\nUsing Delta Lake can improve data lake performance in several ways, including:\\nData skipping\\nDelta Lake can skip over irrelevant data when reading from a Delta Table, which\\ncan greatly improve query performance.\\nCaching\\nDelta Lake supports data caching in Spark, which can significantly improve the\\nperformance of repeated queries.\\nFast indexing\\nDelta Lake uses an optimized indexing structure to quickly locate data, reducing\\nthe time required to execute queries.\\nQuery optimization\\nDelta Lake integrates with Spark SQL and can take advantage of Spark’s query\\noptimization capabilities, resulting in faster and more efficient queries.\\nPredicate pushdown\\nDelta Lake supports predicate pushdown, which means that filter conditions are\\npushed down to the storage layer, reducing the amount of data that needs to be\\nprocessed.\\nColumn pruning\\nWith column pruning, only the columns required for a specific query are read,\\nreducing the amount of data that needs to be processed.\\nVectorized execution\\nIn vectorized execution , multiple data points are processed in a single CPU\\ninstruction, leading to improved performance.\\n162 | Chapter 12: Data Lakehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='498cbc6f-a99e-44e2-ae4a-d97b241864ae', embedding=None, metadata={'page_label': '162', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Parallel processing\\nDelta Lake supports parallel processing, which means that multiple tasks can be\\nexecuted in parallel, leading to improved performance.\\nZ-order\\nZ-order, also known as Morton order, is a data-indexing technique used in Delta\\nLake architectures to organize data for fast, efficient access and querying.\\nThe Data Lakehouse Architecture\\nIn a data lakehouse, data moves through the same five stages you saw with the MDW\\nand data fabric architectures, as Figure 12-2 shows. These are (1) ingestion, (2) stor‐\\nage, (3) transformation, (4) modeling, and (5) visualization.\\nFigure 12-2. Data lakehouse architecture\\nThe Data Lakehouse Architecture | 163', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e7c25914-a3b5-42f3-9f58-77a9a5260663', embedding=None, metadata={'page_label': '163', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As you can see in Figure 12-2 , with a data lakehouse architecture, there’s only one\\nrepository for your data (the data lake that is using Delta Lake), rather than two (a\\ndata lake and a RDW). This solves six problems commonly seen in the MDW and\\ndata fabric architectures:\\nReliability\\nKeeping a data lake and an RDW consistent can be a problem, especially if large\\namounts of data frequently need to be copied from the data lake to the RDW . If\\nthe jobs to copy the data fail, do not copy the data accurately, or put the data in\\nthe wrong spot in the data lake, this can cause reliability issues. For instance, run‐\\nning reports against the RDW could return different results than running reports\\nagainst the same data in the data lake. With a data lakehouse, since there is no\\nRDW to copy data to, this is not an issue.\\nData staleness\\nData in an RDW will be older than the equivalent data in the data lake. How old\\nwill depend on how often data is copied from the data lake to the RDW—and to\\navoid affecting query and report performance, you don’t want to run those jobs\\ntoo often. As with the reliability issues above, this can result in reports against the\\nRDW and data lake returning different results. With a data lakehouse, since there\\nis no RDW to copy data to, this is not an issue.\\nLimited support for advanced analytics\\nFew AI/ML systems and tools work well on RDWs, because data scientists usu‐\\nally prefer working with files in a data lake. They get this with a data lakehouse\\narchitecture.\\nTotal cost of ownership\\nEven though storage is relatively cheap, there are extra costs for the compute\\nneeded to copy data to an RDW . Also, the result is two copies of the data, adding\\nextra storage costs. By contrast, in a data lakehouse, there is only one copy of the\\ndata. Also, the compute used for queries and reporting in the RDW usually costs\\nmuch more than the compute used with a data lake. In addition, managing a data\\nlake and managing a RDW are different skill sets, and hiring for both can mean\\nextra costs as well.\\nData governance\\nHaving two copies of the data in two different storage environments, possibly\\nwith two different types of security, increases the risk of someone seeing data that\\nthey should not see. It’s also challenging to ensure that both systems follow the\\nsame rules for data quality and data transformations. With a data lakehouse and\\nits one copy of data, these problems do not exist.\\n164 | Chapter 12: Data Lakehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='23d76028-71ed-44d5-8a4e-8b37b1119b65', embedding=None, metadata={'page_label': '164', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Complexity\\nManaging both a data lake and an RDW can be complex, requiring specialized\\nskills and resources. Since a data lakehouse doesn’t have a RDW , fewer specialized\\nskills are required.\\nWhat If You Skip the Relational Data Warehouse?\\nHere is where we open a can of worms. Not long ago, I would have told you that skip‐\\nping an RDW in your architecture was a very bad idea. Now, as Delta Lake continues\\nadding RDW-like features to the data lake, I am starting to see more and more use\\ncases where a data lakehouse is the best architecture. The case for using a data lake‐\\nhouse is especially compelling with smaller datasets. Because most cloud vendors\\nhave serverless compute that you can use against a Delta Lake, you can save costs by\\npaying per query. Y ou also save costs by not having to copy data into relational stor‐\\nage, which is more expensive and uses more expensive relational compute. And if you\\nuse dedicated compute for an RDW instead of going serverless, you are paying even if\\nyou aren’t using that compute. As with any architecture, there are trade-offs and con‐\\ncerns, and it’s very important to be aware of them if you choose not to have an RDW\\nin your architecture.\\nThe first trade-off is that relational database queries are faster than queries against a\\nDelta Lake, especially when your RDW uses MPP technology (see Chapter 7). RDWs\\nhave features to improve query performance that aren’t available in a Delta Lake,\\nincluding:\\n• Advanced indexing (such as clustered columnstore indexes and full-text indexes)\\n• Advanced statistics\\n• Caching (unless you’re using Spark)\\n• Advanced query plan optimization\\n• Materialized views\\n• Advanced join optimization\\nSome Delta Lake performance features, such as Z-order, can alleviate some of these\\nmissing features.\\nDelta Lake also lacks some of the common staples of RDW security, such as row-level\\nsecurity, column-level security, data-at-rest encryption, column-level encryption,\\ntransparent data encryption (TDE), and dynamic data masking (which automatically\\nreplaces or obscures portions of the data so that unauthorized users see a masked ver‐\\nsion of the data instead of the actual sensitive information). Nor does it provide SQL\\nviews; referential integrity; workload management; or advanced auditing and compli‐\\nance features, such as auditing trails, data retention policies, and compliance\\nWhat If You Skip the Relational Data Warehouse? | 165', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='520d8fe2-36ee-4768-9c1a-12b853a3db60', embedding=None, metadata={'page_label': '165', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='certifications. RDWs also support higher concurrency than Delta Lake, because they\\nprovide advanced features such as advanced locking, isolation levels, and transaction\\nmanagement.\\nComplexity is also an issue. With an RDW , you have a forced metadata layer—you\\nmust create a database, schema, and a table with fields that describe the data type and\\nthen load the data (schema-on-write). This means you always have the metadata sit‐\\nting on top of the actual data. This requires up-front work, but the big benefit is that\\nthe metadata and data are always locked together, so it’s easy to use the metadata to\\nfind the data. Y ou will never “lose” the metadata, and it will always accurately describe\\nthe data. This is hugely different from Delta Lake, which is a folder- and file-based\\nworld. That’s the main reason end users who are used to an RDW often struggle with\\nDelta Lake.\\nIn Delta Lake, metadata isn’t required to exist with the data. Y ou might find it in one\\nor more separate files, within the file that contains the data, within the same folder, in\\na separate folder, or not at all. To top it off, the metadata might be wildly inaccurate,\\nsince it doesn’t have the one-to-one relationship with the data that it has in an RDW .\\nY ou can see how confusing this can be to end users, who might struggle to find the\\nmetadata, wrongly decide that there is no metadata, or even use the wrong metadata.\\nFinally, the metadata can fall out of sync with the data when changes are made out‐\\nside Delta Lake.\\nUsing certain features of Delta Lake architectures could lock you into having to use\\nSpark. Also, if you migrate from a product that uses a SQL version other than Spark\\nSQL, prepare to rewrite your stored procedures, views, report, dashboards, and so on.\\nIf you do need to use Spark SQL, that might mean retraining end users who are\\nalready used to interacting with RDWs and familiar with tools for doing so. They\\nmay be using ANSI-compliant SQL or an RDW product like T-SQL (which is used\\nwith Microsoft products). They are also used to the relational model and know how\\nto quickly query and report off of it. Switching to a folder-file world would likely\\nforce them to learn to use new tools; get used to schema-on-read; and learn to handle\\nthe issues with the speed, security, missing features, complexity, and SQL changes\\ndescribed earlier. That’s a lot of training.\\nExisting technology can alleviate some of these concerns and could eventually render\\nthem irrelevant. But until that happens, consider them carefully in relation to your\\nneeds as you determine whether to use the data lakehouse architecture. For example,\\nif queries to the data lake take an average of five seconds in the data lakehouse archi‐\\ntecture, is that a problem? If end users are OK with that, you can ignore this concern,\\nbut if they’re using dashboards and need millisecond query response times, you\\nwould need to copy the data used for the dashboards into an RDW (meaning you’ d\\nend up using an MDW architecture). Y ou could instead use a reporting product that\\nallows you to import data into its memory for millisecond query response times, but\\n166 | Chapter 12: Data Lakehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca455acc-4b11-45f6-baf2-c035e1ff88bf', embedding=None, metadata={'page_label': '166', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the data would not be as updated as it is in Delta Lake, and you would need to refresh\\nthe data in the reporting tool’s memory at certain intervals.\\nOnce again, we are talking about trade-offs. If you are an architect, a major part of\\nyour role will be identifying products that could be used in each architecture and\\ndetermining and analyzing their trade-offs to choose the best architecture and prod‐\\nucts for your particular use case.\\nNone of the above concerns is necessarily a showstopper in itself, but taken in combi‐\\nnation, they could provide enough of a reason to use an RDW . A lot of companies\\nstart with some proofs of concept to determine whether any of the trade-offs of skip‐\\nping the RDW will cause an issue. If not, they proceed with a data lakehouse.\\nRelational Serving Layer\\nBecause Delta Lake is schema-on-read (see Chapter 2), the schema is applied to the\\ndata when it is read, not beforehand. Delta Lake is a file-folder system, so it doesn’t\\nprovide context for what the data is. (Contrast that with the RDW’s metadata presen‐\\ntation layer, which is on top of and tied directly to the data.) Defined relationships\\ndon’t exist within Delta Lake. Each file is in its own isolated island, so you need to\\ncreate a “serving layer” on top of the data in Delta Lake to tie the metadata directly to\\nthe data. To help end users understand the data, you will likely want to present it in a\\nrelational data model, so that makes what you’re building a “relational serving layer. ”\\nWith this layer on top of the data, if you need to join more than one file together, you\\ncan define the relationships between them. The relational serving layer can take many\\nforms: a SQL view, a dataset in a reporting tool, an Apache Hive table, or in an ad hoc\\nSQL query. If done correctly, the end user will have no idea they are actually pulling\\ndata from a Delta Lake—they will think it is from the RDW .\\nMany companies create SQL views on top of files in Delta Lake, then use a reporting\\ntool to call those views. This makes it easy for end users to create reports and\\ndashboards.\\nEven with a relational serving layer, Delta Lake still presents some challenges. The\\nrelational serving layer could portray the data incorrectly, for instance, or you could\\nend up with two layers that point to the same data but have different metadata. Meta‐\\ndata not being tied to the data is an inherent problem with Delta Lake. RDWs avoid\\nthis problem by having a universal data model that everyone can use.\\nSummary\\nThis chapter explored the concept of the data lakehouse, focusing on the role of Delta\\nLake as a transactional storage layer that significantly enhances existing data lakes’\\nreliability, security, and performance.\\nSummary | 167', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='760d43eb-f438-4683-a042-5f2ab6b9f7f5', embedding=None, metadata={'page_label': '167', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Y ou learned about the potential drawbacks associated with bypassing a traditional\\nRDW , with emphasis on challenges related to speed, security, and concurrency. Just\\nbe aware of the trade-offs, and if there are no immediate big concerns, then use a data\\nlakehouse until you can’t. If one of those trade-offs becomes too much to overcome\\nfor a particular dataset, you can copy that data to an RDW (no need to copy all the\\ndata). As technology for Delta Lake, for similar technologies like Apache Iceberg and\\nApache Hudi, and for storage and compute continues to improve, there will be fewer\\nand fewer reasons to maintain an RDW . Most new data architectures will be data\\nlakehouses.\\nY ou saw the unique schema-on-read approach of Delta Lake, where data interpreta‐\\ntion occurs at the point of reading, not in advance. Y ou also learned about its file-\\nfolder structure, which is devoid of context and thus significantly deviates from the\\nstructured metadata presentation layer of an RDW . This necessitates a “relational\\nserver layer” for establishing a direct context-based link to the data in Delta Lake.\\nRapid technological advancements continue to influence data architecture strategies.\\nA few years ago, omitting an RDW from your architecture would have been consid‐\\nered a significant misstep, but current trends indicate an increasing number of use\\ncases where data lakehouse emerges as the optimal architecture.\\nAll of the architectures I’ve discussed so far are centralized solutions, meaning that\\nsource data is copied to a central location owned by IT. Y ou’ll see a major difference\\nin the next chapter, as we discuss the decentralized solution of the data mesh.\\n168 | Chapter 12: Data Lakehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='17778549-d6c2-43e8-8fd1-c8fe05f873d9', embedding=None, metadata={'page_label': '168', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 See Zhamak Dehghani, “How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh, ” Martin‐\\nFowler.com, May 20, 2019; “Data Mesh Principles and Logical Architecture, ” MartinFowler.com, December 3,\\n2020, and Data Mesh: Delivering Data-Driven Value at Scale (O’Reilly, 2022).\\nCHAPTER 13\\nData Mesh Foundation\\nA data mesh is a decentralized data architecture with four specific characteristics.\\nFirst, it requires independent teams within desginated domains to own their analyti‐\\ncal data. Second, in a data mesh, data is treated and served as a product to help the\\ndata consumer to discover, trust, and utilize it for whatever purpose they like. Third,\\nit relies on automated infrastructure provisioning. And fourth, it uses governance to\\nensure that all the independent data products are secure and follow global rules.\\nAlthough the concepts that make up a data mesh are not new, Zhamak Dehghani,\\nCEO and founder of Nextdata, deserves credit for coining the term data mesh and\\ncombining those concepts. 1 Although others have formed various opinions about\\nwhat a data mesh is, this book bases its definition on Dehghani’s work, in particular\\nher four data mesh principles, which I discuss in this chapter. It is very important to\\nunderstand that data mesh is a concept, not a technology. It is all about an organiza‐\\ntional and cultural shift within companies. The technology used to build a data mesh\\ncould follow the modern data warehouse, data fabric, or data lakehouse architecture\\n—or domains could even follow different architectures.\\nAs we delve into the nuanced world of data mesh architectures in this chapter and the\\nnext, I offer a road map to navigate the complex data mesh landscape grounded in the\\nprinciple of decentralization. In this chapter, I dissect the decentralized data architec‐\\nture, clarifying the buzz surrounding data mesh and explicating its four foundational\\nprinciples. We’ll dive deep into the essence of the pure data mesh and journey\\nthrough the intricacies of data domains, then discover the logical architecture and\\n169', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1a08da43-ec95-42fa-a168-95f7e045ce0a', embedding=None, metadata={'page_label': '169', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='varied topologies that support the infrastructure and draw contrasts between data\\nmesh and data fabric. Finally, I’ll discuss the proper use cases for data mesh.\\nShifting focus in Chapter 14, I address potential challenges in implementing a data\\nmesh, debunking common myths to assist organizations in informed decision mak‐\\ning regarding its adoption. Next I’ll discuss conducting an organizational assessment\\nto determine if adopting a data mesh aligns with your organization’s needs. From\\nthere, I’ll delve into recommendations for implementing a successful data mesh,\\nensuring a seamless transition and maximum benefits. Then I’ll gaze ahead at the\\nanticipated trajectory of data mesh in the ever-evolving data landscape and conclude\\nwith a discussion of when to use each of the four architectures: modern data ware‐\\nhouse, data fabric, data lakehouse, and data mesh.\\nJoin me as I unravel the intricacies of data mesh architectures, providing you with a\\ndeep yet concise insight into its decentralized framework.\\nA Decentralized Data Architecture\\nThe data architectures I have talked about in this book so far—modern data ware‐\\nhouse, data fabric, and data lakehouse—are all centralized architectures where all\\nanalytical data is created and owned by the IT team. A big difference between data\\nmesh and those other data architectures is that data mesh is decentralized. That\\nmeans the data, along with everything used to manage it, is owned and managed by\\nindividual teams or “domains” within an organization and grouped by business\\ndomain, as opposed to a central authority or single team in IT. These domain teams\\nare responsible for collecting, processing, and managing their own data, and they\\nhave the autonomy to make decisions about how their data is used and shared. The\\ndata is kept within the domain; end users can access and query the data where it lives,\\nwithout copying it to a central location. This leads to greater accountability and\\nensures that data is managed and maintained by those with the greatest expertise\\nabout it.\\nThis solves the main problem with centralized architectures, which is organizational\\nand technical scaling. As a company grows and more data has to be centralized\\nquickly, IT often gets overloaded and becomes a bottleneck.\\nWhen I talk about all the other data architectures in this book being centralized, I’m\\nreferring to a wide variety of aspects of these architectures. Not only does a central IT\\nteam own and control data, that team is also responsible for:\\n• Integrating data from different sources\\n• Storing data in a centralized location that it owns\\n• Building data models and housing them in a central repository\\n170 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cd5b9c1c-337d-42a6-aaa1-1b867ccd44ff', embedding=None, metadata={'page_label': '170', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='• Governance and compliance, including defining and enforcing data quality, secu‐\\nrity, and privacy standards\\n• Creating and maintaining data pipelines and transformation logic\\n• Enforcing data quality via profiling, validation, cleansing, and other processes\\n• Optimizing performance\\n• Managing all compute technology\\n• Managing analytics and reporting tools\\n• Storing and managing metadata\\n• Disaster recovery and backup\\nIn addition, in a centralized architecture, the hardware and processing power are\\ndesigned to scale vertically.\\nData Mesh Hype\\nData mesh has drawn a lot of attention since it was first introduced in 2019. Gartner\\nhas a well-known “hype cycle” for data management, which measures the maturity of\\ntechnologies based on the current level of adoption and the number of years to main‐\\nstream adoption. In its Hype Cycle for Data Management 2023 , it placed data mesh\\nnear the top of the hype cycle (approaching the “peak of inflated expectations”) and\\n“moderate” on the benefit rating. Data mesh has a market penetration of 5% to 20%,\\nand interestingly enough, Gartner predicts that “Data mesh will be obsolete before\\nthe plateau. The practice and supporting technology will evolve toward data fabric as\\norganizations start collecting passive metadata. ”\\nMany who hype the data mesh will claim that data warehousing projects fail at a very\\nhigh rate, are unsustainable, and can no longer handle big data because they can’t\\nscale. With all the hype, you would think building a data mesh is the answer to all of\\nthese “problems” with data warehousing. The truth is that while data warehouse\\nprojects do fail, it is rarely because they can’t scale enough to handle big data or\\nbecause the architecture or the technology isn’t capable. Failure is almost always\\nbecause of problems with the people and/or the process, or that the organization\\nchose the completely wrong technology. Sometimes I feel as if data mesh is a solution\\nlooking for a problem, and the people hyping data mesh are oversimplifying the\\nwhole challenge of managing and integrating data.\\nI have seen big data solutions work very well for many years without data mesh, from\\nthe late 1980s (when a terabyte was considered “big data”) to today’s petabytes of data.\\nThe technology hasn’t stood still. Centralized data architectures and cloud products\\nare quite capable of handling just about any amount of batch and streaming data.\\nData Mesh Hype | 171', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d6c771e0-9b1e-46bb-8cff-1a25cc35e2b1', embedding=None, metadata={'page_label': '171', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 See Zhamak Dehghani, Data Mesh: Delivering Data-Driven Value at Scale (O’Reilly, 2022), page 15 for princi‐\\nple #1, page 29 for principle #2, page 47 for principle #3, and page 67 for principle #4.\\nThese solutions have worked for big data for decades. Why should we assume they\\nwon’t work in the future?\\nOf course, this is all guesswork. Data mesh is still new, but clearly not everyone\\nbelieves the hype. I fall into the skeptics’ camp. I have been working with databases\\nfor over 35 years and data warehousing for 15, and I’ve seen plenty of much-hyped\\ntechnologies come and go. While I don’t believe data mesh will become obsolete, I\\ndon’t believe any companies will build a “pure” data mesh. A small number of compa‐\\nnies will build a modified data mesh, and the rest will use another architecture.\\nI have seen very few companies put a modified data mesh into production. Many\\nclaim they have, wanting to seem cutting-edge, but most of the time it’s really a data\\nfabric or data lakehouse. But even with a very loose interpretation of “data mesh, ” the\\npercentage of companies building one is very small. When I speak on data architec‐\\nture, I always ask the crowd, “How many have heard of data mesh?” Usually, at least\\n75% raise their hands. I then follow up with, “How many are building a data mesh?”\\nMaybe one or two hands will be raised, in a crowd of a hundred. Then I ask, “How\\nmany have a data mesh in production?” I have yet to see a single hand raised. Other\\ncolleagues report similar experiences. While this is anecdotal, it appears to indicate\\nthat data mesh is not living up to the hype. Nevertheless, data mesh architectures\\nhave a lot of positive aspects that can improve any of the other data architectures.\\nThis chapter and the next will explain in more detail.\\nDehghani’s Four Principles of Data Mesh\\nThe data mesh tries to solve the four biggest challenges of centralized data architec‐\\ntures: lack of ownership, low data quality, technical scaling, and organizational scal‐\\ning. The four principles that Dehghani lays out address these challenges. 2 Let’s look at\\neach in turn.\\nPrinciple #1: Domain Ownership\\nPrinciple #1 recommends that you decentralize and distribute responsibility to peo‐\\nple who are closest to the data in order to support continuous change and scalability\\n(for example, manufacturing, sales, supplier).\\nIn a centralized architecture, there is usually confusion (or, in some meetings, loud\\narguments) about who “owns” the analytical data.\\nMost source data is generated by homegrown operational systems, CRM tools such as\\nSalesforce and Microsoft Dynamics, and enterprise resource planning (ERP) tools\\n172 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4d077b6e-a5f9-416a-aa79-dbf9fb95438a', embedding=None, metadata={'page_label': '172', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='such as SAP and Microsoft Dynamics. Each of these domain-specific systems and\\napplications is run by dedicated operational people who usually have little to no com‐\\nmunication with the central IT data platform team. Often, the operational people are\\nunaware that “their” data is being sent to a data platform.\\nWhen data is in an operational database, it belongs to the operational domain that\\nowns the database. When it’s copied to the centralized data store (usually a data lake),\\nIT usually owns it—but some argue that it should still belong to the domain.\\nFor its part, the central IT data platform team rarely has a solid understanding of how\\nthe operational systems or business applications operate. They simply don’t under‐\\nstand the data well and have no context for how the data was generated or what it\\nactually means. They just see table and field names or files. As a result, they struggle\\nto produce quality data and usable analytics.\\nThis challenge is cleared up by the first principle of the data mesh—domain owner‐\\nship. The people in charge of the operational databases are always the owners of the\\nanalytical data, and they are responsible for managing and maintaining the data.\\nResponsibility and ownership are decentralized and distributed to the people who are\\nclosest to the data, to support continuous change and scalability. These people are\\ngrouped according to business domain, such as manufacturing, sales, or supplier;\\ndefining a company’s domains can be challenging. Each domain owns its operational\\nand analytical data.\\nPrinciple #2: Data as a Product\\nPrinciple #2 suggests treating analytical data provided by the domains as a product,\\nand the consumers of that data as customers. Each domain has domain teams, API\\ncode, data and metadata, and infrastructure.\\nIn a centralized architecture, the IT team owns the analytical data and therefore is\\nresponsible for data quality—but they usually don’t know the data well and may make\\nmistakes transforming it. The domain team, on the other hand, has a deep under‐\\nstanding of the data and its context within their business domains. This knowledge\\nallows them to identify and address data quality issues that may be missed by central\\nIT teams, who have less context to aid in understanding the business needs.\\nThe second principle of data mesh is that instead of thinking of analytical data as an\\ninput, an asset, or a byproduct of a process that others manage, data owners treat data\\nas a fully contained product that they are responsible for. They treat the consumers of\\nthat data (data users, data analysts, data scientists) as customers. This means that\\ndomain teams apply product thinking to their data to make sure it can be easily dis‐\\ncovered and that it’s trustworthy, accessible, secure, and understandable.\\nY our first step in treating data like a product is to identify exactly what a data product\\nis in your organization. A data product is a self-contained, independently deployable\\nDehghani’s Four Principles of Data Mesh | 173', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0206fc03-fb49-4df1-926c-d23a6ff3b8cc', embedding=None, metadata={'page_label': '173', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='unit of data that delivers business value. It contains analytical data sourced from\\noperational or transactional applications. It is designed, built, and managed by a sin‐\\ngle product team. Each data product is autonomous and managed completely inde‐\\npendently of the other data products. It solves a particular business problem and\\nshould be designed with a clear understanding of the needs of its target users. A data\\nproduct is associated with a particular data domain, so it serves a specific area of the\\nbusiness and should be aligned to that area’s business goals. There are usually many\\nproducts under each domain.\\nBe aware that data product is not the same thing as data as a product. Data as a prod‐\\nuct describes the idea that data owners treat data as a fully contained product that\\nthey are responsible for, rather than a byproduct of a process that others manage, and\\nshould make the data available to other domains and consumers. Data product refers\\nto the architecture of implementing data as a product. Some within the IT industry\\nview datasets, analytical models, and dashboard reports as data products, focusing on\\nthe physical representation of data, and do not consider any metadata, code, or infra‐\\nstructure to be data products.\\nNote that a data product can be composed of subproducts, each responsible for pro‐\\nviding a specific subset of data or functionality that is organized around a subject\\narea. For example, a data product for customer data could have subproducts for\\ntransactional, demographic, and behavioral data, each managed by a separate team.\\nData products are consumed through data contracts, which are agreements between\\ndata domains or data products that define the format and structure of the data\\nexchange between a producer and a consumer. The data contract is a very rich docu‐\\nment that typically contains metadata, data schemas, data transformation info, and\\ndata access rules. It also contains information on how other domains, products, or\\nconsumers will access the data product, typically through APIs, a database, a standard\\nfile format, an event stream, or a graph.\\nSince domain teams are directly responsible for the data, they can quickly identify\\nand address any quality issues. This leads to faster resolution times, reducing the risk\\nof inaccurate data being used in business decisions. Finally, domain teams are embed‐\\nded within the business, so they have a better understanding of the specific data needs\\nfor their domains. This ensures that data is managed and maintained in a way that\\naligns with the business’s needs, resulting in higher-quality and more relevant analyti‐\\ncal data.\\nOne more item about principle #2: each domain team is responsible for acquiring\\nand managing its own infrastructure and resource allocations, as well as hiring people\\nto build the architecture and manage the data. This is a big change; instead of IT\\ncleaning and processing data for consumption (by copying operational data into ana‐\\nlytical data), these responsibilities now fall on the domain team. The domain teams\\nmust also determine the business needs for their data. This means that each team can\\n174 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='07fc44b2-8122-4bc2-b7ff-08442e68ef43', embedding=None, metadata={'page_label': '174', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='determine its own resource needs based on the domain’s requirements and data vol‐\\nume. Each team can also independently provision and scale resources as needed to\\nhandle fluctuations in demand, so multiple domain teams aren’t competing for infra‐\\nstructure resources from a central IT team.\\nPrinciple #3: Self-Serve Data Infrastructure as a Platform\\nPrinciple #3 recommends that you simplify data product creation and management\\nby automating infrastructure provisioning (for example, storage, compute, data pipe‐\\nline, and access control).\\nAs I noted in the beginning of this chapter, in centralized architectures, the central IT\\nteam can become a bottleneck, making it difficult to scale effectively. This team\\nstands up and monitors all of the architecture, including storage, compute, data pipe‐\\nlines, and data access policies. Often, they just don’t have enough people to make data\\navailable in a timely manner.\\nIn a data mesh, because each domain team owns its domain data and treats its data as\\na product, it needs its own engineering teams and infrastructure to build, deploy,\\nmonitor, and provide access to its data product. This extends domain teams’ respon‐\\nsibility from building operational applications to include building analytics and creat‐\\ning a solution to share the data. In short, the first two data mesh principles impose\\nadditional effort on the domain engineering teams. This principle is about reducing\\nthat.\\nDomain teams might be dismayed at the thought of managing, staffing, overseeing,\\nand paying to develop and manage their data product. To avoid frustration and push‐\\nback, you need to give them a “shortcut” to overcome some of this extra work. Y ou\\ndon’t want each domain going off and building its own infrastructure from scratch,\\neither; “reinventing the wheel” by duplicating efforts in each domain would signifi‐\\ncantly delay putting the data mesh into production, increase costs, and risk large-\\nscale inconsistencies and incompatibilities across domains.\\nTo prevent these problems, a data mesh platform should be built by a dedicated, cen‐\\ntral platform team. Not everything in a data mesh is decentralized. The platform must\\nimplement all the tools and interfaces a domain-engineering team needs to simplify\\nthe lifecycle of building, testing, deploying, securing, maintaining, and sharing a data\\nproduct with a consumer or with other data domains. The domain teams shouldn’t\\nneed to worry about the underlying infrastructure resource provisioning. Y ou can do\\nthis by exposing a set of standardized platform APIs or scripts that the data product\\ndeveloper can use to state their infrastructure needs. Then you can let the platform\\nhandle the creation and management of storage, compute, data pipelines, and access\\ncontrol. This will result in a standardized way of creating data products, securing\\nthem, finding them, connecting to them, and reading data from them. In a data mesh,\\nthe domain-engineering teams are made up of generalist technologists instead of\\nDehghani’s Four Principles of Data Mesh | 175', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='53915398-b7d1-488c-b826-b232568f142c', embedding=None, metadata={'page_label': '175', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='specialists, so reducing complexity through a data mesh platform can free them up to\\ninnovate with their data.\\nThe aim of this principle is to save costs, reduce complexity, lighten the load on\\ndomain teams, reduce the need for technical specialization, and automate governance\\npolicies.\\nPrinciple #4: Federated Computational Governance\\nPrinciple #4 states that a collaborative data governance between domains and a cen‐\\ntral data team exists to define, implement, and monitor global rules (for example,\\ninteroperability, data quality, data security, regulations, and data modeling).\\nData governance in a centralized architecture is much easier than in a data mesh,\\nbecause with a centralized architecture approach, just one central data team owns all\\nthe data and defines, implements, and monitors all the data governance. However,\\nthis centralized approach often results in a bottleneck, slowing down the decision-\\nmaking process and limiting the ability of individual domains to respond swiftly to\\nthe changing conditions and specific situations they face. It can also lead to a lack of\\nlocalized expertise and contextual understanding, as the central team has to govern a\\nvast variety of data without in-depth knowledge of each domain’s nuances.\\nIn a data mesh, a central data team defines and oversees all the data governance\\nstandards and policies, including such things as data quality, data security, regula‐\\ntions, and data modeling. The central data team is composed of domain representa‐\\ntives and subject matter experts (on compliance, legal, security, and so forth).\\nHowever, implementing and monitoring data governance is left to the domains, since\\nthey own the data and know it best.\\nThe challenge with distributing responsibilities across multiple domain teams is the\\npotential for inconsistencies and conflicting practices, leading to operational ineffi‐\\nciencies and increased complexity. Domains might implement different standards for\\ndata quality (for instance, using full names of US states versus two-letter postal abbre‐\\nviations). They might use different security systems or data-modeling techniques. But\\nnot all governance rules can be set globally; some are context specific and really\\nshould be set by the domain. For example, the central data team can specify that users\\nmust be authenticated, while the domain teams define what data products each user\\nhas access to.\\nThis kind of inconsistency can lead to bad data, people seeing data they should not\\nsee, and other issues. Centralized systems often struggle with these nuances as they\\ntry to impose a one-size-fits-all policy, which can lead to misalignments and a failure\\nto meet specific needs and demands of different departments or divisions.\\nThat’s why the concept of federated data governance, which combines centralized\\noversight with domain-specific autonomy, is crucial for maintaining consistency and\\n176 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='704cf4ea-c6cd-4bb7-9999-c32fe83dc26c', embedding=None, metadata={'page_label': '176', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='security across the organization. The hope is to strike a balance between domain\\nindependence and central data team oversight. Whenever possible, it helps to auto‐\\nmate implementing and monitoring policies and to tag data for easier identification.\\nThe “Pure” Data Mesh\\nNow that you understand the four principles, take a look at Figure 13-1. It shows a\\nhigh-level data mesh architecture that uses the four principles. The first two princi‐\\nples are decentralized, with each domain creating analytical data using its own IT\\nteam and infrastructure. But the other two principles require centralized teams—one\\nto implement a data infrastructure platform and one to implement a shared gover‐\\nnance for all the domains to use.\\nFigure 13-1. Data mesh architecture\\nSo what makes a true data mesh? This is where the confusion starts. There are no uni‐\\nversal rules on when a solution can be called a data mesh.\\nIf a company builds a data mesh that completely follows all four principles, we would\\nall feel confident calling that solution a “pure” data mesh. But what if the solution\\nuses only three of the principles? Can we still call it a data mesh? What about two?\\nWhat if the solution uses all four principles, but in a way that doesn’t fulfill all the\\nrequirements of the principle? For example, you could follow principle #2, but have\\nthe domains share the same infrastructure by having one physical data lake that is\\nlogically separated into domain-specific folders that are accessible only by their\\nrespective domains, or perhaps by assigning multiple domains to one of a handful of\\ncompute clusters. As another example, using principle #3, central IT might offer\\nscripts that help build domain storage and management, but leave the domains to\\nThe “Pure” Data Mesh | 177', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9c9b6ba-9f8a-4535-996d-b8b80f9e401f', embedding=None, metadata={'page_label': '177', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='build out the rest of the infrastructure themselves. And consider one more: for prin‐\\nciple #4, central IT could establish standards for data security and regulation, but\\nhave domains self-monitor for compliance and leave them free to do data quality and\\nmodeling as they see fit.\\nSome companies build a solution with two or three partially implemented principles\\nand call it a data mesh. Others create separate data lakes for each domain and say\\nthey’ve built a data mesh. However, simply grouping data by business departments or\\norganizing data into domains is not a data mesh. Even assuming they’ve properly\\ndesigned their domains (the subject of the next section), that approach applies princi‐\\nple #1 but none of the others.\\nSome vendors will tell you that if you use their product, you will have a data mesh,\\neven though data mesh is all about an organizational and cultural shift, not about\\ntechnology. I have even heard people say that all you need to do to create a data mesh\\nis to connect all your data lakes and data warehouses using virtualization software!\\nNever mind that this doesn’t use any of the four principles.\\nAs you can see, there can be lots of “exceptions” to the pure data mesh within each\\nprinciple. How many exceptions would it take for a solution not to be considered a\\ndata mesh (think of this as the minimum viable mesh)? If the four principles aren’t\\nuniversally accepted in total, we can’t even try to answer that question. And should\\nwe all conform 100% to Dehghani’s definitions or adjust them based on feedback\\nfrom others in the industry? Do we need an industry committee to come up with a\\nrevised definition of a data mesh? And how do we avoid feedback based solely on\\nself-interest?\\nI’m afraid I don’t have all the answers. I hope this book will start a healthy discussion\\nby providing some clarity on common data architecture concepts and the architec‐\\ntures that have preceded data mesh.\\nData Domains\\nThe process of designing a domain-oriented architecture, called domain-driven design\\n(DDD), is very difficult and time-consuming. DDD comes from software design, but\\nit gets much more challenging when dealing with data. The first step is to define what\\ndomain means in the context of your company.\\nAccording to Dehghani’s first principle, analytical data should be owned by the busi‐\\nness domains that are closest to it—either the source of the data or its main consum‐\\ners. She defines three types of domain-oriented analytical data:\\nSource-aligned domain data\\nSource-aligned domain data is analytical data that corresponds closely to the\\noperational sources where the data originates. Data is copied from the source\\n178 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3cf5d7b5-e0c1-419f-ae27-7cc161a86a3c', embedding=None, metadata={'page_label': '178', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='application’s operational databases to the domain and transformed to become\\ncollaborating analytical data. This data is not fitted or modeled for a particular\\nconsumer. Source-aligned domains can have one or more sources: the data usu‐\\nally come from that domain’s operational systems but could be operational or\\nanalytical data from other domains. For example, a company that sells books\\nonline could divide its data into domains based on the customer’s purchasing\\njourney: product search, browsing, checkout, and payment. The challenge of cre‐\\nating domains is ownership; for example, if a business is currently divided into\\napplication domains (domains based solely on supporting a particular applica‐\\ntion) but instead needs to be divided into business domains (based on business\\ncapabilities), a business domain could own more than one application.\\nAggregated domain data\\nAggregated domain data is analytical data that combines and/or aggregates data\\nfrom other domains, generally to help with query performance. For example, you\\ncould combine data from the manufacturing domain and the sales domain to\\nmore easily and quickly create a profit and loss report or to create a data model\\nthat is much easier for consumers to use.\\nConsumer-aligned domain data\\nConsumer-aligned domain data is analytical data that has been transformed to fit\\nthe needs of one or more specific departments or use cases. Consumer-aligned\\ndomains almost always receive their data from source-aligned domains. For\\nexample, the domain data for a manufacturing domain can be modeled and\\ndescribed in two different ways: one for those who know manufacturing (source\\naligned) and one for those outside manufacturing (consumer aligned). Another\\nexample is transforming source-aligned data into consumer-aligned data as a way\\nto make it easier to train machine learning models.\\nFor more details on data domains and domain-driven design, I highly recommend\\nPiethein Strengholt’s Data Management at Scale, second edition (O’Reilly, 2023).\\nData Mesh Logical Architecture\\nFigure 13-2  shows how a data mesh logical architecture could look for a company\\nwith a handful of domains. Each of these domains would have one or more products.\\nIn Figure 13-2 , manufacturing, sales, and supplier are all source-aligned domains.\\nEach domain has its own operational and analytical data. For example, the sales\\ndomain could use Salesforce to track all the information about its customers (opera‐\\ntional data). Sales could then put its data in a data lakehouse architecture (see Chap‐\\nter 12) and combine it with other data sources that contain operational data and with\\nanalytical data from the supplier domain. This would create sales analytical data that\\nData Mesh Logical Architecture | 179', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='73c01af2-85cc-4e1f-bf4e-53911b8b699e', embedding=None, metadata={'page_label': '179', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='could then land in the data lakehouse. Sales also creates APIs to share that analytical\\ndata and its associated metadata with other domains and consumers.\\nFigure 13-2. Data mesh logical architecture\\nNote the consumer-aligned domain for supplier. This was created because the analyt‐\\nical data in the supplier source-aligned domain was too complex for people outside\\nthe supplier domain to understand. In the supplier consumer-aligned domain, the\\ndata model is simplified and the supplier jargon made easier to understand.\\nThere is also a profit and loss (P&L) aggregate domain that is built using data from\\nthe manufacturing source-aligned domain and the sales source-aligned domain. In\\nlarge part, this is done for performance reasons. Executing a query or report that\\ncombines data from multiple domains can be too slow if those domains have separate\\ndata lakes that are very far apart geographically.\\nFinally, there is a customer 360 domain. This takes customer data (such as demo‐\\ngraphic, behavioral, transactional, and customer feedback data) from various sources,\\ncleans and masters it (see Chapter 6), and combines it to get a full picture of the cus‐\\ntomer. All domains that need customer data pull from this customer 360 domain.\\nThis is a much better solution than having each domain pull, clean, and master cus‐\\ntomer data themselves.\\nAs you’ve seen, having many domains can lead to a complicated mess. Imagine what\\nthe diagram in Figure 13-2 could look like with dozens or even hundreds of domains.\\nThis is why domain design is so important. I covered it earlier in this chapter, but I\\nencourage you to look at other resources that go into depth on this topic. Y ou will\\nneed to spend a lot of time up front designing domains and products before you\\n180 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eea5ab52-01d6-4eb5-9be5-32ab63e999bb', embedding=None, metadata={'page_label': '180', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='begin building your data mesh to ensure you won’t have to go back and redesign it\\nmonths in.\\nDifferent Topologies\\nThere are many possible variations or topologies of a data mesh architecture.\\nFigure 13-3 shows three types that I will discuss:\\nMesh Type 1\\nIn this architecture, all domains use the same technology. For example, they\\nmight all use the same cloud provider and be limited to that provider’s products,\\nso each domain would be using the same products for storage, data pipelines,\\nsecurity, relational data warehouses, reporting, metadata catalog, MDM, data vir‐\\ntualization, APIs, ML tools, and so on. There may be a few options within some\\nproducts, but these choices would be minimal. Each domain would have its own\\ninfrastructure, so everything would be decentralized—except storage. Instead of\\neach domain having its own data lake, there would be one enterprise data lake,\\nwith each domain getting its own container or folder in the lake that only it can\\naccess.\\nLogically, then, the domains each have their own data lakes, but physically, the\\ndata for all the domains is in one data lake. Many customers use Mesh Type 1\\nbecause of performance problems with combining data when you have multiple\\nphysical data lakes, which could be located many miles apart. In addition, having\\none physical data lake greatly simplifies securing and monitoring the data, as well\\nas doing disaster recovery backups.\\nMesh Type 2\\nIn this architecture, domains use the same technology as in Mesh Type 1. Instead\\nof one enterprise data lake for all the domain data, each domain has its own data\\nlake. All the data lakes use the same technology. This makes the infrastructure\\ntruly decentralized, but at a cost: the technical challenge of linking all the data\\nlakes together and getting acceptable performance when combining data from\\nmultiple domains. This is why I see more companies using Mesh Type 1.\\nMesh Type 3\\nIn this architecture, each domain can use whatever technologies and whichever\\ncloud provider it wants and have its own data lake (which can also use any tech‐\\nnology). Y ou might have DomainA and DomainD using Azure, DomainB using\\nAWS, and DomainC using GCP , while DomainA chooses Azure Synapse for its\\nrelational data warehouse and DomainD chooses SQL Server in a VM for its data\\nwarehouse. The challenges would include dealing with different types of security\\nfor each domain, finding and supporting experts for dozens of products, creating\\ngovernance standards for many products in different clouds (principle #4),\\nDifferent Topologies | 181', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a5834dc1-3fbb-43b1-b0d0-d834de434db2', embedding=None, metadata={'page_label': '181', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='creating automated infrastructure that can allow for all this variety (principle #3),\\nand trying to combine data from multiple products in different clouds. This fits\\ninto the category of the pure data mesh discussed earlier. This formidable list is\\nwhy I feel Mesh Type 3 will never be adopted.\\nFigure 13-3. Three data mesh topologies on a spectrum from control to agility\\nOn the left of Figure 13-3 , the architectures have the most centralization, and the\\narchitectures become more distributed as you move to the right.\\nData Mesh Versus Data Fabric\\nData fabric and data mesh are both instrumental concepts in today’s data landscape,\\nbut they serve distinct roles and should not be confused. At its core, a data fabric is an\\narchitectural framework, designed to be employed within one or more domains inside\\n182 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd2de08b-7501-4caf-80ca-552188002b89', embedding=None, metadata={'page_label': '182', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='a data mesh. The data mesh, however, is a holistic concept, encompassing technology,\\nstrategies, and methodologies. In fact, its definition is so encompassing that I hesitate\\nto even label it an architecture.\\nOn the other side of the spectrum, a data fabric’s technological and architectural\\nunderpinnings can be tailored to support the various intricacies and components\\nwithin a data mesh. Their relationship is not one of competition or interchangeability\\nbut rather of synergy. A single data mesh might incorporate multiple data fabrics,\\neach tailored to the unique needs of its respective domain.\\nIn this context, you can think of each domain in the data mesh as its own ecosystem.\\nWithin this ecosystem, the domain can build, refine, and operate its own data fabric,\\nensuring that it aligns with specific requirements and goals. To put it another way,\\nwhile the data mesh provides a blueprint for a decentralized and domain-oriented\\napproach to data, data fabrics furnish the architectural and technological infrastruc‐\\nture that powers these domains. Utilizing them in tandem can lead to a more integra‐\\nted and efficient data infrastructure. This also applies to combining data mesh with a\\nmodern data warehouse or data lakehouse architecture.\\nUse Cases\\nHopefully by now you have a good understanding of data mesh as described by Deh‐\\nghani, which I will call the “pure” data mesh. While the pure data mesh sounds great\\nin theory, it is very hypothetical and has limited technology to support it. Therefore,\\npractical data mesh solutions will include many exceptions to Dehghani’s vision.\\nWith most of the built solutions, there are so many exceptions to the pure data mesh\\nthat these “data mesh” implementations should not even be called a data mesh.\\nThe main question to ask is this: are you trying to accomplish just data federation—\\nintegrating data from multiple, disparate sources without physically consolidating or\\nstoring the data in a single repository owned and managed by IT—or are you really\\ntrying to build a data mesh? Y ou can accomplish data federation without building a\\ndata mesh; basic data federation can be thought of as satisfying principle #1 of data\\nmesh and not the other three principles. If you go this route, should your solution\\nshould still be called a data mesh? I think you can call it whatever you like, but it may\\nbe more accurate to call it an enterprise data lakehouse or data fabric architecture,\\nespecially when each domain has its own workspace; the most accurate description of\\nwhat you have built is that it is an enterprise data lakehouse or data fabric architec‐\\nture with multiple workspaces (one workspace for each domain). Remember that you\\ncan satisfy principle #1 of a data mesh if each domain owns the analytical data\\nderived from their operational data, even if IT creates the analytical data. However,\\nyou can’t satisfy principle #2 unless each domain creates their own analytic data and\\ncreates the interface to share that data.\\nUse Cases | 183', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85d8d347-43ed-4317-824c-39be75a5b34a', embedding=None, metadata={'page_label': '183', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Also keep in mind that to satisfy principle #1, your organization has to have gone\\nthrough the process of reorganizing your company by business domains. I question\\nwhether doing the work of creating domains is worth it if you are not also satisfying\\nprinciple #2 (data as a product); if IT, not the domains, will be the cleaning that data,\\nthen you will be missing a big point of having a data mesh.\\nAs an example, say your company acquires a lot of other companies. Y ou have each of\\nthose acquisitions own their data, and IT use virtualization software to access it. In\\nthis scenario, you have accomplished data federation, but you certainly do not have a\\ndata mesh; you are not splitting your organization out by data domains (unless you\\ndefine domains by business function and decide that acquired businesses are their\\nown domains, which doesn’t make sense because you would then, for example, have\\nmultiple HR domains with duplicate HR products). Even so, again, organizing\\ndomains based on acquisitions is only satisfying principle #1. I would argue that prin‐\\nciple #2 needs to be satisfied to call the architecture a data mesh, as it realizes the two\\nbiggest benefits of a data mesh: organizational and technical scaling.\\nOne last point: in the quest to strike a balance between centralization and decentrali‐\\nzation within a data mesh, understanding the distribution of responsibilities is cru‐\\ncial. Figure 13-4 is a scale that shows where each responsibility area might fit for a\\nfictional use case.\\nFigure 13-4. Division of responsibilities and ownership between a centralized IT depart‐\\nment and decentralized business domains (fictional example)\\n184 | Chapter 13: Data Mesh Foundation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f4c9a552-2bc5-4670-8e29-ff3b343f5ffb', embedding=None, metadata={'page_label': '184', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the left of Figure 13-4  is centralized IT, and on the right are the decentralized\\nbusiness domains. A dot in the far-left means centralized IT does all of the particular\\nitem, and a dot in the far-right means each decentralized business domain does all of\\nthe particular item. Dots in between show where each responsibility area falls on this\\nspectrum.\\nIf most of your dots are on the far left, there is so much centralization in your solu‐\\ntion that calling it a data mesh might not be appropriate. It might be better classified\\nas an enterprise data fabric or data lakehouse.\\nSummary\\nThis chapter provided a comprehensive overview of the data mesh concept and its\\nfundamental principles. It began with a comparative analysis of centralization and\\ndecentralization, emphasizing the shift from a monolithic architecture to a decentral‐\\nized data landscape where data is treated as a valuable product owned by cross-\\nfunctional teams. I then explained Zhamak Dehghani’s four principles of the data\\nmesh: domain-oriented decentralized data ownership, data as a product, self-serve\\ndata infrastructure as a platform, and federated computational governance. These\\nconcepts highlight the importance of understanding data as a holistic entity that is\\nowned, maintained, and utilized by specific domain teams. The idea is to treat each\\ndata unit as an individual product with its own lifecycle, governed by the team that\\nbest understands its value and use.\\nAfter detailing foundational principles, the chapter dove into the pure data mesh,\\nemphasizing its unadulterated form. We then explored data domains, highlighting\\ntheir role in structuring the decentralized environment. The data mesh logical archi‐\\ntecture and the various topologies supporting the data mesh were discussed, showcas‐\\ning their influence on data flow and interaction. A comparison between the data\\nmesh and data fabric clarified their distinct characteristics and applications. Finally,\\nuse cases for the data mesh were discussed.\\nEven though the data mesh approach can be a game changer in how we handle data,\\nit’s not without hurdles and difficulties, which I’ll delve into in the next chapter. I’ll\\nalso address some of the common misconceptions about the data mesh and help you\\nfigure out if adopting a data mesh is the right move for your organization.\\nSummary | 185', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90d44027-1564-4533-a0ff-1e36e791960a', embedding=None, metadata={'page_label': '185', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cc848e8e-5766-40c4-8ab7-6b34f418c684', embedding=None, metadata={'page_label': '186', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 14\\nShould You Adopt Data Mesh?\\nMyths, Concerns, and the Future\\nI’ll be up-front—this chapter about the challenges of data mesh is one of the book’s\\nlongest chapters. That’s not because I think a data mesh is a bad idea or that the other\\narchitectures I’ve discussed are better; it’s that there are a lot of myths, concerns, and\\nchallenges you need to be aware of. If you decide to pursue building a data mesh, I\\nwant you to make an educated choice and not be swayed too much by the hype.\\nIn the pages that follow, I’ll dissect the misconceptions surrounding the data mesh,\\naddress the genuine concerns that often go unspoken, help you assess its fit within\\nyour organizational structure, and provide actionable recommendations for success‐\\nfully implementing it. Finally, I’ll glance toward the potential horizon of data mesh’s\\njourney and end with a discussion of the best use case for each of the four data\\narchitectures.\\nMyths\\nAs the concept of data mesh gains traction in the tech community, a variety of mis‐\\nconceptions have emerged, often clouding the understanding of data mesh’s actual\\nscope, benefits, and challenges. In this section, I aim to demystify these myths, pro‐\\nviding a nuanced perspective on what data mesh truly entails and how it fits into the\\nbroader data architecture landscape.\\nMyth: Using Data Mesh Is a Silver Bullet That\\nSolves All Data Challenges Quickly\\nIn fact, the reality is the exact opposite. Building a data mesh takes much longer than\\nbuilding the other architectures mentioned in this book. Dehghani doesn’t claim that\\n187', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='166e34b6-5c4b-4e3a-a43b-59bb5db5eebe', embedding=None, metadata={'page_label': '187', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='a data mesh is a silver bullet, but in the media hype, her ideas often come across as\\nsuch. That’s why it’s important to discuss the complexity and challenges of using data\\nmesh.\\nMyth: A Data Mesh Will Replace Your Data Lake and Data Warehouse\\nAs you learned in Chapter 13, each domain in a data mesh still needs a data lake and\\npossibly also a relational data warehouse. What changes is that you no longer have a\\n“central” data lake and RDW owned by a central IT team. Instead of having all data\\nfor all domains land in one data lake and one RDW , each domain keeps its own data.\\nThus, with a data mesh, you in fact have more data lakes and data warehouses, with\\nwhich the domains must work in collaboration.\\nMyth: Data Warehouse Projects Are All Failing,\\nand a Data Mesh Will Solve That Problem\\nI often hear people claim that current big data solutions don’t scale and propose a\\ndata mesh as the solution. I have to take issue with that argument. While hundreds of\\nthousands of organizations have implemented successful big data solutions, there are\\nvery few data meshes in production (none of which are “pure” data meshes).\\nIn my 15 years working in data warehousing, I’ve seen many “monolithic” architec‐\\ntures scale their technology and IT people very well, even supporting petabytes of\\ndata.\\nSure, many big data projects fail. However, most fail for reasons (usually related to\\npeople and processes) that would also make a data mesh fail. The available technol‐\\nogy for centralizing data has improved greatly and will continue to improve. Server‐\\nless options now meet the big data needs of most organizations, allowing solutions to\\nscale along with saving costs.\\nMyth: Building a Data Mesh Means\\nDecentralizing Absolutely Everything\\nThis is perhaps the most common myth. The reality is that even with a pure data\\nmesh, a central IT team is still responsible for creating infrastructure as a service\\n(principle #3) to jumpstart each domain’s infrastructure. A central IT team is also\\nresponsible for creating and monitoring the data governance policies that each\\ndomain follows (principle #4). It wouldn’t make sense for each domain to build its\\nown infrastructure from scratch. Nor would it make sense for each domain to create\\nits own data governance policies—you’ d end up with as many types of security and\\ninterpretations of standards and regulations as there are domains.\\n188 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='353cc29d-e778-4639-8ea3-dc1721339454', embedding=None, metadata={'page_label': '188', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Myth: You Can Use Data Virtualization to Create a Data Mesh\\nI’ve heard many data virtualization advocates say that if you use data virtualization to\\nconnect all your data lakes and data warehouses, you will have a data mesh! But that\\napproach completely ignores the four principles of the data mesh (see Chapter 13).\\nIt is a myth that you can take a single-enterprise data fabric ( Chapter 11), apply data\\nvirtualization to it, and turn it into a data mesh. Y ou can try to do that, but the result\\nis far from a data mesh—it’s data federation, which means integrating data from mul‐\\ntiple, disparate sources without physically consolidating or storing it in a single cen‐\\ntral repository. Decentralization is one small piece of what makes up a data mesh—\\njust adding virtualization won’t satisfy any of the four data mesh principles!\\nFor example, say a company has an enterprise data fabric that uses data virtualization.\\nEach domain generates its own analytical data, and the data fabric connects to that\\ndata via data virtualization software. Is this a data mesh? Let’s apply the four princi‐\\nples to find out:\\n• Principle #1 is about domain ownership. Are these true data domains or simply\\nseparate organizations within the company?\\n• Principle #2 is about treating data as a product. Does each domain have its own\\nteam building the analytical solution, using that domain’s own infrastructure?\\nAre those teams following a data contract that governs how they make data and\\nmetadata available to everyone else? And is this organization using data virtuali‐\\nzation to connect to all of its domains, or only some?\\nImportantly, if each domain doesn’t have its own domain team and infrastruc‐\\nture, then a siloed data engineering team is still acting as an IT bottleneck. Like‐\\nwise, if any domains aren’t using data virtualization, their data needs to be\\ncollected and centralized. Helping to scale infrastructure and the organization are\\nthe two main benefits of a data mesh, and if the domains aren’t following princi‐\\nple #2, they’re not realizing those benefits.\\n• For principle #3, we can ask: Is a separate team creating self-serve data infrastruc‐\\nture as a platform?\\n• And for Principle 4: Is there a separate team creating federated computational\\ngovernance?\\nAs you can see, there is a big difference between a true data mesh solution and a data\\nfabric solution that uses data virtualization.\\nMyths | 189', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='67a82190-6696-40c5-b260-e26cc16d4cac', embedding=None, metadata={'page_label': '189', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 Zhamak Dehghani, Data Mesh: Delivering Data-Driven Value at Scale (O’Reilly, 2022), p. 26.\\nConcerns\\nThere’s plenty of data mesh hype, but so far very little has been published about its\\ndownsides and challenges. This section attempts to cover those challenges compre‐\\nhensively, grouping them by subject area and offering solutions to help overcome the\\nconcerns when possible. If, after reading all these concerns, you’re sure you can over‐\\ncome any that pose a problem, great! A data mesh may be for you. And when you run\\ninto any of these concerns down the road, you won’t be surprised—you’ll be pre‐\\npared. My hope is that by knowing about these issues, you can address them before\\nyou get started, greatly increasing your odds of success.\\nWhile this list focuses on data mesh, many of these concerns apply to other architec‐\\ntures, so it’s good to review them all. Even if you are using one of the other data archi‐\\ntectures, I’ve seen firsthand that learning about data mesh can start lively\\nconversations that can help you improve whatever architecture you use.\\nPhilosophical and Conceptual Matters\\nZhamak Dehghani’s original blogs and book on data mesh set the standard, but peo‐\\nple have interpreted her writings in many different ways. Some have come up with\\n“exceptions” to the pure data mesh and other ideas about what a data mesh should be.\\nSo, if you want to build a data mesh, what definition are you going to use? Just getting\\neveryone involved in building the data mesh to come to a consensus can take months.\\nIt’s especially challenging when the makers of the many products used to build a data\\nmesh all have different ideas of what a data mesh is.\\nTo align your team on a shared understanding of data mesh, consider forming a\\ncross-functional governance committee to set foundational definitions and standards.\\nUtilize stakeholder workshops and documented definitions to ensure everyone is on\\nthe same page. Pilot projects can serve as practical tests for these definitions, allowing\\nyou to adapt as needed. Open communication and periodic reviews will help main‐\\ntain this alignment as you implement your data mesh.\\nHaving a single source of truth helps ensure that the organization bases its decisions\\non reliable data, with no ambiguity or disagreement about which data is correct.\\nEveryone working from the same set of information makes operations more efficient\\nand effective.\\nIn a data mesh, data can be read from one domain and transformed and stored by\\nanother domain. This dynamic topology can make it very difficult to maintain a sin‐\\ngle source of truth. In fact, Dehghani has called the idea of a single source of truth a\\n“myth. ”1 I strongly disagree; I personally have seen dozens of companies implement a\\n190 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56dd4076-b8be-46a7-90a2-ea78a2620841', embedding=None, metadata={'page_label': '190', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='single source of truth in various data architectures and have read about hundreds\\nmore. It can be done with a data mesh if the organization establishes rigorous data\\ngovernance and utilizes a centralized data catalog (see Chapter 6 ). Coupled with\\ncross-domain data quality metrics and strong collaboration among data domains, not\\nonly is a single source of truth feasible, but it can also enhance the mesh’s flexibility\\nand scalability.\\nThere are huge risks in implementing a data mesh, especially compared to the proven\\nsuccess of the data warehouse. It requires drastic organizational change and a whole\\nnew architecture. Plus, the data mesh assumes that each source system can dynami‐\\ncally scale to meet consumer demand. That can get particularly challenging when cer‐\\ntain data assets become “hot spots” within the ecosystem, seeing a surge in queries or\\nusage.\\nCombining Data in a Decentralized Environment\\nY ou will always need to combine data from multiple domains for queries or reports.\\nSometimes you might need to combine data from dozens of domains, which could be\\nin many separate data lakes that are geographically far apart. Y ou’ll need to think\\ncarefully about how to overcome performance issues because solutions like data vir‐\\ntualization (see Chapter 6) and aggregate domains have their own trade-offs.\\nAnother trade-off of having autonomous domains is that each domain tends to focus\\nonly on data products for its own analytical needs. Often, the owners forget to think\\nabout how their data might be combined with data from other domains and do little\\nto make it easier to combine their data model with others. When someone needs to\\ncombine data from multiple domains, they could face a lot of difficulty.\\nPrinciple #4 can help you define a set of data modeling rules for all domains to follow,\\nbut that requires someone to understand all of the domains—a feature of centralized\\narchitectures that the data mesh tries to get away from. With the internal focus of\\ndomains in a data mesh, the company might not have someone who investigates ways\\nto get value out of combined data. Thus, it’s important to ensure that such a person\\nexists.\\nIf you are using a Common Data Model (CDM; see Chapter 8), then every domain\\nneeds to use it. Y ou will have to coordinate to make sure each domain has its own set\\nof unique IDs for records with the same types of data as other domains (such as cus‐\\ntomers or products). Otherwise, when you combine data from multiple domains, you\\ncould end up with duplicate IDs. That could introduce counting errors, aggregation\\nerrors, and data inconsistencies into queries and reports. To prevent duplicate IDs in\\na CDM across multiple domains, you can use a globally unique identifier (GUID) or\\nestablish a centralized ID management system to allocate unique IDs. Alternatively,\\nyou can apply namespacing, where each ID is prefixed or suffixed with a domain-\\nspecific code.\\nConcerns | 191', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c7b59ebc-ca6c-4500-a510-377828726ef5', embedding=None, metadata={'page_label': '191', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Additionally, as domains build their products and data models, other domains and\\nconsumers will consume their data and combine data from multiple domains based\\non those data models. When a domain changes its data model or interface, dependen‐\\ncies can cause problems for the other domains and consumers using it with core data\\nmodels or queries that join domain data models. Y ou’ll need to arrange for all\\ndomains to coordinate when any domain changes its data model.\\nFinally, the domain teams may have different ideas about what counts as “clean” or\\n“standardized” data. If domains disagree on questions like whether to define states\\nwith abbreviations or full state name, combining data from multiple domains can\\ncause havoc. And what if the domains don’t have the time to build the code to clean\\nthe data properly? To tackle inconsistencies in data cleanliness across domains, estab‐\\nlish a centralized governance framework that sets standard definitions for clean or\\nstandardized data. Offer data-cleaning templates or shared libraries to save time and\\nensure uniformity. A governance committee or data steward can help enforce these\\nstandards, making it easier to combine data from multiple domains without issues.\\nOther Issues of Decentralization\\nAggregate and consumer-aligned domains (see Chapter 13) aren’t decentralized. To\\ncreate an aggregate domain, you are taking data from multiple domains and central‐\\nizing that data. With aggregate and consumer-aligned domains, you must decide who\\nowns, builds, and maintains them—the problem that principle #1 is intended to pre‐\\nvent. Building such domains also results in duplicating data. All of this seems to run\\ncounter to the spirit of a data mesh.\\nTo reconcile aggregate and consumer-aligned domains with the principles of a data\\nmesh, consider shared governance and designated data stewards to manage these\\ndomains. Utilize data virtualization to minimize duplication and implement an API\\nlayer for abstraction. Maintain transparency through robust metadata and audit\\ntrails.\\nSecurity is also an issue. In a centralized architecture, one team is responsible for\\nsecuring all data. In a data mesh, that responsibility is farmed out to dozens or even\\nhundreds of domains, creating many more doors a hacker can break through. This\\ngreatly increases the risk of security infringement.\\nY ou’ll need to assign someone to scan for personally identifiable information (PII)\\nand see who is accessing it, as well as someone to fix the issue if anyone is seeing PII\\nthat they should not be allowed to see. Someone should be responsible for making\\nsure all domains have sufficient security.\\nIt’s also hard to choose and implement technology consistently across domains.\\nAlthough principle #3 (infrastructure as a service) is supposed to prevent this, if you\\nhave dozens of domains whose people have different levels of skills and experience,\\n192 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='83141eae-fcc4-48b7-a5a8-941541c12c10', embedding=None, metadata={'page_label': '192', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 Dehghani, “How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh, ” MartinFowler.com,\\nMay 20, 2019.\\nthey’ll likely choose different technologies, set up their infrastructure differently, and\\nconfigure their cloud-product settings differently. This can lead to major issues with\\nconsuming data.\\nY ou could limit what technologies you automatically provision for each domain, but\\nthat violates principle #1 by limiting each domain’s freedom to choose technology for\\ntheir own use case. Supporting many different technologies may be challenging, as\\nwill creating a common interface across all domains. As a compromise, you could\\ncreate a large catalog of approved, vetted technologies—offering domains a choice,\\nbut within a framework that ensures easier integration.\\nFurthermore, although principle #4 (federated governance) creates data mesh poli‐\\ncies, each domain is left to implement them. This could lead to inconsistent imple‐\\nmentations, potentially resulting in performance degradations and differing cost\\nprofiles.\\nComplexity\\nThe ideas behind data mesh are complicated. I’ll freely admit that I had to read Deh‐\\nghani’s blogs dozens of times to fully understand this architecture. Even years later, I\\nsometimes still realize that I did not understand a part of it correctly. I’m not the only\\none; nearly everyone I talk to is confused about data mesh. This complexity, com‐\\nbined with the lack of a standard definition of a data mesh, means that most teams\\nthat plan to build a data mesh will have to invest a lot of time up front in making sure\\neveryone understands it (and understands it correctly). If it’s going to take a long time\\nto get started and ramp up, be sure that the data mesh will bring you enough benefits\\nto make it worth the time and effort.\\nThis complicated nature extends to organizational complexity, though data mesh pro‐\\nponents claim the opposite. Distributed teams involve many more people, and they\\nmay not always communicate effectively.\\nDuplication\\nDehghani argues that duplicating source data isn’t a problem in data mesh, writing\\nthat for each domain, “it’s very likely that we won’t need a data lake, because the dis‐\\ntributed logs and storage that hold the original data are available for exploration from\\ndifferent addressable immutable datasets as products. ”2 But you can’t always directly\\naccess the original data from those immutable datasets, as there are often problems\\nwith security or immediate retrieval. (I still have many customers using mainframes\\nthat are impossible to connect to.) So, to do anything with that source data, you’ll still\\nConcerns | 193', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2ff29cda-a14f-413f-ad0c-5817d20cfdbd', embedding=None, metadata={'page_label': '193', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='have to make a copy and store it in a data lake. In fact, a data mesh may result in more\\ncopies of data than in other architectures, since domains may need to copy data from\\nother domains to complete their data products or to create aggregate or customer-\\noriented domains. Keeping all the copies in sync can incur extra cost and effort.\\nSimilarly, a conformed dimension, used in a star schema (see Chapter 8), has the same\\nmeaning to every fact with which it relates. Conformed dimensions allow you to cate‐\\ngorize and describe facts and measures (like a date or customer dimension) in the\\nsame way, ensuring consistent reporting across the enterprise. Conformed dimen‐\\nsions are very popular in a star schema solution. In a data mesh, those conformed\\ndimensions must be duplicated in each domain that has related fact tables. That’s a lot\\nof duplicated data that needs to be kept synchronized.\\nChances are, your company is purchasing third-party data, such as weather, stock, or\\ncompetitor data. In a centralized architecture, this is handled by one person or group,\\nso the chances of two teams in the same company purchasing the same data are\\nremote. However, in a data mesh, each domain purchases its own data. How do you\\nprevent two domains from wasting money by purchasing the same datasets? None of\\nthe data mesh principles seem to cover this area, but you can enhance principle #4 by\\nsetting up a centralized team that handles all requests for third-party data.\\nDespite principle #3, some domains will likely build similar data ingestion or analyt‐\\nics platforms. With a centralized approach, there is only one team building a plat‐\\nform, but in a data mesh, dozens of domains might do so. It’s a good idea to promote\\nsharing these solutions so domains aren’t duplicating one another’s efforts or missing\\nout on helpful solutions other domains create.\\nFeasibility\\nMigrating to a data mesh is a massive undertaking that requires a huge investment in\\norganizational change and technical implementation, far more than with the other\\ndata architectures. This will result in a much higher cost and a much longer time‐\\nline— many months, possibly even years, before you can put it into production. It\\ntakes a lot to get domains using their own created reports and dashboards well\\nenough that central IT can stop supplying them. In the meantime, unless you are a\\nbrand-new company, you will have to keep your existing platform running. This can\\nbe quite challenging, especially as new domains transition from supplying opera‐\\ntional data to central IT to keeping it within their domain.\\nAgain, data mesh is a concept, not a technology. It’s all about creating organizational\\nand cultural shifts within a company. Dehghani’s book doesn’t discuss how to use\\nexisting technology to build a data mesh. That’s likely because there are major gaps in\\nthe available technology in areas such as data quantum, automation, product sharing,\\ndomain infrastructure setup, and collaborative data governance.\\n194 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='41753029-bd13-44b2-9576-bd207f14e94f', embedding=None, metadata={'page_label': '194', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Principle #2 requires that technology be available to publicize and share domains, but\\nthe current technology for this is in its infancy. Principle #3 requires very sophistica‐\\nted solutions for automating infrastructure provisioning, but current solutions do\\nonly a small piece of this at best. And creating and managing principle #4’s global\\nrules—at the same time that domains create and manage their own rules—means\\nrelying heavily on automation and coding solutions for data product lifecycle man‐\\nagement, standards as code, policies as code, automated tests, automated monitoring,\\nand embedding policy execution in each data product. This requires very sophistica‐\\nted software that, where it exists at all, is so far quite limited. Companies have to build\\nit themselves or just skip this step, thus violating the data mesh principles.\\nDehghani describes data products as having output data ports (interfaces) with\\nexplicitly defined contracts and APIs. These APIs would serve multimodel access,\\nbitemporal data, and immutable read-only access, and they could transfer huge\\namounts of data and scale to a huge number of requests. In addition, these APIs\\nwould span one or more multiple physical infrastructures, multiple clouds, and on-\\nprem hosting environments. So far, the technology for APIs to support all of that does\\nnot exist. It likely never will.\\nFurther, APIs are rarely used with data; interacting with data is usually left to SQL\\nqueries. Creating APIs for all interactions with data is a big culture shift that would\\ntake way more time than using SQL queries.\\nIn her book, Dehghani introduces a new unit of logical architecture called a data\\nquantum, which controls and encapsulates all the structural components needed to\\nshare data autonomously as a data product that others can consume. This includes\\ndata, metadata, code (compute), policy, and infrastructure dependencies. Data and\\ncompute are logically joined as one unit, but the underlying physical infrastructures\\nthat host code and data are kept separate. There is currently no technology for this.\\nWhile Dehghani has started a company to create a data quantum using data product\\ncontainers, it could be years before her company or any other builds components to\\ntackle the majority of these requirements.\\nI think that data quantum is a very risky concept. Y ou can certainly build a data mesh\\nwithout making use of it. Think of data lineage using a data quantum. If each\\ndomain’s metadata is only available within the data product itself, to see its metadata,\\nlineage, or data quality information, you’ d need to perform complex federated queries\\nagainst all the data products. Y ou could replicate all the metadata to a central loca‐\\ntion, but that would require constant syncing and make the overall architecture much\\nmore complicated. Then there is the matter of security; you’ d need to update meta‐\\ndata such as sensitivity labels, privacy classifications, and ownership information for\\ndata that is physically located in multiple places.\\nThe issues with copying data extend to data products. If they are copied, their meta‐\\ndata is copied too, resulting in multiple copies of the same metadata that need to be\\nConcerns | 195', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56bccb57-af2a-48a3-b76f-2b5453b6ad0f', embedding=None, metadata={'page_label': '195', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3 Dehghani, Data Mesh, p. 300.\\nkept in sync. Any updates to the metadata would need to be carried out simultane‐\\nously over multiple data product containers, requiring these containers to always be\\navailable and accessible. Y ou risk ending up with different owners for the same\\nmetadata.\\nFinally, running hundreds or even thousands of small data products side by side dra‐\\nmatically increases complexity, increases utilization of infrastructure resources and\\nnetworks, complicates configuration management, and introduces challenges with\\nperformance and reference data consistency.\\nDehghani holds that since no data mesh–native technologies have yet been created,\\n“you can utilize data warehousing technologies as an underlying storage and SQL\\naccess mode but allocate a separate schema to each data product, ”3 thus sharing com‐\\npute. But this results in a centralized infrastructure, violating principle #1 and losing\\nthe biggest benefit of using a data mesh: technical scaling.\\nPeople\\nY ou’ll need to find and hire quality engineering people and other skilled workers for\\neach domain. It’s hard enough finding good people for a central IT team, but in a data\\nmesh, each domain is tasked not only with learning to build an infrastructure and\\nanalytical platform, but also creating and hiring its own mini IT team. If you have a\\nlot of domains, that’s many more people than you’ d need in a centralized architec‐\\nture—a huge trade-off when you’re doing horizontal organizational scaling.\\nPrinciple #3 is designed to take away the need for deep technical skills and instead use\\nhigher cross-functional skills. This often entices organizations to have people from\\nbusiness domains, instead of experienced people with strong technical skills, do the\\nimplementation in each product group, especially since it’s hard to find enough peo‐\\nple with quality technical skills. Inexperienced people tend to build expensive, com‐\\nplex, badly performing data products.\\nData mesh assumes that central IT data engineers don’t have business and domain\\nknowledge and thus that it’s better to use people familiar with the domains that own\\nthe data. It further assumes that each domain team has the necessary skills to build\\nrobust data products or can acquire these skills.\\nThat’s not always true. Surprisingly, I’ve seen central IT people with more domain\\nknowledge than the domain experts! The IT people also understand how to combine\\ndata from different domains. Sometimes the people running the operational systems\\nhave very little understanding of their data, especially from an analytical perspective;\\nthey’re just focused on keeping the lights on. If central IT data engineers don’t have\\n196 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b696a083-6aab-41e7-8243-123b74c00d89', embedding=None, metadata={'page_label': '196', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the domain knowledge they need, why not have them obtain that knowledge and\\nimprove communication between IT and the domains? That could be a better solu‐\\ntion than introducing a whole new way of working.\\nData mesh proponents boast that instead of having data specialists with deep exper‐\\ntise in only one area, data mesh encourages generalists. No one works on one isolated\\nstep. Data mesh envisions cross-functional teams of people who have intermediate\\nskills in many areas: for instance, streaming, ETL batch processing, data warehouse\\ndesign, and data visualization, along with software-engineering practices such as cod‐\\ning, testing, and continuous delivery (DataOps). Think “jack of all trades, master of\\nnone. ” Again, if you have many domains, this means many roles to fill and salaries to\\npay. It’s not easy to find and recruit people with such a wide range of skills, let alone\\nlots of people. It’s debatable whether these multiskilled people would be better than\\nindividuals with years of deep expertise in one area.\\nDomain-Level Barriers\\nLet’s say your domain has finally been granted permission to have central IT add des‐\\nperately needed features to the domain’s analytics so that you can get real value out of\\nyour data. Y ou’ve mapped out a plan with central IT to create the analytics in the next\\nfew months. Then you learn that your company will be building a data mesh and it’s\\non you to hire a whole new team to create those needed features and the APIs to go\\nwith them. Then you’re asked to delay the project for months while the company\\nimplements principles #3 and #4, designs the data domains, and writes the contract\\nthat will guide each domain’s upgrades.\\nHow would you feel? And what if people in dozens of domains feel that way? The\\ncompany can’t have a rogue domain; consumers as well as other domains depend on\\nhaving data available from each domain, so it’s crucial that every domain buys into\\nthe data mesh project. Y ou’ll need to:\\n• Provide compelling incentives to make up for the extra work, hassle, and delay.\\nIt’s not enough incentive to tell them they’re doing it “for the greater good. ”\\n• Gain buy-in from every domain for implementing a data mesh.\\n• Secure top-level executive sponsorship.\\n• Communicate a clear vision that shows benefits for each domain.\\n• Run pilot programs to demonstrate the efficacy of the new approach.\\n• Offer incentives, both financial and nonfinancial, to encourage participation.\\n• Establish transparent channels for ongoing communication.\\n• Offer support through competency centers or centers of excellence.\\nConcerns | 197', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='74d4224b-3c36-406a-963c-1f0d2350191e', embedding=None, metadata={'page_label': '197', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4 Dehghani, Data Mesh, pp. 271–75.\\nPlan for what you’ll do if a domain does not want to follow the rules. This could be\\nespecially troublesome for data security and data quality standards. If you decide that\\ndomains that don’t follow the rules can’t join the data mesh, you’ll have data silos.\\nThere could also be domains that can’t join the data mesh, even if they want to. Per‐\\nhaps they don’t have the budget to build a team to create an analytics platform, or\\ncircumstances force them to keep using older technology that can’t handle an analyt‐\\nics platform. To incorporate their data into the data mesh, you might need a central‐\\nized architecture alongside the data mesh.\\nEffective product sharing is crucial to your data mesh’s success. Sharing products\\nwithin and outside each domain in a data mesh is very different from sharing data.\\nY ou’ll need a way for other domains or consumers to keep track of all the domains in\\nyour company and their data products. This will require all the domain metadata to\\nbe cataloged somewhere with a search mechanism and a way to request access: a sort\\nof data marketplace, as described in Chapter 6.\\nThe major cloud providers offer many software solutions for sharing data, but very\\nfew for sharing data products (and these are all recently released as of this writing).\\nY ou’ll need to decide: Will you purchase one or create your own?\\nA data mesh results in a shift toward a self-serve model for data requests, which\\npresents challenges. Traditionally, domains have relied on central IT departments to\\nhandle the entirety of their analytical data needs, which encompassed everything\\nfrom managing data to addressing specific requests like queries and reports. With\\ndata mesh, this responsibility is decentralized, landing squarely on the individual\\ndomains to satisfy those data requests. A potential risk is that some domains may lack\\nthe expertise to manage these tasks efficiently. Additionally, they can’t merely repli‐\\ncate the solutions implemented by central IT, they have to improve them to show the\\nvalue of a data mesh. As they craft their unique solutions, there’s a possibility that\\nthese might not even be as effective or robust as the centralized systems they replace.\\nOrganizational Assessment:\\nShould You Adopt a Data Mesh?\\nBy now, you understand what a data mesh is and the potential concerns that come\\nwith it. If you still feel like a data mesh is an option, the next step is to do a thorough\\nassessment to find out if your organization is ready to build a successful data mesh.\\nY ou can use the criteria in Table 14-1, which are drawn from Dehghani’s book.4 Deh‐\\nghani recommends that companies consider building a data mesh only if all criteria\\nare somewhat or highly applicable.\\n198 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1481bab0-60c4-4274-86fa-0a132ae4d4da', embedding=None, metadata={'page_label': '198', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 14-1. Self-assessment of your company’s readiness for data mesh\\nCriterion Rating (select one)\\nWe have too much organizational complexity for our current system and our\\nexisting data architecture has become a blocker; it can’t scale quickly enough\\nto get value from data from many parts of the business.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nOur company has a data-oriented strategy and a strategic vision for using and\\ngetting value out of data.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nOur executives support the culture and work changes needed for a data mesh\\nand are committed to motivating the entire organization to change how\\npeople work.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nWe see data as a valuable asset and regularly collect it, analyze it, and use it\\nto inform business decisions. We have built our operations, decision-making\\nprocesses, culture, and strategies around data. We invest heavily in a data\\ntechnology infrastructure, analytics platforms, and machine learning. Our\\nemployees are data literate.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nOur company likes to be on the “bleeding edge” and is always one of the first\\nto adopt and implement new technologies and innovations, even when it\\ninvolves some risk.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nOur company employs modern engineering practices and technologies,\\nincluding data-driven decision making and user-centered design, that enable\\nmany smaller teams instead of one large centralized team.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nOur organization is structured around specific business areas or data domains,\\neach with dedicated technical teams who build and support the products that\\nserve that domain. Every domain has (or will soon have) a budget for creating\\nanalytical data from its operational data and for building its own data mesh\\ninfrastructure.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nEven if it takes a long time, our company is committed to using machine\\nlearning and analytics to gain a competitive advantage over our competitors.\\nWe are committed to a process of culture change and to designing and\\nbuilding the technology necessary for data mesh, as well as a network of\\ndomain-oriented teams.\\nNot\\napplicable\\nSomewhat\\napplicable\\nHighly\\napplicable\\nThe reality, based on my experience, is that only a tiny percentage of companies—\\nmaybe 1%—find all of these categories somewhat or highly applicable. High scores on\\nthe early adopter, domain-oriented, and modern engineering categories are particu‐\\nlarly rare.\\nRecommendations for Implementing a\\nSuccessful Data Mesh\\nIf you have a current data solution and decide to build a data mesh, I recommend a\\nhub-and-spoke model, in which you build the data mesh as an extension to a central‐\\nized data solution (Figure 14-1). Y ou might start by using new data to create new data\\nmesh domains, but supplement those domains with your current centralized data\\nRecommendations for Implementing a Successful Data Mesh | 199', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a23f905d-bb66-43b6-af29-a9e127e73794', embedding=None, metadata={'page_label': '199', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='solution in alignment with business needs. Slowly migrate your centralized data into\\ndata mesh domains over time as you continue adding more new domains from new\\ndata—don’t try to convert all of your centralized data into data mesh domains in one\\nfell swoop.\\nFigure 14-1. A hub-and-spoke model, with a data mesh supplementing and extending a\\ncentralized data solution\\nImplementing the data mesh gradually helps you manage the risks associated with a\\ncomplete overhaul of your existing data architecture. If there’s a failure in one part of\\nthe mesh, it’s contained and doesn’t affect the entire system. Y ou get better control\\nover compliance and security measures and time to establish proper governance and\\nsecurity protocols. This gradual approach also spreads out the cost and the demands\\non the workforce. Existing business processes that rely on the centralized data can\\ncontinue to function smoothly during the transition, minimizing disruption to the\\nbusiness.\\nWhat I’m suggesting is an iterative approach. Implementing new domains gradually\\nallows you to learn from each phase, make necessary adjustments, and apply those\\nlessons to subsequent domains, creating a more robust system in the long run. Fur‐\\nthermore, it allows you to leverage your existing technology investment while moving\\ntoward a more modern architecture.\\nThis approach is also an agile one. It gives you the flexibility to prioritize certain\\ndomains that provide more immediate value or align with strategic business initia‐\\ntives. It allows for alignment with business needs rather than a forced migration all at\\nonce. Y ou also get a chance to optimize the performance of each domain’s infrastruc‐\\nture and can tailor it to that domain’s specific needs and constraints.\\nFinally, the hub-and-spoke approach paves the way for a cultural shift over time, fos‐\\ntering cross-functional, domain-driven collaboration among data product owners,\\nengineers, and business stakeholders.\\nThis piecemeal data mesh implementation is a strategic approach that allows for care‐\\nful planning, risk management, and alignment with business goals. It supports a grad‐\\nual transition from a centralized data architecture to a decentralized one, leveraging\\nboth the old and the new systems during the transition period. It’s a more adaptable\\nand resilient approach than attempting a complete transition all at once, which is\\nfraught with risk and complexity.\\n200 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0083bfcd-7eae-4e8d-a1a9-887cea1ea211', embedding=None, metadata={'page_label': '200', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The Future of Data Mesh\\nI have talked to many people who believe that the idea of a centralized team building\\nthe entire data architecture is dying out and that eventually every company will move\\nto a decentralized model. After all, centralized models don’t scale well.\\nThis was discussed back in the days of the Kimball versus Inmon debates (see Chap‐\\nter 8). In the Inmon camp, IT centralized everything; but in the Kimball camp, the\\nmethodology was to build loosely centralized subject matter data marts and hold\\nthem together using conformed dimensions with a bus architecture. In practice, how‐\\never, you had many domains building their own data marts in isolation, sometimes\\nwith different technologies, resulting in a decentralized architecture. When people\\nneeded to join and aggregate data from multiple domains, they created additional\\ndata marts and/or OLAP cubes.\\nSometimes this strategy worked fine, but often it led to duplicated efforts, using all\\nsorts of different technology and products. It was also difficult and commonly sent\\ncosts spiraling out of control. The company would then move back to a centralized\\nmodel to prevent these problems.\\nWill the data mesh lead to the same cycle of problems? What makes this time any\\ndifferent? Although technology has come a long way since the data mart days, the\\nproblem really isn’t one of technology—it’s a people and process problem, and we\\nhaven’t come nearly as far in solving those issues. Come up with all the cool new\\nbuzzwords and methods you like, but it’s extremely hard to get people to change—\\nand data mesh requires a ton of change.\\nI predict that very few companies will adopt data mesh architectures, and I don’t\\nbelieve that any of those solutions will be a pure data mesh. For those that do call\\ntheir solution a data mesh, I estimate that most—perhaps 90%—will adopt domain\\nownership (principle #1), while fewer—perhaps 70%—will adopt data as a product\\n(principle #2). Perhaps a very small percentage of those will take it to the level of a\\ndata quantum. About a third of companies will use some form of automated infra‐\\nstructure for each domain (principle #3: self-serve data infrastructure as a platform),\\nand maybe half will adopt federated computational governance (principle #4). I\\nexpect a very large percentage of solutions to place limits on the technologies that\\neach domain can use.\\nInstead of trying to implement every element of a data mesh, I recommend focusing\\non how you can empower your data teams to deliver value faster and more frequently\\nfor your customers. From there, you can work backward to identify and adopt spe‐\\ncific elements of a data mesh that will help you to achieve this goal.\\nThe Future of Data Mesh | 201', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1e683b44-b7e9-4657-956d-6a60d20fb82b', embedding=None, metadata={'page_label': '201', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In the end, a data mesh is a battle between centralization and decentralization. There\\nwill always be a mixture of both in any data mesh solution. It’s about finding the right\\nbalance for your use case.\\nZooming Out: Understanding Data Architectures\\nand Their Applications\\nIn the modern world of rapid technological advancement, businesses stand at a cross‐\\nroads, surrounded by an abundance of data. This data, if harnessed correctly, can\\nunlock unprecedented insights, streamline operations, and provide a competitive\\nedge. The key to unlocking this potential lies in the architecture—the framework that\\ndecides how data will be stored, processed, and accessed.\\nOver the course of this book, I’ve delved deep into the intricacies of various data\\narchitectures, their evolution, and their practical application. Now, as I prepare to\\nclose this chapter, it’s crucial to take a step back and view the landscape from a higher\\nvantage point.\\nY ou have a great many choices. How can you know which one is right for your orga‐\\nnization? To address this, let’s look at some high-level use cases for each architecture,\\ncategorized by cost and complexity:\\nModern data warehouse\\nMDWs are most suitable for businesses that handle a small amount of data and\\nare already accustomed to relational data warehouses, making for a gentle\\ntransition.\\nData fabric\\nAs businesses grow and diversify, they often find themselves grappling with data\\nfrom myriad sources, each differing in size, speed, and type. Data fabric is the\\narchitectural choice for those who need a seamless way to integrate these diverse\\ndata streams.\\nData lakehouse\\nThis can be visualized as a middle-ground solution. Y ou should engage with a\\ndata lakehouse as your primary solution until you encounter its limitations.\\nWhen you do, you can transfer specific datasets to an RDW to better cater to\\nyour organization’s needs.\\nData mesh\\nTailored for very large, domain-oriented companies, the data mesh is for organi‐\\nzations that are confronting major scalability challenges and have the resources\\nand time to invest in a robust solution.\\n202 | Chapter 14: Should You Adopt Data Mesh? Myths, Concerns, and the Future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fc61f78b-2825-4513-98b3-05b11ca8625c', embedding=None, metadata={'page_label': '202', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='These architectures are not mutually exclusive. The real world rarely deals in abso‐\\nlutes. Most organizations will find that their unique needs demand a mosaic of fea‐\\ntures from multiple architectures. Such a tailored solution can combine aspects of any\\nor all of these architectures, adapted to the organization’s specific data use cases and\\nits evolving business capabilities.\\nSummary\\nThis chapter addressed common myths about data mesh, clarified valid concerns,\\nand assessed its fit within various organizational structures. I provided key recom‐\\nmendations for successful implementation, grounded in real-world practices and case\\nstudies. We explored the future of data mesh in the context of evolving technologies\\nand business trends. Finally, I summarized all the data architectures to help guide you\\nin the selection of the most appropriate data architecture for your specific business\\nneeds. Hopefully you now have both a theoretical understanding and practical\\ninsights that will allow you make informed decisions about data mesh adoption.\\nThe future of business is inextricably linked with the future of data. As we move for‐\\nward, the architectures we choose will determine not only how we store and access\\nour data but also how our businesses grow, innovate, and compete. It’s essential to\\napproach this choice with deep understanding, adaptability, and a vision for the\\nfuture. Remember, the architecture you choose today will pave the path for your busi‐\\nness tomorrow. Choose wisely, and let your data lead the way.\\nThe previous chapters of this book delved into technological aspects of data architec‐\\ntures. In Chapters 15 and 16, we’ll explore the crucial topics of people and processes,\\nexamining team organization, the causes of project failures, and the keys to project\\nsuccess.\\nSummary | 203', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='928e6790-b2e9-4bed-9c4a-24cd3b42656a', embedding=None, metadata={'page_label': '203', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ecdb1e37-ef66-433f-8b89-f32aa8b0eb8d', embedding=None, metadata={'page_label': '204', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='PART IV\\nPeople, Processes, and Technology\\nCongratulations! Y ou have gotten through the foundational sections, the common\\ndata architecture concepts, and the four data architectures. But you’re not done yet.\\nChapter 15 covers the biggest determining factors in the success of whatever solution\\nyou build: people and processes. Chapter 16 covers data architecture technologies. To\\nensure that it stays relevant for many years to come, this discussion takes a vendor-\\nneutral, historical, and high-level perspective.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd85927f-06b6-4b75-a1d0-6fdbe2d26610', embedding=None, metadata={'page_label': '205', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6a961dbe-095c-48c5-b7a1-e4cf0394fb52', embedding=None, metadata={'page_label': '206', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 15\\nPeople and Processes\\nIn the complicated world of data architectures, while technology and tools might\\ncommand the spotlight, people and processes form its heart and soul. As organiza‐\\ntions transition from traditional data models to innovative structures like MDW , data\\nfabric, or data lakehouse, and even further into decentralized paradigms like data\\nmesh, the human element remains central. But why?\\nIn my many years building data warehouses, applications, and reporting projects, I\\nhave seen what helps projects succeed and what makes them fail. Imagine construct‐\\ning an architectural marvel—a skyscraper with state-of-the-art amenities. The blue‐\\nprint is flawless, the materials top-notch. Y et if the builders, engineers, and managers\\naren’t aligned in their vision, or if processes aren’t in place to ensure cohesive action,\\nthe project can crumble before it even begins.\\nIn much the same way, the transformation of a data architecture demands not just\\ntechnological reshuffling but profound shifts in organizational culture. Changing\\nhow data is managed, processed, and valued means realigning how teams think about\\nand interact with data. It means challenging established norms and embracing new\\nmethodologies. Such transitions are never merely technical; they are deeply human.\\nCentral to this human element is delineating roles. Who does what? Who shoulders\\nthe responsibility for ensuring that data flows smoothly, that it is processed correctly,\\nand that it is used ethically and effectively? Misunderstand or underestimate these\\nroles, and you teeter on the precipice of inefficiency or, worse, project failure. Con‐\\nversely, getting the roles right—ensuring that each team member understands their\\nresponsibilities and has the resources to fulfill them—is akin to setting solid founda‐\\ntional stones for your architectural wonder.\\nY et, like any major endeavor, data projects come with their success stories and cau‐\\ntionary tales. The line between a triumphant data transition and a failed solution can\\n207', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9dc992df-6472-489a-a831-fa3410c90089', embedding=None, metadata={'page_label': '207', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='often be traced back to how people were managed and how processes were designed.\\nLearning from both the victories and the pitfalls offers invaluable insights into navi‐\\ngating the challenges and complexities of data architecture transformations.\\nAs we delve into this chapter, we’ll explore the intricacies of team organization, the\\nnuances of roles in various data frameworks, and the critical factors that spell the dif‐\\nference between the success and failure of data projects. By the end of this journey,\\nyou’ll appreciate that while tools and technologies evolve, it’s the people and pro‐\\ncesses that determine whether they’re wielded for success or relegated to the annals of\\nmissed opportunities.\\nTeam Organization: Roles and Responsibilities\\nBuilding a data architecture requires a team (or many teams) with a diverse set of\\nskills and roles. This section outlines some key roles, their responsibilities, and how\\nthey can collaborate to build a modern data warehouse, data fabric, or data lakehouse.\\n(For a data mesh, you’ll need additional roles; I’ll talk about those next.) In smaller\\ncompanies, one person might take on multiple roles; for example, the same person\\nmight serve as both data governance manager and data quality manager. In larger\\ncompanies, several people may fill the same role—for example, you might have a\\ndozen data engineers.\\nRoles for MDW, Data Fabric, or Data Lakehouse\\nLet’s look first at the general roles:\\nData architect\\nDesigns the high-level structure of the data architecture (MDW , data fabric, or\\ndata lakehouse) and decides what technologies and data governance policies the\\nproject should use.\\nData engineer\\nResponsible for building, testing, and maintaining the data architecture. They\\nwrite scripts to extract, load, and transform data (ELT) from various sources into\\nthe data solution and work closely with the data architect to implement the\\ndesigned architecture.\\nData steward\\nIn charge of data quality and governance. They define and implement rules for\\ndata usage and standards, oversee data cleansing, manage data access rights, and\\nwork with users to resolve any data quality or access issues.\\n208 | Chapter 15: People and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='526c095a-71b6-4098-b588-28ceba83d11a', embedding=None, metadata={'page_label': '208', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Database administrator (DBA)\\nResponsible for the operation, maintenance, and performance of the data system.\\nThey implement backup and recovery plans, optimize queries for performance,\\nand keep the system running smoothly.\\nData analyst\\nCollects, processes, and performs statistical descriptive and diagnostic analyses\\non large datasets, identifying patterns and trends, transforming these complex\\nfindings into understandable insights, and communicating them to stakeholders.\\nThey create comprehensive reports and dashboards to track business metrics.\\nData scientist\\nUses advanced statistical techniques, predictive modeling, and machine learning\\nto extract valuable insights from data and solve complex problems. They design\\nand implement ML models, perform predictive and prescriptive analysis, con‐\\nduct A/B tests, and mine the data. They also contribute data-driven insights and\\nrecommendations to influence strategy and operations decisions.\\nBusiness analyst\\nCommunicates between the data team and business users and helps define the\\nbusiness requirements for the data architecture, then translates those into techni‐\\ncal requirements. They also validate data analysts’ and scientists’ insights and\\ncommunicate them to business users.\\nProject manager/scrum master\\nCoordinates the team’s work, ensures they are on track to meet their goals, and\\nresolves any blockers. They also communicate the project’s status to stakeholders\\nand manage any risks.\\nData privacy officer\\nEnsures compliance with data privacy regulations, works with the data steward to\\nset usage policies, and responds to any data privacy incidents. In some sectors or\\nregions, regulations mandate this role.\\nData governance manager\\nEnsures the availability, usability, integrity, and security of the organization’s data\\nand creates and enforces data management policies and procedures for to ensure\\nbest practices across all business units.\\nData quality manager\\nOversees the completeness, consistency, and accuracy of data, including imple‐\\nmenting data quality rules, monitoring data quality metrics, and coordinating\\nwith other teams to fix any data quality issues.\\nTeam Organization: Roles and Responsibilities | 209', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='67714173-859e-4624-a0a2-509f929fa434', embedding=None, metadata={'page_label': '209', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Roles for Data Mesh\\nAs you learned in Chapters 13 and 14, the data mesh approach views data as a shared\\nasset across different domains within an organization. Instead of having a central data\\nteam handle all aspects of data, the data mesh approach distributes responsibilities to\\ncross-functional domain teams. Each team is responsible for the data it produces,\\nwhich includes providing data as a product, ensuring its quality, and making it acces‐\\nsible to others. This changes the required teams, roles, and responsibilities quite a bit.\\nLet’s break it down team by team.\\nDomain teams\\nEach domain has one domain team. Its members are responsible for building their\\ndomain’s analytical solution and implementing governance policies and the data\\nmesh principles (outlined in Chapter 13). Most of the roles in the domain team, such\\nas domain data engineers, data analysts, and domain data stewards, mirror the roles\\nlisted above for an MDW , data fabric, or data lakehouse, except their scope is a spe‐\\ncific domain instead of the entire organization. Since each domain has its own solu‐\\ntion, most, if not all, of these roles exist within each domain. One additional role\\nspecific to data mesh is the data product owner.\\nA domain data product owner takes responsibility for the data that their domain pro‐\\nduces. This includes managing its lifecycle, ensuring its quality, making it accessible,\\nand aligning it to business needs. The product owner should understand both the\\ntechnical and business aspects of the data.\\nSelf-service data infrastructure platform team\\nA platform team focused on self-service data infrastructure, in alignment with data\\nmesh principle #3, can provide domain teams with the technology infrastructure,\\ntools, and guidelines to build and manage their data products. Roles on this team\\ninclude:\\nData mesh architect\\nDesigns the general architecture and guidelines, coordinates the efforts of the\\ndomain teams, and facilitates communication and learning. While each domain\\nteam manages its data, this architect role ensures the data mesh’s overall\\ncoherence.\\nPlatform architect\\nDesigns the overall architecture of the data platform to supports the needs of the\\ndomains and accommodate a variety of data types, workloads, and analytical\\ntools.\\n210 | Chapter 15: People and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a259105d-9388-40f4-beda-07e20a3f5489', embedding=None, metadata={'page_label': '210', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Platform engineers/developers\\nBuilds and maintains the platform infrastructure, including databases, APIs, con‐\\ntainer orchestration, and other aspects.\\nData engineer\\nWorks on the pipelines that move and transform data within the platform, ensur‐\\ning that the data is clean, consistent, and available to all who need it.\\nDevOps engineer\\nAutomates platform deployment, scaling, and management, ensuring that the\\nplatform is stable and reliable and can handle the data teams’ needs.\\nDataOps engineer\\nManages and optimizes the lifecycle of data and data science models from end to\\nend, including by monitoring data quality and ensuring that data privacy and\\ncompliance requirements are met.\\nSecurity and compliance officer\\nEnsures that the platform follows all necessary regulations and best practices for\\ndata security and privacy.\\nProduct owner/manager\\nActs as the link between the technical team and the business stakeholders, priori‐\\ntizing and managing platform features and enhancements to align with users’\\nneeds and the business’s objectives.\\nUser experience (UX) designer\\nDesigns the interfaces that domain teams use to interact with the platform, aim‐\\ning for ease of use and effectiveness.\\nSupport and training staff\\nAssists domain teams in using the platform, troubleshoots issues, and trains new\\nusers.\\nFederated computational governance platform team\\nIn accordance with data mesh principle #4, another platform team is focused specifi‐\\ncally on federated computational governance, providing each domain with the blue‐\\nprints to help it comply with security and privacy regulations. Most domain teams\\nimplement these blueprints themselves, but this platform team may sometimes\\nimplement global polices. It provides the necessary guidance to help each domain\\nteam manage its data in alignment with the organization’s policies and standards.\\nThis team must have a deep understanding of each domain’s specific data needs and\\nwork to balance those needs with the organization’s overall data governance require‐\\nments. Roles in this team include:\\nTeam Organization: Roles and Responsibilities | 211', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='875177d5-de24-4d74-89cf-e6c6ab6b471a', embedding=None, metadata={'page_label': '211', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data governance lead\\nGuides the overall governance strategy and ensures that the team’s initiatives\\nalign with the organization’s strategic goals. They coordinate with various\\ndomain representatives and subject matter experts to define governance stand‐\\nards and policies.\\nData governance specialist\\nEstablishes and oversees data standards and policies, working closely with\\ndomain teams to ensure they apply the guidelines correctly.\\nData privacy and compliance officer\\nEnsures that domain teams adhere to data privacy regulations and internal com‐\\npliance policies, guiding them on legal and regulatory considerations in collabo‐\\nration with external legal and compliance experts.\\nData security analyst\\nDefines the security standards and procedures that domain teams must follow,\\nprovides them with guidance, and ensures that they follow the security aspects of\\nthe governance policies.\\nData quality specialist\\nDefines quality standards and procedures to ensure the quality of the data being\\ngenerated across the organization. They work with domain teams to set up data\\nquality metrics and monitoring systems.\\nDomain representative\\nServes as a representative from their domain teams, bringing domain-specific\\ndata knowledge to the governance platform team and ensuring that governance\\npolicies align with their domain’s needs.\\nData architect\\nDefines data-modeling standards, including designing and managing logical and\\nphysical data models.\\nDataOps engineer\\nAutomates the implementation and monitoring of data governance policies. May\\nalso maintain the systems and tools that enable data tagging and other data iden‐\\ntification and management mechanisms.\\nThe exact roles and responsibilities of each team will vary, depending on the specifics\\nof the organization and its data mesh. What’s most important is that each domain\\nteam takes responsibility for that domain’s data as a product.\\nThese roles need to collaborate closely to make sure that the data architecture meets\\nthe needs of the business and users. The architecture should be flexible to handle\\n212 | Chapter 15: People and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eda26b53-ab3e-4e91-8c4f-0bb6d228bcbb', embedding=None, metadata={'page_label': '212', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='changes in the business and technology landscape and robust enough to ensure data\\nquality and availability.\\nAs we delve deeper into understanding the intricacies of team dynamics, it’s impera‐\\ntive to recognize that even the best-organized teams can face unforeseen challenges.\\nLet’s shift our focus from the roles that drive success to the common pitfalls that can\\nhinder a project—and how you can take preventative measures.\\nWhy Projects Fail: Pitfalls and Prevention\\nNow let’s talk about the common reasons big data projects fail. Some studies  show\\nhigh failure rates; for instance, in 2019, VentureBeat reported that “87% of data sci‐\\nence projects never make it into production. ” This section outlines some of the most\\ncommon reasons I have seen them fail, along with ways to avoid these pitfalls.\\nPitfall: Allowing Executives to Think That BI Is “Easy”\\nI’ve met a number of executives with very unrealistic ideas about how long it takes to\\nbuild a data architecture. They often pressure project managers into adopting unreal‐\\nistic timelines and milestones, thus setting the project up to fail.\\nTo prevent succumbing to this pitfall, you must educate the executives. Help them\\nunderstand the process and the time it takes to properly build and test a solution. I’ve\\neven had executives attend a one-day coding session so they can see firsthand how\\nhard it is to build a data solution.\\nPitfall: Using the Wrong Technologies\\nWhen companies fail to educate themselves about all of the available data solution\\nproducts and their use cases, they can end up using the wrong product for the job.\\n(For example, I’ve seen a company use SQL Server as a data lake!) People don’t know\\nwhat they don’t know, so they only use products they’ve heard of. As the saying goes,\\nwhen you have a hammer, everything is a nail.\\nDecision makers must invest time in understanding all the possible tools, then choos‐\\ning the right tool for the job to prevent this. I recommend forming several commit‐\\ntees, each one focused on researching a specific group of products, such as ETL,\\nanalytical, or reporting tools.\\nPitfall: Gathering Too Many Business Requirements\\nWhen I was a developer, I worked for a company that spent over a year gathering\\nbusiness requirements for a data solution while we sat around waiting for them to\\nfinish. They wound up producing hundreds of pages of requirements, putting\\nWhy Projects Fail: Pitfalls and Prevention | 213', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='01e769e0-0110-4f15-870c-8c64e8619424', embedding=None, metadata={'page_label': '213', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='everything behind schedule. No one read the whole document; the requirements\\nphase had taken so long that we read just a few sections before starting development.\\nTo avoid this pitfall, spend just a few weeks on requirements, gathering enough to\\nallow the developers to start coding, then continue gathering requirements in parallel\\nwith coding.\\nPitfall: Gathering Too Few Business Requirements\\nSome project teams spend barely any time with end users gathering requirements;\\nsome leave it up to the developers to guess what the end users want. They often end\\nup developing the wrong features, not at all giving users what they need or want.\\nIf you adopt an iterative method for project development, you can avoid this issue.\\nThis approach involves constructing parts of the solution, then revisiting design and\\nplanning phases, allowing for adjustments based on feedback and evolving require‐\\nments in a continuous cycle.\\nPitfall: Presenting Reports Without Validating Their Contents First\\nIf you present an end user with a report from the new solution you’re building and it\\ncontains an incorrect number, their first impression will be a bad one. When users\\nlose confidence in reports, it’s very hard to win back their trust. Companies don’t\\nalways take the time to make sure everything in the report is correct, and the result\\ncan be a major setback.\\nHave a rigorous QA process that includes comparing the report totals in the new sol‐\\nution with those in a production report that is known to be valid. Ask end users for\\nhelp, if possible; not only will they be able to confirm the reports are correct, but\\nthey’ll also feel some ownership from being part of building the new solution.\\nPitfall: Hiring an Inexperienced Consulting Company\\nMost companies don’t have the time or expertise to build a data warehouse solution\\non their own, so they hire consultants to build it. That’s a great idea, as long as you\\npick the right consulting company. They could be experts—but in a completely differ‐\\nent type of project. Consulting companies sometimes involve experts with relevant\\nexperience when they are trying to win your business, but then substitute less experi‐\\nenced people once the project starts.\\nTo prevent falling prey to this pitfall, ask questions. Do the consultants have experi‐\\nence building the particular solution you want? If so, will those people be working on\\nyour project?\\n214 | Chapter 15: People and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b87efd15-97e6-4ce3-937c-5733b03a33c1', embedding=None, metadata={'page_label': '214', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pitfall: Hiring a Consulting Company That Outsources\\nDevelopment to Offshore Workers\\nConsulting companies use offshore developers to save costs, but that can lead to\\nproblems. Time zone differences can make it difficult to meet, language barriers can\\nmake it hard to communicate the requirements properly, and the quality of work may\\nbe lower.\\nAsk questions to ensure that the consulting company isn’t passing your project off to\\nan offshore team to avoid this issue.\\nPitfall: Passing Project Ownership Off to Consultants\\nIt can be tempting to “hand over the keys” of the project to the consulting company,\\nhaving them not only develop the project but manage it entirely. This can result in a\\nlack of transparency; the project can become a “black box, ” where you have no idea\\nwhat the consultants are working on or how well they’re doing the job.\\nMake sure you’re getting the full, true story of the project’s progress. Status reports\\nwon’t cut it; put project managers in place to keep a close eye on things.\\nPitfall: Neglecting the Need to Transfer Knowledge\\nBack into the Organization\\nIf you use a consulting company, you don’t want them to come in, do the project, and\\nleave without educating you on what they’ve built. Y ou need to be able to fix bugs and\\nadd features without having to hire the consulting company again.\\nHave some of your people work alongside the consultants as they build the solution,\\neven if just for a few weeks, to become familiar with it. Set aside time for knowledge\\ntransfer near the end of the project and have the consulting company hold training\\nand review sessions to educate your organization about the new solution.\\nPitfall: Slashing the Budget Midway Through the Project\\nPoorly planned projects sometimes max out their budgets, running out of money just\\nas they near the finish line. If you want to ensure failure, start making cuts. This\\nincludes actions like laying off developers, testers, program managers, or DBAs; cut‐\\nting back on the solution’s hardware; or reducing end-user training on the new sys‐\\ntem. Don’t make cuts in those areas.\\nEither increase the budget or make cuts in less critical areas to avoid this pitfall. For\\ninstance, you might reduce the number of data sources to ingest initially, leaving\\nthem for the next phase of the project.\\nWhy Projects Fail: Pitfalls and Prevention | 215', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8372e01f-c814-40a0-b0e6-388903349448', embedding=None, metadata={'page_label': '215', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pitfall: Starting with an End Date and Working Backward\\nFocusing on a strict deadline can compromise the project’s quality. First, rushing the\\nintricate stages of data sourcing, integration, establishing quality standards, building\\nETL processes, designing data models, and creating user interfaces can lead to errors\\nand inefficiencies. Second, a strict timeline can also limit the essential discovery and\\nexploration phases of the project, potentially leading to a system that doesn’t meet the\\norganization’s actual needs. Third, it’s unrealistic to assume that you’ll start the\\nproject with a perfect understanding of its scope. A rigid end date makes it difficult to\\naccommodate unexpected changes or new requirements that emerge as the project\\nprogresses, which can cause delays or incomplete deliverables. Finally, an inflexible\\nend date can restrict essential stakeholder collaboration, resulting in a product that\\ndoesn’t adequately serve all users.\\nDon’t make deadlines and schedules inflexible. Accept that schedules may need to\\nchange as new circumstances arise.\\nPitfall: Structuring the Data Warehouse to Reflect the\\nSource Data Rather Than the Business’s Needs\\nSource data is derived from operational systems that aren’t typically designed for\\ncomprehensive data analysis or business intelligence tasks, which can limit your\\nMDW’s usability. Those systems aren’t aligned with the business’s strategic goals and\\nKPIs, either, which can mean missing out on valuable insights (which, in turn, has a\\nnegative impact on decision making). Modeling your system after the source data also\\nmeans that changes in operational systems, which are common as businesses evolve,\\nwill necessitate major modifications in your warehouse, leading to instability and\\nincreased maintenance.\\nStructure your MDW (or data lakehouse) according to the needs of your business.\\nDon’t just mindlessly mirror the structure of the source data.\\nPitfall: Presenting End Users with a Solution with\\nSlow Response Times or Other Performance Issues\\nA surefire way to get inundated with complaints from end users is to give them a sol‐\\nution that is slow to produce the reports, dashboards, or queries they need—espe‐\\ncially if the previous solution did it faster. Just like with incorrect data, if slow\\nresponse times give users a poor first impression, they lose confidence and trust in\\nyou and the solution, which will be very difficult to win back.\\nDon’t wait for complaints before you make performance improvements. Measure the\\nresponse times from the migrated reports and tune the new reports to be at least\\nequal. If these reports haven’t been migrated, ask users to determine an acceptable\\nbaseline level of performance for report creation and interaction.\\n216 | Chapter 15: People and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2b13032a-c7c9-4b2c-86c0-b795334e53c9', embedding=None, metadata={'page_label': '216', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pitfall: Overdesigning (or Underdesigning) Your Data Architecture\\nIf you spend too little time on designing the data architecture for your solution, you\\nrisk building something that can’t handle data sources of a certain size, type, or speed;\\nlacks scalability; performs poorly; or introduces security risks. On the other hand, too\\nmuch architecture design can lead to overcomplexity, lack of flexibility, analysis para‐\\nlysis, and increased costs.\\nIt’s essential to strike a balance when creating a data architecture. The design should\\nbe comprehensive enough to provide a robust, scalable, and secure structure but flex‐\\nible and simple enough to be easily understood, implemented, and adapted to chang‐\\ning needs.\\nPitfall: Poor Communication Between IT and the Business Domains\\nThese two critical groups often use different language, have different priorities, and\\nsee things from different perspectives, so rifts between them aren’t unusual. IT per‐\\nsonnel typically focus on technical aspects and system implementation, while busi‐\\nness professionals concentrate on strategic objectives and profitability. Poor\\ncommunication can lead to critical problems such as misaligned objectives, ineffi‐\\ncient resource use, missed opportunities, and decreased morale.\\nA data project’s success heavily relies on these two groups working collaboratively. IT\\nmust comprehend the business needs, while the business team must appreciate the\\npotential and limitations of the IT infrastructure. It’s crucial to foster a culture of\\nopen dialogue and mutual respect. Roles such as business analyst can help bridge this\\ngap by translating business requirements into technical specifications and vice versa.\\nThe two groups should meet regularly, hold transparent discussions, and clearly\\ndocument requirements. Effective collaboration can make a big difference to whether\\nthe project succeeds or fails.\\nTips for Success\\nAfter having navigated the minefield of potential challenges and pitfalls that can\\nderail even the most promising projects, it’s essential to balance your perspective. As\\nmuch as it’s vital to be aware of what can go wrong, it’s equally crucial to understand\\nwhat drives a project toward its goals. These tips will help you harness proven strate‐\\ngies, best practices, and insightful wisdom, steering you clear of potential missteps\\nand toward success.\\nDon’t Skimp on Your Investment\\nSometimes it’s better to take twice as long and spend twice the money to build a solu‐\\ntion that provides 50 times the value of your old solution. Emphasize the long-term\\nTips for Success | 217', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='580443b9-59e8-4c8d-949a-085677293dbb', embedding=None, metadata={'page_label': '217', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='value of investing time and resources in building high-quality, robust, scalable data\\nsolutions that support advanced analytics capabilities. This is crucial to fostering a\\ndata-driven culture that encourages innovation and operational efficiency.\\nRushing projects or skimping on resources can lead to system flaws, inaccurate\\nresults, and compliance issues. Ultimately, dealing with constant maintenance or\\noverhauling a flawed system can end up being more expensive than just investing in\\nthe best tools to begin with. Investing more time and resources up front allows for\\nbetter planning, better people, and better tools, all of which contribute to creating\\nreliable, accurate, and scalable systems.\\nGetting the right expertise and technical knowledge on the team might mean paying\\nmore in salaries, but building a data solution requires a priceless combination of busi‐\\nness understanding, technical skills, and management capabilities. Furthermore, you\\nneed at least one person on the project who is well versed in data architectures. They\\nshould lead the process of deciding which data architecture to use. This is a huge\\ndecision. If you leave it to nonexperts, the resulting solution is likely to be off the\\nmark, resulting in performance problems, inability to ingest data from certain sour‐\\nces, incorrect results, and other issues. Involve people who know what they’re doing.\\nThis also goes for reporting tools and any other tools you’re using; make sure you\\nhave people who understand their features and function so you can get the most out\\nof your investment.\\nOverall, the initial costs and time investment will be offset by a substantial long-term\\nreturn on investment, which includes reduced long-term operational costs as well as\\nstrategic advantages and valuable insights.\\nInvolve Users, Show Them Results, and Get Them Excited\\nGet end users involved early so they’re working with you instead of against you. Early\\nin the project, invite them to help decide questions like which data sources to use and\\nhow to clean, aggregate, and validate the data. If they feel involved, they’ll cheer the\\nproject on and will want to help solve any problems that arise rather than complain\\nabout them. If you don’t get them involved, they will feel like the changes are being\\nforced upon them. They’ll be quick to complain about the solution and push back\\nagainst using it.\\nIf your solution will require users to work with unfamiliar new technologies (such as\\nreporting tools), train them! Participating in training is a great way for users to pre‐\\npare for the new solution and even learn faster, better ways to do their jobs. Help\\nthem understand any helpful new features and functionality the new tool offers so\\nthat they’ll look forward to a successful rollout of the new solution.\\n218 | Chapter 15: People and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3eacdb1b-05c8-4a5e-82d1-f6f533466c28', embedding=None, metadata={'page_label': '218', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To that end, show results quickly and often. Don’t let months of project work go by\\nwithout showing users and stakeholders any results. Use an interactive, agile\\napproach to project delivery, with frequent releases. Break the project into phases and\\ngo for a quick win that shows lots of benefits you can promote. For example, when\\nyou begin ingesting sources, start with one that’s high priority but also small and easy.\\nThen repeat the process. This builds a solid foundation and helps developers create\\nbest practices for building the solution.\\nAdd Value to New Reports and Dashboards\\nIf you are doing a migration—say, from on-prem to the cloud—don’t just re-create\\nyour existing reports; add value by giving users new features and functionality. Create\\ndashboards with KPIs they haven’t seen before; show them the art of the possible and\\nnew ways to get insights into their data. Find ways to help them do their jobs faster\\nand more easily. If the new solution isn’t better than the old one, they’ll ask, “Why did\\nwe spend all this money to re-create what we already have?” Convince them that\\nbuilding a new solution is money well spent.\\nAsk End Users to Build a Prototype\\nThe only part of the data solution most end users will see and use is the reporting\\ntool. Y ou want to make sure they are all familiar with most, if not all, of its features.\\nMany users aren’t even aware of features that could be saving them time and helping\\nthem gain insights. It’s like driving a Lamborghini in first gear because you don’t\\nknow it has other gears! Train them on the reporting tool and have an expert demon‐\\nstrate it using their own data.\\nEngaging end users in prototype creation plays a key role in this understanding. As\\nyou determine the project’s business requirements and train people on the reporting\\ntool, have the end users build a prototype. This prototype should consist of sample\\nreports that include all the information they need. By crafting this hands-on repre‐\\nsentation, users can better grasp the enhanced capabilities of new reports and dash‐\\nboards. Today’s sophisticated reporting tools have become so easy to use that you\\nneed only minimum technical skills to build a prototype. They say a picture is worth\\na thousand words; having a working prototype is a great way for end users to first‐\\nhand see the added value and insights the new solution offers.\\nFind a Project Champion/Sponsor\\nA project champion is a senior executive or influential individual who supports and\\nadvocates for the project. Their role is essential and multifaceted. First, they serve as\\nan internal advocate, communicating the long-term benefits of a well-constructed\\ndata architecture, including enhanced decision making, operational efficiency, and\\nstrategic planning. Second, they are instrumental in securing the necessary resources\\nTips for Success | 219', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0fac751a-6009-442b-8785-1f2e85504f0b', embedding=None, metadata={'page_label': '219', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and funding. Given that data projects require substantial investments in technology,\\nhuman resources, and time, a project champion who can marshal these resources can\\nbe pivotal.\\nThird, the project champion plays a vital role in facilitating cross-functional collabo‐\\nration between departments with different interests, aligning everyone’s priorities\\ntoward the common project goal. Finally, they manage resistance to change—a signif‐\\nicant challenge in most data projects—by helping to engage employees and address‐\\ning their apprehensions about new systems and processes. This helps ensure\\nsmoother transitions and increases the project’s likelihood of success.\\nMake a Project Plan That Aims for 80% Efficiency\\nThe individual responsible for the day-to-day management and execution of the\\nproject—often referred to as the project manager or project lead—should ensure that\\neveryone is optimally engaged. This role is distinct from that of the project champion\\nor sponsor, who provides strategic support and advocacy from a higher level. The\\nproject manager is more “in the trenches, ” coordinating tasks, timelines, and\\nresources. Ideally, each team member should be utilizing about 80% of their work\\nhours on the project. (Reserve the other 20% for things such as vacation or sick time,\\nnon-project meetings, learning, and so on.) Y ou’ll need a well-thought-out project\\nplan geared toward minimizing the time each person has to wait for someone else to\\nfinish some dependent task. It also means ensuring the right people are performing\\nthe right tasks and that everyone is trained before the project begins. Anything less\\nthan 80% efficiency could lead to missed deadlines and cost overruns.\\nSummary\\nIn this chapter, we’ve journeyed through the pivotal role of the human element in\\ndata architectures. While tools and technology might dazzle and inspire awe, they\\nserve as mere instruments in the hands of their human wielders. How organizations\\nharness these tools, and how they navigate cultural shifts in the process, can make or\\nbreak their architectural endeavors. As you’ve learned, setting the right team struc‐\\ntures, fostering clarity in roles and responsibilities, and having a deep understanding\\nof potential pitfalls and success markers are indispensable to a project’s success.\\nTake a moment to internalize that the most intricate and advanced technology will be\\nonly as powerful as the people using it and the processes governing its use. A per‐\\nfectly designed data lakehouse or a robust data mesh can have an impact only when\\ncomplemented by a well-organized, informed, and empowered team. The true suc‐\\ncess of data transformations lies in this delicate balance between humans and\\ntechnology.\\n220 | Chapter 15: People and Processes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='844c5d52-3b07-4f40-a509-b07fa8d0a393', embedding=None, metadata={'page_label': '220', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='However, while this human-centric perspective is foundational, we cannot ignore the\\ntechnological marvels that have reshaped the landscape of data architectures in recent\\nyears. The next chapter dives deep into the technological realm, exploring the inno‐\\nvations and intricacies of open source movements, the groundbreaking capabilities of\\nHadoop, the transformative benefits of the cloud, and the offerings of major cloud\\nproviders. Additionally, I’ll take you into the world of multi-cloud strategies and\\nfamiliarize you with game changers like Databricks and Snowflake.\\nPeople and technology aren’t an either-or choice. They need to be a harmonious\\nblend, with each elevating the other. Let’s continue our exploration, understanding\\nthat our technological efforts are truly realized only when complemented by the right\\npeople and processes.\\nSummary | 221', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='34dd83ce-fa7e-4c5b-82f7-d03eae8e4fd3', embedding=None, metadata={'page_label': '221', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5892187c-a209-4b53-968d-1db9b43b948a', embedding=None, metadata={'page_label': '222', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CHAPTER 16\\nTechnologies\\nAs you delve into this chapter, your first crucial decision is choosing between open\\nsource solutions and cloud provider offerings. However, there’s more to consider. I’ll\\nguide you in thinking about the scale of your data needs and your organization’s agil‐\\nity requirements.\\nBeyond this, we’ll explore the landscape of cloud service models, from major provid‐\\ners to the complexity of multi-cloud solutions. The chapter ends by introducing you\\nto three essential software frameworks: Hadoop, Databricks, and Snowflake.\\nY our answers to the questions in this chapter will profoundly shape your data archi‐\\ntecture. It’s not just open source versus proprietary; it’s about crafting the perfect\\ntechnology mix to suit your unique needs in the world of data architecture.\\nChoosing a Platform\\nThe decision to use open source software or software from a cloud service provider\\n(CSP) depends on various factors, including specific requirements, budget, expertise,\\ndata sensitivity, and control preferences. Evaluate your organization’s needs carefully\\nand consider the trade-offs between the advantages of CSP software and the flexibility\\nand customization offered by open source software (OSS).\\nOpen Source Solutions\\nOpen source software refers to software that is distributed for free with its source code\\nopenly available for anyone to view, modify, and distribute. It is typically developed\\ncollaboratively by a community of programmers who contribute their knowledge and\\nexpertise to create and improve the software.\\n223', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='931e45c8-36e1-4457-ac82-5c457a6eda54', embedding=None, metadata={'page_label': '223', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='OSS has a rich history that spans several decades. It originated from the Free Software\\nMovement in the 1980s, led by Richard Stallman, which advocated for software free‐\\ndom. The Open Source Initiative  (OSI), founded in 1998, created the Open Source\\nDefinition and established criteria for OSS licenses, which ensure that the software\\nremains free and open for anyone to use. Linux played a significant role in populariz‐\\ning OSS. Adopting open source solutions became popular in the 2010s because of\\ntheir flexibility and low cost. Collaborative platforms like GitHub facilitated OSS\\ndevelopment, and the movement has expanded beyond software into open data and\\nhardware initiatives. Some of the top OSS projects include MySQL, PostgreSQL,\\nMongoDB, Apache Hadoop, Apache Spark, Apache Kafka, Presto, and Apache Air‐\\nflow.\\nMany data architectures use OSS, but few are built entirely with it. Most architectures\\nuse OSS in combination with cloud provider products. Including OSS is well worth\\nconsidering for your data architecture.\\nSome of the main reasons you might want to use OSS over cloud provider products\\ninclude:\\nCost savings\\nOSS is typically free to use. This makes it an attractive option for organizations\\nlooking to optimize their budgets.\\nFlexibility and customization\\nOSS provides the freedom to modify and customize the source code to suit spe‐\\ncific needs. This allows you to tailor the software to your organization’s require‐\\nments, adding or removing features as necessary.\\nTransparency and trust\\nOSS source code is openly available, and anyone can review it for security vulner‐\\nabilities, bugs, and potential back doors. This transparency promotes trust and\\nfosters a collaborative environment for code review and improvement.\\nRapid innovation and collaboration\\nOSS benefits from a global community of developers who contribute their exper‐\\ntise and improvements. This collaborative environment fosters rapid innovation,\\naccelerates software development cycles, and results in more reliable and feature-\\nrich solutions.\\nVendor independence\\nWith OSS, organizations are not tied to a specific vendor or locked into propriet‐\\nary software. They have the freedom to switch service providers or customize the\\nsoftware as needed.\\n224 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a1043c50-ba81-4b32-acc0-d6ecb52dd86d', embedding=None, metadata={'page_label': '224', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='There are also a few potential disadvantages to consider:\\nLimited support\\nUnlike cloud provider software, OSS may not have dedicated customer support\\nor service-level agreements. Relying on community support can mean varying\\nresponse times, so critical issues may take longer to address. To avoid this prob‐\\nlem, companies often purchase a support contract from a vendor. Additionally,\\nsome hardware and software vendors prioritize compatibility and support for\\ncloud-provider solutions over OSS. This can result in limited hardware driver\\navailability or lack of support for certain software applications.\\nLearning curve and user experience\\nWorking with OSS solutions often requires skilled developers or technical staff to\\neffectively implement, customize, and maintain them. This can result in addi‐\\ntional training or hiring costs. OSS projects also vary in terms of documentation\\nquality, user-friendliness of interfaces, and ease of installation. Some projects may\\nlack comprehensive documentation or intuitive user interfaces, making initial\\nsetup and configuration more challenging.\\nFragmentation, standardization, and compatibility\\nDue to the distributed nature of OSS development and the vast number of OSS\\nprojects and versions, there is little standardization in coding styles, conventions,\\nor approaches. This can lead to fragmentation, making it challenging to choose\\nthe right solution or ensure compatibility between different OSS components.\\nIntegrating with existing systems may require additional effort. (To learn more,\\nsee “Hadoop” on page 235.)\\nSecurity and liability\\nEven under the scrutiny of a global community, vulnerabilities and bugs still exist\\nin OSS. If you use it, you must stay vigilant in applying security patches and\\nupdates. Additionally, if your organization modifies the OSS code, it assumes\\nresponsibility for maintaining the security and integrity of the customized\\nversion.\\nUncertain project maintenance\\nNot all OSS projects are actively developed or offer long-term maintenance.\\nSometimes projects may become stagnant, with few updates or security patches.\\nAssess the level of activity and community support before adopting an OSS\\nsolution.\\nIntellectual property considerations\\nWhen using or contributing to OSS, organizations must navigate intellectual\\nproperty rights and licensing obligations. It is crucial to understand the terms\\nand conditions of the OSS licenses to ensure compliance and avoid legal issues.\\nChoosing a Platform | 225', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='480656db-447d-49c8-92b7-eb44fd353da4', embedding=None, metadata={'page_label': '225', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On-Premises Solutions\\nIf your company has an on-prem data warehouse solution, you may be looking at\\nmigrating it to the cloud, or at least building new greenfield projects in the cloud.\\nHowever, there are still a few situations where you might want to have your data\\nwarehouse solution on-prem:\\n• If your internet connection is slow or nonexistent, such as in a deep mine or on a\\nsubmarine, an offshore oil rig, or a cruise ship.\\n• If you require millisecond performance, such as for servers in a high-volume\\nmail-processing facility.\\n• If moving your applications to the cloud would break a third-party support\\ncontract.\\n• If you have a long-term locked-in lease with your datacenter or have just made a\\nvery large equipment purchase.\\n• If you have a very large amount of on-prem-born data that would need to be\\nmoved to the cloud and your pipeline to the cloud is too constrained to meet\\nusers’ reporting needs. Y our reports may not reflect the most current data, poten‐\\ntially delaying decision making.\\n• If you plan to replace or retire your applications and databases in the near future.\\n• If your data is extremely sensitive (though this no longer prevents many organi‐\\nzations from putting data in the cloud—most CSPs now have top-secret clouds).\\nIf any of this applies to databases at your company, you’ll want to keep those data‐\\nbases on-prem, but you can still move others to the cloud. I’ve talked to hundreds of\\ncompanies about this over the last few years, and nearly every one of them is at least\\npartially in the cloud. Most have already shut down all of their on-prem data centers.\\nAnd for any new company, it’s a no-brainer to be in the cloud.\\nIf your data warehouse is on-prem, you should be aware of the constraints:\\nLimited scalability\\nY our ability to scale your infrastructure is limited by what hardware you can pur‐\\nchase. Y our hardware may be a constraint if you have to use certain vendors, if\\nthere are compatibility issues with existing hardware, or if hardware vendors are\\nexperiencing production delays. Likewise, datacenters have only so much physi‐\\ncal space. When you run out of room to add more servers, you are left with two\\nvery expensive options: expanding your datacenter or building another one. Both\\ncan take many months.\\n226 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c68ef66e-df43-4eb7-a215-9eea40d68f2e', embedding=None, metadata={'page_label': '226', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Up-front costs\\nPurchasing the hardware for on-prem solutions incurs a very large up-front capi‐\\ntal expenditure. Hardware includes server hardware, datacenter space to house\\nthe hardware, additional storage devices, firewalls, networking switches, a high-\\nspeed network (with redundancy) to access the data, the power and redundant\\npower supplies needed to keep the system up and running, and the costs of\\nsecuring and backing up the data.\\nIf your warehouse is mission critical, then you’ll also need to configure a disaster\\nrecovery site, effectively doubling the cost.\\nI often tell companies that if they have their own on-prem datacenter, they are in\\nthe air-conditioning business, because they have to keep all that hardware cool.\\nWouldn’t you rather focus all your efforts on analyzing data? Most companies\\nprefer the yearly operating expense of working with a cloud provider.\\nPersonnel costs\\nY ou’ll need to pay employees or consultants with the specialized expertise to set\\nup, manage, administer, and support the hardware and software, tune the prod‐\\nucts for performance, and deploy products and solutions into production. This\\ncreates a potential bottleneck when issues arise and keeps responsibility for the\\nsystem with the customer, not the vendor.\\nSoftware costs\\nOrganizations frequently pay hundreds of thousands of dollars in software-\\nlicensing fees for data warehouse software and add-on packages, as well as licens‐\\ning fees to give additional end users, such as customers and suppliers, access to\\nthe data. Annual support contracts for such software can easily run to 20% of the\\noriginal license cost.\\nNow let’s look at the cloud.\\nCloud Provider Solutions\\nThe cloud is like a massive, invisible storage room that exists somewhere out there in\\nthe ether. It allows us to store, access, and share data from anywhere, while saving us\\nfrom running out of space or losing valuable data. Its influence spans all the architec‐\\ntures described in this book, reshaping how we work with data. Cloud computing\\nprovides on-demand access to shared computing resources such as servers, storage,\\nand applications over the internet, so you no longer have to buy, own, and maintain\\nphysical datacenters and servers. This makes for scalable, flexible, and cost-efficient\\ncomputing.\\nChoosing a Platform | 227', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3d2e8047-c9e5-4d3a-b03a-8f5301fde684', embedding=None, metadata={'page_label': '227', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='There are many benefits to having a data-warehousing solution in the cloud:\\nScalability and elasticity\\nCSP software is designed to scale seamlessly. Y ou can dynamically provision stor‐\\nage and compute resources on the fly to meet the demands of changing work‐\\nloads in peak and steady usage periods. Capacity is whatever you need whenever\\nyou need it.\\nCost\\nIn the cloud system, the complexities and costs of capacity planning and admin‐\\nistration (such as sizing, balancing, and tuning the system) are built in, automa‐\\nted, and covered by the cost of your cloud subscription. CSPs also offer flexible\\npricing models, so you can optimize costs by paying only for the resources you\\nactually use. Y ou can reduce hardware as demand lessens or use serverless\\noptions.\\nThis means that you can build a solution quickly, and if it doesn’t work out, you\\ncan simply delete all the cloud resources the project used. The cost of failure is\\ngreatly reduced, which means you can take chances and try new things.\\nIt’s also important to consider indirect cost savings: you’re not paying for hard‐\\nware, power, the staff who build and maintain the datacenter, or development of\\nyour own upgrades.\\nEasy deployment and management\\nCSP software often comes with integrated deployment and management tools\\nthat simplify the setup and configuration processes. These tools streamline the\\nprovisioning of resources and scaling, monitoring, and maintenance tasks,\\nreducing operational overhead. Y ou can create all the cloud resources you need\\n(such as compute, storage, and networking) in hours, if not minutes—whereas\\nstarting a solution on-prem can take weeks or even months.\\nVendor support and service-level agreements (SLAs)\\nCSPs offer dedicated support and SLAs, providing assistance, troubleshooting,\\nand guaranteed service availability.\\nIntegrated ecosystems\\nTypically, a CSP’s software integrates seamlessly with that provider’s other serv‐\\nices. This can enable easier integration and interoperability between different\\nservices and help you leverage the provider’s broader capabilities.\\n228 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bb64a463-737d-4184-ba28-aca7ff9d83a6', embedding=None, metadata={'page_label': '228', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Managed services\\nCSPs often handle specific aspects of software infrastructure, such as managed\\ndatabases, machine learning services, or serverless computing. These managed\\nservices can offload operational complexities, allowing organizations to focus on\\ntheir core business rather than managing the underlying infrastructure. This also\\nsaves you from having to find and pay for services from third-party vendors.\\nContinuous innovation and feature updates\\nCSPs regularly introduce new features, services, and updates to their software\\nofferings. Y ou benefit from their innovation without having to invest in upgrad‐\\ning or integrating new features yourself.\\nEasier hiring\\nHuge numbers of people use products from CSPs; OSS can have a much smaller\\nuser base. Therefore, it can be much easier to find job candidates who know the\\ncloud products your organization uses.\\nGlobal availability and high availability\\nCSPs’ datacenters are distributed worldwide, which lets you deploy your software\\ncloser to your target audience or leverage multi-region redundancy for high\\navailability. CSPs use load balancing to distribute traffic evenly across multiple\\nservers, ensuring that no single server gets overloaded and causes downtime.\\nThis global infrastructure ensures low latency services and is fault tolerant. In\\ncase of a hardware or software failure, the service will automatically failover to a\\nsecondary instance, ensuring uninterrupted and reliable access. Also, you can set\\nup disaster recovery for storage in a matter of minutes.\\nSecurity and compliance\\nCSPs invest heavily in security measures such as encryption, access controls, and\\nregular audits to protect data and ensure compliance with industry standards and\\nregulations.\\nTotal Cost of Ownership\\nAll of the CSPs offer a total cost of ownership (TCO) calculator\\nthat you can use to estimate the cost savings you could realize by\\nmigrating your application and database workloads to the cloud.\\nSimply provide a brief description of your on-premises environ‐\\nment to get an instant report from Microsoft Azure, Amazon Web\\nServices, or Google Cloud.\\nChoosing a Platform | 229', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b75a535e-c16e-423a-9df7-948d7b90464d', embedding=None, metadata={'page_label': '229', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cloud Service Models\\nThere are three basic service models for delivering and managing IT resources in the\\ncloud:\\nInfrastructure as a Service (IaaS)\\nThe CSP hosts and manages the hardware infrastructure. It offers virtualized\\ncomputing resources, such as servers, storage, and networking, which can be\\neasily scaled up or down as needed; the customer manages the operating system,\\napplications, and data. Most services are housed within a virtual machine (VM)\\nthat the end user will access remotely. This model offers more flexibility and scal‐\\nability than on-prem, but managing its operating system, applications, and data\\narchitectures still requires some level of IT expertise.\\nPlatform as a Service (PaaS)\\nThe CSP offers a platform for developers to build, test, and deploy applications\\nand databases, as well as an underlying infrastructure (similar to IaaS). It also\\nincludes development tools, database management systems, and business intelli‐\\ngence services. The CSP controls the runtime, middleware, and O/S environ‐\\nments—there are no VMs. This allows developers to focus on coding and\\nmanaging the applications without worrying about managing the platform and\\nunderlying infrastructure. This model is even more abstract and easier to use\\nthan IaaS, but it is limited in its flexibility and customizability.\\nSoftware as a Service (SaaS)\\nThe CSP hosts and fully manages both the software and infrastructure; custom‐\\ners access the software over the internet having to manage any hardware or soft‐\\nware themselves. This is the simplest and easiest-to-use model, but it may have\\nlimited flexibility in terms of customization and control. Some CSPs offer data\\narchitecture tools that are “SaaS-like”: that is, they’re not quite as fully managed\\nas a true SaaS tool like Salesforce, requiring just some minor application\\nmanagement.\\nWith any of the cloud service models, you get the latest version of whatever database\\nproduct you are using. For example, Azure’s cloud database, SQL Database, is a newer\\nversion of an on-prem database product called SQL Server 2022. Think of SQL Data‐\\nbase as SQL Server 2022+. New features are added every few weeks—you can choose\\nto use them or not. Every couple of years, those new features are gathered up and\\nadded to the boxed version of SQL Server, but you get them right away with SQL\\nDatabase. And your code won’t break when there is an upgrade.\\nAll of the CSPs offer built-in advanced threat detection, using monitoring to detect\\nanomalous activities that could indicate unusual and potentially harmful attempts to\\naccess or exploit databases; assessments that help you to discover, track, and\\n230 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90ec8fd5-0108-4e59-a6c6-b46c2f8cd586', embedding=None, metadata={'page_label': '230', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='remediate potential database vulnerabilities; and advanced capabilities for discover‐\\ning, classifying, labeling, and reporting the sensitive data in your databases.\\nOverall, as Figure 16-1 shows, these four models represent different levels of abstrac‐\\ntion and control over the IT infrastructure and software stack. On-prem offers com‐\\nplete control but requires significant investment and expertise, while IaaS, PaaS, and\\nSaaS offer increasing levels of abstraction and ease of use—at the expense of control\\nand customization.\\nFigure 16-1. Comparison of service models\\nWhich service model you choose will depend on your organization’s needs and goals.\\nHowever, I personally prefer PaaS for data warehouses where possible, for a few rea‐\\nsons. First, you don’t have to deal with a virtual machine (VM). It exists somewhere\\nand hosts the databases, but you never have to remote into a server or manage it in\\nany way—you do everything via the CSP portal or third-party tools. Nor are you\\nresponsible for patching or upgrading: most CSP databases are patched with no\\ndowntime. Y ou get database backups out of the box—no more setup and monitoring.\\nFinally, PaaS makes disaster recovery (DR) a lot simpler. Most IaaS DR solutions are a\\npain to set up and monitor, but with PaaS, you basically just have to click which area\\nof the country you want the DR database to reside in. The CSP takes care of setting it\\nup and keeping it working.\\nCloud Service Models | 231', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d17c6b27-a2fd-4fda-8ced-229d8c6e96c1', embedding=None, metadata={'page_label': '231', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Major Cloud Providers\\nThe three major CSPs, Microsoft Azure, Amazon Web Services  (AWS), and Google\\nCloud Platform (GCP), have the majority of the market share ( estimated at 65%  in\\nQ2 2023). For more detailed and up-to-date information on each provider’s offerings,\\nI encourage you to visit their respective websites. Table 16-1 compares their major\\nfeatures to give you a comprehensive overview.\\nTable 16-1. Comparison of the big three cloud service providers\\nCriteria Azure AWS GCP\\nYear announced 2008 2006 2008\\nData center regions 60 31 24\\nGlobal market share 23% 32% 10%\\nServices offered 200+ services across various\\ncategories\\nAround 200+ services across\\nvarious categories\\n100+ services, known for data\\nanalytics and machine learning\\nPricing Cost benefits for Microsoft\\nusers\\nMore options for instance\\ntypes\\nStraightforward and customer\\nfriendly\\nIntegration/open\\nsource\\nStrong integration with\\nMicrosoft products\\nWide variety of open source\\nintegrations\\nGood integrations with various\\nopen source platforms\\nLearning curve Easier for those familiar with\\nother Microsoft products\\nWide range of services leading\\nto steeper learning curve\\nUser-friendly interface, easier\\nlearning curve\\nAnalytic products Azure Synapse Analytics,\\nMicrosoft Fabric\\nAmazon Redshift Google BigQuery\\nEach of these three major cloud service providers has unique strengths, and the\\nchoice among them often boils down to your specific needs and existing infrastruc‐\\nture. A business deeply embedded in the Microsoft ecosystem might gravitate toward\\nAzure, one focused on advanced data processing might opt for GCP , and a business\\nrequiring a broad range of services might lean toward AWS. Ultimately, the decision\\ninvolves balancing technical requirements, cost, and the strategic direction of the\\nbusiness.\\nMulti-Cloud Solutions\\nMany organizations find themselves deliberating between adopting one of the big\\nthree CSPs or employing services from more than one, which is often called a multi-\\ncloud approach. Some companies feel like relying on just one CSP is “putting all their\\neggs in one basket” or fear that the CSP might become a competitor to their business.\\n(This happened, for instance, when Amazon bought Whole Foods, becoming a com‐\\npetitor to other supermarket businesses.)\\n232 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1fa45cef-dccc-4727-923a-22f813968386', embedding=None, metadata={'page_label': '232', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This approach does have some major benefits, mostly because it leverages each CSP’s\\nstrengths and weaknesses. If one CSP has lower latency and another has better\\navailability or disaster recovery, using both could improve the organization’s ability to\\nmeet its SLAs. Y ou can do similar comparisons for cost, capacity, and various features\\nand products; if one CSP doesn’t have everything you want, you can lean on another\\nto make up for it. However, this is less relevant than it used to be; today, all three CSPs\\nnow have very similar offerings, with few unique features or products.\\nData sovereignty is also a factor. Not all CSPs have the same cloud regions, which can\\nbe very important for regulatory compliance. For example, if you are using MongoDB\\nin Azure but Azure does not have a region in Taiwan, your Taiwan operations could\\nuse MongoDB in AWS.\\nImagine a fictional global ecommerce giant we’ll call TechTreasures Inc. TechTreas‐\\nures is based in the United States—specifically Ponte Vedra, Florida—but operates\\naround the world, including in North America, Europe, and Asia.\\nTo ensure optimal performance, reliability, and compliance with regional data regula‐\\ntions, TechTreasures adopts a multi-cloud approach. It leverages Microsoft Azure for\\nits North American operations due to Azure’s robust low-latency network and data\\ncenters in the region. In Europe, it utilizes GCP for its advanced data analytics capa‐\\nbilities and strong presence in European cloud regions. For its Asian operations,\\nTechTreasures relies on AWS, which offers cloud regions in key Asian markets,\\nincluding Taiwan.\\nThis multi-cloud strategy allows TechTreasures to provide fast, reliable services to its\\nglobal customer base while ensuring compliance with data sovereignty regulations.\\nIt’s a strategic choice that enables the business to harness the unique strengths of each\\nmajor CSP to optimize business operations.\\nBut there are plenty of drawbacks to multi-cloud. Since these clouds aren’t designed\\nto work together, moving data and compute back and forth can be slow and clunky.\\nInteroperability is an issue too; products from one CSP don’t always work well with\\nthose from different CSPs. Moving data and applications from one CSP to another\\nand launching another product could be very costly, especially if you have to re-\\nengineer applications to support multiple clouds. CSPs also charge egress fees if you\\nmove data from their cloud to a competitor’s cloud, and they don’t make it easy to\\nmigrate to another CSP . And if you’ve invested in a larger internet pipeline, like\\nAzure’s ExpressRoute, you’ d need to make a similar investment with any other CSPs\\nyou use. Moreover, storing your data in multiple clouds increases its exposure to\\nsecurity threats.\\nIf you want to make it easy to move from one cloud to another, you will have to use\\nan IaaS service model instead of PaaS, which takes away many of the higher-value\\nservices that each CSP offers—a big opportunity cost.\\nCloud Service Models | 233', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7b8d87fe-b1c8-4fd6-ac68-b729ec104fe3', embedding=None, metadata={'page_label': '233', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 CSPs use different names for these commitment-based discounts; you might see them called “reserved\\ninstance pricing, ” “pre-commit discounts, ” “commercial pricing discounts, ” or “volume-based discounts. ”\\nIn terms of personnel, you’ll need to hire and/or train your people to understand two\\nor more clouds, greatly increasing costs. Managing an additional CSP; handling\\nsecurity; and creating policies, standards, and procedures for it also requires more\\npeople and skill sets. Y ou’ll also have to get different types of security to get to work\\ntogether, and you’ll have two places to add, delete, or update users; two places to\\nmonitor; and two sets of billing statements, adding significant administrative com‐\\nplexity (and cost).\\nFinally, having two cloud providers is not likely to save you from an outage. The CSPs\\nhave specifically designed their networks so that an outage at one region doesn’t\\nimpact other regions. Y our disaster recovery solution should be to failover to another\\nregion within the same CSP , not to another CSP .\\nMany people believe that using the multi-cloud approach forces CSPs into a competi‐\\ntive situation, allowing you to negotiate a better deal. However, this is mostly a myth. \\nFor enterprise license agreements (ELAs), the CSPs give higher discounts based on\\nhow much consumption you can commit to on their cloud—the more you use, the\\nbigger your discounts.1 When you add in tier-based pricing and sustained usage dis‐\\ncounts, you will likely save more by sticking with just one CSP . However, some com‐\\npanies will threaten to jump to another CSP to control their pricing. Going with just\\none CSP can also allow for partnership benefits if your company is a large enterprise.\\nFor example, Microsoft has a collaboration with Ernst & Y oung to drive a $15 billion\\ngrowth opportunity and technology innovation across industries.\\nI don’t expect the multi-cloud approach to gain wider adoption. In time, I think most\\ncompanies will choose one CSP and stick with it, to go deeper and fully leverage its\\nservices, PaaS and SaaS, and ecosystem. Some very large companies will settle on a\\ncouple of clouds if the CSPs offer different things that are beneficial to the workload.\\nThat makes sense if the company is big enough to invest the time and people to go\\ndeep on both and take advantage of each provider’s PaaS services.\\nWe now move from a discussion of cloud service models to software frameworks,\\ntransitioning from establishing the foundation of data architecture to exploring the\\ntools that leverage this foundation. In the upcoming section, we’ll delve into key soft‐\\nware frameworks that harness cloud infrastructure to unlock the full potential of your\\ndata.\\n234 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a4841f89-6583-4d23-9d21-2f34b81f9299', embedding=None, metadata={'page_label': '234', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Software Frameworks\\nIn the world of data architecture, your choice of software frameworks can make all\\nthe difference in how effectively you process, analyze, and derive insights from your\\ndata. These frameworks serve as the engines that power your data operations, allow‐\\ning you to extract valuable information and drive data-driven decisions. In this sec‐\\ntion, we will journey into the realm of software frameworks, exploring three\\nprominent players: Hadoop, Databricks, and Snowflake. Each of these frameworks\\nbrings unique strengths to the table, and understanding their capabilities is essential\\nfor building a data architecture that meets your specific needs and objectives.\\nHadoop\\nHadoop is an OSS framework created in 2005 by the Apache Foundation. It’s\\ndesigned to harness the power of distributed computing. Various tools and libraries\\nextend Hadoop’s capabilities, making it a popular choice for organizations that deal\\nwith massive volumes of data and are seeking cost-effective scalability.\\nAt its core, Hadoop consists of two main components. First, the Hadoop Distributed\\nFile System (HDFS) can store and process big data workloads efficiently by splitting\\ndata into smaller blocks and distributing it across multiple nodes and clusters of com‐\\nputers, as pictured in Figure 16-2. This enables high fault tolerance and tasks such as\\nbatch processing, data integration, and analytics. HDFS enables high-throughput\\naccess to data and provides reliability even when hardware fails.\\nFigure 16-2. Hadoop’s cluster architecture\\nSoftware Frameworks | 235', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='355fc8a1-28ec-475f-b47d-0fb668725cd7', embedding=None, metadata={'page_label': '235', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Second, the MapReduce framework is a programming model used for processing and\\nanalyzing large datasets in parallel across a cluster, again by dividing the workload\\ninto smaller tasks and distributing them across multiple nodes in the cluster. It breaks\\ndown large, complex computations into multiple tasks and assigns those tasks to indi‐\\nvidual worker nodes, which coordinate and consolidate the results. The MapReduce\\nframework automatically handles parallelization, fault tolerance, and data distribu‐\\ntion. A MapReduce program is composed of a Map() procedure that filters and sorts\\n(such as sorting students by first name into queues, one queue for each name) and a\\nReduce() procedure that summarizes the data (such as counting the number of stu‐\\ndents in each queue, yielding name frequencies). MapReduce libraries have been\\nwritten in many programming languages (usually Java), with different levels of\\noptimization.\\nHadoop offers several benefits that make it a popular choice for handling big data:\\nScalability\\nHadoop provides a scalable architecture that can handle large volumes of data. It\\nallows you to scale your storage and processing capabilities easily by adding more\\ncommodity hardware to the cluster, making it cost-effective and flexible.\\nDistributed computing\\nHadoop distributes data and processing tasks across a cluster of machines, ena‐\\nbling parallel processing. Dividing the work across multiple nodes allows it to\\nprocess large datasets faster.\\nFault tolerance\\nHadoop replicates data across multiple nodes in the cluster, ensuring that data\\nendures and remains available even if hardware fails. If a node fails, Hadoop\\nautomatically redirects tasks to other available nodes, ensuring uninterrupted\\nprocessing.\\nCost-effective storage\\nHDFS allows you to store large amounts of data on standard, affordable hard‐\\nware that you can buy off the shelf, rather than expensive, specialized storage\\nsystems.\\nData locality\\nHadoop optimizes data processing by moving computation closer to where the\\ndata is stored instead of transferring data over the network, reducing network\\noverhead and improving overall efficiency.\\nFlexibility and compatibility\\nHadoop is compatible with various data formats and can process structured,\\nsemi-structured, and unstructured data. It supports a wide range of\\n236 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b1b2693c-7b3a-4be3-9b27-c7b5c4636129', embedding=None, metadata={'page_label': '236', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='programming languages and tools, allowing developers to work with their prefer‐\\nred languages and integrate different components into their data pipelines.\\nEcosystem of tools\\nHadoop has a rich ecosystem of tools and frameworks that extend its capabilities\\nand provide higher-level abstractions, data-processing languages, and analytics.\\nThese include:\\n• YARN, for resource management and job scheduling\\n• Hive, for data warehousing\\n• Pig, for data analysis\\n• HBase, a NoSQL database\\n• Spark, for in-memory processing\\n• Presto, for distributed SQL\\nHadoop vendors sell platforms that incorporate, test, and pre-integrate several\\nOSS products together, saving you the time and effort of integrating and testing\\nvarious components yourself.\\nCommunity support\\nHadoop has a large and active open source community that contributes to its\\ndevelopment, improvement, and support and provides a wealth of resources,\\ndocumentation, and expertise to users.\\nHadoop’s potential disadvantages include:\\nComplexity\\nHadoop has a steep learning curve and can be complex to set up and manage. It\\nrequires specialized knowledge and expertise in distributed systems, which may\\npose a challenge for organizations without prior experience or dedicated\\nresources.\\nLatency\\nMapReduce, Hadoop’s core processing framework, is based on batch processing,\\nwhich introduces latency, so it may not be suitable for real-time or interactive\\ndata-processing scenarios. Hadoop is most effective for large-scale data-\\nprocessing tasks, such as batch analytics and processing massive datasets. If you\\nneed immediate or near-real-time insights, alternative solutions or frameworks\\nlike Apache Kafka or Apache Flink might be more suitable.\\nHardware requirements\\nHadoop’s distributed architecture requires a cluster of machines to function effi‐\\nciently. Setting up and maintaining such a cluster, including hardware and net‐\\nwork infrastructure, can involve significant costs and complexity.\\nSoftware Frameworks | 237', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='47260d39-dbd1-4b2d-ace3-a4b49d52d5b1', embedding=None, metadata={'page_label': '237', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 Spark was open sourced in 2010 under a Berkeley Software Distribution license, and it became an Apache\\nSoftware Foundation project in 2013.\\nData management overhead\\nWhile Hadoop’s HDFS provides fault tolerance and replication, replicating data\\nacross nodes consumes storage space. This introduces additional data storage\\noverhead.\\nComplex ecosystem\\nWhile the Hadoop ecosystem is rich and wide-ranging, it’s also complex, so man‐\\naging and integrating its components can be challenging. It requires understand‐\\ning the capabilities and compatibility of each tool and maintaining the tools’\\nconfigurations and dependencies.\\nData security and governance\\nHadoop’s distributed nature and data replication can introduce unique challenges\\nin terms of data security, privacy, and governance. Unlike traditional data storage\\nsystems, Hadoop’s distributed nature requires organizations to go beyond stan‐\\ndard security practices and adopt specialized security measures and access con‐\\ntrols tailored to its intricacies. These measures are essential to ensure data\\nprotection and maintain compliance with Hadoop-specific requirements and\\nbest practices.\\nHadoop gained significant adoption in the early 2010s, but as the big data landscape\\nand cloud computing have evolved, introducing technologies like serverless compute,\\ncloud data warehouses, and data-streaming platforms, use of Hadoop has waned.\\nToday, cloud infrastructure can replace Hadoop clusters and cloud object storage can\\nreplace HDFS. Its core principles, such as distributed storage and processing, remain\\nrelevant, but its specific components and tools have seen varying degrees of adoption.\\nFor example, technologies like Apache Spark have gained popularity as alternatives or\\ncomplements to MapReduce.\\nDatabricks\\nDatabricks is a cloud-based platform for running workloads in Apache Spark, an\\nopen source data analytics–processing engine. Spark was originally developed at UC\\nBerkeley’s AMPLab in 2009 as a way to deal with the limitations of MapReduce. 2 It\\nwas designed to be faster and easier to use and to handle both batch processing and\\nreal-time data streaming.\\nSome of Spark’s original creators were among Databricks’ founders in 2013. Data‐\\nbricks has continued to work to commercialize Spark, providing a Unified Data Ana‐\\nlytics Platform, a fully managed service that offers a comprehensive, simplified\\nplatform for big data analytics, integrating data science, engineering, and business. It\\n238 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d073016-50fc-43ac-944f-47c08a1c2e11', embedding=None, metadata={'page_label': '238', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='is designed to make it easier for businesses to use Spark without having to worry\\nabout setting up and maintaining their own infrastructure, making advanced analyt‐\\nics and machine learning accessible to a broader set of users.\\nSince its inception, Databricks has been a significant contributor to Apache Spark,\\nworking to improve its processing speed through Project Tungsten, a series of opti‐\\nmizations aimed at leveraging modern CPU architectures to increase efficiency. It has\\nalso developed an optimized autoscaling system that dynamically adjusts the number\\nof machines allocated to a job in response to computational demands, improving\\nboth cost-efficiency and performance.\\nDatabricks has introduced various libraries to expand Spark’s capabilities, such as\\nMLlib (for machine learning), GraphX (for graph computation), and Spark Stream‐\\ning (for real-time data). It has also played a crucial role in training and supporting the\\nSpark community, which has accelerated adoption and enabled users to better lever‐\\nage Spark’s capabilities.\\nDatabricks also focused on making Spark easier to use, introducing high-level APIs in\\nPython, Java, and Scala to make writing Spark applications more accessible. It also\\ndeveloped Databricks notebooks, interactive environments where data scientists and\\nengineers can write Spark code, visualize their results, and build models. This plat‐\\nform allowed data scientists and data engineers to work together more effectively by\\nunifying data science, data engineering, and business analytics.\\nTo address the limitations of data lakes and improve reliability and data quality, Data‐\\nbricks also introduced Delta Lake (see Chapter 12), an open source storage layer that\\nruns on top of existing data lakes. It enhances Spark’s big data capacities with ACID\\ntransactions, scalable metadata handling, and unified streaming and batch data pro‐\\ncessing. Delta Lake addresses traditional data lakes’ challenges with data quality, relia‐\\nbility, and performance. Databricks made Data Lake open source in 2019,\\ncontributing the project to the Linux Foundation to allow the broader community to\\ncontribute. Today, Delta Lake has been widely adopted.\\nIn 2020, Databricks introduced a novel data architecture, the data lakehouse (also\\ncovered in Chapter 12). This innovation represented a significant shift in data man‐\\nagement paradigms. Instead of being forced to choose between data lakes and data\\nwarehouses, organizations can opt for a hybrid architecture that fuses data lakes’ scal‐\\nability and cost-effectiveness with the robust structure, reliability, and performance of\\ndata warehouses.\\nIn Databricks’ view, modern data architectures should be open, collaborative, and\\nable to handle all types of data and advanced analytics use cases. The company thus\\nsupports architectures that allow businesses to scale as they grow and change over\\ntime.\\nSoftware Frameworks | 239', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d77a38f-f18d-406c-97f1-06b8d0a6e254', embedding=None, metadata={'page_label': '239', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='With its Unified Data Analytics Platform and tools like Delta Lake, Databricks could\\nprovide the underlying infrastructure for each data product or domain within a data\\nmesh (see Chapters 13 and 14). Its platform can handle a variety of data types and\\nsupports both batch and real-time data processing, making it versatile enough to\\nserve the needs of different data domains. The key features of Delta Lake, such as\\nACID transactions, schema enforcement, and versioning, can ensure data quality and\\nconsistency within each data domain. Moreover, it offers the ability to “time travel”\\n(access previous versions of data). This can be particularly useful in a data mesh,\\nwhere teams may need to audit or revert changes in their respective domains. Data‐\\nbricks’ collaborative environment can also enable cross-functional teams within a\\ndata mesh to work together more efficiently, aligning with the principle of enabling\\ndomain-oriented, decentralized teams.\\nSnowflake\\nSnowflake is a fully managed, cloud-based data-warehousing solution for storing,\\norganizing, and analyzing large amounts of data and, uniquely, can serve as a data\\nlake in addition to providing traditional data-warehousing functionalities.\\nThe company was founded in 2012 to address the limitations of traditional data-\\nwarehousing solutions. Its founders developed a unique architecture that separates\\ncompute and storage, allowing efficient data analytics and workloads that can scale\\nindependently and automatically based on demand. The platform, launched in 2014,\\ngained recognition for its ability to handle diverse data types and ability to store and\\nprocess a wide range of data formats, including CSV , JSON, Parquet, and Avro. Its\\ncloud-native approach and architecture have attracted a wide range of customers\\nacross industries. It offers pay-per-use pricing, robust security, flexibility, and\\nreliability.\\nY ou can load data in various formats directly into Snowflake as you would with a data\\nlake, eliminating the need to transform or preprocess data before ingestion. Snow‐\\nflake further simplifies the process by automatically inferring the schema of the inges‐\\nted data. Once the data is loaded, Snowflake provides a structured and scalable\\nenvironment for organizing and cataloging it, allowing users to create tables, define\\nschemas, and apply metadata tags. This ensures efficient organization and easy\\naccessibility.\\nUsing Snowflake as a data lake provides the benefits of a unified data platform, elimi‐\\nnating the need for a separate data lake infrastructure and reducing complexity. It\\noffers a scalable and performant environment for storing, managing, and analyzing\\ndiverse data types, making it an attractive choice for organizations looking to leverage\\nthe power of a data lake within a unified ecosystem.\\n240 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a73e685e-896e-4e45-b8f2-9f53805a580f', embedding=None, metadata={'page_label': '240', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Snowflake’s strength lies in its highly optimized data-warehousing capabilities and\\nefficient querying and analysis within its platform. However, if you require specific\\ncomputations or specialized processing that can be better served by other compute\\nengines, exporting the data from Snowflake and utilizing those engines is the com‐\\nmon approach. While Snowflake provides seamless data export capabilities, it does\\nnot directly support running external compute engines or tools on data stored within\\nits platform.\\nSnowflake aligns well with the data mesh principles (covered in Chapter 13 ). The\\ncompany acknowledges the importance of empowering domain experts and teams to\\ntake ownership of their data. With Snowflake, different business units or domains can\\nhave separate accounts or virtual warehouses, enabling them to manage and control\\ntheir own data assets. This decentralized approach promotes agility and autonomy,\\nallowing domain teams to make data-driven decisions and take responsibility for the\\nquality and usability of their data.\\nSnowflake’s robust data management capabilities, including schema management,\\nmetadata tagging, and cataloging, let you treat your data as a valuable asset that can\\nbe accessed and consumed by other teams within the organization, promoting data\\nproduct thinking. Snowflake also aligns with the scalability and elasticity aspects of\\nthe data mesh concept: its cloud-native architecture allows for independent scaling of\\ncompute resources, helping teams adapt to changing requirements and handle data\\nprocessing and analytics efficiently. In summary, Snowflake empowers organizations\\nto adopt a data mesh approach that promotes collaboration, autonomy, and agility in\\ntheir data ecosystems.\\nSummary\\nThe decisions you make about data architecture technologies will be pivotal to the\\nsuccess of your data initiatives. This chapter guided you through the fundamental\\nchoices that lay the foundation for your data infrastructure.\\nWe explored three essential software frameworks: Hadoop, Databricks, and Snow‐\\nflake, each offering a unique approach to data processing and analytics. Whether you\\nopt for open source solutions, embrace the offerings of cloud providers, or combine\\nthe two, consider the scale of your data needs and your organization’s agility\\nrequirements.\\nMoreover, remember that the world of data technology is not binary. It extends to the\\nlandscape of cloud service models, where major providers offer a diverse array of\\nservices and multi-cloud solutions add a layer of complexity to your decision making.\\nSummary | 241', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b184dd26-69b8-47fd-a269-b5baa3c7b1eb', embedding=None, metadata={'page_label': '241', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As you move forward in your data architecture journey, keep in mind that the choices\\nmade in this chapter are not isolated; they are interconnected and will shape the land‐\\nscape of your data environment. It’s not merely about selecting open source or pro‐\\nprietary solutions; it’s about crafting a harmonious blend of technologies that align\\nwith your organization’s goals and challenges. With these considerations in mind, you\\nare well equipped to navigate the ever-evolving world of data architecture\\ntechnologies.\\nHappy deciphering!\\n242 | Chapter 16: Technologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a47b08d0-854d-4943-b7c2-b8a4c3e46005', embedding=None, metadata={'page_label': '242', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Index\\nA\\nACID (atomicity, consistency, isolation, and\\ndurability), 161\\nADS (see architecture design session)\\naggregated domain data, 179\\nall-in-one architecture, for MDW, 146\\nAmazon Web Services (AWS), 232\\nanalytical data, defined, 94\\nanalytics layer (data lake), 65\\nApache Hadoop (see Hadoop)\\nApache Spark\\nData Lake and, 166\\nDatabricks and, 238-240\\nAPIs, data fabric and, 155\\napplication layer (data lake), 65\\napproaches to design (see design approaches)\\narchitecture design session (ADS), 25-40\\nconducting the session, 31-37\\ndefined, 25\\ndeliverables, 25\\ndiscovery, 31-32\\nintroductions, 31\\ninviting participants, 29-30\\npreparation, 27-29\\nquestions to ask, 32-36\\nreasons to hold a session, 26\\ntasks before the session, 27-30\\ntasks following the session, 37-38\\ntips for conducting, 38-40\\nwhiteboarding, 36\\navailability, multiple data lakes and, 71\\nAWS (Amazon Web Services), 232\\nB\\nbase layer (data lake), 64\\nbatch layer (Lambda architecture), 97\\nbatch processing, 5\\ndefined, 129\\npros and cons, 130\\nreal-time processing versus, 127-128\\nbig data, 3-10\\ndata lake, 18-20\\ndata maturity, 7-9\\ndefined, 4\\ngrowth in volume (2018–2024), 3\\nMDW, 20\\nself-service business intelligence, 9-10\\n“six Vs”, 4-6\\nuses for, 6-7\\nbinary data sources, 5\\nbronze layer (data lake), 64\\nbudgets, slashing midway through project, 215\\nbusiness analyst, 209\\nbusiness intelligence, self-service, 9-10\\nbusiness requirements, gathering too few/too\\nmany, 213\\nC\\ncaching (Delta Lake), 162\\nCDM (common data model), 111, 191\\ncenter of excellence (CoE), 131\\nchange data capture (CDC), 55\\nCIF (corporate information factory), 114\\ncleansed layer (data lake), 64\\ncloud service models, 230-234\\nbasic service models, 230\\nmajor cloud providers, 232\\n243', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90775c15-64d2-403e-af69-206918c33f4a', embedding=None, metadata={'page_label': '243', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='multi-cloud solutions, 232-234\\ncloud service providers (CSPs)\\nbenefits as data-warehousing solution,\\n228-229\\non-prem solutions versus, 226\\nopen source software versus, 224-225\\nas platform choice, 227-229\\ncloud subscription, 70\\nCodd, Edgar F ., 103\\ncolumn pruning (Delta Lake), 162\\ncommon data model (CDM), 111, 191\\ncommunication, poor, project failures from,\\n217\\ncompliance, multiple data lakes and, 70\\nconformed dimensions, 116, 194\\nconformed layer (data lake), 64\\nconsulting companies\\nhiring a company that outsources develop‐\\nment to offshore workers, 215\\nhiring an inexperienced company, 214\\nneglecting the need to transfer consultants’\\nknowledge back into the organization,\\n215\\nconsumer-aligned domain data, 179\\nconsumption layer (data lake), 65\\ncorporate information factory (CIF), 114\\ncosts, of CSPs, 228, 229\\nCRUD (create, read, update, delete) operations,\\n17\\nCSPs (see cloud service providers)\\ncurated layer (data lake), 65\\nD\\ndashboards, adding value to, 219\\ndata access policies, for data fabric, 154\\ndata analyst, 209\\ndata architect, 208, 212\\ndata architectures (generally), 13-23\\n(see also specific data architectures)\\nevolution of, 14-15\\nhigh-level use cases for individual architec‐\\ntures, 202\\noverdesigning/underdesigning, 217\\npeople and processes, 207-221\\ndata as product, 173-175\\ndata catalogs, 87\\ndata consumption layer (Lambda architecture),\\n96\\ndata contracts, 174\\ndata design, data modeling versus, 91\\ndata domains, 178-179\\ndata engineer, 208, 211\\ndata exchanges (data marketplaces), 87-89\\ndata fabric, 151-157\\nAPIs, 155\\nbasics, 21, 152-156\\ndata access policies, 154\\ndata mesh versus, 182\\ndata virtualization, 155\\ndefined, 151\\nGartner definition, 151\\nhigh-level use cases for, 202\\nmaster data management, 155\\nmetadata catalog, 154\\npotential drawbacks, 157\\nas product, 156\\nreal-time processing, 155\\nreasons to transition from MDW to data\\nfabric architecture, 156\\nservices, 156\\nteam roles for, 208-209\\ndata federation\\ndata mesh creation myths and, 189\\ndata virtualization versus, 82\\nfederated queries versus, 84\\ndata governance, 131\\ndata access policies, 154\\ndefined, 131\\nfederated computation governance platform\\nteam for data mesh, 211-213\\nfederated computational governance for\\ndata mesh, 176-177\\nmultiple data lakes and, 70\\ndata governance lead, 212\\ndata governance manager, 209\\ndata governance specialist, 212\\ndata hubs, 79-81\\ndata ingestion, 125-132\\nbatch processing versus real-time process‐\\ning, 127-128\\ndata governance issues, 131\\nETL versus ELT, 125-127\\nreverse ETL, 127-128\\ndata lake, 18-20, 59-73\\nbasics, 142\\nbest practices for designing, 63-68\\nbottom-up approach, 62-63\\ncombining with RDWs, 142-143\\n244 | Index', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bf49c8cc-923d-4233-be0f-505494e6e442', embedding=None, metadata={'page_label': '244', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='defined, 60\\nlayers, 63-68\\nMDW construction with temporary data\\nlake plus EDW, 145-146\\nmultiple data lakes, 69-72\\nreasons to use, 60-62\\ndata lakehouses, 159-168\\narchitecture, 163-165\\nbasics, 21\\nDelta Lake features, 160-162\\nhigh-level use cases for, 202\\nperformance improvements from Delta\\nLake, 162\\nrelational serving layer, 167\\nskipping the RDW, 165-167\\nteam roles for, 208-209\\ndata lineage, 154\\ndata manipulation language (DML) commands,\\n160\\ndata marketplaces, 87-89\\ndata marts, 76-77\\ndata maturity, 7-9\\ndata mesh, 22-23, 169-185\\ncomplexity concerns, 193\\ndata domains, 178-179\\ndata fabric versus, 182\\ndecentralization concerns, 191-193\\nas decentralized data architecture, 170-171\\nDehghani’s four principles, 172-177\\ndata as product, 173-175\\ndomain ownership, 172\\nfederated computational governance,\\n176-177\\nself-serve data infrastructure as a plat‐\\nform, 175-176\\ndomain-level barriers to implementation,\\n197-198\\nduplication concerns, 193\\nfeasibility of migrating to, 194-196\\nfuture of, 201-202\\nhigh-level use cases for, 202\\nhiring talent for each domain, 196-197\\nhype surrounding, 171-172\\nlogical architecture, 179-181\\nmyths, 187-189\\ndata mesh as replacement for data ware‐\\nhouse/data lake, 188\\ndecentralization as result of data mesh,\\n188\\nsilver bullet myth, 187\\nusing data virtualization to create data\\nmesh, 189\\norganizational assessment on decision to\\nadopt, 198-199\\nphilosophical/conceptual challenges, 190\\n“pure” data mesh, 177-178\\nrecommendations for successful implemen‐\\ntation, 199-200\\nteam roles, 210-213\\ndomain teams, 210\\nfederated computation governance plat‐\\nform team, 211-213\\nself-service data infrastructure platform\\nteam, 210\\ntopologies/possible variations, 181-182\\nuse cases, 183-185\\ndata mesh architect, 210\\ndata modeling, 103-123\\ncommon data model, 111\\ndata design versus, 91\\ndata vault, 111-113\\ndata warehousing\\nchoosing a methodology, 116\\nhybrid models, 118-119\\nInmon’s top-down methodology,\\n114-115\\nKimball’s bottom-up methodology,\\n115-116\\nmethodology myths, 120-122\\ndefined, 63, 103\\ndimensional modeling, 107-110\\ndenormalization, 109-110\\nfacts/dimensions/keys, 107\\ntracking changes, 108\\nrelational modeling, 103-123\\nentity–relationship diagrams, 104\\nkeys, 103\\nnormalization rules and forms, 104-105\\ntracking changes, 106\\ndata movement, 36\\ndata privacy and compliance officer, 212\\ndata privacy officer, 209\\ndata processes, 81-89\\ndata catalogs, 87\\ndata marketplaces, 87-89\\ndata virtualization/data federation, 82-86\\nmaster data management, 81\\ndata product, 174\\nIndex | 245', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='61bb81fa-dfc4-40b5-beb7-bb3840a19163', embedding=None, metadata={'page_label': '245', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data quality manager, 209\\ndata quality specialist, 212\\ndata quantum, 195\\ndata retention, multiple data lakes for, 72\\ndata science workspace (layer of data lake), 65\\ndata scientist, 209\\ndata security analyst, 212\\ndata skipping (Delta Lake), 162\\ndata steward, 208\\ndata stockpiling, 61\\ndata storage solutions, 75-81\\ndata hubs, 79-81\\ndata marts, 76-77\\noperational data stores, 77-79\\ndata vault, 111-113\\ndata virtualization, 82-86\\nfor data fabric, 155\\ndata federation versus, 82\\nas replacement for data warehouse, 83-85\\nas replacement for ETL or data movement,\\n85-86\\ndata warehouse (DW) (see relational data ware‐\\nhouse [RDW])\\ndatabase administrator (DBA), 209\\ndatabase indexes, 84\\ndatabase schema, 109\\ndatabase statistics, 84\\ndatabase views, 119\\nDatabricks, 238-240\\nDataOps engineer, 211, 212\\nDDD (domain-driven design), 178-179\\ndeadlines, excessive focus on, 216\\ndecentralization, data mesh architecture and,\\n170-171\\ndecentralized architecture, 22\\ndecentralized data warehouse (see data virtuali‐\\nzation)\\nDehghani, Zhamak\\ndata mesh blog post, 22\\ndata mesh origins, 169\\nfour principles of data mesh, 172-177\\nDelta Lake (Databricks software), 159\\ndata lake performance improvements from,\\n162\\nfeatures, 160-162\\norigins and purpose, 239\\nDelta Table, 161-162\\ndenormalization, 109-110\\ndependent data marts, 116\\ndescriptive analytics, 47\\ndesign approaches, 91-101\\nKappa architecture, 98-99\\nLambda architecture, 96-98\\nOLTP versus OLAP, 92-94\\noperational and analytical data, 94\\npolyglot persistence and polyglot data\\nstores, 100-101\\nSMP and MPP, 94\\ndevelopment layer (data lake), 65\\nDevlin, Barry, 113\\nDevOps engineer, 211\\ndiagnostic analytics, 47\\ndigital transformation, 7\\ndimensional modeling, 107-110\\ndenormalization, 109-110\\nfacts/dimensions/keys, 107\\ntracking changes, 108\\ndimensionalized view, 116\\ndimensions, defined, 107\\ndisaster recovery (DR)\\nmultiple data lakes and, 71\\nPaaS and, 231\\ndiscovery phase of ADS, 31-32\\nDML (data manipulation language) commands,\\n160\\ndomain data product owner (data mesh), 210\\ndomain ownership principle of data mesh, 172\\ndomain representative, 212\\ndomain-driven design (DDD), 178-179\\ndomain-oriented analytical data, 178\\nDR (see disaster recovery)\\nDW (data warehouse) (see relational data ware‐\\nhouse [RDW])\\nDW bus, 116\\nE\\nEDW (see enterprise data warehouse)\\nELAs (enterprise license agreements), 234\\nELT (extract, load, and transform)\\nETL versus, 125-127\\nhow to avoid confusing ETL and ELT, 126\\nend users, involvement as key to project suc‐\\ncess, 218\\nenriched layer (data lake), 64\\nenterprise data maturity, 7-9\\nenterprise data warehouse (EDW)\\naugmentation for MDW, 143-145\\ndata warehouse versus, 114\\n246 | Index', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a3e31771-bb25-4413-916b-37a9f79cd508', embedding=None, metadata={'page_label': '246', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='defined, 44\\nMDW construction with temporary data\\nlake plus EDW, 145-146\\nenterprise license agreements (ELAs), 234\\nentity–relationship (ER) diagrams, 104\\nenvironment management, for multiple data\\nlakes, 72\\nETL (extract, transform, and load)\\ndata virtualization as replacement for, 85-86\\nELT versus, 125-127\\nhow to avoid confusing ETL and ELT, 126\\nreverse ETL, 127-128\\nsteps, 45\\nevolution of data architectures, 14-15\\nexploration layer (data lake), 65\\nextract, load, and transform (see ELT)\\nextract, transform, and load (see ETL)\\nF\\nfacts, in dimensional modeling context, 107\\nfailure of projects, causes of\\nallowing executives to think that BI is easy,\\n213\\ngathering too few/too many business\\nrequirements, 213\\nhiring a consulting company that outsour‐\\nces development to offshore workers,\\n215\\nhiring an inexperienced consulting com‐\\npany, 214\\nneglecting the need to transfer consultants’\\nknowledge back into the organization,\\n215\\noverdesigning/underdesigning data archi‐\\ntecture, 217\\npassing project ownership off to consul‐\\ntants, 215\\npoor communication between IT and busi‐\\nness domains, 217\\npresenting end users with solution with\\nslow response times/performance issues,\\n216\\npresenting reports without validating con‐\\ntents, 214\\nslashing budgets midway through project,\\n215\\nstarting with end date and working back‐\\nward, 216\\nstructuring data warehouse to reflect source\\ndata over business’s needs, 216\\nusing wrong technologies, 213\\nfast indexing (Delta Lake), 162\\nfederated computational governance (data\\nmesh)\\nplatform team, 211-213\\nprinciple, 176-177\\nfederated queries, 84\\nfirst normal form (1NF), 105\\nforced metadata layer, 166\\nforeign key, 103\\nFree Software Movement, 224\\nfull extraction, 54\\nG\\nGartner\\ndata fabric definition, 151\\nHype Cycle for Data Management, 171\\ngold layer (data lake), 65\\nGoogle Cloud Platform (GCP), 232\\ngoverned layer (data lake), 65\\nH\\nHadoop, 18, 235-238\\nbenefits, 236-237\\npotential disadvantages, 237\\nHadoop Distributed File System (HDFS), 235\\nhiring, of talent for data mesh, 196-197\\nhub-and-spoke data mesh model, 199\\nhubs, in data vault model, 112\\nHype Cycle for Data Management (Gartner),\\n171\\nI\\nImhoff, Claudia, 120\\nincremental extraction, 54\\nindependent data marts, 116\\ninformation bus, 116\\ninformative stage of enterprise data maturity, 8\\ninfrastructure as a service (IaaS), 230\\nInmon data warehouse methodology, 114-115\\ndata warehouse methodology myths,\\n120-122\\nKimball methodology combined with,\\n118-119\\nKimball methodology versus, 116\\nInmon, Bill, 113\\nIndex | 247', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d936a500-5c62-495a-9f82-379a24f8939a', embedding=None, metadata={'page_label': '247', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='integrated layer (data lake), 64\\nK\\nKappa architecture, 98-99\\nkeys\\nin dimensional modeling context, 107\\nin relational modeling, 103\\nKimball data warehouse methodology, 115-116\\ndata warehouse methodology myths,\\n120-122\\nInmon methodology combined with,\\n118-119\\nInmon methodology versus, 116\\nKimball, Ralph, 114\\nL\\nLambda architecture, 96-98\\nkey principles, 96\\npotential drawbacks, 97\\nlanding area (data lake layer), 64\\nlatency, multiple data lakes and, 71\\nlayers, of data lake, 63-68\\nlinks, in data vault model, 112\\nlogical data warehouse (see data virtualization)\\nM\\nMapReduce framework, 236\\nmassively parallel processing (MPP), 95\\nmaster data management (MDM)\\nas ADS topic, 34\\nfor data fabric, 155\\ndefined, 81\\nfor relational data warehouse, 51\\nMDW (see modern data warehouse)\\nMERGE statements, 55\\nmetadata catalog, 154\\nMicrosoft Azure, 232\\nmirrored OLTP, 119\\nmodern data warehouse (MDW), 135-149\\narchitecture, 135-140\\nbasics, 20\\ncase study, 147-148\\ncombining RDW and data lake, 142-143\\ndata fabric and, 151\\nhigh-level use cases for, 202\\npros and cons of architecture, 140-142\\nreasons to transition to data fabric architec‐\\nture from, 156\\nstages of architecture, 138-139\\nstepping-stone architectures, 143-147\\nall-in-one architecture, 146\\nEDW augmentation, 143-145\\ntemporary data lake plus EDW, 145-146\\nteam roles for, 208-209\\nMorton order/Z-order (Delta Lake), 163\\nMPP (massively parallel processing), 95\\nmulti-cloud approach, 232-234\\nmultidimensional model, 93\\nN\\nnatural keys, 107\\nnon-relational data warehouses, 44\\nnormalization rules, 104-105\\nnormalized data base schema, 105\\nO\\nODSs (operational data stores), 77-79\\noffshore workers, consulting companies who\\noutsource to, 215\\non-prem (on-premise) data warehouse solu‐\\ntions, 226-227\\ndefined, 9\\nlimitations/constraints, 226\\nonline analytical processing (OLAP), 92-94\\nonline transaction processing (OLTP), 17,\\n92-94\\nOpen Source Initiative (OSI), 224\\nopen source software (OSS), 223-225\\noperational analytics, 127\\noperational data stores (ODSs), 77-79\\noperational data, defined, 94\\nOSI (Open Source Initiative), 224\\nOSS (open source software), 223-225\\noutsourcing, 215\\noverdesign of data architecture, 217\\nP\\nparallel processing (Delta Lake), 163\\nplatform architect (data mesh), 210\\nplatform as a service (PaaS), 230\\nplatform choice, 223-229\\ncloud provider solutions, 227-229\\non-premise solutions, 226-227\\nopen source solutions, 223-225\\nplatform engineers/developers, 211\\npolyglot data stores, 100\\n248 | Index', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fe8981bf-c478-456d-8916-61e632e3c5db', embedding=None, metadata={'page_label': '248', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='polyglot persistence, 100-101\\npredicate pushdown (Delta Lake), 162\\npredictive analytics, 6, 62\\npredictive stage of enterprise data maturity, 9\\nprescriptive analytics, 63\\npresentation layer\\ndata lake, 65\\nLambda architecture, 97\\nprimary key, 103\\nprocessed layer (data lake), 64\\nproduct\\ndata as, 173-175\\ndata fabric as, 156\\nproduct owner/manager, 211\\nproduction ready layer (data lake), 65\\nproject champion, 219\\nproject failures (see failure of projects)\\nproject manager (project lead), 220\\nproject manager (scrum master), 209\\nprototype creation, involving end users in, 219\\n“pure” data mesh, 177-178, 183\\npush-down query, 84\\nQ\\nquery optimization (Delta Lake), 162\\nR\\nraw layer (data lake), 64\\nRDW (see relational data warehouse)\\nreactive stage of enterprise data maturity, 8\\nreal-time processing\\nbatch processing versus, 127-128\\nfor data fabric, 155\\ndefined, 129\\npros and cons, 130\\nreal-time streaming, 6\\nrefined layer (data lake), 64\\nrelational data warehouse (RDW), 43-57\\nbasics, 43-45, 142\\nchoosing a methodology for, 116\\ncombining with data lake, 142-143\\ndata lakes as solution to limitations of, 59\\ndata lakes used in conjunction with, 60-62\\ndetermining what data has changed since\\nlast extraction, 54-55\\ndrawbacks to using, 52-53\\nEDW versus, 114\\nextraction methods, 54\\nfailures stemming from reflecting source\\ndata instead of business’s needs, 216\\nhow often to extract data, 53\\nhybrid models, 118-119\\nInmon’s top-down methodology, 114-115\\nKimball’s bottom-up methodology, 115-116\\nmethodology myths, 120-122\\non-prem solutions, 226-227\\norigins and brief history, 16-17\\npopulating a data warehouse, 53-55\\nreasons to use, 49-52\\nrelevance to current IT environment, 56\\nskipping when constructing a data lake‐\\nhouse, 165-167\\nsolutions that are not data warehouses,\\n46-47\\ntop-down design approach, 47-49\\nvirtualization as replacement for, 83-85\\nwhat it is not, 46-47\\nrelational database, 14, 16-17\\nrelational modeling, 103-123\\nentity–relationship diagrams, 104\\nkeys, 103\\nnormalization rules and forms, 104-105\\ntracking changes, 106\\nrelational serving layer (data lakehouse), 167\\nreports\\nadding value to, 219\\npresenting without validating contents, 214\\nresource group, 68\\nreverse ETL, 127-128\\nS\\nsandbox layer (data lake), 65\\nsatellites, in data vault model, 112\\nSCDs (slowly changing dimensions), 108\\nschema enforcement, 162\\nschema, defined, 14\\nschema-on-read, 15\\nschema-on-write, 14\\nsecond normal form (2NF), 105\\nsecure layer (data lake), 65\\nsecurity and compliance officer, 211\\nsecurity, multiple data lakes and, 70\\nself-serve data infrastructure as a platform,\\n175-176\\nself-service business intelligence, 9-10\\nsemi-structured data sources, 5\\nservices, data fabric and, 156\\nIndex | 249', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7771a0bc-8161-46aa-9aac-012c33ed0ee0', embedding=None, metadata={'page_label': '249', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='serving layer\\ndata lake, 65\\nLambda architecture, 97\\nsilver layer (data lake), 64\\nsingle version of truth (SVOT), 44\\n“six Vs” of big data, 4-6\\nslowly changing dimensions (SCDs), 108\\nsmall files problem, 161\\nSMEs (subject matter experts), 28\\nSMP (symmetric multiprocessing), 94\\nSnowflake, 240-241\\nsoftware as a service (SaaS), 230\\nsource-aligned domain data, 178\\nSpark (see Apache Spark)\\nspeed layer (Lambda architecture), 96\\nspreadmarts (spreadsheet data marts), 8\\nSQL (Structured Query Language), 14\\nstaging layer (data lake), 64\\nStallman, Richard, 224\\nstandardized layer (data lake), 64\\nstar schema, 109, 194\\nstorage (see data storage solutions)\\nstream layer (Lambda architecture), 96\\nstructured data sources, 5\\nStructured Query Language (SQL), 14\\nsubject matter experts (SMEs), 28\\nsuccess, tips for, 217-220\\nadding value to new reports and dash‐\\nboards, 219\\nfinding a project champion/sponsor, 219\\ninvolving end users in project, 218\\nmaking a project plan that aims for 80%\\nefficiency, 220\\nnot skimping on investment, 217\\nsupport and training staff, 211\\nsurrogate keys, 107\\nSVOT (single version of truth), 44\\nsymmetric multiprocessing (SMP), 94\\nT\\ntabular data model, 93\\nTCO (see total cost of ownership)\\nteam organization, 208-213\\ndata mesh team, 210-213\\nMDW/data fabric/data lakehouse team,\\n208-209\\ntechnologies, 223-242\\nchoosing a platform, 223-229\\ncloud provider solutions, 227-229\\non-prem solutions, 226-227\\nopen source solutions, 223-225\\ncloud service models, 230-234\\nbasic service models, 230\\nmajor cloud providers, 232\\nmulti-cloud solutions, 232-234\\nsoftware frameworks, 235-241\\nDatabricks, 238-240\\nHadoop, 235-238\\nSnowflake, 240-241\\nthird normal form (3NF), 105\\ntop-down design approach, for RDWs, 47-49\\ntotal cost of ownership (TCO)\\ncalculator, 229\\nfor data lakehouse, 164\\ntransformative stage of enterprise data matur‐\\nity, 9\\ntransformed layer (data lake), 64\\ntrusted layer (data lake), 65\\nTwitter, 59\\nU\\nunderdesign of data architecture, 217\\nunstructured data sources, 5\\nuser experience (UX) designer, 211\\nV\\nvectorized execution (Delta Lake), 162\\nvirtual data warehouse (see data virtualization)\\nvirtual sandbox, 86\\nvirtualization of data (see data virtualization)\\nW\\nwhiteboarding, for ADS, 36\\nworkspace layer (data lake), 65\\nZ\\nZ-order/Morton order (Delta Lake), 163\\nzones (layers), 63-68\\n250 | Index', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='096f9ed7-cd8e-45b4-a204-786bde876806', embedding=None, metadata={'page_label': '250', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='About the Author\\nJames Serra works at Microsoft as a big data and data warehousing solution architect\\nwhere he has been for most of the last nine years. He is a thought leader in the use\\nand application of big data and advanced analytics, including data architectures such\\nas the modern data warehouse, data lakehouse, data fabric, and data mesh. Previously\\nhe was an independent consultant working as a Data Warehouse/Business Intelli‐\\ngence architect and developer. He is a prior SQL Server MVP with over 35 years of IT\\nexperience. He is a popular blogger and speaker, having presented at dozens of major\\nevents including SQLBits, PASS Summit, Data Summit and the Enterprise Data\\nWorld conference.\\nColophon\\nThe animal on the cover of Deciphering Data Architectures is a blue discus fish ( Sym‐\\nphysodon aequifasciata), a freshwater fish found in the tributaries of the Amazon\\nRiver. The wavy markings and coloration make blue discus fish popular for aquari‐\\nums, but their natural habitat can be difficult to replicate and maintain.\\nBlue discus fish eat worms and small crustaceans and grow up to 9 inches long. The\\nyoung feed off of the secretions of their parents. Many of the animals on O’Reilly cov‐\\ners are endangered; all of them are important to the world.\\nThe cover illustration is by Karen Montgomery, based on a black-and-white engrav‐\\ning from Akademie der Wissenschaften. The series design is by Edie Freedman, Ellie\\nVolckhausen, and Karen Montgomery. The cover fonts are Gilroy Semibold and\\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0e44f112-b2ae-46cb-b1e8-693118d82c4d', embedding=None, metadata={'page_label': '251', 'file_name': 'Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Deciphering Data Architectures_ Choosing Between a Modern -- James Serra.pdf', 'file_type': 'application/pdf', 'file_size': 7458555, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Learn from experts.  \\nBecome one yourself.\\nBooks | Live online courses   \\nInstant answers | Virtual events \\nVideos | Interactive learning\\nGet started at oreilly.com. \\n©2023 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc.  175  7x9.1975', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='101685fc-aab2-47bd-9802-fdbe9facf855', embedding=None, metadata={'page_label': '1', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22a95b7e-01a3-4161-9562-fc88e241afa1', embedding=None, metadata={'page_label': '2', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fundamentals of Data\\nEngineering\\nPlan and Build Robust Data Systems\\nJoe Reis and Matt Housley', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='71cdfd75-b633-40ee-8268-ee6a67bd92d1', embedding=None, metadata={'page_label': '3', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fundamentals of Data Engineering\\nby Joe Reis and Matt Housley\\nCopyright © 2022 Joseph Reis and Matthew Housley. All rights\\nreserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North,\\nSebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales\\npromotional use. Online editions are also available for most titles\\n(http://oreilly.com). For more information, contact our\\ncorporate/institutional sales department: 800-998-9938 or\\ncorporate@oreilly.com.\\nAcquisitions Editor: Jessica Haberman\\nDevelopment Editor: Michele Cronin\\nProduction Editor: Gregory Hyman\\nCopyeditor: Sharon Wilkey\\nProofreader: Amnet Systems, LLC\\nIndexer: Judith McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nJune 2022: First Edition\\nRevision History for the First Edition', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c1292028-e96a-481f-b403-e4c8a02a8e67', embedding=None, metadata={'page_label': '4', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2022-06-22: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098108304 for\\nrelease details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc.\\nFundamentals of Data Engineering, the cover image, and related\\ntrade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors, and do\\nnot represent the publisher’s views. While the publisher and the\\nauthors have used good faith efforts to ensure that the information\\nand instructions contained in this work are accurate, the publisher\\nand the authors disclaim all responsibility for errors or omissions,\\nincluding without limitation responsibility for damages resulting from\\nthe use of or reliance on this work. Use of the information and\\ninstructions contained in this work is at your own risk. If any code\\nsamples or other technology this work contains or describes is\\nsubject to open source licenses or the intellectual property rights of\\nothers, it is your responsibility to ensure that your use thereof\\ncomplies with such licenses and/or rights.\\n978-1-098-10830-4\\n[LSI]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='48870041-de85-43b6-be91-2b7fa2aeffc7', embedding=None, metadata={'page_label': '5', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preface\\nHow did this book come about? The origin is deeply rooted in our\\njourney from data science into data engineering. We often jokingly\\nrefer to ourselves as recovering data scientists. We both had the\\nexperience of being assigned to data science projects, then\\nstruggling to execute these projects due to a lack of proper\\nfoundations. Our journey into data engineering began when we\\nundertook data engineering tasks to build foundations and\\ninfrastructure.\\nWith the rise of data science, companies splashed out lavishly on\\ndata science talent, hoping to reap rich rewards. Very often, data\\nscientists struggled with basic problems that their background and\\ntraining did not address—data collection, data cleansing, data\\naccess, data transformation, and data infrastructure. These are\\nproblems that data engineering aims to solve.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc2069ae-dfb3-41ef-8bad-76b6b09e001e', embedding=None, metadata={'page_label': '6', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What This Book Isn’t\\nBefore we cover what this book is about and what you’ll get out of it,\\nlet’s quickly cover what this book isn’t. This book isn’t about data\\nengineering using a particular tool, technology, or platform. While\\nmany excellent books approach data engineering technologies from\\nthis perspective, these books have a short shelf life. Instead, we try\\nto focus on the fundamental concepts behind data engineering.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='03c8fadd-58ed-4c5c-ad4c-e34ffc797224', embedding=None, metadata={'page_label': '7', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What This Book Is About\\nThis book aims to fill a gap in current data engineering content and\\nmaterials. While there’s no shortage of technical resources that\\naddress specific data engineering tools and technologies, people\\nstruggle to understand how to assemble these components into a\\ncoherent whole that applies in the real world. This book connects the\\ndots of the end-to-end data lifecycle. It shows you how to stitch\\ntogether various technologies to serve the needs of downstream\\ndata consumers such as analysts, data scientists, and machine\\nlearning engineers. This book works as a complement to O’Reilly\\nbooks that cover the details of particular technologies, platforms and\\nprogramming languages.\\nThe big idea of this book is the data engineering lifecycle: data\\ngeneration, storage, ingestion, transformation, and serving Since the\\ndawn of data, we’ve seen the rise and fall of innumerable specific\\ntechnologies and vendor products, but the data engineering life cycle\\nstages have remained essentially unchanged. With this framework,\\nthe reader will come away with a sound understanding for applying\\ntechnologies to real-world business problems.\\nOur goal here is to map out principles that reach across two axes.\\nFirst, we wish to distill data engineering into principles that can\\nencompass any relevant technology. Second, we wish to present\\nprinciples that will stand the test of time. We hope that these ideas\\nreflect lessons learned across the data technology upheaval of the\\nlast twenty years and that our mental framework will remain useful\\nfor a decade or more into the future.\\nOne thing to note: we unapologetically take a cloud-first approach.\\nWe view the cloud as a fundamentally transformative development\\nthat will endure for decades; most on-premises data systems and\\nworkloads will eventually move to cloud hosting. We assume that\\ninfrastructure and systems are ephemeral and scalable, and that\\ndata engineers will lean toward deploying managed services in the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a7c69ab-8518-4def-bfe5-309ffd3b584b', embedding=None, metadata={'page_label': '8', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='cloud. That said, most concepts in this book will translate to non-\\ncloud environments.\\nWho Should Read This Book\\nOur primary intended audience for this book consists of technical\\npractitioners, mid- to senior-level software engineers, data scientists,\\nor analysts interested in moving into data engineering; or data\\nengineers working in the guts of specific technologies, but wanting to\\ndevelop a more comprehensive perspective. Our secondary target\\naudience consists of data stakeholders who work adjacent to\\ntechnical practitioners—e.g., a data team lead with a technical\\nbackground overseeing a team of data engineers, or a director of\\ndata warehousing wanting to migrate from on-premises technology\\nto a cloud-based solution.\\nIdeally, you’re curious and want to learn—why else would you be\\nreading this book? You stay current with data technologies and\\ntrends by reading books and articles on data warehousing/data\\nlakes, batch and streaming systems, orchestration, modeling,\\nmanagement, analysis, developments in cloud technologies, etc.\\nThis book will help you weave what you’ve read into a complete\\npicture of data engineering across technologies and paradigms.\\nPrerequisites\\nWe assume a good deal of familiarity with the types of data systems\\nfound in a corporate setting. In addition, we assume that readers\\nhave some familiarity with SQL and Python (or some other\\nprogramming language), and experience with cloud services.\\nNumerous resources are available for aspiring data engineers to\\npractice Python and SQL. Free online resources abound (blog posts,\\ntutorial sites, YouTube videos), and many new Python books are\\npublished every year.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3f5d934-d3a3-4044-a6db-d33e2ab87aa3', embedding=None, metadata={'page_label': '9', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The cloud provides unprecedented opportunities to get hands-on\\nexperience with data tools. We suggest that aspiring data engineers\\nset up accounts with cloud services such as AWS, Azure, Google\\nCloud Platform, Snowflake, Databricks, etc. Note that many of these\\nplatforms have free tier options, but readers should keep a close eye\\non costs, and work with small quantities of data and single node\\nclusters as they study.\\nDeveloping familiarity with corporate data systems outside of a\\ncorporate environment remains difficult and this creates certain\\nbarriers for aspiring data engineers who have yet to land their first\\ndata job. This book can help. We suggest that data novices read for\\nhigh level ideas, and then look at materials in the additional\\nresources section at the end of each chapter. On a second read\\nthrough, note any unfamiliar terms and technologies. You can utilize\\nGoogle, Wikipedia, blog posts, YouTube videos, and vendor sites to\\nbecome familiar with new terms and fill gaps in your understanding.\\nWhat You’ll Learn and How It Will Improve Your\\nAbilities\\nThis book aims to help you build a solid foundation for solving real\\nworld data engineering problems.\\nBy the end of this book you will understand:\\nHow data engineering impacts your current role (data scientist,\\nsoftware engineer, or data team lead).\\nHow to cut through the marketing hype and choose the right\\ntechnologies, data architecture, and processes.\\nHow to use the data engineering lifecycle to design and build a\\nrobust architecture.\\nBest practices for each stage of the data lifecycle.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d7be4064-15a1-488c-8f01-5cb2c5996171', embedding=None, metadata={'page_label': '10', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='And you will be able to:\\nIncorporate data engineering principles in your current role (data\\nscientist, analyst, software engineer, data team lead, etc.)\\nStitch together a variety of cloud technologies to serve the\\nneeds of downstream data consumers.\\nAssess data engineering problems with an end-to-end\\nframework of best practices\\nIncorporate data governance and security across the data\\nengineering lifecycle.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5802b8a4-2a7f-4fa4-944f-b30a99e488b0', embedding=None, metadata={'page_label': '11', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The Book Outline\\nThis book is composed of four parts:\\nPart I, “Foundation and Building Blocks”\\nPart II, “The Data Engineering Lifecycle in Depth”\\nPart III, “Security, Privacy, and the Future of Data Engineering”\\nAppendices A and B: cloud networking, serialization and\\ncompression\\nIn Part I, we begin by defining data engineering in Chapter 1, then\\nmap out the data engineering lifecycle in Chapter 2. In Chapter 3, we\\ndiscuss good architecture. In Chapter 4, we introduce a framework\\nfor choosing the right technology—while we frequently see\\ntechnology and architecture conflated, these are in fact very different\\ntopics.\\nPart II builds on Chapter 2 to cover the data engineering lifecycle in\\ndepth; each lifecycle stage—data generation, storage, ingestion,\\ntransformation and serving—is covered in its own chapter. Part II is\\narguably the heart of the book, and the other chapters exist to\\nsupport the core ideas covered here.\\nPart III covers additional topics. In Chapter 10, we discuss security\\nand privacy. While security has always been an important part of the\\ndata engineering profession, it has only become more critical with\\nthe rise of for profit hacking and state sponsored cyber attacks. And\\nwhat can we say of privacy? The era of corporate privacy nihilism is\\nover—no company wants to see its name appear in the headline of\\nan article on sloppy privacy practices. Reckless handling of personal\\ndata can also have significant legal ramifications with the advent of\\nGDPR, CCPA and other regulations. In short, security and privacy\\nmust be top priorities in any data engineering work.\\nIn the course of working in data engineering, doing research for this\\nbook and interviewing numerous experts, we thought a good deal', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3afa66b1-c265-42da-ae86-059ff8c6588f', embedding=None, metadata={'page_label': '12', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='about where the field is going in the near and long term. Chapter 11\\noutlines our highly speculative ideas on the future of data\\nengineering. By its nature, the future is a slippery thing. Time will tell\\nif some of our ideas are correct. We would love to hear from our\\nreaders on how their visions of the future agree with or differ from\\nour own.\\nIn the appendix, we cover a handful of technical topics that are\\nextremely relevant to the day to day practice of data engineering, but\\ndidn’t fit into the main body of the text. Specifically, cloud networking\\nis a critical topic as data engineering shifts into the cloud, and\\nengineers need to understand serialization and compression both to\\nwork directly with data files, and to assess performance\\nconsiderations in data systems.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file\\nextensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to\\nprogram elements such as variable or function names,\\ndatabases, data types, environment variables, statements, and\\nkeywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by\\nthe user.\\nConstant width italic', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa6ce3b3-bdbf-401a-bdf7-15e89c9448f9', embedding=None, metadata={'page_label': '13', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Shows text that should be replaced with user-supplied values or\\nby values determined by context.\\nTIP\\nThis element signifies a tip or suggestion.\\nNOTE\\nThis element signifies a general note.\\nWARNING\\nThis element indicates a warning or caution.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the\\npublisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4e04f9db-0aec-407c-ae84-9fbc9e3cec55', embedding=None, metadata={'page_label': '14', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We have a web page for this book, where we list errata, examples,\\nand any additional information. You can access this page at\\nhttps://oreil.ly/fundamentals-of-data.\\nEmail bookquestions@oreilly.com to comment or ask technical\\nquestions about this book.\\nFor news and information about our books and courses, visit\\nhttps://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\\nFollow us on Twitter: https://twitter.com/oreillymedia\\nWatch us on YouTube: https://www.youtube.com/oreillymedia\\nAcknowledgments\\nWhen we started writing this book, we were warned by many people\\nthat we faced a hard task. A book like this has a lot of moving parts,\\nand due to its comprehensive view of the field of data engineering, it\\nrequired a ton of research, interviews, discussions, and deep\\nthinking. We won’t claim to have captured every nuance of data\\nengineering, but we hope that the results resonate with you.\\nNumerous individuals contributed to our efforts, and we’re grateful\\nfor the support we received from many experts.\\nFirst, thanks to our amazing crew of technical reviewers. They\\nslogged through many readings, and gave invaluable (and often\\nruthlessly blunt) feedback. This book would be a fraction of itself\\nwithout their efforts. In no particular order, we give endless thanks to\\nBill Inmon, Andy Petrella, Matt Sharp, Tod Hanseman, Chris Tabb,\\nDanny Lebzyon, Martin Kleppman, Scott Lorimor, Nick Schrock, Lisa\\nSteckman, and Alex Woolford.\\nSecond, we’ve had a unique opportunity to talk with the leading\\nexperts in the field of data on our live shows, podcasts, meetups,\\nand endless private calls. Their ideas helped shape our book. There', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a690162b-ee11-496d-945f-3846c75b0f8b', embedding=None, metadata={'page_label': '15', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='are too many people to name individually, but we’d like to give\\nshoutouts to Bill Inmon, Jordan Tigani, Zhamak Dehghani, Shruti\\nBhat, Eric Tschetter, Benn Stancil, Kevin Hu, Michael Rogove, Ryan\\nWright, Egor Gryaznov, Chad Sanderson, Julie Price, Matt Turck,\\nMonica Rogati, Mars Lan, Pardhu Gunnam, Brian Suk, Barr Moses,\\nLior Gavish, Bruno Aziza, Gian Merlino, DeVaris Brown, Todd\\nBeauchene, Tudor Girba, Scott Taylor, Ori Rafael, Lee Edwards,\\nBryan Offutt, Ollie Hughes, Gilbert Eijkelenboom, Chris Bergh,\\nFabiana Clemente, Andreas Kretz, Ori Reshef, Nick Singh, Mark\\nBalkenende, Kenten Danas, Brian Olsen, Lior Gavish, Rhaghu\\nMurthy, Greg Coquillo, David Aponte, Demetrios Brinkmann, Sarah\\nCatanzaro, Michel Tricot, Levi Davis, Ted Walker, Carlos Kemeny,\\nJosh Benamram, Chanin Nantasenamat, George Firican, Jordan\\nGoldmeir, Minhaaj Rehmam, Luigi Patruno, Vin Vashista, Danny Ma,\\nJesse Anderson, Alessya Visnjic, Vishal Singh, Dave Langer, Roy\\nHasson, Todd Odess, Che Sharma, Scott Breitenother, Ben Taylor,\\nThom Ives, John Thompson, Brent Dykes, Josh Tobin, Mark Kosiba,\\nTyler Pugliese, Douwe Maan, Martin Traverso, Curtis Kowalski, Bob\\nDavis, Koo Ping Shung, Ed Chenard, Matt Sciorma, Tyler Folkman,\\nJeff Baird, Tejas Manohar, Paul Singman, Kevin Stumpf, Willem\\nPineaar, and Michael Del Balso from Tecton, Emma Dahl, Harpreet\\nSahota, Ken Jee, Scott Taylor, Kate Strachnyi, Kristen Kehrer, Taylor\\nMiller, Abe Gong, Ben Castleton, Ben Rogojan, David Mertz,\\nEmmanuel Raj, Andrew Jones, Avery Smith, Brock Cooper, Jeff\\nLarson, Jon King, Holden Ackerman, Miriah Peterson, Felipe Hoffa,\\nDavid Gonzalez, Richard Wellman, Susan Walsh, Ravit Jain, Lauren\\nBalik, Mikiko Bazeley, Mark Freeman, Mike Wimmer, Alexey\\nShchedrin, Mary Clair Thompson, Julie Burroughs, Jason Pedley,\\nFreddy Drennan, Jake Carter, Jason Pedley, Kelly and Matt\\nPhillipps, Brian Campbell, Faris Chebib, Dylan Gregerson, Ken\\nMyers, and many others.\\nIf you’re not mentioned specifically, don’t take it personally. You know\\nwho you are. Let us know and we’ll get you on the next edition.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f64f2e71-f15c-4155-ae33-efe583bb970d', embedding=None, metadata={'page_label': '16', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We’d also like to thank the Ternary Data team, our students, and the\\ncountless people around the world who’ve supported us. It’s a great\\nreminder the world is a very small place.\\nWorking with the O’Reilly crew was amazing! Special thanks to Jess\\nHaberman for having confidence in us during the book proposal\\nprocess, our amazing and extremely patient development editors\\nNicole Taché and Michele Cronin for invaluable editing, feedback\\nand support. Thank you also to the superb production crew at\\nO’Reilly (Greg and crew).\\nJoe would like to thank his family—Cassie, Milo, and Ethan—for\\nletting him write a book. They had to endure a ton, and Joe promises\\nto never write another book again ;)\\nMatt would like to thank his friends and family for their enduring\\npatience and support. He’s still hopeful that Seneca will deign to give\\na five star review after a good deal of toil and missed family time\\naround the holidays.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5ecd964c-f913-480c-8af2-3b9dd0128c33', embedding=None, metadata={'page_label': '17', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Part I. Foundation and Building\\nBlocks', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='396d1d6d-cb7f-4010-bece-2a76ebe3fe87', embedding=None, metadata={'page_label': '18', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 1. Data Engineering\\nDescribed\\nIf you work in data or software, you may have noticed data\\nengineering emerging from the shadows and now sharing the stage\\nwith data science. Data engineering is one of the hottest fields in\\ndata and technology, and for a good reason. It builds the foundation\\nfor data science and analytics in production. This chapter explores\\nwhat data engineering is, how the field was born and its evolution,\\nthe skills of data engineers, and with whom they work.\\nWhat Is Data Engineering?\\nDespite the current popularity of data engineering, there’s a lot of\\nconfusion about what data engineering means and what data\\nengineers do. Data engineering has existed in some form since\\ncompanies started doing things with data—such as predictive\\nanalysis, descriptive analytics, and reports—and came into sharp\\nfocus alongside the rise of data science in the 2010s. For the\\npurpose of this book, it’s critical to define what data engineering and\\ndata engineer mean.\\nFirst, let’s look at the landscape of how data engineering is\\ndescribed and develop some terminology we can use throughout this\\nbook. Endless definitions of data engineering exist. In early 2022, a\\nGoogle exact-match search for “what is data engineering?” returns\\nover 91,000 unique results. Before we give our definition, here are a\\nfew examples of how some experts in the field define data\\nengineering:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b08440ff-e4f0-4969-be06-82422ea16c4e', embedding=None, metadata={'page_label': '19', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data engineering is a set of operations aimed at creating\\ninterfaces and mechanisms for the flow and access of information.\\nIt takes dedicated specialists—data engineers—to maintain data\\nso that it remains available and usable by others. In short, data\\nengineers set up and operate the organization’s data\\ninfrastructure, preparing it for further analysis by data analysts and\\nscientists.\\n—From “Data Engineering and Its Main Concepts” by\\nAlexSoft\\nThe first type of data engineering is SQL-focused. The work and\\nprimary storage of the data is in relational databases. All of the\\ndata processing is done with SQL or a SQL-based language.\\nSometimes, this data processing is done with an ETL tool. The\\nsecond type of data engineering is Big Data–focused. The work\\nand primary storage of the data is in Big Data technologies like\\nHadoop, Cassandra, and HBase. All of the data processing is\\ndone in Big Data frameworks like MapReduce, Spark, and Flink.\\nWhile SQL is used, the primary processing is done with\\nprogramming languages like Java, Scala, and Python.\\n—Jesse Anderson\\nIn relation to previously existing roles, the data engineering field\\ncould be thought of as a superset of business intelligence and\\ndata warehousing that brings more elements from software\\nengineering. This discipline also integrates specialization around\\nthe operation of so-called “big data” distributed systems, along\\nwith concepts around the extended Hadoop ecosystem, stream\\nprocessing, and in computation at scale.\\n—Maxime Beauchemin\\nData engineering is all about the movement, manipulation, and\\nmanagement of data.\\n—Lewis Gavin\\n1\\n2\\n3\\n4\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='181e4f7e-6c47-47ef-be8b-c9b0e874fccc', embedding=None, metadata={'page_label': '20', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Wow! It’s entirely understandable if you’ve been confused about data\\nengineering. That’s only a handful of definitions, and they contain an\\nenormous range of opinions about the meaning of data engineering.\\nData Engineering Defined\\nWhen we unpack the common threads of how various people define\\ndata engineering, an obvious pattern emerges: a data engineer gets\\ndata, stores it, and prepares it for consumption by data scientists,\\nanalysts, and others. We define data engineering and data engineer\\nas follows:\\nData engineering is the development, implementation, and\\nmaintenance of systems and processes that take in raw data and\\nproduce high-quality, consistent information that supports\\ndownstream use cases, such as analysis and machine learning.\\nData engineering is the intersection of security, data management,\\nDataOps, data architecture, orchestration, and software\\nengineering. A data engineer manages the data engineering\\nlifecycle, beginning with getting data from source systems and\\nending with serving data for use cases, such as analysis or\\nmachine learning.\\nThe Data Engineering Lifecycle\\nIt is all too easy to fixate on technology and miss the bigger picture\\nmyopically. This book centers around a big idea called the data\\nengineering lifecycle (Figure 1-1), which we believe gives data\\nengineers the holistic context to view their role.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f9d82cc9-944a-491a-ad02-5717b9bce4f6', embedding=None, metadata={'page_label': '21', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1-1. The data engineering lifecycle\\nThe data engineering lifecycle shifts the conversation away from\\ntechnology and toward the data itself and the end goals that it must\\nserve. The stages of the data engineering lifecycle are as follows:\\nGeneration\\nStorage\\nIngestion\\nTransformation\\nServing\\nThe data engineering lifecycle also has a notion of undercurrents—\\ncritical ideas across the entire lifecycle. These include security, data\\nmanagement, DataOps, data architecture, orchestration, and\\nsoftware engineering. We cover the data engineering lifecycle and its\\nundercurrents more extensively in Chapter 2. Still, we introduce it\\nhere because it is essential to our definition of data engineering and\\nthe discussion that follows in this chapter.\\nNow that you have a working definition of data engineering and an\\nintroduction to its lifecycle, let’s take a step back and look at a bit of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42cb5617-53fa-4487-a753-370177d439ea', embedding=None, metadata={'page_label': '22', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='history.\\nEvolution of the Data Engineer\\nHistory doesn’t repeat itself, but it rhymes.\\n—A famous adage often attributed to Mark Twain\\nUnderstanding data engineering today and tomorrow requires a\\ncontext of how the field evolved. This section is not a history lesson,\\nbut looking at the past is invaluable in understanding where we are\\ntoday and where things are going. A common theme constantly\\nreappears: what’s old is new again.\\nThe early days: 1980 to 2000, from data warehousing to the web\\nThe birth of the data engineer arguably has its roots in data\\nwarehousing, dating as far back as the 1970s, with the business\\ndata warehouse taking shape in the 1980s and Bill Inmon officially\\ncoining the term data warehouse in 1990. After engineers at IBM\\ndeveloped the relational database and Structured Query Language\\n(SQL), Oracle popularized the technology. As nascent data systems\\ngrew, businesses needed dedicated tools and data pipelines for\\nreporting and business intelligence (BI). To help people correctly\\nmodel their business logic in the data warehouse, Ralph Kimball and\\nInmon developed their respective eponymous data-modeling\\ntechniques and approaches, which are still widely used today.\\nData warehousing ushered in the first age of scalable analytics, with\\nnew massively parallel processing (MPP) databases that use\\nmultiple processors to crunch large amounts of data coming on the\\nmarket and supporting unprecedented volumes of data. Roles such\\nas BI engineer, ETL developer, and data warehouse engineer\\naddressed the various needs of the data warehouse. Data\\nwarehouse and BI engineering were a precursor to today’s data\\nengineering and still play a central role in the discipline.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9ada3f18-4b04-4f8f-b3c6-508e2e32437d', embedding=None, metadata={'page_label': '23', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The internet went mainstream around the mid-1990s, creating a\\nwhole new generation of web-first companies such as AOL, Yahoo,\\nand Amazon. The dot-com boom spawned a ton of activity in web\\napplications and the backend systems to support them—servers,\\ndatabases, and storage. Much of the infrastructure was expensive,\\nmonolithic, and heavily licensed. The vendors selling these backend\\nsystems likely didn’t foresee the sheer scale of the data that web\\napplications would produce.\\nThe early 2000s: The birth of contemporary data engineering\\nFast-forward to the early 2000s, when the dot-com boom of the late\\n’90s went bust, leaving behind a tiny cluster of survivors. Some of\\nthese companies, such as Yahoo, Google, and Amazon, would grow\\ninto powerhouse tech companies. Initially, these companies\\ncontinued to rely on the traditional monolithic, relational databases\\nand data warehouses of the 1990s, pushing these systems to the\\nlimit. As these systems buckled, updated approaches were needed\\nto handle data growth. The new generation of the systems must be\\ncost-effective, scalable, available, and reliable.\\nCoinciding with the explosion of data, commodity hardware—such as\\nservers, RAM, disks, and flash drives—also became cheap and\\nubiquitous. Several innovations allowed distributed computation and\\nstorage on massive computing clusters at a vast scale. These\\ninnovations started decentralizing and breaking apart traditionally\\nmonolithic services. The “big data” era had begun.\\nThe Oxford English Dictionary defines big data as “extremely large\\ndata sets that may be analyzed computationally to reveal patterns,\\ntrends, and associations, especially relating to human behavior and\\ninteractions.” Another famous and succinct description of big data is\\nthe three V’s of data: velocity, variety, and volume.\\nIn 2003, Google published a paper on the Google File System, and\\nshortly after that, in 2004, a paper on MapReduce, an ultra-scalable\\ndata-processing paradigm. In truth, big data has earlier antecedents', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7ec9ef6c-5218-4e84-ac17-f1490c18f7a4', embedding=None, metadata={'page_label': '24', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='in MPP data warehouses and data management for experimental\\nphysics projects, but Google’s publications constituted a “big bang”\\nfor data technologies and the cultural roots of data engineering as\\nwe know it today. You’ll learn more about MPP systems and\\nMapReduce in Chapters 3 and 8, respectively.\\nThe Google papers inspired engineers at Yahoo to develop and later\\nopen source Apache Hadoop in 2006. It’s hard to overstate the\\nimpact of Hadoop. Software engineers interested in large-scale data\\nproblems were drawn to the possibilities of this new open source\\ntechnology ecosystem. As companies of all sizes and types saw\\ntheir data grow into many terabytes and even petabytes, the era of\\nthe big data engineer was born.\\nAround the same time, Amazon had to keep up with its own\\nexploding data needs and created elastic computing environments\\n(Amazon Elastic Compute Cloud, or EC2), infinitely scalable storage\\nsystems (Amazon Simple Storage Service, or S3), highly scalable\\nNoSQL databases (Amazon DynamoDB), and many other core data\\nbuilding blocks. Amazon elected to offer these services for internal\\nand external consumption through Amazon Web Services (AWS),\\nbecoming the first popular public cloud. AWS created an ultra-flexible\\npay-as-you-go resource marketplace by virtualizing and reselling\\nvast pools of commodity hardware. Instead of purchasing hardware\\nfor a data center, developers could simply rent compute and storage\\nfrom AWS.\\nAs AWS became a highly profitable growth engine for Amazon, other\\npublic clouds would soon follow, such as Google Cloud, Microsoft\\nAzure, and DigitalOcean. The public cloud is arguably one of the\\nmost significant innovations of the 21st century and spawned a\\nrevolution in the way software and data applications are developed\\nand deployed.\\nThe early big data tools and public cloud laid the foundation for\\ntoday’s data ecosystem. The modern data landscape—and data\\n6\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='82df9734-b94c-4fbf-b4b4-4d10f67b5a87', embedding=None, metadata={'page_label': '25', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='engineering as we know it now—would not exist without these\\ninnovations.\\nThe 2000s and 2010s: Big data engineering\\nOpen source big data tools in the Hadoop ecosystem rapidly\\nmatured and spread from Silicon Valley to tech-savvy companies\\nworldwide. For the first time, any business had access to the same\\nbleeding-edge data tools used by the top tech companies. Another\\nrevolution occurred with the transition from batch computing to event\\nstreaming, ushering in a new era of big “real-time” data. You’ll learn\\nabout batch and event streaming throughout this book.\\nEngineers could choose the latest and greatest—Hadoop, Apache\\nPig, Apache Hive, Dremel, Apache HBase, Apache Storm, Apache\\nCassandra, Apache Spark, Presto, and numerous other new\\ntechnologies that came on the scene. Traditional enterprise-oriented\\nand GUI-based data tools suddenly felt outmoded, and code-first\\nengineering was in vogue with the ascendance of MapReduce. We\\n(the authors) were around during this time, and it felt like old dogmas\\ndied a sudden death upon the altar of big data.\\nThe explosion of data tools in the late 2000s and 2010s ushered in\\nthe big data engineer. To effectively use these tools and techniques\\n—namely, the Hadoop ecosystem including Hadoop, YARN, Hadoop\\nDistributed File System (HDFS), and MapReduce—big data\\nengineers had to be proficient in software development and low-level\\ninfrastructure hacking, but with a shifted emphasis. Big data\\nengineers typically maintained massive clusters of commodity\\nhardware to deliver data at scale. While they might occasionally\\nsubmit pull requests to Hadoop core code, they shifted their focus\\nfrom core technology development to data delivery.\\nBig data quickly became a victim of its own success. As a buzzword,\\nbig data gained popularity during the early 2000s through the mid-\\n2010s. Big data captured the imagination of companies trying to\\nmake sense of the ever-growing volumes of data and the endless', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c2fd210f-02cc-4989-807f-def1e768138f', embedding=None, metadata={'page_label': '26', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='barrage of shameless marketing from companies selling big data\\ntools and services. Because of the immense hype, it was common to\\nsee companies using big data tools for small data problems,\\nsometimes standing up a Hadoop cluster to process just a few\\ngigabytes. It seemed like everyone wanted in on the big data action.\\nDan Ariely tweeted, “Big data is like teenage sex: everyone talks\\nabout it, nobody really knows how to do it, everyone thinks everyone\\nelse is doing it, so everyone claims they are doing it.”\\nFigure 1-2 shows a snapshot of Google Trends for the search term\\n“big data” to get an idea of the rise and fall of big data.\\nFigure 1-2. Google Trends for “big data” (March 2022)\\nDespite the term’s popularity, big data has lost steam. What\\nhappened? One word: simplification. Despite the power and\\nsophistication of open source big data tools, managing them was a\\nlot of work and required constant attention. Often, companies\\nemployed entire teams of big data engineers, costing millions of\\ndollars a year, to babysit these platforms. Big data engineers often\\nspent excessive time maintaining complicated tooling and arguably\\nnot as much time delivering the business’s insights and value.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='74e86ea7-2d2d-4aa2-b7b4-69d48bc46d4c', embedding=None, metadata={'page_label': '27', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Open source developers, clouds, and third parties started looking for\\nways to abstract, simplify, and make big data available without the\\nhigh administrative overhead and cost of managing their clusters,\\nand installing, configuring, and upgrading their open source code.\\nThe term big data is essentially a relic to describe a particular time\\nand approach to handling large amounts of data.\\nToday, data is moving faster than ever and growing ever larger, but\\nbig data processing has become so accessible that it no longer\\nmerits a separate term; every company aims to solve its data\\nproblems, regardless of actual data size. Big data engineers are now\\nsimply data engineers.\\nThe 2020s: Engineering for the data lifecycle\\nAt the time of this writing, the data engineering role is evolving\\nrapidly. We expect this evolution to continue at a rapid clip for the\\nforeseeable future. Whereas data engineers historically tended to\\nthe low-level details of monolithic frameworks such as Hadoop,\\nSpark, or Informatica, the trend is moving toward decentralized,\\nmodularized, managed, and highly abstracted tools.\\nIndeed, data tools have proliferated at an astonishing rate (see\\nFigure 1-3). Popular trends in the early 2020s include the modern\\ndata stack, representing a collection of off-the-shelf open source and\\nthird-party products assembled to make analysts’ lives easier. At the\\nsame time, data sources and data formats are growing both in\\nvariety and size. Data engineering is increasingly a discipline of\\ninteroperation, and connecting various technologies like LEGO\\nbricks, to serve ultimate business goals.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='35d31dd4-0b24-42af-b1fd-3180f7f369c5', embedding=None, metadata={'page_label': '28', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1-3. Matt Turck’s Data Landscape in 2012 versus 2021\\nThe data engineer we discuss in this book can be described more\\nprecisely as a data lifecycle engineer. With greater abstraction and\\nsimplification, a data lifecycle engineer is no longer encumbered by\\nthe gory details of yesterday’s big data frameworks. While data\\nengineers maintain skills in low-level data programming and use\\nthese as required, they increasingly find their role focused on things\\nhigher in the value chain: security, data management, DataOps, data\\narchitecture, orchestration, and general data lifecycle management.\\nAs tools and workflows simplify, we’ve seen a noticeable shift in the\\nattitudes of data engineers. Instead of focusing on who has the\\n“biggest data,” open source projects and services are increasingly\\nconcerned with managing and governing data, making it easier to\\nuse and discover, and improving its quality. Data engineers are now\\nconversant in acronyms such as CCPA and GDPR;  as they\\nengineer pipelines, they concern themselves with privacy,\\nanonymization, data garbage collection, and compliance with\\nregulations.\\nWhat’s old is new again. While “enterprisey” stuff like data\\nmanagement (including data quality and governance) was common\\nfor large enterprises in the pre-big-data era, it wasn’t widely adopted\\nin smaller companies. Now that many of the challenging problems of\\nyesterday’s data systems are solved, neatly productized, and\\npackaged, technologists and entrepreneurs have shifted focus back\\n8\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9b21f73f-e5e3-4a94-a50c-b3165df1a8ea', embedding=None, metadata={'page_label': '29', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='to the “enterprisey” stuff, but with an emphasis on decentralization\\nand agility, that contrasts with the traditional enterprise command-\\nand-control approach.\\nWe view the present as a golden age of data lifecycle management.\\nData engineers managing the data engineering lifecycle have better\\ntools and techniques than ever before. We discuss the data\\nengineering lifecycle and its undercurrents in greater detail in the\\nnext chapter.\\nData Engineering and Data Science\\nWhere does data engineering fit in with data science? There’s some\\ndebate, with some arguing data engineering is a subdiscipline of\\ndata science. We believe data engineering is separate from data\\nscience and analytics. They complement each other, but they are\\ndistinctly different. Data engineering sits upstream from data science\\n(Figure 1-4), meaning data engineers provide the inputs used by\\ndata scientists (downstream from data engineering), who convert\\nthese inputs into something useful.\\nFigure 1-4. Data engineering sits upstream from data science\\nConsider the Data Science Hierarchy of Needs (Figure 1-5). In 2017,\\nMonica Rogati published this hierarchy in an article that showed\\nwhere AI and machine learning (ML) sat in proximity to more\\n“mundane” areas such as data movement/storage, collection, and\\ninfrastructure.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0c3ee28f-353c-40f6-ad89-5b1a6ba8b880', embedding=None, metadata={'page_label': '30', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1-5. The Data Science Hierarchy of Needs\\nAlthough many data scientists are eager to build and tune ML\\nmodels, the reality is an estimated 70% to 80% of their time is spent\\ntoiling in the bottom three parts of the hierarchy—gathering data,\\ncleaning data, processing data—and only a tiny slice of their time on\\nanalysis and ML. Rogati argues that companies need to build a solid\\ndata foundation (the bottom three levels of the hierarchy) before\\ntackling areas such as AI and ML.\\nData scientists aren’t typically trained to engineer production-grade\\ndata systems, and they end up doing this work haphazardly because\\nthey lack the support and resources of a data engineer. In an ideal\\nworld, data scientists should spend more than 90% of their time\\nfocused on the top layers of the pyramid: analytics, experimentation,\\nand ML. When data engineers focus on these bottom parts of the\\nhierarchy, they build a solid foundation for data scientists to succeed.\\nWith data science driving advanced analytics and ML, data\\nengineering straddles the divide between getting data and getting\\nvalue from data (see Figure 1-6). We believe data engineering is of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ebc9632-05e3-4cd2-860c-c8eb2cc847bd', embedding=None, metadata={'page_label': '31', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='equal importance and visibility to data science, with data engineers\\nplaying a vital role in making data science successful in production.\\nFigure 1-6. A data engineer gets data and provides value from the data\\nData Engineering Skills and Activities\\nThe skill set of a data engineer encompasses the “undercurrents” of\\ndata engineering: security, data management, DataOps, data\\narchitecture, and software engineering. This skill set requires an\\nunderstanding of how to evaluate data tools and how they fit\\ntogether across the data engineering lifecycle. It’s also critical to\\nknow how data is produced in source systems and how analysts and\\ndata scientists will consume and create value after processing and\\ncurating data. Finally, a data engineer juggles a lot of complex\\nmoving parts and must constantly optimize along the axes of cost,\\nagility, scalability, simplicity, reuse, and interoperability (Figure 1-7).\\nWe cover these topics in more detail in upcoming chapters.\\nFigure 1-7. The balancing act of data engineering\\nAs we discussed, in the recent past, a data engineer was expected\\nto know and understand how to use a small handful of powerful and\\nmonolithic technologies (Hadoop, Spark, Teradata, Hive, and many\\nothers) to create a data solution. Utilizing these technologies often\\nrequires a sophisticated understanding of software engineering,\\nnetworking, distributed computing, storage, or other low-level details.\\nTheir work would be devoted to cluster administration and\\nmaintenance, managing overhead, and writing pipeline and\\ntransformation jobs, among other tasks.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97040ebb-61bd-49bf-a6e9-6b69e89ef9a6', embedding=None, metadata={'page_label': '32', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Nowadays, the data-tooling landscape is dramatically less\\ncomplicated to manage and deploy. Modern data tools considerably\\nabstract and simplify workflows. As a result, data engineers are now\\nfocused on balancing the simplest and most cost-effective, best-of-\\nbreed services that deliver value to the business. The data engineer\\nis also expected to create agile data architectures that evolve as new\\ntrends emerge.\\nWhat are some things a data engineer does not do? A data engineer\\ntypically does not directly build ML models, create reports or\\ndashboards, perform data analysis, build key performance indicators\\n(KPIs), or develop software applications. A data engineer should\\nhave a good functioning understanding of these areas to serve\\nstakeholders best.\\nData Maturity and the Data Engineer\\nThe level of data engineering complexity within a company depends\\na great deal on the company’s data maturity. This significantly\\nimpacts a data engineer’s day-to-day job responsibilities and career\\nprogression. What is data maturity, exactly?\\nData maturity is the progression toward higher data utilization,\\ncapabilities, and integration across the organization, but data\\nmaturity does not simply depend on the age or revenue of a\\ncompany. An early-stage startup can have greater data maturity than\\na 100-year-old company with annual revenues in the billions. What\\nmatters is the way data is leveraged as a competitive advantage.\\nData maturity models have many versions, such as Data\\nManagement Maturity (DMM) and others, and it’s hard to pick one\\nthat is both simple and useful for data engineering. So, we’ll create\\nour own simplified data maturity model. Our data maturity model\\n(Figure 1-8) has three stages: starting with data, scaling with data,\\nand leading with data. Let’s look at each of these stages and at what\\na data engineer typically does at each stage.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90852810-8745-4246-8b76-3d7e46ef1331', embedding=None, metadata={'page_label': '33', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1-8. Our simplified data maturity model for a company\\nStage 1: Starting with data\\nA company getting started with data is, by definition, in the very early\\nstages of its data maturity. The company may have fuzzy, loosely\\ndefined goals or no goals. Data architecture and infrastructure are in\\nthe very early stages of planning and development. Adoption and\\nutilization are likely low or nonexistent. The data team is small, often\\nwith a headcount in the single digits. At this stage, a data engineer is\\nusually a generalist and will typically play several other roles, such\\nas data scientist or software engineer. A data engineer’s goal is to\\nmove fast, get traction, and add value.\\nThe practicalities of getting value from data are typically poorly\\nunderstood, but the desire exists. Reports or analyses lack formal\\nstructure, and most requests for data are ad hoc. While it’s tempting\\nto jump headfirst into ML at this stage, we don’t recommend it.\\nWe’ve seen countless data teams get stuck and fall short when they\\ntry to jump to ML without building a solid data foundation.\\nThat’s not to say you can’t get wins from ML at this stage—it is rare\\nbut possible. Without a solid data foundation, you likely won’t have\\nthe data to train reliable ML models nor the means to deploy these\\nmodels to production in a scalable and repeatable way. We half-\\njokingly call ourselves “recovering data scientists”, mainly from\\npersonal experience with being involved in premature data science\\nprojects without adequate data maturity or data engineering support.\\nA data engineer should focus on the following in organizations\\ngetting started with data:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='11686bbf-6f21-4949-9fb3-1f27585167d5', embedding=None, metadata={'page_label': '34', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Get buy-in from key stakeholders, including executive\\nmanagement. Ideally, the data engineer should have a sponsor\\nfor critical initiatives to design and build a data architecture to\\nsupport the company’s goals.\\nDefine the right data architecture (usually solo, since a data\\narchitect likely isn’t available). This means determining business\\ngoals and the competitive advantage you’re aiming to achieve\\nwith your data initiative. Work toward a data architecture that\\nsupports these goals. See Chapter 3 for our advice on “good”\\ndata architecture.\\nIdentify and audit data that will support key initiatives and\\noperate within the data architecture you designed.\\nBuild a solid data foundation for future data analysts and data\\nscientists to generate reports and models that provide\\ncompetitive value. In the meantime, you may also have to\\ngenerate these reports and models until this team is hired.\\nThis is a delicate stage with lots of pitfalls. Here are some tips for\\nthis stage:\\nOrganizational willpower may wane if a lot of visible successes\\ndon’t occur with data. Getting quick wins will establish the\\nimportance of data within the organization. Just keep in mind\\nthat quick wins will likely create technical debt. Have a plan to\\nreduce this debt, as it will otherwise add friction for future\\ndelivery.\\nGet out and talk to people, and avoid working in silos. We often\\nsee the data team working in a bubble, not communicating with\\npeople outside their departments and getting perspectives and\\nfeedback from business stakeholders. The danger is you’ll\\nspend a lot of time working on things of little use to people.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2de3e181-6dd8-4c1c-8b3c-bb62577fcbaf', embedding=None, metadata={'page_label': '35', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Avoid undifferentiated heavy lifting. Don’t box yourself in with\\nunnecessary technical complexity. Use off-the-shelf, turnkey\\nsolutions wherever possible.\\nBuild custom solutions and code only where this creates a\\ncompetitive advantage.\\nStage 2: Scaling with data\\nAt this point, a company has moved away from ad hoc data requests\\nand has formal data practices. Now the challenge is creating\\nscalable data architectures and planning for a future where the\\ncompany is genuinely data-driven. Data engineering roles move from\\ngeneralists to specialists, with people focusing on particular aspects\\nof the data engineering lifecycle.\\nIn organizations that are in stage 2 of data maturity, a data\\nengineer’s goals are to do the following:\\nEstablish formal data practices\\nCreate scalable and robust data architectures\\nAdopt DevOps and DataOps practices\\nBuild systems that support ML\\nContinue to avoid undifferentiated heavy lifting and customize\\nonly when a competitive advantage results\\nWe return to each of these goals later in the book.\\nIssues to watch out for include the following:\\nAs we grow more sophisticated with data, there’s a temptation\\nto adopt bleeding-edge technologies based on social proof from\\nSilicon Valley companies. This is rarely a good use of your time\\nand energy. Any technology decisions should be driven by the\\nvalue they’ll deliver to your customers.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='65cf384b-591c-4272-a517-79c882cd36ff', embedding=None, metadata={'page_label': '36', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The main bottleneck for scaling is not cluster nodes, storage, or\\ntechnology but the data engineering team. Focus on solutions\\nthat are simple to deploy and manage to expand your team’s\\nthroughput.\\nYou’ll be tempted to frame yourself as a technologist, a data\\ngenius who can deliver magical products. Shift your focus\\ninstead to pragmatic leadership and begin transitioning to the\\nnext maturity stage; communicate with other teams about the\\npractical utility of data. Teach the organization how to consume\\nand leverage data.\\nStage 3: Leading with data\\nAt this stage, the company is data-driven. The automated pipelines\\nand systems created by data engineers allow people within the\\ncompany to do self-service analytics and ML. Introducing new data\\nsources is seamless, and tangible value is derived. Data engineers\\nimplement proper controls and practices to ensure that data is\\nalways available to the people and systems. Data engineering roles\\ncontinue to specialize more deeply than in stage 2.\\nIn organizations in stage 3 of data maturity, a data engineer will\\ncontinue building on prior stages, plus they will do the following:\\nCreate automation for the seamless introduction and usage of\\nnew data\\nFocus on building custom tools and systems that leverage data\\nas a competitive advantage\\nFocus on the “enterprisey” aspects of data, such as data\\nmanagement (including data governance and quality) and\\nDataOps\\nDeploy tools that expose and disseminate data throughout the\\norganization, including data catalogs, data lineage tools, and\\nmetadata management systems', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a2b4e998-a700-4252-9396-28756ba7a825', embedding=None, metadata={'page_label': '37', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Collaborate efficiently with software engineers, ML engineers,\\nanalysts, and others\\nCreate a community and environment where people can\\ncollaborate and speak openly, no matter their role or position\\nIssues to watch out for include the following:\\nAt this stage, complacency is a significant danger. Once\\norganizations reach stage 3, they must constantly focus on\\nmaintenance and improvement or risk falling back to a lower\\nstage.\\nTechnology distractions are a more significant danger here than\\nin the other stages. There’s a temptation to pursue expensive\\nhobby projects that don’t deliver value to the business. Utilize\\ncustom-built technology only where it provides a competitive\\nadvantage.\\nThe Background and Skills of a Data Engineer\\nData engineering is a fast-growing field, and a lot of questions\\nremain about how to become a data engineer. Because data\\nengineering is a relatively new discipline, little formal training is\\navailable to enter the field. Universities don’t have a standard data\\nengineering path. Although a handful of data engineering boot\\ncamps and online tutorials cover random topics, a common\\ncurriculum for the subject doesn’t yet exist.\\nPeople entering data engineering arrive with varying backgrounds in\\neducation, career, and skill set. Everyone entering the field should\\nexpect to invest a significant amount of time in self-study. Reading\\nthis book is a good starting point; one of the primary goals of this\\nbook is to give you a foundation for the knowledge and skills we\\nthink are necessary to succeed as a data engineer.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7fed8cd3-7da1-4a18-b06a-0e08630e470a', embedding=None, metadata={'page_label': '38', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='If you’re pivoting your career into data engineering, we’ve found that\\nthe transition is easiest when moving from an adjacent field, such as\\nsoftware engineering, ETL development, database administration,\\ndata science, or data analysis. These disciplines tend to be “data\\naware” and provide good context for data roles in an organization.\\nThey also equip folks with the relevant technical skills and context to\\nsolve data engineering problems.\\nDespite the lack of a formalized path, a requisite body of knowledge\\nexists that we believe a data engineer should know to be successful.\\nBy definition, a data engineer must understand both data and\\ntechnology. With respect to data, this entails knowing about various\\nbest practices around data management. On the technology end, a\\ndata engineer must be aware of various options for tools, their\\ninterplay, and their trade-offs. This requires a good understanding of\\nsoftware engineering, DataOps, and data architecture.\\nZooming out, a data engineer must also understand the\\nrequirements of data consumers (data analysts and data scientists)\\nand the broader implications of data across the organization. Data\\nengineering is a holistic practice; the best data engineers view their\\nresponsibilities through business and technical lenses.\\nBusiness Responsibilities\\nThe macro responsibilities we list in this section aren’t exclusive to\\ndata engineers, but are crucial for anyone working in a data or\\ntechnology field. Because a simple Google search will yield tons of\\nresources to learn about these areas, we will simply list them for\\nbrevity:\\nKnow how to communicate with nontechnical and technical people.\\nCommunication is key, and you need to be able to establish\\nrapport and trust with people across the organization. We\\nsuggest paying close attention to organizational hierarchies, who', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a4768b2-0d08-49db-ae39-f0f5637da96d', embedding=None, metadata={'page_label': '39', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='reports to whom, how people interact, and which silos exist.\\nThese observations will be invaluable to your success.\\nUnderstand how to scope and gather business and product\\nrequirements.\\nYou need to know what to build and ensure that your\\nstakeholders agree with your assessment. In addition, develop a\\nsense of how data and technology decisions impact the business.\\nUnderstand the cultural foundations of Agile, DevOps, and DataOps.\\nMany technologists mistakenly believe these practices are solved\\nthrough technology. We feel this is dangerously wrong. Agile,\\nDevOps, and DataOps are fundamentally cultural, requiring buy-\\nin across the organization.\\nControl costs.\\nYou’ll be successful when you can keep costs low while providing\\noutsized value. Know how to optimize for time to value, the total\\ncost of ownership, and opportunity cost. Learn to monitor costs to\\navoid surprises.\\nLearn continuously.\\nThe data field feels like it’s changing at light speed. People who\\nsucceed in it are great at picking up new things while sharpening\\ntheir fundamental knowledge. They’re also good at filtering,\\ndetermining which new developments are most relevant to their\\nwork, which are still immature, and which are just fads. Stay\\nabreast of the field and learn how to learn.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6fa8944f-6a9b-4a6b-a628-a7e7d68b9ca7', embedding=None, metadata={'page_label': '40', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A successful data engineer always zooms out to understand the big\\npicture and how to achieve outsized value for the business.\\nCommunication is vital, both for technical and nontechnical people.\\nWe often see data teams succeed based on their communication\\nwith other stakeholders; success or failure is rarely a technology\\nissue. Knowing how to navigate an organization, scope and gather\\nrequirements, control costs, and continuously learn will set you apart\\nfrom the data engineers who rely solely on their technical abilities to\\ncarry their career.\\nTechnical Responsibilities\\nYou must understand how to build architectures that optimize\\nperformance and cost at a high level, using prepackaged or\\nhomegrown components. Ultimately, architectures and constituent\\ntechnologies are building blocks to serve the data engineering\\nlifecycle. Recall the stages of the data engineering lifecycle:\\nGeneration\\nStorage\\nIngestion\\nTransformation\\nServing\\nThe undercurrents of the data engineering lifecycle are the following:\\nSecurity\\nData management\\nDataOps\\nData architecture\\nSoftware engineering', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='adcc8e0b-d88a-4e54-8650-494490b21de9', embedding=None, metadata={'page_label': '41', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Zooming in a bit, we discuss some of the tactical data and\\ntechnology skills you’ll need as a data engineer in this section; we\\ndiscuss these in more detail in subsequent chapters.\\nPeople often ask, should a data engineer know how to code? Short\\nanswer: yes. A data engineer should have production-grade software\\nengineering chops. We note that the nature of software development\\nprojects undertaken by data engineers has changed fundamentally\\nin the last few years. Fully managed services now replace a great\\ndeal of low-level programming effort previously expected of\\nengineers, who now use managed open source, and simple plug-\\nand-play software-as-a-service (SaaS) offerings. For example, data\\nengineers now focus on high-level abstractions or writing pipelines\\nas code within an orchestration framework.\\nEven in a more abstract world, software engineering best practices\\nprovide a competitive advantage, and data engineers who can dive\\ninto the deep architectural details of a codebase give their\\ncompanies an edge when specific technical needs arise. In short, a\\ndata engineer who can’t write production-grade code will be severely\\nhindered, and we don’t see this changing anytime soon. Data\\nengineers remain software engineers, in addition to their many other\\nroles.\\nWhat languages should a data engineer know? We divide data\\nengineering programming languages into primary and secondary\\ncategories. At the time of this writing, the primary languages of data\\nengineering are SQL, Python, a Java Virtual Machine (JVM)\\nlanguage (usually Java or Scala), and bash:\\nSQL\\nThe most common interface for databases and data lakes. After\\nbriefly being sidelined by the need to write custom MapReduce\\ncode for big data processing, SQL (in various forms) has\\nreemerged as the lingua franca of data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='446ef3f8-5bfd-43a7-bebb-34c9e89e6cc5', embedding=None, metadata={'page_label': '42', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Python\\nThe bridge language between data engineering and data\\nscience. A growing number of data engineering tools are written\\nin Python or have Python APIs. It’s known as “the second-best\\nlanguage at everything.” Python underlies popular data tools\\nsuch as pandas, NumPy, Airflow, sci-kit learn, TensorFlow,\\nPyTorch, and PySpark. Python is the glue between underlying\\ncomponents and is frequently a first-class API language for\\ninterfacing with a framework.\\nJVM languages such as Java and Scala\\nPrevalent for Apache open source projects such as Spark, Hive,\\nand Druid. The JVM is generally more performant than Python\\nand may provide access to lower-level features than a Python\\nAPI (for example, this is the case for Apache Spark and Beam).\\nUnderstanding Java or Scala will be beneficial if you’re using a\\npopular open source data framework.\\nbash\\nThe command-line interface for Linux operating systems.\\nKnowing bash commands and being comfortable using CLIs will\\nsignificantly improve your productivity and workflow when you\\nneed to script or perform OS operations. Even today, data\\nengineers frequently use command-line tools like awk or sed to\\nprocess files in a data pipeline or call bash commands from', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fdd95ac7-871e-40e5-b0c2-f65d4cd72841', embedding=None, metadata={'page_label': '43', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='orchestration frameworks. If you’re using Windows, feel free to\\nsubstitute PowerShell for bash.\\nTHE UNREASONABLE EFFECTIVENESS OF SQL\\nThe advent of MapReduce and the big data era relegated SQL to\\npassé status. Since then, various developments have\\ndramatically enhanced the utility of SQL in the data engineering\\nlifecycle. Spark SQL, Google BigQuery, Snowflake, Hive, and\\nmany other data tools can process massive amounts of data by\\nusing declarative, set-theoretic SQL semantics. SQL is also\\nsupported by many streaming frameworks, such as Apache\\nFlink, Beam, and Kafka. We believe that competent data\\nengineers should be highly proficient in SQL.\\nAre we saying that SQL is a be-all and end-all language? Not at\\nall. SQL is a powerful tool that can quickly solve complex\\nanalytics and data transformation problems. Given that time is a\\nprimary constraint for data engineering team throughput,\\nengineers should embrace tools that combine simplicity and high\\nproductivity. Data engineers also do well to develop expertise in\\ncomposing SQL with other operations, either within frameworks\\nsuch as Spark and Flink or by using orchestration to combine\\nmultiple tools. Data engineers should also learn modern SQL\\nsemantics for dealing with JavaScript Object Notation (JSON)\\nparsing and nested data and consider leveraging a SQL\\nmanagement framework such as dbt (Data Build Tool).\\nA proficient data engineer also recognizes when SQL is not the\\nright tool for the job and can choose and code in a suitable\\nalternative. A SQL expert could likely write a query to stem and\\ntokenize raw text in a natural language processing (NLP) pipeline\\nbut would also recognize that coding in native Spark is a far\\nsuperior alternative to this masochistic exercise.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e4e005f0-714f-4e7a-9cdd-d74e0711df5d', embedding=None, metadata={'page_label': '44', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data engineers may also need to develop proficiency in secondary\\nprogramming languages, including R, JavaScript, Go, Rust, C/C++,\\nC#, and Julia. Developing in these languages is often necessary\\nwhen popular across the company or used with domain-specific data\\ntools. For instance, JavaScript has proven popular as a language for\\nuser-defined functions in cloud data warehouses. At the same time,\\nC# and Pow er Shell are essential in companies that leverage Azure\\nand the Microsoft ecosystem.\\nKEEPING PACE IN A FAST-MOVING FIELD\\nOnce a new technology rolls over you, if you’re not part of the\\nsteamroller, you’re part of the road.\\n—Stewart Brand\\nHow do you keep your skills sharp in a rapidly changing field like\\ndata engineering? Should you focus on the latest tools or deep\\ndive into fundamentals? Here’s our advice: focus on the\\nfundamentals to understand what’s not going to change; pay\\nattention to ongoing developments to know where the field is\\ngoing. New paradigms and practices are introduced all the time,\\nand it’s incumbent on you to stay current. Strive to understand\\nhow new technologies will be helpful in the lifecycle.\\nThe Continuum of Data Engineering Roles, from\\nA to B\\nAlthough job descriptions paint a data engineer as a “unicorn” who\\nmust possess every data skill imaginable, data engineers don’t all do\\nthe same type of work or have the same skill set. Data maturity is a\\nhelpful guide to understanding the types of data challenges a\\ncompany will face as it grows its data capability. It’s beneficial to look\\nat some critical distinctions in the kinds of work data engineers do.\\nThough these distinctions are simplistic, they clarify what data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a7116f0d-a95c-4a27-8ef6-56ffb7694a58', embedding=None, metadata={'page_label': '45', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='scientists and data engineers do and avoid lumping either role into\\nthe unicorn bucket.\\nIn data science, there’s the notion of type A and type B data\\nscientists. Type A data scientists—where A stands for analysis—\\nfocus on understanding and deriving insight from data. Type B data\\nscientists—where B stands for building—share similar backgrounds\\nas type A data scientists and possess strong programming skills.\\nThe type B data scientist builds systems that make data science\\nwork in production. Borrowing from this data scientist continuum,\\nwe’ll create a similar distinction for two types of data engineers:\\nType A data engineers\\nA stands for abstraction. In this case, the data engineer avoids\\nundifferentiated heavy lifting, keeping data architecture as\\nabstract and straightforward as possible and not reinventing the\\nwheel. Type A data engineers manage the data engineering\\nlifecycle mainly by using entirely off-the-shelf products, managed\\nservices, and tools. Type A data engineers work at companies\\nacross industries and at all levels of data maturity.\\nType B data engineers\\nB stands for build. Type B data engineers build data tools and\\nsystems that scale and leverage a company’s core competency\\nand competitive advantage. In the data maturity range, a type B\\ndata engineer is more commonly found at companies in stage 2\\nand 3 (scaling and leading with data), or when an initial data use\\ncase is so unique and mission-critical that custom data tools are\\nrequired to get started.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a94e9aef-b0ea-4a9c-9447-7036d8999519', embedding=None, metadata={'page_label': '46', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Type A and type B data engineers may work in the same company\\nand may even be the same person! More commonly, a type A data\\nengineer is first hired to set the foundation, with type B data engineer\\nskill sets either learned or hired as the need arises within a company.\\nData Engineers Inside an Organization\\nData engineers don’t work in a vacuum. Depending on what they’re\\nworking on, they will interact with technical and nontechnical people\\nand face different directions (internal and external). Let’s explore\\nwhat data engineers do inside an organization and with whom they\\ninteract.\\nInternal-Facing Versus External-Facing Data\\nEngineers\\nA data engineer serves several end users and faces many internal\\nand external directions (Figure 1-9). Since not all data engineering\\nworkloads and responsibilities are the same, it’s essential to\\nunderstand whom the data engineer serves. Depending on the end-\\nuse cases, a data engineer’s primary responsibilities are external\\nfacing, internal facing, or a blend of the two.\\nFigure 1-9. The directions a data engineer faces\\nAn external-facing data engineer typically aligns with the users of\\nexternal-facing applications, such as social media apps, Internet of\\nThings (IoT) devices, and ecommerce platforms. This data engineer\\narchitects, builds, and manages the systems that collect, store, and\\nprocess transactional and event data from these applications. The\\nsystems built by these data engineers have a feedback loop from the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0eccc603-18fc-41b9-9c6d-8d88b4638455', embedding=None, metadata={'page_label': '47', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='application to the data pipeline, and then back to the application\\n(Figure 1-10).\\nFigure 1-10. External-facing data engineer systems\\nExternal-facing data engineering comes with a unique set of\\nproblems. External-facing query engines often handle much larger\\nconcurrency loads than internal-facing systems. Engineers also need\\nto consider putting tight limits on queries that users can run to limit\\nthe infrastructure impact of any single user. In addition, security is a\\nmuch more complex and sensitive problem for external queries,\\nespecially if the data being queried is multitenant (data from many\\ncustomers and housed in a single table).\\nAn internal-facing data engineer typically focuses on activities crucial\\nto the needs of the business and internal stakeholders (Figure 1-11).\\nExamples include creating and maintaining data pipelines and data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e619d9c4-cc23-475f-ac3d-4f2f52b31217', embedding=None, metadata={'page_label': '48', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='warehouses for BI dashboards, reports, business processes, data\\nscience, and ML models.\\nFigure 1-11. Internal-facing data engineer\\nExternal-facing and internal-facing responsibilities are often blended.\\nIn practice, internal-facing data is usually a prerequisite to external-\\nfacing data. The data engineer has two sets of users with very\\ndifferent requirements for query concurrency, security, and more.\\nData Engineers and Other Technical Roles\\nIn practice, the data engineering lifecycle cuts across many domains\\nof responsibility. Data engineers sit at the nexus of various roles,\\ndirectly or through managers, interacting with many organizational\\nunits.\\nLet’s look at whom a data engineer may impact. In this section, we’ll\\ndiscuss technical roles connected to data engineering (Figure 1-12).\\nFigure 1-12. Key technical stakeholders of data engineering\\nThe data engineer is a hub between data producers, such as\\nsoftware engineers, data architects, and DevOps or site-reliability', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eacfe0a5-6f50-4c4b-901e-f88a7acfb917', embedding=None, metadata={'page_label': '49', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='engineers (SREs), and data consumers, such as data analysts, data\\nscientists, and ML engineers. In addition, data engineers will interact\\nwith those in operational roles, such as DevOps engineers.\\nGiven the pace at which new data roles come into vogue (analytics\\nand ML engineers come to mind), this is by no means an exhaustive\\nlist.\\nUpstream stakeholders\\nTo be successful as a data engineer, you need to understand the\\ndata architecture you’re using or designing and the source systems\\nproducing the data you’ll need. Next, we discuss a few familiar\\nupstream stakeholders: data architects, software engineers, and\\nDevOps engineers.\\nData architects\\nData architects function at a level of abstraction one step removed\\nfrom data engineers. Data architects design the blueprint for\\norganizational data management, mapping out processes and\\noverall data architecture and systems. They also serve as a bridge\\nbetween an organization’s technical and nontechnical sides.\\nSuccessful data architects generally have “battle scars” from\\nextensive engineering experience, allowing them to guide and assist\\nengineers while successfully communicating engineering challenges\\nto nontechnical business stakeholders.\\nData architects implement policies for managing data across silos\\nand business units, steer global strategies such as data\\nmanagement and data governance, and guide significant initiatives.\\nData architects often play a central role in cloud migrations and\\ngreenfield cloud design.\\nThe advent of the cloud has shifted the boundary between data\\narchitecture and data engineering. Cloud data architectures are\\nmuch more fluid than on-premises systems, so architecture\\ndecisions that traditionally involved extensive study, long lead times,\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='60d53567-b88d-40a5-916f-8f985136228a', embedding=None, metadata={'page_label': '50', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='purchase contracts, and hardware installation are now often made\\nduring the implementation process, just one step in a larger strategy.\\nNevertheless, data architects will remain influential visionaries in\\nenterprises, working hand in hand with data engineers to determine\\nthe big picture of architecture practices and data strategies.\\nDepending on the company’s data maturity and size, a data engineer\\nmay overlap with or assume the responsibilities of a data architect.\\nTherefore, a data engineer should have a good understanding of\\narchitecture best practices and approaches.\\nNote that we have placed data architects in the upstream\\nstakeholders section. Data architects often help design application\\ndata layers that are source systems for data engineers. Architects\\nmay also interact with data engineers at various other stages of the\\ndata engineering lifecycle. We cover “good” data architecture in\\nChapter 3.\\nSoftware engineers\\nSoftware engineers build the software and systems that run a\\nbusiness; they are largely responsible for generating the internal\\ndata that data engineers will consume and process. The systems\\nbuilt by software engineers typically generate application event data\\nand logs, which are significant assets in their own right. This internal\\ndata contrasts with external data pulled from SaaS platforms or\\npartner businesses. In well-run technical organizations, software\\nengineers and data engineers coordinate from the inception of a new\\nproject to design application data for consumption by analytics and\\nML applications.\\nA data engineer should work together with software engineers to\\nunderstand the applications that generate data, the volume,\\nfrequency, and format of the generated data, and anything else that\\nwill impact the data engineering lifecycle, such as data security and\\nregulatory compliance. For example, this might mean setting\\nupstream expectations on what the data software engineers need to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f14e39fe-a9ad-48cc-9527-5dca587612bd', embedding=None, metadata={'page_label': '51', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='do their jobs. Data engineers must work closely with the software\\nengineers.\\nDevOps engineers and site-reliability engineers\\nDevOps and SREs often produce data through operational\\nmonitoring. We classify them as upstream of data engineers, but\\nthey may also be downstream, consuming data through dashboards\\nor interacting with data engineers directly in coordinating operations\\nof data systems.\\nDownstream stakeholders\\nThe modern data engineering profession exists to serve downstream\\ndata consumers and use cases. This section discusses how data\\nengineers interact with various downstream roles. We’ll also\\nintroduce a few service models, including centralized data\\nengineering teams and cross-functional teams.\\nData scientists\\nData scientists build forward-looking models to make predictions and\\nrecommendations. These models are then evaluated on live data to\\nprovide value in various ways. For example, model scoring might\\ndetermine automated actions in response to real-time conditions,\\nrecommend products to customers based on the browsing history in\\ntheir current session, or make live economic predictions used by\\ntraders.\\nAccording to common industry folklore, data scientists spend 70% to\\n80% of their time collecting, cleaning, and preparing data. In our\\nexperience, these numbers often reflect immature data science and\\ndata engineering practices. In particular, many popular data science\\nframeworks can become bottlenecks if they are not scaled up\\nappropriately. Data scientists who work exclusively on a single\\nworkstation force themselves to downsample data, making data\\npreparation significantly more complicated and potentially\\ncompromising the quality of the models they produce. Furthermore,\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='271f64a0-6a63-4bb6-b45d-91da65427c78', embedding=None, metadata={'page_label': '52', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='locally developed code and environments are often difficult to deploy\\nin production, and a lack of automation significantly hampers data\\nscience workflows. If data engineers do their job and collaborate\\nsuccessfully, data scientists shouldn’t spend their time collecting,\\ncleaning, and preparing data after initial exploratory work. Data\\nengineers should automate this work as much as possible.\\nThe need for production-ready data science is a significant driver\\nbehind the emergence of the data engineering profession. Data\\nengineers should help data scientists to enable a path to production.\\nIn fact, we (the authors) moved from data science to data\\nengineering after recognizing this fundamental need. Data engineers\\nwork to provide the data automation and scale that make data\\nscience more efficient.\\nData analysts\\nData analysts (or business analysts) seek to understand business\\nperformance and trends. Whereas data scientists are forward-\\nlooking, a data analyst typically focuses on the past or present. Data\\nanalysts usually run SQL queries in a data warehouse or a data lake.\\nThey may also utilize spreadsheets for computation and analysis\\nand various BI tools such as Microsoft Power BI, Looker, or Tableau.\\nData analysts are domain experts in the data they work with\\nfrequently and become intimately familiar with data definitions,\\ncharacteristics, and quality problems. A data analyst’s typical\\ndownstream customers are business users, management, and\\nexecutives.\\nData engineers work with data analysts to build pipelines for new\\ndata sources required by the business. Data analysts’ subject-matter\\nexpertise is invaluable in improving data quality, and they frequently\\ncollaborate with data engineers in this capacity.\\nMachine learning engineers and AI researchers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ecade577-1787-42ad-9520-12d01b9b72f5', embedding=None, metadata={'page_label': '53', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Machine learning engineers (ML engineers) overlap with data\\nengineers and data scientists. ML engineers develop advanced ML\\ntechniques, train models, and design and maintain the infrastructure\\nrunning ML processes in a scaled production environment. ML\\nengineers often have advanced working knowledge of ML and deep\\nlearning techniques, and frameworks such as PyTorch or\\nTensorFlow.\\nML engineers also understand the hardware, services, and systems\\nrequired to run these frameworks, both for model training and model\\ndeployment at a production scale. It’s common for ML flows to run in\\na cloud environment where ML engineers can spin up and scale\\ninfrastructure resources on demand or rely on managed services.\\nAs we’ve mentioned, the boundaries between ML engineering, data\\nengineering, and data science are blurry. Data engineers may have\\nsome DevOps responsibilities over ML systems, and data scientists\\nmay work closely with ML engineering in designing advanced ML\\nprocesses.\\nThe world of ML engineering is snowballing and parallels a lot of the\\nsame developments occurring in data engineering. Whereas several\\nyears ago, the attention of ML was focused on how to build models,\\nML engineering now increasingly emphasizes incorporating best\\npractices of machine learning operations (MLOps) and other mature\\npractices previously adopted in software engineering and DevOps.\\nAI researchers work on new, advanced ML techniques. AI\\nresearchers may work inside large technology companies,\\nspecialized intellectual property startups (OpenAI, DeepMind), or\\nacademic institutions. Some practitioners are dedicated to part-time\\nresearch in conjunction with ML engineering responsibilities inside a\\ncompany. Those working inside specialized ML labs are often 100%\\ndedicated to research. Research problems may target immediate\\npractical applications or more abstract demonstrations of AI.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='98ed7dff-269e-4fb2-93d5-3fb76c95fdad', embedding=None, metadata={'page_label': '54', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='AlphaGo and GPT-3/GPT-4 are great examples of ML research\\nprojects. We’ve provided some references in “Additional Resources”.\\nAI researchers in well-funded organizations are highly specialized\\nand operate with supporting teams of engineers to facilitate their\\nwork. ML engineers in academia usually have fewer resources but\\nrely on teams of graduate students, postdocs, and university staff to\\nprovide engineering support. ML engineers who are partially\\ndedicated to research often rely on the same support teams for\\nresearch and production.\\nData Engineers and Business Leadership\\nWe’ve discussed technical roles with which a data engineer\\ninteracts. But data engineers also operate more broadly as\\norganizational connectors, often in a nontechnical capacity.\\nBusinesses have come to rely increasingly on data as a core part of\\nmany products or a product in itself. Data engineers now participate\\nin strategic planning and lead key initiatives that extend beyond the\\nboundaries of IT. Data engineers often support data architects by\\nacting as the glue between the business and data science/analytics.\\nData in the C-suite\\nC-level executives are increasingly involved in data and analytics, as\\nthese are recognized as significant assets for modern businesses.\\nCEOs now concern themselves with initiatives that were once the\\nexclusive province of IT, such as cloud migrations or deployment of a\\nnew customer data platform.\\nChief executive officer\\nChief executive officers (CEOs) at nontech companies generally\\ndon’t concern themselves with the nitty-gritty of data frameworks and\\nsoftware. Instead, they define a vision in collaboration with technical\\nC-suite roles and company data leadership. Data engineers provide\\na window into what’s possible with data. Data engineers and their', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b336c378-9415-40bc-9060-10c4e273ca76', embedding=None, metadata={'page_label': '55', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='managers maintain a map of what data is available to the\\norganization—both internally and from third parties—in what time\\nframe. They are also tasked to study primary data architectural\\nchanges in collaboration with other engineering roles. For example,\\ndata engineers are often heavily involved in cloud migrations,\\nmigrations to new data systems, or deployment of streaming\\ntechnologies.\\nChief information officer\\nA chief information officer (CIO) is the senior C-suite executive\\nresponsible for information technology within an organization; it is an\\ninternal-facing role. A CIO must possess deep knowledge of\\ninformation technology and business processes—either alone is\\ninsufficient. CIOs direct the information technology organization,\\nsetting ongoing policies while also defining and executing significant\\ninitiatives under the direction of the CEO.\\nCIOs often collaborate with data engineering leadership in\\norganizations with a well-developed data culture. If an organization is\\nnot very high in its data maturity, a CIO will typically help shape its\\ndata culture. CIOs will work with engineers and architects to map out\\nmajor initiatives and make strategic decisions on adopting major\\narchitectural elements, such as enterprise resource planning (ERP)\\nand customer relationship management (CRM) systems, cloud\\nmigrations, data systems, and internal-facing IT.\\nChief technology officer\\nA chief technology officer (CTO) is similar to a CIO but faces\\noutward. A CTO owns the key technological strategy and\\narchitectures for external-facing applications, such as mobile, web\\napps, and IoT—all critical data sources for data engineers. The CTO\\nis likely a skilled technologist and has a good sense of software\\nengineering fundamentals and system architecture. In some\\norganizations without a CIO, the CTO or sometimes the chief', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='37d32e07-22b3-478d-a75a-7752fa455d1c', embedding=None, metadata={'page_label': '56', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='operating officer (COO) plays the role of CIO. Data engineers often\\nreport directly or indirectly through a CTO.\\nChief data officer\\nThe chief data officer (CDO) was created in 2002 at Capital One to\\nrecognize the growing importance of data as a business asset. The\\nCDO is responsible for a company’s data assets and strategy. CDOs\\nare focused on data’s business utility but should have a strong\\ntechnical grounding. CDOs oversee data products, strategy,\\ninitiatives, and core functions such as master data management and\\nprivacy. Occasionally, CDOs manage business analytics and data\\nengineering.\\nChief analytics officer\\nThe chief analytics officer (CAO) is a variant of the CDO role. Where\\nboth roles exist, the CDO focuses on the technology and\\norganization required to deliver data. The CAO is responsible for\\nanalytics, strategy, and decision making for the business. A CAO\\nmay oversee data science and ML, though this largely depends on\\nwhether the company has a CDO or CTO role.\\nChief algorithms officer\\nA chief algorithms officer (CAO-2) is a recent innovation in the C-\\nsuite, a highly technical role focused specifically on data science and\\nML. CAO-2s typically have experience as individual contributors and\\nteam leads in data science or ML projects. Frequently, they have a\\nbackground in ML research and a related advanced degree.\\nCAO-2s are expected to be conversant in current ML research and\\nhave deep technical knowledge of their company’s ML initiatives. In\\naddition to creating business initiatives, they provide technical\\nleadership, set research and development agendas, and build\\nresearch teams.\\nData engineers and project managers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a81b8c97-b95d-4190-a6a1-53dbf9737f7c', embedding=None, metadata={'page_label': '57', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data engineers often work on significant initiatives, potentially\\nspanning many years. As we write this book, many data engineers\\nare working on cloud migrations, migrating pipelines and\\nwarehouses to the next generation of data tools. Other data\\nengineers are starting greenfield projects, assembling new data\\narchitectures from scratch by selecting from an astonishing number\\nof best-of-breed architecture and tooling options.\\nThese large initiatives often benefit from project management (in\\ncontrast to product management, discussed next). Whereas data\\nengineers function in an infrastructure and service delivery capacity,\\nproject managers direct traffic and serve as gatekeepers. Most\\nproject managers operate according to some variation of Agile and\\nScrum, with Waterfall still appearing occasionally. Business never\\nsleeps, and business stakeholders often have a significant backlog\\nof things they want to address and new initiatives they want to\\nlaunch. Project managers must filter a long list of requests and\\nprioritize critical deliverables to keep projects on track and better\\nserve the company.\\nData engineers interact with project managers, often planning sprints\\nfor projects and ensuing standups related to the sprint. Feedback\\ngoes both ways, with data engineers informing project managers and\\nother stakeholders about progress and blockers, and project\\nmanagers balancing the cadence of technology teams against the\\never-changing needs of the business.\\nData engineers and product managers\\nProduct managers oversee product development, often owning\\nproduct lines. In the context of data engineers, these products are\\ncalled data products. Data products are either built from the ground\\nup or are incremental improvements upon existing products. Data\\nengineers interact more frequently with product managers as the\\ncorporate world has adopted a data-centric focus. Like project', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='418ba097-d844-4f09-97e7-b46acb8a0f1e', embedding=None, metadata={'page_label': '58', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='managers, product managers balance the activity of technology\\nteams against the needs of the customer and business.\\nData engineers and other management roles\\nData engineers interact with various managers beyond project and\\nproduct managers. However, these interactions usually follow either\\nthe services or cross-functional models. Data engineers either serve\\na variety of incoming requests as a centralized team or work as a\\nresource assigned to a particular manager, project, or product.\\nFor more information on data teams and how to structure them, we\\nrecommend John Thompson’s Building Analytics Teams (Packt) and\\nJesse Anderson’s Data Teams (Apress). Both books provide strong\\nframeworks and perspectives on the roles of executives with data,\\nwho to hire, and how to construct the most effective data team for\\nyour company.\\nNOTE\\nCompanies don’t hire engineers simply to hack on code in isolation. To\\nbe worthy of their title, engineers should develop a deep understanding\\nof the problems they’re tasked with solving, the technology tools at their\\ndisposal, and the people they work with and serve.\\nConclusion\\nThis chapter provided you with a brief overview of the data\\nengineering landscape, including the following:\\nDefining data engineering and describing what data engineers\\ndo\\nDescribing the types of data maturity in a company\\nType A and type B data engineers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='06f9a089-646e-4c12-b5bd-9367d2e40c48', embedding=None, metadata={'page_label': '59', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whom data engineers work with\\nWe hope that this first chapter has whetted your appetite, whether\\nyou are a software development practitioner, data scientist, ML\\nengineer, business stakeholder, entrepreneur, or venture capitalist.\\nOf course, a great deal still remains to elucidate in subsequent\\nchapters. Chapter 2 covers the data engineering lifecycle, followed\\nby architecture in Chapter 3. The following chapters get into the nitty-\\ngritty of technology decisions for each part of the lifecycle. The entire\\ndata field is in flux, and as much as possible, each chapter focuses\\non the immutables—perspectives that will be valid for many years\\namid relentless change.\\nAdditional Resources\\n“On Complexity in Big Data” by Jesse Anderson (O’Reilly)\\n“Which Profession Is More Complex to Become, a Data\\nEngineer or a Data Scientist?” thread on Quora\\n“The Future of Data Engineering Is the Convergence of\\nDisciplines” by Liam Hausmann\\nThe Information Management Body of Knowledge website\\n“Doing Data Science at Twitter” by Robert Chang\\n“A Short History of Big Data” by Mark van Rijmenam\\n“Data Engineering: A Quick and Simple Definition” by James\\nFurbush (O’Reilly)\\n“Big Data Will Be Dead in Five Years” by Lewis Gavin\\n“The AI Hierarchy of Needs” by Monica Rogati\\nChapter 1 ofWhat Is Data Engineering? by Lewis Gavin\\n(O’Reilly)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='96a7e04a-e8cc-4415-8f84-7daa6c242c7f', embedding=None, metadata={'page_label': '60', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“The Three Levels of Data Analysis: A Framework for Assessing\\nData Organization Maturity” by Emilie Schario\\n“Data as a Product vs. Data as a Service” by Justin Gage\\n“The Downfall of the Data Engineer” by Maxime Beauchemin\\n“The Rise of the Data Engineer” by Maxime Beauchemin\\n“Skills of the Data Architect” by Bob Lambert\\n“What Is a Data Architect? IT’s Data Framework Visionary” by\\nThor Olavsrud\\n“OpenAI’s New Language Generator GPT-3 Is Shockingly Good\\n—and Completely Mindless” by Will Douglas Heaven\\nThe AlphaGo research web page\\n“How CEOs Can Lead a Data-Driven Culture” by Thomas H.\\nDavenport and Nitin Mittal\\n“Why CEOs Must Lead Big Data Initiatives” by John\\nWeathington\\n“How Creating a Data-Driven Culture Can Drive Success” by\\nFrederik Bussler\\nBuilding Analytics Teams by John K. Thompson (Packt)\\nData Teams by Jesse Anderson (Apress)\\n“Information Management Body of Knowledge” Wikipedia page\\n“Information management” Wikipedia page\\n1  “Data Engineering and Its Main Concepts,” AlexSoft, last updated August\\n26, 2021, https://oreil.ly/e94py.\\n2  ETL stands for extract, transform, load, a common pattern we cover in the\\nbook.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4ca7027b-7efe-4da8-a3c9-08dded0e54d9', embedding=None, metadata={'page_label': '61', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3  Jesse Anderson, “The Two Types of Data Engineering,” June 27, 2018,\\nhttps://oreil.ly/dxDt6.\\n4  Maxime Beauchemin, “The Rise of the Data Engineer,” January 20, 2017,\\nhttps://oreil.ly/kNDmd.\\n5  Lewis Gavin, What Is Data Engineering? (Sebastapol, CA: O’Reilly, 2020),\\nhttps://oreil.ly/ELxLi.\\n6  Cade Metz, “How Yahoo Spawned Hadoop, the Future of Big Data,” Wired,\\nOctober 18, 2011, https://oreil.ly/iaD9G.\\n7  Ron Miller, “How AWS Came to Be,” TechCrunch, July 2, 2016,\\nhttps://oreil.ly/VJehv.\\n8  DataOps is an abbreviation for data operations. We cover this topic in\\nChapter 2. For more information, read the DataOps Manifesto.\\n9  These acronyms stand for California Consumer Privacy Act and General\\nData Protection Regulation, respectively.\\n10  Robert Chang, “Doing Data Science at Twitter,” Medium, June 20, 2015,\\nhttps://oreil.ly/xqjAx.\\n11  Paramita (Guha) Ghosh, “Data Architect vs. Data Engineer,” Dataversity,\\nNovember 12, 2021, https://oreil.ly/TlyZY.\\n12  A variety of references exist for this notion. Although this cliche is widely\\nknown, a healthy debate has arisen around its validity in different practical\\nsettings. For more details, see Leigh Dodds, “Do Data Scientists Spend 80%\\nof Their Time Cleaning Data? Turns Out, No?” Lost Boy blog, January 31,\\n2020, https://oreil.ly/szFww; and Alex Woodie, “Data Prep Still Dominates\\nData Scientists’ Time, Survey Finds,” Datanami, July 6, 2020,\\nhttps://oreil.ly/jDVWF.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9420507-3a80-4778-b028-27d79af59d69', embedding=None, metadata={'page_label': '62', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 2. The Data\\nEngineering Lifecycle\\nThe major goal of this book is to encourage you to move beyond\\nviewing data engineering as a specific collection of data\\ntechnologies. The data landscape is undergoing an explosion of new\\ndata technologies and practices, with ever-increasing levels of\\nabstraction and ease of use. Because of increased technical\\nabstraction, data engineers will increasingly become data lifecycle\\nengineers, thinking and operating in terms of the principles of data\\nlifecycle management.\\nIn this chapter, you’ll learn about the data engineering lifecycle,\\nwhich is the central theme of this book. The data engineering\\nlifecycle is our framework describing “cradle to grave” data\\nengineering. You will also learn about the undercurrents of the data\\nengineering lifecycle, which are key foundations that support all data\\nengineering efforts.\\nWhat Is the Data Engineering Lifecycle?\\nThe data engineering lifecycle comprises stages that turn raw data\\ningredients into a useful end product, ready for consumption by\\nanalysts, data scientists, ML engineers, and others. This chapter\\nintroduces the major stages of the data engineering lifecycle,\\nfocusing on each stage’s core concepts and saving details for later\\nchapters.\\nWe divide the data engineering lifecycle into the following five stages\\n(Figure 2-1, top):\\nGeneration', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56a32afc-3418-41a7-831c-08edaf811acd', embedding=None, metadata={'page_label': '63', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Storage\\nIngestion\\nTransformation\\nServing data\\nFigure 2-1. Components and undercurrents of the data engineering lifecycle\\nWe begin the data engineering lifecycle by getting data from source\\nsystems and storing it. Next, we transform the data and then\\nproceed to our central goal, serving data to analysts, data scientists,\\nML engineers, and others. In reality, storage occurs throughout the\\nlifecycle as data flows from beginning to end—hence, the diagram\\nshows the storage “stage” as a foundation that underpins other\\nstages.\\nIn general, the middle stages—storage, ingestion, transformation—\\ncan get a bit jumbled. And that’s OK. Although we split out the\\ndistinct parts of the data engineering lifecycle, it’s not always a neat,\\ncontinuous flow. Various stages of the lifecycle may repeat\\nthemselves, occur out of order, overlap, or weave together in\\ninteresting and unexpected ways.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0c6e64de-5dcc-41e4-a9c7-f2058001984a', embedding=None, metadata={'page_label': '64', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Acting as a bedrock are undercurrents (Figure 2-1, bottom) that cut\\nacross multiple stages of the data engineering lifecycle: security,\\ndata management, DataOps, data architecture, orchestration, and\\nsoftware engineering. No part of the data engineering lifecycle can\\nadequately function without these undercurrents.\\nThe Data Lifecycle Versus the Data Engineering\\nLifecycle\\nYou may be wondering about the difference between the overall data\\nlifecycle and the data engineering lifecycle. There’s a subtle\\ndistinction between the two. The data engineering lifecycle is a\\nsubset of the whole data lifecycle (Figure 2-2). Whereas the full data\\nlifecycle encompasses data across its entire lifespan, the data\\nengineering lifecycle focuses on the stages a data engineer controls.\\nFigure 2-2. The data engineering lifecycle is a subset of the full data lifecycle\\nGeneration: Source Systems\\nA source system is the origin of the data used in the data\\nengineering lifecycle. For example, a source system could be an IoT', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='78120ffd-e046-4414-9385-ee38b3e4230e', embedding=None, metadata={'page_label': '65', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='device, an application message queue, or a transactional database.\\nA data engineer consumes data from a source system, but doesn’t\\ntypically own or control the source system itself. The data engineer\\nneeds to have a working understanding of the way source systems\\nwork, the way they generate data, the frequency and velocity of the\\ndata, and the variety of data they generate.\\nEngineers also need to keep an open line of communication with\\nsource system owners on changes that could break pipelines and\\nanalytics. Application code might change the structure of data in a\\nfield, or the application team might even choose to migrate the\\nbackend to an entirely new database technology.\\nA major challenge in data engineering is the dizzying array of data\\nsource systems engineers must work with and understand. As an\\nillustration, let’s look at two common source systems, one very\\ntraditional (an application database) and the other a more recent\\nexample (IoT swarms).\\nFigure 2-3 illustrates a traditional source system with several\\napplication servers supported by a database. This source system\\npattern became popular in the 1980s with the explosive success of\\nrelational database management systems (RDBMSs). The\\napplication + database pattern remains popular today with various\\nmodern evolutions of software development practices. For example,\\napplications often consist of many small service/database pairs with\\nmicroservices rather than a single monolith.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fadaa8ff-e24b-4d19-bfad-4bcf748cbace', embedding=None, metadata={'page_label': '66', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 2-3. Source system example: an application database\\nLet’s look at another example of a source system. Figure 2-4\\nillustrates an IoT swarm: a fleet of devices (circles) sends data\\nmessages (rectangles) to a central collection system. This IoT\\nsource system is increasingly common as IoT devices such as\\nsensors, smart devices, and much more increase in the wild.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4c2507b9-7b34-4150-8683-0c2d3418b86c', embedding=None, metadata={'page_label': '67', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 2-4. Source system example: an IoT swarm and messaging queue\\nEvaluating source systems: Key engineering considerations\\nThere are many things to consider when assessing source systems,\\nincluding how the system handles ingestion, state, and data\\ngeneration. The following is a starting set of evaluation questions of\\nsource systems that data engineers must consider:\\nWhat are the essential characteristics of the data source? Is it\\nan application? A swarm of IoT devices?\\nHow is data persisted in the source system? Is data persisted\\nlong term, or is it temporary and quickly deleted?\\nAt what rate is data generated? How many events per second?\\nHow many gigabytes per hour?\\nWhat level of consistency can data engineers expect from the\\noutput data? If you’re running data-quality checks against the\\noutput data, how often do data inconsistencies occur—nulls\\nwhere they aren’t expected, lousy formatting, etc.?\\nHow often do errors occur?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0fcfb84e-16ae-4b10-8ecf-bd816b4c4fef', embedding=None, metadata={'page_label': '68', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Will the data contain duplicates?\\nWill some data values arrive late, possibly much later than other\\nmessages produced simultaneously?\\nWhat is the schema of the ingested data? Will data engineers\\nneed to join across several tables or even several systems to\\nget a complete picture of the data?\\nIf schema changes (say, a new column is added), how is this\\ndealt with and communicated to downstream stakeholders?\\nHow frequently should data be pulled from the source system?\\nFor stateful systems (e.g., a database tracking customer\\naccount information), is data provided as periodic snapshots or\\nupdate events from change data capture (CDC)? What’s the\\nlogic for how changes are performed, and how are these\\ntracked in the source database?\\nWho/what is the data provider that will transmit the data for\\ndownstream consumption?\\nWill reading from a data source impact its performance?\\nDoes the source system have upstream data dependencies?\\nWhat are the characteristics of these upstream systems?\\nAre data-quality checks in place to check for late or missing\\ndata?\\nSources produce data consumed by downstream systems, including\\nhuman-generated spreadsheets, IoT sensors, and web and mobile\\napplications. Each source has its unique volume and cadence of\\ndata generation. A data engineer should know how the source\\ngenerates data, including relevant quirks or nuances. Data engineers\\nalso need to understand the limits of the source systems they\\ninteract with. For example, will analytical queries against a source', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cd4eb2ea-dd1d-4a2a-a99c-f629e0fb383a', embedding=None, metadata={'page_label': '69', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='application database cause resource contention and performance\\nissues?\\nOne of the most challenging nuances of source data is the schema.\\nThe schema defines the hierarchical organization of data. Logically,\\nwe can think of data at the level of a whole source system, drilling\\ndown into individual tables, all the way to the structure of respective\\nfields. The schema of data shipped from source systems is handled\\nin various ways. Two popular options are schemaless and fixed\\nschema.\\nSchemaless doesn’t mean the absence of schema. Rather, it means\\nthat the application defines the schema as data is written, whether to\\na messaging queue, a flat file, a blob, or a document database such\\nas MongoDB. A more traditional model built on relational database\\nstorage uses a fixed schema enforced in the database, to which\\napplication writes must conform.\\nEither of these models presents challenges for data engineers.\\nSchemas change over time; in fact, schema evolution is encouraged\\nin the Agile approach to software development. A key part of the\\ndata engineer’s job is taking raw data input in the source system\\nschema and transforming this into valuable output for analytics. This\\njob becomes more challenging as the source schema evolves.\\nWe dive into source systems in greater detail in Chapter 5; we also\\ncover schemas and data modeling in Chapters 6 and 8, respectively.\\nStorage\\nAfter ingesting data, you need a place to store it. Choosing a storage\\nsolution is key to success in the rest of the data lifecycle, and it’s\\nalso one of the most complicated stages of the data lifecycle for a\\nvariety of reasons. First, data architectures in the cloud often\\nleverage several storage solutions. Second, few data storage\\nsolutions function purely as storage, with many supporting complex\\ntransformation queries; even object storage solutions may support', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='15da1d0b-3c1c-4b44-9b39-021afe5b94b3', embedding=None, metadata={'page_label': '70', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='powerful query capabilities—e.g., Amazon S3 Select. Third, while\\nstorage is a stage of the data engineering lifecycle, it frequently\\ntouches on other stages, such as ingestion, transformation, and\\nserving.\\nStorage runs across the entire data engineering lifecycle, often\\noccurring in multiple places in a data pipeline, with storage systems\\ncrossing over with source systems, ingestion, transformation, and\\nserving. In many ways, the way data is stored impacts how it is used\\nin all of the stages of the data engineering lifecycle. For example,\\ncloud data warehouses can store data, process data in pipelines,\\nand serve it to analysts. Streaming frameworks such as Apache\\nKafka and Pulsar can function simultaneously as ingestion, storage,\\nand query systems for messages, with object storage being a\\nstandard layer for data transmission.\\nEvaluating storage systems: Key engineering considerations\\nHere are a few key engineering questions to ask when choosing a\\nstorage system for a data warehouse, data lakehouse, database, or\\nobject storage:\\nIs this storage solution compatible with the architecture’s\\nrequired write and read speeds?\\nWill storage create a bottleneck for downstream processes?\\nDo you understand how this storage technology works? Are you\\nutilizing the storage system optimally or committing unnatural\\nacts? For instance, are you applying a high rate of random\\naccess updates in an object storage system? (This is an\\nantipattern with significant performance overhead.)\\nWill this storage system handle anticipated future scale? You\\nshould consider all capacity limits on the storage system: total\\navailable storage, read operation rate, write volume, etc.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c3f875d0-b9cd-473b-b8f6-5725910b0db0', embedding=None, metadata={'page_label': '71', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Will downstream users and processes be able to retrieve data in\\nthe required service-level agreement (SLA)?\\nAre you capturing metadata about schema evolution, data flows,\\ndata lineage, and so forth? Metadata has a significant impact on\\nthe utility of data. Metadata represents an investment in the\\nfuture, dramatically enhancing discoverability and institutional\\nknowledge to streamline future projects and architecture\\nchanges.\\nIs this a pure storage solution (object storage), or does it\\nsupport complex query patterns (i.e., a cloud data warehouse)?\\nIs the storage system schema-agnostic (object storage)?\\nFlexible schema (Cassandra)? Enforced schema (a cloud data\\nwarehouse)?\\nHow are you tracking master data, golden records data quality,\\nand data lineage for data governance? (We have more to say\\non these in “Data Management”.)\\nHow are you handling regulatory compliance and data\\nsovereignty? For example, can you store your data in certain\\ngeographical locations but not others?\\nUnderstanding data access frequency\\nNot all data is accessed in the same way. Retrieval patterns will\\ngreatly vary based on the data being stored and queried. This brings\\nup the notion of the “temperatures” of data. Data access frequency\\nwill determine the temperature of your data.\\nData that is most frequently accessed is called hot data. Hot data is\\ncommonly retrieved many times per day, perhaps even several times\\nper second, in systems that serve user requests. This data should be\\nstored for fast retrieval, where “fast” is relative to the use case.\\nLukewarm data might be accessed every so often—say, every week\\nor month.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6b034081-f8b2-4da2-9750-96f9d46c3c8c', embedding=None, metadata={'page_label': '72', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cold data is seldom queried and is appropriate for storing in an\\narchival system. Cold data is often retained for compliance purposes\\nor in case of a catastrophic failure in another system. In the “old\\ndays,” cold data would be stored on tapes and shipped to remote\\narchival facilities. In cloud environments, vendors offer specialized\\nstorage tiers with very cheap monthly storage costs but high prices\\nfor data retrieval.\\nSelecting a storage system\\nWhat type of storage solution should you use? This depends on your\\nuse cases, data volumes, frequency of ingestion, format, and size of\\nthe data being ingested—essentially, the key considerations listed in\\nthe preceding bulleted questions. There is no one-size-fits-all\\nuniversal storage recommendation. Every storage technology has its\\ntrade-offs. Countless varieties of storage technologies exist, and it’s\\neasy to be overwhelmed when deciding the best option for your data\\narchitecture.\\nChapter 6 covers storage best practices and approaches in greater\\ndetail, as well as the crossover between storage and other lifecycle\\nstages.\\nIngestion\\nAfter you understand the data source and the characteristics of the\\nsource system you’re using, you need to gather the data. The\\nsecond stage of the data engineering lifecycle is data ingestion from\\nsource systems.\\nIn our experience, source systems and ingestion represent the most\\nsignificant bottlenecks of the data engineering lifecycle. The source\\nsystems are normally outside your direct control and might randomly\\nbecome unresponsive or provide data of poor quality. Or, your data\\ningestion service might mysteriously stop working for many reasons.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c6e7e608-877e-481e-b697-7f17cff9d9ae', embedding=None, metadata={'page_label': '73', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As a result, data flow stops or delivers insufficient data for storage,\\nprocessing, and serving.\\nUnreliable source and ingestion systems have a ripple effect across\\nthe data engineering lifecycle. But you’re in good shape, assuming\\nyou’ve answered the big questions about source systems.\\nKey engineering considerations for the ingestion phase\\nWhen preparing to architect or build a system, here are some\\nprimary questions about the ingestion stage:\\nWhat are the use cases for the data I’m ingesting? Can I reuse\\nthis data rather than create multiple versions of the same\\ndataset?\\nAre the systems generating and ingesting this data reliably, and\\nis the data available when I need it?\\nWhat is the data destination after ingestion?\\nHow frequently will I need to access the data?\\nIn what volume will the data typically arrive?\\nWhat format is the data in? Can my downstream storage and\\ntransformation systems handle this format?\\nIs the source data in good shape for immediate downstream\\nuse? If so, for how long, and what may cause it to be unusable?\\nIf the data is from a streaming source, does it need to be\\ntransformed before reaching its destination? Would an in-flight\\ntransformation be appropriate, where the data is transformed\\nwithin the stream itself?\\nThese are just a sample of the factors you’ll need to think about with\\ningestion, and we cover those questions and more in Chapter 7.\\nBefore we leave, let’s briefly turn our attention to two major data\\ningestion concepts: batch versus streaming and push versus pull.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f3082809-fafc-45cf-af4a-f16e564b91b0', embedding=None, metadata={'page_label': '74', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Batch versus streaming\\nVirtually all data we deal with is inherently streaming. Data is nearly\\nalways produced and updated continually at its source. Batch\\ningestion is simply a specialized and convenient way of processing\\nthis stream in large chunks—for example, handling a full day’s worth\\nof data in a single batch.\\nStreaming ingestion allows us to provide data to downstream\\nsystems—whether other applications, databases, or analytics\\nsystems—in a continuous, real-time fashion. Here, real-time (or near\\nreal-time) means that the data is available to a downstream system a\\nshort time after it is produced (e.g., less than one second later). The\\nlatency required to qualify as real-time varies by domain and\\nrequirements.\\nBatch data is ingested either on a predetermined time interval or as\\ndata reaches a preset size threshold. Batch ingestion is a one-way\\ndoor: once data is broken into batches, the latency for downstream\\nconsumers is inherently constrained. Because of limitations of legacy\\nsystems, batch was for a long time the default way to ingest data.\\nBatch processing remains an extremely popular way to ingest data\\nfor downstream consumption, particularly in analytics and ML.\\nHowever, the separation of storage and compute in many systems\\nand the ubiquity of event-streaming and processing platforms make\\nthe continuous processing of data streams much more accessible\\nand increasingly popular. The choice largely depends on the use\\ncase and expectations for data timeliness.\\nKey considerations for batch versus stream ingestion\\nShould you go streaming-first? Despite the attractiveness of a\\nstreaming-first approach, there are many trade-offs to understand\\nand think about. The following are some questions to ask yourself\\nwhen determining whether streaming ingestion is an appropriate\\nchoice over batch ingestion:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1e437696-f2c9-4ac7-ba46-97dcc5523199', embedding=None, metadata={'page_label': '75', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='If I ingest the data in real time, can downstream storage\\nsystems handle the rate of data flow?\\nDo I need millisecond real-time data ingestion? Or would a\\nmicro-batch approach work, accumulating and ingesting data,\\nsay, every minute?\\nWhat are my use cases for streaming ingestion? What specific\\nbenefits do I realize by implementing streaming? If I get data in\\nreal time, what actions can I take on that data that would be an\\nimprovement upon batch?\\nWill my streaming-first approach cost more in terms of time,\\nmoney, maintenance, downtime, and opportunity cost than\\nsimply doing batch?\\nAre my streaming pipeline and system reliable and redundant if\\ninfrastructure fails?\\nWhat tools are most appropriate for the use case? Should I use\\na managed service (Amazon Kinesis, Google Cloud Pub/Sub,\\nGoogle Cloud Dataflow) or stand up my own instances of Kafka,\\nFlink, Spark, Pulsar, etc.? If I do the latter, who will manage it?\\nWhat are the costs and trade-offs?\\nIf I’m deploying an ML model, what benefits do I have with\\nonline predictions and possibly continuous training?\\nAm I getting data from a live production instance? If so, what’s\\nthe impact of my ingestion process on this source system?\\nAs you can see, streaming-first might seem like a good idea, but it’s\\nnot always straightforward; extra costs and complexities inherently\\noccur. Many great ingestion frameworks do handle both batch and\\nmicro-batch ingestion styles. We think batch is an excellent\\napproach for many common use cases, such as model training and\\nweekly reporting. Adopt true real-time streaming only after identifying\\na business use case that justifies the trade-offs against using batch.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='37b77ebb-3a9e-43d7-849d-98e285e725be', embedding=None, metadata={'page_label': '76', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Push versus pull\\nIn the push model of data ingestion, a source system writes data out\\nto a target, whether a database, object store, or filesystem. In the\\npull model, data is retrieved from the source system. The line\\nbetween the push and pull paradigms can be quite blurry; data is\\noften pushed and pulled as it works its way through the various\\nstages of a data pipeline.\\nConsider, for example, the extract, transform, load (ETL) process,\\ncommonly used in batch-oriented ingestion workflows. ETL’s extract\\n(E) part clarifies that we’re dealing with a pull ingestion model. In\\ntraditional ETL, the ingestion system queries a current source table\\nsnapshot on a fixed schedule. You’ll learn more about ETL and\\nextract, load, transform (ELT) throughout this book.\\nIn another example, consider continuous CDC, which is achieved in\\na few ways. One common method triggers a message every time a\\nrow is changed in the source database. This message is pushed to a\\nqueue, where the ingestion system picks it up. Another common\\nCDC method uses binary logs, which record every commit to the\\ndatabase. The database pushes to its logs. The ingestion system\\nreads the logs but doesn’t directly interact with the database\\notherwise. This adds little to no additional load to the source\\ndatabase. Some versions of batch CDC use the pull pattern. For\\nexample, in timestamp-based CDC, an ingestion system queries the\\nsource database and pulls the rows that have changed since the\\nprevious update.\\nWith streaming ingestion, data bypasses a backend database and is\\npushed directly to an endpoint, typically with data buffered by an\\nevent-streaming platform. This pattern is useful with fleets of IoT\\nsensors emitting sensor data. Rather than relying on a database to\\nmaintain the current state, we simply think of each recorded reading\\nas an event. This pattern is also growing in popularity in software\\napplications as it simplifies real-time processing, allows app', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f8f73596-dbb4-4aaf-9f35-cdcb889ff0e6', embedding=None, metadata={'page_label': '77', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='developers to tailor their messages for downstream analytics, and\\ngreatly simplifies the lives of data engineers.\\nWe discuss ingestion best practices and techniques in depth in\\nChapter 7. Next, let’s turn to the transformation stage of the data\\nengineering lifecycle.\\nTransformation\\nAfter you’ve ingested and stored data, you need to do something\\nwith it. The next stage of the data engineering lifecycle is\\ntransformation, meaning data needs to be changed from its original\\nform into something useful for downstream use cases. Without\\nproper transformations, data will sit inert, and not be in a useful form\\nfor reports, analysis, or ML. Typically, the transformation stage is\\nwhere data begins to create value for downstream user\\nconsumption.\\nImmediately after ingestion, basic transformations map data into\\ncorrect types (changing ingested string data into numeric and date\\ntypes, for example), putting records into standard formats, and\\nremoving bad ones. Later stages of transformation may transform\\nthe data schema and apply normalization. Downstream, we can\\napply large-scale aggregation for reporting or featurize data for ML\\nprocesses.\\nKey considerations for the transformation phase\\nWhen considering data transformations within the data engineering\\nlifecycle, it helps to consider the following:\\nWhat’s the cost and return on investment (ROI) of the\\ntransformation? What is the associated business value?\\nIs the transformation as simple and self-isolated as possible?\\nWhat business rules do the transformations support?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1fcd2f23-05f1-4216-bed0-91560339c9ee', embedding=None, metadata={'page_label': '78', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Am I minimizing data movement between the transformation\\nand the storage system during transformation?\\nYou can transform data in batch or while streaming in flight. As\\nmentioned in “Ingestion”, virtually all data starts life as a continuous\\nstream; batch is just a specialized way of processing a data stream.\\nBatch transformations are overwhelmingly popular, but given the\\ngrowing popularity of stream-processing solutions and the general\\nincrease in the amount of streaming data, we expect the popularity\\nof streaming transformations to continue growing, perhaps entirely\\nreplacing batch processing in certain domains soon.\\nLogically, we treat transformation as a standalone area of the data\\nengineering lifecycle, but the realities of the lifecycle can be much\\nmore complicated in practice. Transformation is often entangled in\\nother phases of the lifecycle. Typically, data is transformed in source\\nsystems or in flight during ingestion. For example, a source system\\nmay add an event timestamp to a record before forwarding it to an\\ningestion process. Or a record within a streaming pipeline may be\\n“enriched” with additional fields and calculations before it’s sent to a\\ndata warehouse. Transformations are ubiquitous in various parts of\\nthe lifecycle. Data preparation, data wrangling, and cleaning—these\\ntransformative tasks add value for end consumers of data.\\nBusiness logic is a major driver of data transformation, often in data\\nmodeling. Data translates business logic into reusable elements\\n(e.g., a sale means “somebody bought 12 picture frames from me for\\n$30 each, or $360 in total”). In this case, somebody bought 12\\npicture frames for $30 each. Data modeling is critical for obtaining a\\nclear and current picture of business processes. A simple view of\\nraw retail transactions might not be useful without adding the logic of\\naccounting rules so that the CFO has a clear picture of financial\\nhealth. Ensure a standard approach for implementing business logic\\nacross your transformations.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a8db110e-7851-4a70-bbd2-3769c5a954de', embedding=None, metadata={'page_label': '79', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data featurization for ML is another data transformation process.\\nFeaturization intends to extract and enhance data features useful for\\ntraining ML models. Featurization can be a dark art, combining\\ndomain expertise (to identify which features might be important for\\nprediction) with extensive experience in data science. For this book,\\nthe main point is that once data scientists determine how to featurize\\ndata, featurization processes can be automated by data engineers in\\nthe transformation stage of a data pipeline.\\nTransformation is a profound subject, and we cannot do it justice in\\nthis brief introduction. Chapter 8 delves into queries, data modeling,\\nand various transformation practices and nuances.\\nServing Data\\nYou’ve reached the last stage of the data engineering lifecycle. Now\\nthat the data has been ingested, stored, and transformed into\\ncoherent and useful structures, it’s time to get value from your data.\\n“Getting value” from data means different things to different users.\\nData has value when it’s used for practical purposes. Data that is not\\nconsumed or queried is simply inert. Data vanity projects are a major\\nrisk for companies. Many companies pursued vanity projects in the\\nbig data era, gathering massive datasets in data lakes that were\\nnever consumed in any useful way. The cloud era is triggering a new\\nwave of vanity projects built on the latest data warehouses, object\\nstorage systems, and streaming technologies. Data projects must be\\nintentional across the lifecycle. What is the ultimate business\\npurpose of the data so carefully collected, cleaned, and stored?\\nData serving is perhaps the most exciting part of the data\\nengineering lifecycle. This is where the magic happens. This is\\nwhere ML engineers can apply the most advanced techniques. Let’s\\nlook at some of the popular uses of data: analytics, ML, and reverse\\nETL.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='306a5a5f-39d5-41fe-b61a-c4c8b32c9a5b', embedding=None, metadata={'page_label': '80', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Analytics\\nAnalytics is the core of most data endeavors. Once your data is\\nstored and transformed, you’re ready to generate reports or\\ndashboards, and do ad hoc analysis on the data. Whereas the bulk\\nof analytics used to encompass BI, it now includes other facets such\\nas operational analytics and customer-facing analytics (Figure 2-5).\\nLet’s briefly touch on these variations of analytics.\\nFigure 2-5. Types of analytics\\nBusiness intelligence\\nBI marshals collected data to describe a business’s past and current\\nstate. BI requires using business logic to process raw data. Note that\\ndata serving for analytics is yet another area where the stages of the\\ndata engineering lifecycle can get tangled. As we mentioned earlier,\\nbusiness logic is often applied to data in the transformation stage of\\nthe data engineering lifecycle, but a logic-on-read approach has\\nbecome increasingly popular. Data is stored in a clean but fairly raw\\nform, with minimal postprocessing business logic. A BI system\\nmaintains a repository of business logic and definitions. This\\nbusiness logic is used to query the data warehouse so that reports\\nand dashboards align with business definitions.\\nAs a company grows its data maturity, it will move from ad hoc data\\nanalysis to self-service analytics, allowing democratized data access\\nto business users without needing IT to intervene. The capability to\\ndo self-service analytics assumes that data is good enough that', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5a906aca-6420-476f-b08f-e0cb2e098945', embedding=None, metadata={'page_label': '81', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='people across the organization can simply access it themselves,\\nslice and dice it however they choose, and get immediate insights.\\nAlthough self-service analytics is simple in theory, it’s tough to pull off\\nin practice. The main reason is that poor data quality, organizational\\nsilos, and a lack of adequate data skills get in the way of allowing\\nwidespread use of analytics.\\nOperational analytics\\nOperational analytics focuses on the fine-grained details of\\noperations, promoting actions that a user of the reports can act upon\\nimmediately. Operational analytics could be a live view of inventory\\nor real-time dashboarding of website health. In this case, data is\\nconsumed in real time, either directly from a source system or from a\\nstreaming data pipeline. The types of insights in operational analytics\\ndiffer from traditional BI since operational analytics is focused on the\\npresent and doesn’t necessarily concern historical trends.\\nEmbedded analytics\\nYou may wonder why we’ve broken out embedded analytics\\n(customer-facing analytics) separately from BI. In practice, analytics\\nprovided to customers on a SaaS platform come with a separate set\\nof requirements and complications. Internal BI faces a limited\\naudience and generally presents a limited number of unified views.\\nAccess controls are critical but not particularly complicated. Access\\nis managed using a handful of roles and access tiers.\\nWith customer-facing analytics, the request rate for reports, and the\\ncorresponding burden on analytics systems, go up dramatically;\\naccess control is significantly more complicated and critical.\\nBusinesses may be serving separate analytics and data to\\nthousands or more customers. Each customer must see their data\\nand only their data. An internal data-access error at a company\\nwould likely lead to a procedural review. A data leak between\\ncustomers would be considered a massive breach of trust, leading to\\nmedia attention and a significant loss of customers. Minimize your', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6bb2214a-01a2-422c-9322-c9b1630a3547', embedding=None, metadata={'page_label': '82', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='blast radius related to data leaks and security vulnerabilities. Apply\\ntenant- or data-level security within your storage, and anywhere\\nthere’s a possibility of data leakage.\\nMULTITENANCY\\nMany current storage and analytics systems support\\nmultitenancy in various ways. Data engineers may choose to\\nhouse data for many customers in common tables to allow a\\nunified view for internal analytics and ML. This data is presented\\nexternally to individual customers through logical views with\\nappropriately defined controls and filters. It is incumbent on data\\nengineers to understand the minutiae of multitenancy in the\\nsystems they deploy to ensure absolute data security and\\nisolation.\\nMachine learning\\nThe emergence and success of ML is one of the most exciting\\ntechnology revolutions. Once organizations reach a high level of\\ndata maturity, they can begin to identify problems amenable to ML\\nand start organizing a practice around it.\\nThe responsibilities of data engineers overlap significantly in\\nanalytics and ML, and the boundaries between data engineering, ML\\nengineering, and analytics engineering can be fuzzy. For example, a\\ndata engineer may need to support Spark clusters that facilitate\\nanalytics pipelines and ML model training. They may also need to\\nprovide a system that orchestrates tasks across teams and support\\nmetadata and cataloging systems that track data history and lineage.\\nSetting these domains of responsibility and the relevant reporting\\nstructures is a critical organizational decision.\\nThe feature store is a recently developed tool that combines data\\nengineering and ML engineering. Feature stores are designed to\\nreduce the operational burden for ML engineers by maintaining', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97b08ff7-781b-49da-8a2a-1132ede03072', embedding=None, metadata={'page_label': '83', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='feature history and versions, supporting feature sharing among\\nteams, and providing basic operational and orchestration\\ncapabilities, such as backfilling. In practice, data engineers are part\\nof the core support team for feature stores to support ML\\nengineering.\\nShould a data engineer be familiar with ML? It certainly helps.\\nRegardless of the operational boundary between data engineering,\\nML engineering, business analytics, and so forth, data engineers\\nshould maintain operational knowledge about their teams. A good\\ndata engineer is conversant in the fundamental ML techniques and\\nrelated data-processing requirements, the use cases for models\\nwithin their company, and the responsibilities of the organization’s\\nvarious analytics teams. This helps maintain efficient communication\\nand facilitate collaboration. Ideally, data engineers will build tools in\\npartnership with other teams that neither team can make\\nindependently.\\nThis book cannot possibly cover ML in depth. A growing ecosystem\\nof books, videos, articles, and communities is available if you’re\\ninterested in learning more; we include a few suggestions in\\n“Additional Resources”.\\nThe following are some considerations for the serving data phase\\nspecific to ML:\\nIs the data of sufficient quality to perform reliable feature\\nengineering? Quality requirements and assessments are\\ndeveloped in close collaboration with teams consuming the\\ndata.\\nIs the data discoverable? Can data scientists and ML engineers\\neasily find valuable data?\\nWhere are the technical and organizational boundaries between\\ndata engineering and ML engineering? This organizational\\nquestion has significant architectural implications.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a6591e92-758d-4e49-ba81-b35dfd731004', embedding=None, metadata={'page_label': '84', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Does the dataset properly represent ground truth? Is it unfairly\\nbiased?\\nWhile ML is exciting, our experience is that companies often\\nprematurely dive into it. Before investing a ton of resources into ML,\\ntake the time to build a solid data foundation. This means setting up\\nthe best systems and architecture across the data engineering and\\nML lifecycle. It’s generally best to develop competence in analytics\\nbefore moving to ML. Many companies have dashed their ML\\ndreams because they undertook initiatives without appropriate\\nfoundations.\\nReverse ETL\\nReverse ETL has long been a practical reality in data, viewed as an\\nantipattern that we didn’t like to talk about or dignify with a name.\\nReverse ETL takes processed data from the output side of the data\\nengineering lifecycle and feeds it back into source systems, as\\nshown in Figure 2-6. In reality, this flow is beneficial and often\\nnecessary; reverse ETL allows us to take analytics, scored models,\\netc., and feed these back into production systems or SaaS platforms.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0547d1d6-7d9e-402e-9133-0fad5860367c', embedding=None, metadata={'page_label': '85', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 2-6. Reverse ETL\\nMarketing analysts might calculate bids in Microsoft Excel by using\\nthe data in their data warehouse, and then upload these bids to\\nGoogle Ads. This process was often entirely manual and primitive.\\nAs we’ve written this book, several vendors have embraced the\\nconcept of reverse ETL and built products around it, such as\\nHightouch and Census. Reverse ETL remains nascent as a field, but\\nwe suspect that it is here to stay.\\nReverse ETL has become especially important as businesses rely\\nincreasingly on SaaS and external platforms. For example,\\ncompanies may want to push specific metrics from their data\\nwarehouse to a customer data platform or CRM system. Advertising\\nplatforms are another everyday use case, as in the Google Ads\\nexample. Expect to see more activity in reverse ETL, with an overlap\\nin both data engineering and ML engineering.\\nThe jury is out on whether the term reverse ETL will stick. And the\\npractice may evolve. Some engineers claim that we can eliminate\\nreverse ETL by handling data transformations in an event stream', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f5ac6e60-6456-4a62-a1b9-b9801c7d65b1', embedding=None, metadata={'page_label': '86', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and sending those events back to source systems as needed.\\nRealizing widespread adoption of this pattern across businesses is\\nanother matter. The gist is that transformed data will need to be\\nreturned to source systems in some manner, ideally with the correct\\nlineage and business process associated with the source system.\\nMajor Undercurrents Across the Data\\nEngineering Lifecycle\\nData engineering is rapidly maturing. Whereas prior cycles of data\\nengineering simply focused on the technology layer, the continued\\nabstraction and simplification of tools and practices have shifted this\\nfocus. Data engineering now encompasses far more than tools and\\ntechnology. The field is now moving up the value chain, incorporating\\ntraditional enterprise practices such as data management and cost\\noptimization, and newer practices like DataOps.\\nWe’ve termed these practices undercurrents—security, data\\nmanagement, DataOps, data architecture, orchestration, and\\nsoftware engineering—that support every aspect of the data\\nengineering lifecycle (Figure 2-7). In this section, we give a brief\\noverview of these undercurrents and their major components, which\\nyou’ll see in more detail throughout the book.\\nFigure 2-7. The major undercurrents of data engineering\\nSecurity', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e12e00a4-1d49-4215-b5d1-d7ce175b9472', embedding=None, metadata={'page_label': '87', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Security must be top of mind for data engineers, and those who\\nignore it do so at their peril. That’s why security is the first\\nundercurrent. Data engineers must understand both data and access\\nsecurity, exercising the principle of least privilege. The principle of\\nleast privilege means giving a user or system access to only the\\nessential data and resources to perform an intended function. A\\ncommon antipattern we see with data engineers with little security\\nexperience is to give admin access to all users. This is a catastrophe\\nwaiting to happen!\\nGive users only the access they need to do their jobs today, nothing\\nmore. Don’t operate from a root shell when you’re just looking for\\nvisible files with standard user access. When querying tables with a\\nlesser role, don’t use the superuser role in a database. Imposing the\\nprinciple of least privilege on ourselves can prevent accidental\\ndamage and keep you in a security-first mindset.\\nPeople and organizational structure are always the biggest security\\nvulnerabilities in any company. When we hear about major security\\nbreaches in the media, it often turns out that someone in the\\ncompany ignored basic precautions, fell victim to a phishing attack,\\nor otherwise acted irresponsibly. The first line of defense for data\\nsecurity is to create a culture of security that permeates the\\norganization. All individuals who have access to data must\\nunderstand their responsibility in protecting the company’s sensitive\\ndata and its customers.\\nData security is also about timing—providing data access to exactly\\nthe people and systems that need to access it and only for the\\nduration necessary to perform their work. Data should be protected\\nfrom unwanted visibility, both in flight and at rest, by using\\nencryption, tokenization, data masking, obfuscation, and simple,\\nrobust access controls.\\nData engineers must be competent security administrators, as\\nsecurity falls in their domain. A data engineer should understand', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5d4dfc0-e5e9-4d92-9b8b-8b661b3618db', embedding=None, metadata={'page_label': '88', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='security best practices for the cloud and on prem. Knowledge of user\\nand identity access management (IAM) roles, policies, groups,\\nnetwork security, password policies, and encryption are good places\\nto start.\\nThroughout the book, we highlight areas where security should be\\ntop of mind in the data engineering lifecycle. You can also gain more\\ndetailed insights into security in Chapter 10.\\nData Management\\nYou probably think that data management sounds very…corporate.\\n“Old school” data management practices make their way into data\\nand ML engineering. What’s old is new again. Data management\\nhas been around for decades but didn’t get a lot of traction in data\\nengineering until recently. Data tools are becoming simpler, and\\nthere is less complexity for data engineers to manage. As a result,\\nthe data engineer moves up the value chain toward the next rung of\\nbest practices. Data best practices once reserved for huge\\ncompanies—data governance, master data management, data-\\nquality management, metadata management—are now filtering\\ndown to companies of all sizes and maturity levels. As we like to say,\\ndata engineering is becoming “enterprisey.” This is ultimately a great\\nthing!\\nThe Data Management Association International (DAMA) Data\\nManagement Body of Knowledge (DMBOK), which we consider to\\nbe the definitive book for enterprise data management, offers this\\ndefinition:\\nData management is the development, execution, and supervision\\nof plans, policies, programs, and practices that deliver, control,\\nprotect, and enhance the value of data and information assets\\nthroughout their lifecycle.\\nThat’s a bit lengthy, so let’s look at how it ties to data engineering.\\nData engineers manage the data lifecycle, and data management', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b257357-a207-4db1-bc63-eb1f42cca854', embedding=None, metadata={'page_label': '89', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='encompasses the set of best practices that data engineers will use to\\naccomplish this task, both technically and strategically. Without a\\nframework for managing data, data engineers are simply technicians\\noperating in a vacuum. Data engineers need a broader perspective\\nof data’s utility across the organization, from the source systems to\\nthe C-suite, and everywhere in between.\\nWhy is data management important? Data management\\ndemonstrates that data is vital to daily operations, just as businesses\\nview financial resources, finished goods, or real estate as assets.\\nData management practices form a cohesive framework that\\neveryone can adopt to ensure that the organization gets value from\\ndata and handles it appropriately.\\nData management has quite a few facets, including the following:\\nData governance, including discoverability and accountability\\nData modeling and design\\nData lineage\\nStorage and operations\\nData integration and interoperability\\nData lifecycle management\\nData systems for advanced analytics and ML\\nEthics and privacy\\nWhile this book is in no way an exhaustive resource on data\\nmanagement, let’s briefly cover some salient points from each area\\nas they relate to data engineering.\\nData governance\\nAccording to Data Governance: The Definitive Guide, “Data\\ngovernance is, first and foremost, a data management function to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e06283e7-27ce-4669-ad48-dd0d7cd0a8d5', embedding=None, metadata={'page_label': '90', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ensure the quality, integrity, security, and usability of the data\\ncollected by an organization.”\\nWe can expand on that definition and say that data governance\\nengages people, processes, and technologies to maximize data\\nvalue across an organization while protecting data with appropriate\\nsecurity controls. Effective data governance is developed with\\nintention and supported by the organization. When data governance\\nis accidental and haphazard, the side effects can range from\\nuntrusted data to security breaches and everything in between.\\nBeing intentional about data governance will maximize the\\norganization’s data capabilities and the value generated from data. It\\nwill also (hopefully) keep a company out of the headlines for\\nquestionable or downright reckless data practices.\\nThink of the typical example of data governance being done poorly.\\nA business analyst gets a request for a report but doesn’t know what\\ndata to use to answer the question. They may spend hours digging\\nthrough dozens of tables in a transactional database, wildly guessing\\nat which fields might be useful. The analyst compiles a “directionally\\ncorrect” report but isn’t entirely sure that the report’s underlying data\\nis accurate or sound. The recipient of the report also questions the\\nvalidity of the data. The integrity of the analyst—and of all data in the\\ncompany’s systems—is called into question. The company is\\nconfused about its performance, making business planning\\nimpossible.\\nData governance is a foundation for data-driven business practices\\nand a mission-critical part of the data engineering lifecycle. When\\ndata governance is practiced well, people, processes, and\\ntechnologies align to treat data as a key business driver; if data\\nissues occur, they are promptly handled.\\nThe core categories of data governance are discoverability, security,\\nand accountability.  Within these core categories are subcategories,\\n1\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c29b3166-ad25-4236-a0b1-cc5002fa856f', embedding=None, metadata={'page_label': '91', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='such as data quality, metadata, and privacy. Let’s look at each core\\ncategory in turn.\\nDiscoverability\\nIn a data-driven company, data must be available and discoverable.\\nEnd users should have quick and reliable access to the data they\\nneed to do their jobs. They should know where the data comes from,\\nhow it relates to other data, and what the data means.\\nSome key areas of data discoverability include metadata\\nmanagement and master data management. Let’s briefly describe\\nthese areas.\\nMetadata\\nMetadata is “data about data,” and it underpins every section of the\\ndata engineering lifecycle. Metadata is exactly the data needed to\\nmake data discoverable and governable.\\nWe divide metadata into two major categories: autogenerated and\\nhuman generated. Modern data engineering revolves around\\nautomation, but metadata collection is often manual and error prone.\\nTechnology can assist with this process, removing much of the error-\\nprone work of manual metadata collection. We’re seeing a\\nproliferation of data catalogs, data-lineage tracking systems, and\\nmetadata management tools. Tools can crawl databases to look for\\nrelationships and monitor data pipelines to track where data comes\\nfrom and where it goes. A low-fidelity manual approach uses an\\ninternally led effort where various stakeholders crowdsource\\nmetadata collection within the organization. These data management\\ntools are covered in depth throughout the book, as they undercut\\nmuch of the data engineering lifecycle.\\nMetadata becomes a byproduct of data and data processes.\\nHowever, key challenges remain. In particular, interoperability and\\nstandards are still lacking. Metadata tools are only as good as their\\nconnectors to data systems and their ability to share metadata. In', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ee850236-52dc-4d05-b25f-6ba4946d4732', embedding=None, metadata={'page_label': '92', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='addition, automated metadata tools should not entirely take humans\\nout of the loop.\\nData has a social element; each organization accumulates social\\ncapital and knowledge around processes, datasets, and pipelines.\\nHuman-oriented metadata systems focus on the social aspect of\\nmetadata. This is something that Airbnb has emphasized in its\\nvarious blog posts on data tools, particularly its original Dataportal\\nconcept. Such tools should provide a place to disclose data owners,\\ndata consumers, and domain experts. Documentation and internal\\nwiki tools provide a key foundation for metadata management, but\\nthese tools should also integrate with automated data cataloging. For\\nexample, data-scanning tools can generate wiki pages with links to\\nrelevant data objects.\\nOnce metadata systems and processes exist, data engineers can\\nconsume metadata in useful ways. Metadata becomes a foundation\\nfor designing pipelines and managing data throughout the lifecycle.\\nDMBOK identifies four main categories of metadata that are useful to\\ndata engineers:\\nBusiness metadata\\nTechnical metadata\\nOperational metadata\\nReference metadata\\nLet’s briefly describe each category of metadata.\\nBusiness metadata relates to the way data is used in the business,\\nincluding business and data definitions, data rules and logic, how\\nand where data is used, and the data owner(s).\\nA data engineer uses business metadata to answer nontechnical\\nquestions about who, what, where, and how. For example, a data\\nengineer may be tasked with creating a data pipeline for customer\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f88fd13f-2147-4456-96fb-173058f81995', embedding=None, metadata={'page_label': '93', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='sales analysis. But what is a customer? Is it someone who’s\\npurchased in the last 90 days? Or someone who’s purchased at any\\ntime the business has been open? A data engineer would use the\\ncorrect data to refer to business metadata (data dictionary or data\\ncatalog) to look up how a “customer” is defined. Business metadata\\nprovides a data engineer with the right context and definitions to\\nproperly use data.\\nTechnical metadata describes the data created and used by systems\\nacross the data engineering lifecycle. It includes the data model and\\nschema, data lineage, field mappings, and pipeline workflows. A\\ndata engineer uses technical metadata to create, connect, and\\nmonitor various systems across the data engineering lifecycle.\\nHere are some common types of technical metadata that a data\\nengineer will use:\\nPipeline metadata (often produced in orchestration systems)\\nData lineage\\nSchema\\nOrchestration is a central hub that coordinates workflow across\\nvarious systems. Pipeline metadata captured in orchestration\\nsystems provides details of the workflow schedule, system and data\\ndependencies, configurations, connection details, and much more.\\nData-lineage metadata tracks the origin and changes to data, and its\\ndependencies, over time. As data flows through the data engineering\\nlifecycle, it evolves through transformations and combinations with\\nother data. Data lineage provides an audit trail of data’s evolution as\\nit moves through various systems and workflows.\\nSchema metadata describes the structure of data stored in a system\\nsuch as a database, a data warehouse, a data lake, or a filesystem;\\nit is one of the key differentiators across different storage systems.\\nObject stores, for example, don’t manage schema metadata;', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='204fdae0-e471-4603-be55-529e63e95cd4', embedding=None, metadata={'page_label': '94', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='instead, this must be managed in a metastore. On the other hand,\\ncloud data warehouses manage schema metadata internally.\\nThese are just a few examples of technical metadata that a data\\nengineer should know about. This is not a complete list, and we\\ncover additional aspects of technical metadata throughout the book.\\nOperational metadata describes the operational results of various\\nsystems and includes statistics about processes, job IDs, application\\nruntime logs, data used in a process, and error logs. A data engineer\\nuses operational metadata to determine whether a process\\nsucceeded or failed and the data involved in the process.\\nOrchestration systems can provide a limited picture of operational\\nmetadata, but the latter still tends to be scattered across many\\nsystems. A need for better-quality operational metadata, and better\\nmetadata management, is a major motivation for next-generation\\norchestration and metadata management systems.\\nReference metadata is data used to classify other data. This is also\\nreferred to as lookup data. Standard examples of reference data are\\ninternal codes, geographic codes, units of measurement, and\\ninternal calendar standards. Note that much of reference data is fully\\nmanaged internally, but items such as geographic codes might come\\nfrom standard external references. Reference data is essentially a\\nstandard for interpreting other data, so if it changes, this change\\nhappens slowly over time.\\nData accountability\\nData accountability means assigning an individual to govern a\\nportion of data. The responsible person then coordinates the\\ngovernance activities of other stakeholders. Managing data quality is\\ntough if no one is accountable for the data in question.\\nNote that people accountable for data need not be data engineers.\\nThe accountable person might be a software engineer or product\\nmanager, or serve in another role. In addition, the responsible', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc95fa2c-4c3e-4da4-9f50-f13c7cdaa040', embedding=None, metadata={'page_label': '95', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='person generally doesn’t have all the resources necessary to\\nmaintain data quality. Instead, they coordinate with all people who\\ntouch the data, including data engineers.\\nData accountability can happen at various levels; accountability can\\nhappen at the level of a table or a log stream but could be as fine-\\ngrained as a single field entity that occurs across many tables. An\\nindividual may be accountable for managing a customer ID across\\nmany systems. For enterprise data management, a data domain is\\nthe set of all possible values that can occur for a given field type,\\nsuch as in this ID example. This may seem excessively bureaucratic\\nand meticulous, but it can significantly affect data quality.\\nData quality\\nCan I trust this data?\\n—Everyone in the business\\nData quality is the optimization of data toward the desired state and\\norbits the question, “What do you get compared with what you\\nexpect?” Data should conform to the expectations in the business\\nmetadata. Does the data match the definition agreed upon by the\\nbusiness?\\nA data engineer ensures data quality across the entire data\\nengineering lifecycle. This involves performing data-quality tests, and\\nensuring data conformance to schema expectations, data\\ncompleteness, and precision.\\nAccording to Data Governance: The Definitive Guide, data quality is\\ndefined by three main characteristics:\\nAccuracy\\nIs the collected data factually correct? Are there duplicate\\nvalues? Are the numeric values accurate?\\nCompleteness\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d4a9fa2d-28bc-4262-a01a-01f225399509', embedding=None, metadata={'page_label': '96', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Are the records complete? Do all required fields contain valid\\nvalues?\\nTimeliness\\nAre records available in a timely fashion?\\nEach of these characteristics is quite nuanced. For example, how do\\nwe think about bots and web scrapers when dealing with web event\\ndata? If we intend to analyze the customer journey, we must have a\\nprocess that lets us separate humans from machine-generated\\ntraffic. Any bot-generated events misclassified as human present\\ndata accuracy issues, and vice versa.\\nA variety of interesting problems arise concerning completeness and\\ntimeliness. In the Google paper introducing the Dataflow model, the\\nauthors give the example of an offline video platform that displays\\nads. The platform downloads video and ads while a connection is\\npresent, allows the user to watch these while offline, and then\\nuploads ad view data once a connection is present again. This data\\nmay arrive late, well after the ads are watched. How does the\\nplatform handle billing for the ads?\\nFundamentally, this problem can’t be solved by purely technical\\nmeans. Rather, engineers will need to determine their standards for\\nlate-arriving data and enforce these uniformly, possibly with the help\\nof various technology tools.\\nData quality sits across the boundary of human and technology\\nproblems. Data engineers need robust processes to collect\\nactionable human feedback on data quality and use technology tools\\nto detect quality issues preemptively before downstream users ever\\nsee them. We cover these collection processes in the appropriate\\nchapters throughout this book.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b1f3028e-8701-403a-b2f2-d2254ee65bc6', embedding=None, metadata={'page_label': '97', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='MASTER DATA MANAGEMENT\\nMaster data is data about business entities such as employees,\\ncustomers, products, and locations. As organizations grow larger\\nand more complex through organic growth and acquisitions, and\\ncollaborate with other businesses, maintaining a consistent\\npicture of entities and identities becomes more and more\\nchallenging.\\nMaster data management (MDM) is the practice of building\\nconsistent entity definitions known as golden records. Golden\\nrecords harmonize entity data across an organization and with its\\npartners. MDM is a business operations process facilitated by\\nbuilding and deploying technology tools. For example, an MDM\\nteam might determine a standard format for addresses, and then\\nwork with data engineers to build an API to return consistent\\naddresses and a system that uses address data to match\\ncustomer records across company divisions.\\nMDM reaches across the full data cycle into operational\\ndatabases. It may fall directly under the purview of data\\nengineering, but is often the assigned responsibility of a\\ndedicated team that works across the organization. Even if they\\ndon’t own MDM, data engineers must always be aware of it, as\\nthey will collaborate on MDM initiatives.\\nData modeling and design\\nTo derive business insights from data, through business analytics\\nand data science, the data must be in a usable form. The process for\\nconverting data into a usable form is known as data modeling and\\ndesign. Whereas we traditionally think of data modeling as a problem\\nfor database administrators (DBAs) and ETL developers, data\\nmodeling can happen almost anywhere in an organization. Firmware\\nengineers develop the data format of a record for an IoT device, or', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92ffaa2a-97c6-4ffb-9cd3-47f8bb17d16a', embedding=None, metadata={'page_label': '98', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='web application developers design the JSON response to an API call\\nor a MySQL table schema—these are all instances of data modeling\\nand design.\\nData modeling has become more challenging because of the variety\\nof new data sources and use cases. For instance, strict\\nnormalization doesn’t work well with event data. Fortunately, a new\\ngeneration of data tools increases the flexibility of data models, while\\nretaining logical separations of measures, dimensions, attributes,\\nand hierarchies. Cloud data warehouses support the ingestion of\\nenormous quantities of denormalized and semistructured data, while\\nstill supporting common data modeling patterns, such as Kimball,\\nInmon, and data vault. Data processing frameworks such as Spark\\ncan ingest a whole spectrum of data, from flat structured relational\\nrecords to raw unstructured text. We discuss these data modeling\\nand transformation patterns in greater detail in Chapter 8.\\nWith the wide variety of data that engineers must cope with, there is\\na temptation to throw up our hands and give up on data modeling.\\nThis is a terrible idea with harrowing consequences, made evident\\nwhen people murmur of the write once, read never (WORN) access\\npattern or refer to a data swamp. Data engineers need to understand\\nmodeling best practices as well as develop the flexibility to apply the\\nappropriate level and type of modeling to the data source and use\\ncase.\\nData lineage\\nAs data moves through its lifecycle, how do you know what system\\naffected the data or what the data is composed of as it gets passed\\naround and transformed? Data lineage describes the recording of an\\naudit trail of data through its lifecycle, tracking both the systems that\\nprocess the data and the upstream data it depends on.\\nData lineage helps with error tracking, accountability, and debugging\\nof data and the systems that process it. It has the obvious benefit of\\ngiving an audit trail for the data lifecycle and helps with compliance.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f364b87b-f72f-46b3-b421-bc5f0669efe3', embedding=None, metadata={'page_label': '99', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='For example, if a user would like their data deleted from your\\nsystems, having lineage for that data lets you know where that data\\nis stored and its dependencies.\\nData lineage has been around for a long time in larger companies\\nwith strict compliance standards. However, it’s now being more\\nwidely adopted in smaller companies as data management becomes\\nmainstream. We also note that Andy Petrella’s concept of Data\\nObservability Driven Development (DODD) is closely related to data\\nlineage. DODD observes data all along its lineage. This process is\\napplied during development, testing, and finally production to deliver\\nquality and conformity to expectations.\\nData integration and interoperability\\nData integration and interoperability is the process of integrating data\\nacross tools and processes. As we move away from a single-stack\\napproach to analytics and toward a heterogeneous cloud\\nenvironment in which various tools process data on demand,\\nintegration and interoperability occupy an ever-widening swath of the\\ndata engineer’s job.\\nIncreasingly, integration happens through general-purpose APIs\\nrather than custom database connections. For example, a data\\npipeline might pull data from the Salesforce API, store it to Amazon\\nS3, call the Snowflake API to load it into a table, call the API again to\\nrun a query, and then export the results to S3 where Spark can\\nconsume them.\\nAll of this activity can be managed with relatively simple Python code\\nthat talks to data systems rather than handling data directly. While\\nthe complexity of interacting with data systems has decreased, the\\nnumber of systems and the complexity of pipelines has dramatically\\nincreased. Engineers starting from scratch quickly outgrow the\\ncapabilities of bespoke scripting and stumble into the need for\\norchestration. Orchestration is one of our undercurrents, and we\\ndiscuss it in detail in “Orchestration”.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='49e4fa87-488e-458b-a7f0-457cbf4895aa', embedding=None, metadata={'page_label': '100', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data lifecycle management\\nThe advent of data lakes encouraged organizations to ignore data\\narchival and destruction. Why discard data when you can simply add\\nmore storage ad infinitum? Two changes have encouraged\\nengineers to pay more attention to what happens at the end of the\\ndata engineering lifecycle.\\nFirst, data is increasingly stored in the cloud. This means we have\\npay-as-you-go storage costs instead of large up-front capital\\nexpenditures for an on-premises data lake. When every byte shows\\nup on a monthly AWS statement, CFOs see opportunities for\\nsavings. Cloud environments make data archival a relatively\\nstraightforward process. Major cloud vendors offer archival-specific\\nobject storage classes that allow long-term data retention at an\\nextremely low cost, assuming very infrequent access (it should be\\nnoted that data retrieval isn’t so cheap, but that’s for another\\nconversation). These storage classes also support extra policy\\ncontrols to prevent accidental or deliberate deletion of critical\\narchives.\\nSecond, privacy and data retention laws such as the GDPR and the\\nCCPA require data engineers to actively manage data destruction to\\nrespect users’ “right to be forgotten.” Data engineers must know\\nwhat consumer data they retain and must have procedures to\\ndestroy data in response to requests and compliance requirements.\\nData destruction is straightforward in a cloud data warehouse. SQL\\nsemantics allow deletion of rows conforming to a where clause.\\nData destruction was more challenging in data lakes, where write-\\nonce, read-many was the default storage pattern. Tools such as Hive\\nACID and Delta Lake allow easy management of deletion\\ntransactions at scale. New generations of metadata management,\\ndata lineage, and cataloging tools will also streamline the end of the\\ndata engineering lifecycle.\\nEthics and privacy', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='05cf7084-364c-4b12-9498-d3be6061f9fd', embedding=None, metadata={'page_label': '101', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ethical behavior is doing the right thing when no one else is\\nwatching.\\n—Aldo Leopold\\nThe last several years of data breaches, misinformation, and\\nmishandling of data make one thing clear: data impacts people. Data\\nused to live in the Wild West, freely collected and traded like\\nbaseball cards. Those days are long gone. Whereas data’s ethical\\nand privacy implications were once considered nice to have, like\\nsecurity, they’re now central to the general data lifecycle. Data\\nengineers need to do the right thing when no one else is watching,\\nbecause everyone will be watching someday. We hope that more\\norganizations will encourage a culture of good data ethics and\\nprivacy.\\nHow do ethics and privacy impact the data engineering lifecycle?\\nData engineers need to ensure that datasets mask personally\\nidentifiable information (PII) and other sensitive information; bias can\\nbe identified and tracked in datasets as they are transformed.\\nRegulatory requirements and compliance penalties are only growing.\\nEnsure that your data assets are compliant with a growing number of\\ndata regulations, such as GDPR and CCPA. Please take this\\nseriously. We offer tips throughout the book to ensure that you’re\\nbaking ethics and privacy into the data engineering lifecycle.\\nOrchestration\\nWe think that orchestration matters because we view it as really\\nthe center of gravity of both the data platform as well as the data\\nlifecycle, the software development lifecycle as it comes to data.\\n—Nick Schrock, founder of Elementl\\nOrchestration is not only a central DataOps process, but also a\\ncritical part of the engineering and deployment flow for data jobs. So,\\nwhat is orchestration?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='be52a6a8-d5a9-4052-b37c-3dc89e6ee245', embedding=None, metadata={'page_label': '102', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Orchestration is the process of coordinating many jobs to run as\\nquickly and efficiently as possible on a scheduled cadence. For\\ninstance, people often refer to orchestration tools like Apache Airflow\\nas schedulers. This isn’t quite accurate. A pure scheduler, such as\\ncron, is aware only of time; an orchestration engine builds in\\nmetadata on job dependencies, generally in the form of a directed\\nacyclic graph (DAG). The DAG can be run once or scheduled to run\\nat a fixed interval of daily, weekly, every hour, every five minutes, etc.\\nAs we discuss orchestration throughout this book, we assume that\\nan orchestration system stays online with high availability. This\\nallows the orchestration system to sense and monitor constantly\\nwithout human intervention and run new jobs anytime they are\\ndeployed. An orchestration system monitors jobs that it manages\\nand kicks off new tasks as internal DAG dependencies are\\ncompleted. It can also monitor external systems and tools to watch\\nfor data to arrive and criteria to be met. When certain conditions go\\nout of bounds, the system also sets error conditions and sends alerts\\nthrough email or other channels. You might set an expected\\ncompletion time of 10 a.m. for overnight daily data pipelines. If jobs\\nare not done by this time, alerts go out to data engineers and\\nconsumers.\\nOrchestration systems also build job history capabilities,\\nvisualization, and alerting. Advanced orchestration engines can\\nbackfill new DAGs or individual tasks as they are added to a DAG.\\nThey also support dependencies over a time range. For example, a\\nmonthly reporting job might check that an ETL job has been\\ncompleted for the full month before starting.\\nOrchestration has long been a key capability for data processing but\\nwas not often top of mind nor accessible to anyone except the\\nlargest companies. Enterprises used various tools to manage job\\nflows, but these were expensive, out of reach of small startups, and\\ngenerally not extensible. Apache Oozie was extremely popular in the\\n2010s, but it was designed to work within a Hadoop cluster and was', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b1b53216-a956-4568-91bb-427937abc732', embedding=None, metadata={'page_label': '103', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='difficult to use in a more heterogeneous environment. Facebook\\ndeveloped Dataswarm for internal use in the late 2000s; this inspired\\npopular tools such as Airflow, introduced by Airbnb in 2014.\\nAirflow was open source from its inception, and was widely adopted.\\nIt was written in Python, making it highly extensible to almost any\\nuse case imaginable. While many other interesting open source\\norchestration projects exist, such as Luigi and Conductor, Airflow is\\narguably the mindshare leader for the time being. Airflow arrived just\\nas data processing was becoming more abstract and accessible,\\nand engineers were increasingly interested in coordinating complex\\nflows across multiple processors and storage systems, especially in\\ncloud environments.\\nAt this writing, several nascent open source projects aim to mimic\\nthe best elements of Airflow’s core design while improving on it in\\nkey areas. Some of the most interesting examples are Prefect and\\nDagster, which aim to improve the portability and testability of DAGs\\nto allow engineers to move from local development to production\\nmore easily. Argo is an orchestration engine built around Kubernetes\\nprimitives; Metaflow is an open source project out of Netflix that aims\\nto improve data science orchestration.\\nWe must point out that orchestration is strictly a batch concept. The\\nstreaming alternative to orchestrated task DAGs is the streaming\\nDAG. Streaming DAGs remain challenging to build and maintain, but\\nnext-generation streaming platforms such as Pulsar aim to\\ndramatically reduce the engineering and operational burden. We talk\\nmore about these developments Chapter 8.\\nDataOps\\nDataOps maps the best practices of Agile methodology, DevOps,\\nand statistical process control (SPC) to data. Whereas DevOps aims\\nto improve the release and quality of software products, DataOps\\ndoes the same thing for data products.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97cdd7c8-3967-4218-9f24-60acc2b2e46c', embedding=None, metadata={'page_label': '104', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data products differ from software products because of the way data\\nis used. A software product provides specific functionality and\\ntechnical features for end users. By contrast, a data product is built\\naround sound business logic and metrics, whose users make\\ndecisions or build models that perform automated actions. A data\\nengineer must understand both the technical aspects of building\\nsoftware products, and the business logic, quality, and metrics that\\nwill create excellent data products.\\nLike DevOps, DataOps borrows much from lean manufacturing and\\nsupply chain management, mixing people, processes, and\\ntechnology to reduce time to value. As Data Kitchen (experts in\\nDataOps) describes it:\\nDataOps is a collection of technical practices, workflows, cultural\\nnorms, and architectural patterns that enable:\\nRapid innovation and experimentation delivering new insights\\nto customers with increasing velocity\\nExtremely high data quality and very low error rates\\nCollaboration across complex arrays of people, technology,\\nand environments\\nClear measurement, monitoring, and transparency of results\\nLean practices (such as lead time reduction and minimizing defects)\\nand the resulting improvements to quality and productivity are things\\nwe are glad to see gaining momentum both in software and data\\noperations.\\nFirst and foremost, DataOps is a set of cultural habits; the data\\nengineering team needs to adopt a cycle of communicating and\\ncollaborating with the business, breaking down silos, continuously\\nlearning from successes and mistakes, and rapid iteration. Only\\nwhen these cultural habits are set in place can the team get the best\\nresults from technology and tools.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7392de13-45b1-46c2-b237-7a474a44d2be', embedding=None, metadata={'page_label': '105', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Depending on a company’s data maturity, a data engineer has some\\noptions to build DataOps into the fabric of the overall data\\nengineering lifecycle. If the company has no preexisting data\\ninfrastructure or practices, DataOps is very much a greenfield\\nopportunity that can be baked in from day one. With an existing\\nproject or infrastructure that lacks DataOps, a data engineer can\\nbegin adding DataOps into workflows. We suggest first starting with\\nobservability and monitoring to get a window into the performance of\\na system, then adding in automation and incident response. A data\\nengineer may work alongside an existing DataOps team to improve\\nthe data engineering lifecycle in a data-mature company. In all\\ncases, a data engineer must be aware of the philosophy and\\ntechnical aspects of DataOps.\\nDataOps has three core technical elements: automation, monitoring\\nand observability, and incident response (Figure 2-8). Let’s look at\\neach of these pieces and how they relate to the data engineering\\nlifecycle.\\nFigure 2-8. The three pillars of DataOps\\nAutomation\\nAutomation enables reliability and consistency in the DataOps\\nprocess and allows data engineers to quickly deploy new product\\nfeatures, and improvements to existing workflows. DataOps\\nautomation has a similar framework and workflow to DevOps,\\nconsisting of change management (environment, code, and data\\nversion control), continuous integration/continuous deployment\\n(CI/CD), and configuration as code. Like DevOps, DataOps practices\\nmonitor and maintain the reliability of technology and systems (data\\npipelines, orchestration, etc.), with the added dimension of checking\\nfor data quality, data/model drift, metadata integrity, and more.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6854f306-85b8-45f2-8bce-beb8796a0b7a', embedding=None, metadata={'page_label': '106', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Let’s briefly discuss the evolution of DataOps automation within a\\nhypothetical organization. An organization with a low level of\\nDataOps maturity often attempts to schedule multiple stages of data\\ntransformation processes using cron jobs. This works well for a\\nwhile. As data pipelines become more complicated, several things\\nare likely to happen. If the cron jobs are hosted on a cloud instance,\\nthe instance may have an operational problem, causing the jobs to\\nstop running unexpectedly. As the spacing between jobs becomes\\ntighter, a job will eventually run long, causing a subsequent job to fail\\nor produce stale data. Engineers may not be aware of job failures\\nuntil they hear from analysts that their reports are out-of-date.\\nAs the organization’s data maturity grows, data engineers will\\ntypically adopt an orchestration framework, perhaps Airflow or\\nDagster. Data engineers are aware that Airflow presents an\\noperational burden, but the benefits of orchestration eventually\\noutweigh the complexity. Engineers will gradually migrate their cron\\njobs to Airflow jobs. Now, dependencies are checked before jobs\\nrun. More transformation jobs can be packed into a given time\\nbecause each job can start as soon as upstream data is ready rather\\nthan at a fixed, predetermined time.\\nThe data engineering team still has room for operational\\nimprovements. A data scientist eventually deploys a broken DAG,\\nbringing down the Airflow web server and leaving the data team\\noperationally blind. After enough such headaches, the data\\nengineering team members realize that they need to stop allowing\\nmanual DAG deployments. In their next phase of operational\\nmaturity, they adopt automated DAG deployment. DAGs are tested\\nbefore deployment, and monitoring processes ensure that the new\\nDAGs start running properly. In addition, data engineers block the\\ndeployment of new Python dependencies until installation is\\nvalidated. After automation is adopted, the data team is much\\nhappier and experiences far fewer headaches.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='60878c55-7460-49d9-96c9-7e7c21d94ca3', embedding=None, metadata={'page_label': '107', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='One of the tenets of the DataOps Manifesto is “Embrace change.”\\nThis does not mean change for the sake of change, but goal-\\noriented change. At each stage of our automation journey,\\nopportunities exist for operational improvement. Even at the high\\nlevel of maturity that we’ve described here, further room for\\nimprovement remains. Engineers might embrace a next-generation\\norchestration framework that builds in better metadata capabilities.\\nOr they might try to develop a framework that builds DAGs\\nautomatically based on data-lineage specifications. The main point is\\nthat engineers constantly seek to implement improvements in\\nautomation that will reduce their workload and increase the value\\nthat they deliver to the business.\\nObservability and monitoring\\nAs we tell our clients, “Data is a silent killer.” We’ve seen countless\\nexamples of bad data lingering in reports for months or years.\\nExecutives may make key decisions from this bad data, discovering\\nthe error only much later. The outcomes are usually bad and\\nsometimes catastrophic for the business. Initiatives are undermined\\nand destroyed, years of work wasted. In some of the worst cases,\\nbad data may lead companies to financial ruin.\\nAnother horror story occurs when the systems that create the data\\nfor reports randomly stop working, resulting in reports being delayed\\nby several days. The data team doesn’t know until they’re asked by\\nstakeholders why reports are late or producing stale information.\\nEventually, various stakeholders lose trust in the capabilities of the\\ncore data team and start their own splinter teams. The result is many\\ndifferent unstable systems, inconsistent reports, and silos.\\nIf you’re not observing and monitoring your data and the systems\\nthat produce the data, you’re inevitably going to experience your own\\ndata horror story. Observability, monitoring, logging, alerting, and\\ntracing are all critical to getting ahead of any problems along the\\ndata engineering lifecycle. We recommend you incorporate SPC to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cd0d159e-5d9e-41af-828c-6c32302fffbd', embedding=None, metadata={'page_label': '108', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='understand whether events being monitored are out of line and\\nwhich incidents are worth responding to.\\nPetrella’s DODD method mentioned previously in this chapter\\nprovides an excellent framework for thinking about data\\nobservability. DODD is much like test-driven development (TDD) in\\nsoftware engineering:\\nThe purpose of DODD is to give everyone involved in the data\\nchain visibility into the data and data applications so that everyone\\ninvolved in the data value chain has the ability to identify changes\\nto the data or data applications at every step—from ingestion to\\ntransformation to analysis—to help troubleshoot or prevent data\\nissues. DODD focuses on making data observability a first-class\\nconsideration in the data engineering lifecycle.\\nWe cover many aspects of monitoring and observability throughout\\nthe data engineering lifecycle in later chapters.\\nIncident response\\nA high-functioning data team using DataOps will be able to ship new\\ndata products quickly. But mistakes will inevitably happen. A system\\nmay have downtime, a new data model may break downstream\\nreports, an ML model may become stale and provide bad predictions\\n—countless problems can interrupt the data engineering lifecycle.\\nIncident response is about using the automation and observability\\ncapabilities mentioned previously to rapidly identify root causes of an\\nincident and resolve it as reliably and quickly as possible.\\nIncident response isn’t just about technology and tools, though these\\nare beneficial; it’s also about open and blameless communication,\\nboth on the data engineering team and across the organization. As\\nWerner Vogels is famous for saying, “Everything breaks all the time.”\\nData engineers must be prepared for a disaster and ready to\\nrespond as swiftly and efficiently as possible.\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22709ae1-8c04-46de-807c-75f4f41570a0', embedding=None, metadata={'page_label': '109', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data engineers should proactively find issues before the business\\nreports them. Failure happens, and when the stakeholders or end\\nusers see problems, they will present them. They will be unhappy to\\ndo so. The feeling is different when they go to raise those issues to a\\nteam and see that they are actively being worked on to resolve\\nalready. Which team’s state would you trust more as an end user?\\nTrust takes a long time to build and can be lost in minutes. Incident\\nresponse is as much about retroactively responding to incidents as\\nproactively addressing them before they happen.\\nDataOps summary\\nAt this point, DataOps is still a work in progress. Practitioners have\\ndone a good job of adapting DevOps principles to the data domain\\nand mapping out an initial vision through the DataOps Manifesto and\\nother resources. Data engineers would do well to make DataOps\\npractices a high priority in all of their work. The up-front effort will see\\na significant long-term payoff through faster delivery of products,\\nbetter reliability and accuracy of data, and greater overall value for\\nthe business.\\nThe state of operations in data engineering is still quite immature\\ncompared with software engineering. Many data engineering tools,\\nespecially legacy monoliths, are not automation-first. A recent\\nmovement has arisen to adopt automation best practices across the\\ndata engineering lifecycle. Tools like Airflow have paved the way for\\na new generation of automation and data management tools. The\\ngeneral practices we describe for DataOps are aspirational, and we\\nsuggest companies try to adopt them to the fullest extent possible,\\ngiven the tools and knowledge available today.\\nData Architecture\\nA data architecture reflects the current and future state of data\\nsystems that support an organization’s long-term data needs and\\nstrategy. Because an organization’s data requirements will likely', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30ebadf9-b35c-41c9-a1d8-564870e7ee48', embedding=None, metadata={'page_label': '110', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='change rapidly, and new tools and practices seem to arrive on a\\nnear-daily basis, data engineers must understand good data\\narchitecture. Chapter 3 covers data architecture in depth, but we\\nwant to highlight here that data architecture is an undercurrent of the\\ndata engineering lifecycle.\\nA data engineer should first understand the needs of the business\\nand gather requirements for new use cases. Next, a data engineer\\nneeds to translate those requirements to design new ways to capture\\nand serve data, balanced for cost and operational simplicity. This\\nmeans knowing the trade-offs with design patterns, technologies,\\nand tools in source systems, ingestion, storage, transformation, and\\nserving data.\\nThis doesn’t imply that a data engineer is a data architect, as these\\nare typically two separate roles. If a data engineer works alongside a\\ndata architect, the data engineer should be able to deliver on the\\ndata architect’s designs and provide architectural feedback.\\nSoftware Engineering\\nSoftware engineering has always been a central skill for data\\nengineers. In the early days of contemporary data engineering\\n(2000–-2010), data engineers worked on low-level frameworks and\\nwrote MapReduce jobs in C, C++, and Java. At the peak of the big\\ndata era (the mid-2010s), engineers started using frameworks that\\nabstracted away these low-level details.\\nThis abstraction continues today. Cloud data warehouses support\\npowerful transformations using SQL semantics; tools like Spark have\\nbecome more user-friendly, transitioning away from low-level coding\\ndetails and toward easy-to-use dataframes. Despite this abstraction,\\nsoftware engineering is still critical to data engineering. We want to\\nbriefly discuss a few common areas of software engineering that\\napply to the data engineering lifecycle.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='76b4b792-0933-49dd-a269-63e1de86d319', embedding=None, metadata={'page_label': '111', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Core data processing code\\nThough it has become more abstract and easier to manage, core\\ndata processing code still needs to be written, and it appears\\nthroughout the data engineering lifecycle. Whether in ingestion,\\ntransformation, or data serving, data engineers need to be highly\\nproficient and productive in frameworks and languages such as\\nSpark, SQL, or Beam; we reject the notion that SQL is not code.\\nIt’s also imperative that a data engineer understand proper code-\\ntesting methodologies, such as unit, regression, integration, end-to-\\nend, and smoke.\\nDevelopment of open source frameworks\\nMany data engineers are heavily involved in developing open source\\nframeworks. They adopt these frameworks to solve specific\\nproblems in the data engineering lifecycle, and then continue\\ndeveloping the framework code to improve the tools for their use\\ncases and contribute back to the community.\\nIn the big data era, we saw a Cambrian explosion of data-processing\\nframeworks inside the Hadoop ecosystem. These tools primarily\\nfocused on transforming and serving parts of the data engineering\\nlifecycle. Data engineering tool speciation has not ceased or slowed\\ndown, but the emphasis has shifted up the ladder of abstraction,\\naway from direct data processing. This new generation of open\\nsource tools assists engineers in managing, enhancing, connecting,\\noptimizing, and monitoring data.\\nFor example, Airflow dominated the orchestration space from 2015\\nuntil the early 2020s. Now, a new batch of open source competitors\\n(including Prefect, Dagster, and Metaflow) has sprung up to fix\\nperceived limitations of Airflow, providing better metadata handling,\\nportability, and dependency management. Where the future of\\norchestration goes is anyone’s guess.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d104032f-5908-4689-a707-af548fe90510', embedding=None, metadata={'page_label': '112', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Before data engineers begin engineering new internal tools, they\\nwould do well to survey the landscape of publicly available tools.\\nKeep an eye on the total cost of ownership (TCO) and opportunity\\ncost associated with implementing a tool. There is a good chance\\nthat an open source project already exists to address the problem\\nthey’re looking to solve, and they would do well to collaborate rather\\nthan reinventing the wheel.\\nStreaming\\nStreaming data processing is inherently more complicated than\\nbatch, and the tools and paradigms are arguably less mature. As\\nstreaming data becomes more pervasive in every stage of the data\\nengineering lifecycle, data engineers face interesting software\\nengineering problems.\\nFor instance, data processing tasks such as joins that we take for\\ngranted in the batch processing world often become more\\ncomplicated in real time, requiring more complex software\\nengineering. Engineers must also write code to apply a variety of\\nwindowing methods. Windowing allows real-time systems to\\ncalculate valuable metrics such as trailing statistics. Engineers have\\nmany frameworks to choose from, including various function\\nplatforms (OpenFaaS, AWS Lambda, Google Cloud Functions) for\\nhandling individual events or dedicated stream processors (Spark,\\nBeam, Flink or Pulsar) for analyzing streams to support reporting\\nand real-time actions.\\nInfrastructure as code\\nInfrastructure as code (IaC) applies software engineering practices\\nto the configuration and management of infrastructure. The\\ninfrastructure management burden of the big data era has decreased\\nas companies have migrated to managed big data systems (such as\\nDatabricks and Amazon EMR) and cloud data warehouses. When\\ndata engineers have to manage their infrastructure in a cloud', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='00a3348b-8344-4d61-95e2-989afe020881', embedding=None, metadata={'page_label': '113', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='environment, they increasingly do this through IaC frameworks\\nrather than manually spinning up instances and installing software.\\nSeveral general-purpose and cloud-platform-specific frameworks\\nallow automated infrastructure deployment based on a set of\\nspecifications. Many of these frameworks can manage cloud\\nservices as well as infrastructure. There is also a notion of IaC with\\ncontainers and Kubernetes, using tools like Helm.\\nThese practices are a vital part of DevOps, allowing version control\\nand repeatability of deployments. Naturally, these capabilities are\\nprecious throughout the data engineering lifecycle, especially as we\\nadopt DataOps practices.\\nPipelines as code\\nPipelines as code is the core concept of present-day orchestration\\nsystems, which touch every stage of the data engineering lifecycle.\\nData engineers use code (typically Python) to declare data tasks and\\ndependencies among them. The orchestration engine interprets\\nthese instructions to run steps using available resources.\\nGeneral-purpose problem solving\\nIn practice, regardless of which high-level tools they adopt, data\\nengineers will run into corner cases throughout the data engineering\\nlifecycle that require them to solve problems outside the boundaries\\nof their chosen tools and to write custom code. When using\\nframeworks like Fivetran, Airbyte, or Singer, data engineers will\\nencounter data sources without existing connectors and need to\\nwrite something custom. They should be proficient in software\\nengineering to understand APIs, pull and transform data, handle\\nexceptions, and so forth.\\nConclusion', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c60f875b-b450-407a-b102-08bfea82180c', embedding=None, metadata={'page_label': '114', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Most discussions we’ve seen in the past about data engineering\\ninvolve technologies but miss the bigger picture of data lifecycle\\nmanagement. As technologies become more abstract and do more\\nheavy lifting, a data engineer has the opportunity to think and act on\\na higher level. The data engineering lifecycle, supported by its\\nundercurrents, is an extremely useful mental model for organizing\\nthe work of data engineering.\\nWe break the data engineering lifecycle into the following stages:\\nGeneration\\nStorage\\nIngestion\\nTransformation\\nServing data\\nSeveral themes cut across the data engineering lifecycle as well.\\nThese are the undercurrents of the data engineering lifecycle. At a\\nhigh level, the undercurrents are as follows:\\nSecurity\\nData management\\nDataOps\\nData architecture\\nOrchestration\\nSoftware engineering\\nA data engineer has several top-level goals across the data lifecycle:\\nproduce optimum ROI and reduce costs (financial and opportunity),\\nreduce risk (security, data quality), and maximize data value and\\nutility.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a8e9df6b-8ca4-4050-8bab-52180dfb3ac5', embedding=None, metadata={'page_label': '115', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The next two chapters discuss how these elements impact good\\narchitecture design, along with choosing the right technologies. If\\nyou feel comfortable with these two topics, feel free to skip ahead to\\nPart II, where we cover each of the stages of the data engineering\\nlifecycle.\\nAdditional Resources\\nManaging the data engineering lifecycle:\\n“Staying Ahead of Debt” by Etai Mizrahi\\nData transformation and processing:\\n“Data transformation” Wikipedia page\\n“Data processing” Wikipedia page\\n“A Comparison of Data Processing Frameworks” by Ludovic\\nSantos\\nUndercurrents:\\n“The Dataflow Model: A Practical Approach to Balancing\\nCorrectness, Latency, and Cost in Massive-Scale, Unbounded,\\nOut-of-Order Data Processing” by Tyler Akidau et al.\\nDataOps:\\n“Getting Started with DevOps Automation” by Jared Murrell\\n“Incident Management in the Age of DevOps” Atlassian web\\npage\\n“The Seven Stages of Effective Incident Response” Atlassian\\nweb page\\n“Is DevOps Related to DataOps?” by Carol Jang and Jove\\nKuang', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a4ab2cb-c65c-4149-8ad7-dd7bc1164f45', embedding=None, metadata={'page_label': '116', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data management:\\nDAMA International website\\nData management and metadata:\\n“What Is Metadata” by Michelle Knight\\nAirbnb Data Portal:\\n“Democratizing Data at Airbnb” by Chris Williams et al.\\n“Five Steps to Begin Collecting the Value of Your Data” Lean-\\nData web page\\nOrchestration:\\n“An Introduction to Dagster: The Orchestrator for the Full Data\\nLifecycle” video by Nick Schrock\\n1  Evren Eryurek et al., Data Governance: The Definitive Guide (Sebastopol,\\nCA: O’Reilly, 2021), 1, https://oreil.ly/LFT4d.\\n2  Eryurek, Data Governance, 5.\\n3  Chris Williams et al., “Democratizing Data at Airbnb,” The Airbnb Tech Blog,\\nMay 12, 2017, https://oreil.ly/dM332.\\n4  Eryurek, Data Governance, 113.\\n5  Tyler Akidau et al., “The Dataflow Model: A Practical Approach to Balancing\\nCorrectness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order\\nData Processing,” Proceedings of the VLDB Endowment 8 (2015): 1792–\\n1803, https://oreil.ly/Z6XYy.\\n6  “What Is DataOps,” DataKitchen FAQ page, accessed May 5, 2022,\\nhttps://oreil.ly/Ns06w.\\n7  Andy Petrella, “Data Observability Driven Development: The Perfect\\nAnalogy for Beginners,” Kensu, accessed May 5, 2022,\\nhttps://oreil.ly/MxvSX.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8832f79b-3483-49dc-bc42-67338c9670a7', embedding=None, metadata={'page_label': '117', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 3. Designing Good\\nData Architecture\\nGood data architecture provides seamless capabilities across every\\nstep of the data lifecycle and undercurrent. We’ll begin by defining\\ndata architecture and then discuss components and considerations.\\nWe’ll then touch on specific batch patterns (data warehouses, data\\nlakes), streaming patterns, and patterns that unify batch and\\nstreaming. Throughout, we’ll emphasize leveraging the capabilities\\nof the cloud to deliver scalability, availability, and reliability.\\nWhat Is Data Architecture?\\nSuccessful data engineering is built upon rock-solid data\\narchitecture. This chapter aims to review a few popular architecture\\napproaches and frameworks, and then craft our opinionated\\ndefinition of what makes “good” data architecture. Indeed, we won’t\\nmake everyone happy. Still, we will lay out a pragmatic, domain-\\nspecific, working definition for data architecture that we think will\\nwork for companies of vastly different scales, business processes,\\nand needs.\\nWhat is data architecture? When you stop to unpack it, the topic\\nbecomes a bit murky; researching data architecture yields many\\ninconsistent and often outdated definitions. It’s a lot like when we\\ndefined data engineering in Chapter 1—there’s no consensus. In a\\nfield that is constantly changing, this is to be expected. So what do\\nwe mean by data architecture for the purposes of this book? Before\\ndefining the term, it’s essential to understand the context in which it\\nsits. Let’s briefly cover enterprise architecture, which will frame our\\ndefinition of data architecture.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fa68df8c-c488-417a-b2dc-2288f903cf0c', embedding=None, metadata={'page_label': '118', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Enterprise Architecture, Defined\\nEnterprise architecture has many subsets, including business,\\ntechnical, application, and data (Figure 3-1). As such, many\\nframeworks and resources are devoted to enterprise architecture. In\\ntruth, architecture is a surprisingly controversial topic.\\nFigure 3-1. Data architecture is a subset of enterprise architecture\\nThe term enterprise gets mixed reactions. It brings to mind sterile\\ncorporate offices, command-and-control/waterfall planning, stagnant\\nbusiness cultures, and empty catchphrases. Even so, we can learn\\nsome things here.\\nBefore we define and describe enterprise architecture, let’s unpack\\nthis term. Let’s look at how enterprise architecture is defined by\\nsome significant thought leaders: TOGAF, Gartner, and EABOK.\\nTOGAF’s definition\\nTOGAF is The Open Group Architecture Framework, a standard of\\nThe Open Group. It’s touted as the most widely used architecture\\nframework today. Here’s the TOGAF definition:\\nThe term “enterprise” in the context of “enterprise architecture” can\\ndenote an entire enterprise—encompassing all of its information\\nand technology services, processes, and infrastructure—or a\\nspecific domain within the enterprise. In both cases, the\\narchitecture crosses multiple systems, and multiple functional\\ngroups within the enterprise.\\nGartner’s definition', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1001a8d2-7fc9-42ea-9609-fa07e2e36d33', embedding=None, metadata={'page_label': '119', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gartner is a global research and advisory company that produces\\nresearch articles and reports on trends related to enterprises. Among\\nother things, it is responsible for the (in)famous Gartner Hype Cycle.\\nGartner’s definition is as follows:\\nEnterprise architecture (EA) is a discipline for proactively and\\nholistically leading enterprise responses to disruptive forces by\\nidentifying and analyzing the execution of change toward desired\\nbusiness vision and outcomes. EA delivers value by presenting\\nbusiness and IT leaders with signature-ready recommendations\\nfor adjusting policies and projects to achieve targeted business\\noutcomes that capitalize on relevant business disruptions.\\nEABOK’s definition\\nEABOK is the Enterprise Architecture Book of Knowledge, an\\nenterprise architecture reference produced by the MITRE\\nCorporation. EABOK was released as an incomplete draft in 2004\\nand has not been updated since. Though seemingly obsolete,\\nEABOK is still frequently referenced in descriptions of enterprise\\narchitecture; we found many of its ideas helpful while writing this\\nbook. Here’s the EABOK definition:\\nEnterprise Architecture (EA) is an organizational model; an\\nabstract representation of an Enterprise that aligns strategy,\\noperations, and technology to create a roadmap for success.\\nOur definition\\nWe extract a few common threads in these definitions of enterprise\\narchitecture: change, alignment, organization, opportunities,\\nproblem-solving, and migration. Here is our definition of enterprise\\narchitecture, one that we feel is more relevant to today’s fast-moving\\ndata landscape:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0ae56c2b-3d74-4273-93d9-31ac5d9ecc3d', embedding=None, metadata={'page_label': '120', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Enterprise architecture is the design of systems to support change\\nin the enterprise, achieved by flexible and reversible decisions\\nreached through careful evaluation of trade-offs.\\nHere, we touch on some key areas we’ll return to throughout the\\nbook: flexible and reversible decisions, change management, and\\nevaluation of trade-offs. We discuss each theme at length in this\\nsection and then make the definition more concrete in the latter part\\nof the chapter by giving various examples of data architecture.\\nFlexible and reversible decisions are essential for two reasons. First,\\nthe world is constantly changing, and predicting the future is\\nimpossible. Reversible decisions allow you to adjust course as the\\nworld changes and you gather new information. Second, there is a\\nnatural tendency toward enterprise ossification as organizations\\ngrow. As organizations grow, they become increasingly risk averse\\nand cumbersome. Adopting a culture of reversible decisions helps\\novercome this tendency by reducing the risk attached to a decision.\\nJeff Bezos is credited with the idea of one-way and two-way doors.\\nA one-way door is a decision that is almost impossible to reverse.\\nFor example, Amazon could have decided to sell AWS or shut it\\ndown. It would be nearly impossible for Amazon to rebuild a public\\ncloud with the same market position after such an action.\\nOn the other hand, a two-way door is an easily reversible decision:\\nyou walk through and proceed if you like what you see in the room or\\nstep back through the door if you don’t. Amazon might decide to\\nrequire the use of DynamoDB for a new microservices database. If\\nthis policy doesn’t work, Amazon has the option of reversing it and\\nrefactoring some services to use other databases. Since the stakes\\nattached to each reversible decision (two-way door) are low,\\norganizations can make more decisions, iterating, improving, and\\ncollecting data rapidly.\\nChange management is closely related to reversible decisions and is\\na central theme of enterprise architecture frameworks. Even with an\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf587c6d-74a8-44dd-9318-19d772acb545', embedding=None, metadata={'page_label': '121', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='emphasis on reversible decisions, enterprises often need to\\nundertake large initiatives. These are ideally broken into smaller\\nchanges, each one a reversible decision in itself. Returning to\\nAmazon, we note a five-year gap (2007 to 2012) from the publication\\nof a paper on the DynamoDB concept to Werner Vogels’s\\nannouncement of the DynamoDB service on AWS. Behind the\\nscenes, teams took numerous small actions to make DynamoDB a\\nconcrete reality for AWS customers. Managing such small actions is\\nat the heart of change management.\\nArchitects are not simply mapping out IT processes and vaguely\\nlooking toward a distant, utopian future; they actively solve business\\nproblems and create new opportunities. Technical solutions exist not\\nfor their own sake but in support of business goals. Architects\\nidentify problems in the current state (poor data quality, scalability\\nlimits, money-losing lines of business), define desired future states\\n(agile data-quality improvement, scalable cloud data solutions,\\nimproved business processes), and realize initiatives through\\nexecution of small, concrete steps.\\nTechnical solutions exist not for their own sake but in support of\\nbusiness goals.\\nWe found significant inspiration in Fundamentals of Software\\nArchitecture by Mark Richards and Neal Ford (O’Reilly). They\\nemphasize that trade-offs are inevitable and ubiquitous in the\\nengineering space. Sometimes the relatively fluid nature of software\\nand data leads us to believe that we are freed from the constraints\\nthat engineers face in the hard, cold physical world. Indeed, this is\\npartially true; patching a software bug is much easier than\\nredesigning and replacing an airplane wing. However, digital\\nsystems are ultimately constrained by physical limits such as latency,\\nreliability, density, and energy consumption. Engineers also confront\\nvarious nonphysical limits, such as characteristics of programming\\nlanguages and frameworks, and practical constraints in managing\\ncomplexity, budgets, etc. Magical thinking culminates in poor', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56ab0a2e-cab1-4a16-99d7-305b939cec00', embedding=None, metadata={'page_label': '122', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='engineering. Data engineers must account for trade-offs at every\\nstep to design an optimal system while minimizing high-interest\\ntechnical debt.\\nLet’s reiterate one central point in our enterprise architecture\\ndefinition: enterprise architecture balances flexibility and trade-offs.\\nThis isn’t always an easy balance, and architects must constantly\\nassess and reevaluate with the recognition that the world is dynamic.\\nGiven the pace of change that enterprises are faced with,\\norganizations—and their architecture—cannot afford to stand still.\\nData Architecture Defined\\nNow that you understand enterprise architecture, let’s dive into data\\narchitecture by establishing a working definition that will set the\\nstage for the rest of the book. Data architecture is a subset of\\nenterprise architecture, inheriting its properties: processes, strategy,\\nchange management, and technology. Here are a couple of\\ndefinitions of data architecture that influence our definition.\\nTOGAF’s definition\\nTOGAF defines data architecture as follows:\\nA description of the structure and interaction of the enterprise’s\\nmajor types and sources of data, logical data assets, physical data\\nassets, and data management resources.\\nDAMA’s definition\\nThe DAMA DMBOK defines data architecture as follows:\\nIdentifying the data needs of the enterprise (regardless of\\nstructure) and designing and maintaining the master blueprints to\\nmeet those needs. Using master blueprints to guide data\\nintegration, control data assets, and align data investments with\\nbusiness strategy.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='52e83620-5fdd-4262-a3e4-8c0521824e7d', embedding=None, metadata={'page_label': '123', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Our definition\\nConsidering the preceding two definitions and our experience, we\\nhave crafted our definition of data architecture:\\nData architecture is the design of systems to support the evolving\\ndata needs of an enterprise, achieved by flexible and reversible\\ndecisions reached through a careful evaluation of trade-offs.\\nHow does data architecture fit into data engineering? Just as the\\ndata engineering lifecycle is a subset of the data lifecycle, data\\nengineering architecture is a subset of general data architecture.\\nData engineering architecture is the systems and frameworks that\\nmake up the key sections of the data engineering lifecycle. We’ll use\\ndata architecture interchangeably with data engineering architecture\\nthroughout this book.\\nOther aspects of data architecture that you should be aware of are\\noperational and technical (Figure 3-2). Operational architecture\\nencompasses the functional requirements of what needs to happen\\nrelated to people, processes, and technology. For example, what\\nbusiness processes does the data serve? How does the organization\\nmanage data quality? What is the latency requirement from when the\\ndata is produced to when it becomes available to query? Technical\\narchitecture outlines how data is ingested, stored, transformed, and\\nserved along the data engineering lifecycle. For instance, how will\\nyou move 10 TB of data every hour from a source database to your\\ndata lake? In short, operational architecture describes what needs to\\nbe done, and technical architecture details how it will happen.\\nFigure 3-2. Operational and technical data architecture', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e832a4f6-beb3-4dc2-9e56-bb39d3097a37', embedding=None, metadata={'page_label': '124', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Now that we have a working definition of data architecture, let’s\\ncover the elements of “good” data architecture.\\n“Good” Data Architecture\\nNever shoot for the best architecture, but rather the least worst\\narchitecture.\\n—Neal Ford, Mark Richards\\nAccording to Grady Booch, “Architecture represents the significant\\ndesign decisions that shape a system, where significant is measured\\nby cost of change.” Data architects aim to make significant decisions\\nthat will lead to good architecture at a basic level.\\nWhat do we mean by “good” data architecture? To paraphrase an old\\ncliche, you know good when you see it. Good data architecture\\nserves business requirements with a common, widely reusable set of\\nbuilding blocks while maintaining flexibility and making appropriate\\ntrade-offs. Bad architecture is authoritarian and tries to cram a bunch\\nof one-size-fits-all decisions into a big ball of mud.\\nAgility is the foundation for good data architecture; it acknowledges\\nthat the world is fluid. Good data architecture is flexible and easily\\nmaintainable. It evolves in response to changes within the business\\nand new technologies and practices that may unlock even more\\nvalue in the future. Businesses and their use cases for data are\\nalways evolving. The world is dynamic, and the pace of change in\\nthe data space is accelerating. Last year’s data architecture that\\nserved you well might not be sufficient for today, let alone next year.\\nBad data architecture is tightly coupled, rigid, overly centralized, or\\nuses the wrong tools for the job, hampering development and\\nchange management. Ideally, by designing architecture with\\nreversibility in mind, changes will be less costly.\\nThe undercurrents of the data engineering lifecycle form the\\nfoundation of good data architecture for companies at any stage of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4bb6c13a-525e-41ce-87eb-c8b5d579c893', embedding=None, metadata={'page_label': '125', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data maturity. Again, these undercurrents are security, data\\nmanagement, DataOps, data architecture, orchestration, and\\nsoftware engineering.\\nGood data architecture is a living, breathing thing. It’s never finished.\\nIn fact, per our definition, change and evolution are central to the\\nmeaning and purpose of data architecture. Let’s now look at the\\nprinciples of good data architecture.\\nPrinciples of Good Data Architecture\\nThis section takes a 10,000-foot view of good architecture by\\nfocusing on principles—key ideas useful in evaluating major\\narchitectural decisions and practices. We borrow inspiration for our\\narchitecture principles from several sources, especially the AWS\\nWell-Architected Framework and Google Cloud’s Five Principles for\\nCloud-Native Architecture.\\nThe AWS Well-Architected Framework consists of six pillars:\\nOperational excellence\\nSecurity\\nReliability\\nPerformance efficiency\\nCost optimization\\nSustainability\\nGoogle Cloud’s Five Principles for Cloud-Native Architecture are as\\nfollows:\\nDesign for automation.\\nBe smart with state.\\nFavor managed services.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='547a5ab0-fdc8-44ff-a1ec-6d3a4f884e1c', embedding=None, metadata={'page_label': '126', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Practice defense in depth.\\nAlways be architecting.\\nWe advise you to carefully study both frameworks, identify valuable\\nideas, and determine points of disagreement. We’d like to expand or\\nelaborate on these pillars with these principles of data engineering\\narchitecture:\\n1. Choose common components wisely.\\n2. Plan for failure.\\n3. Architect for scalability.\\n4. Architecture is leadership.\\n5. Always be architecting.\\n6. Build loosely coupled systems.\\n7. Make reversible decisions.\\n8. Prioritize security.\\n9. Embrace FinOps.\\nPrinciple 1: Choose Common Components\\nWisely\\nOne of the primary jobs of a data engineer is to choose common\\ncomponents and practices that can be used widely across an\\norganization. When architects choose well and lead effectively,\\ncommon components become a fabric facilitating team collaboration\\nand breaking down silos. Common components enable agility within\\nand across teams in conjunction with shared knowledge and skills.\\nCommon components can be anything that has broad applicability\\nwithin an organization. Common components include object storage,\\nversion-control systems, observability, monitoring and orchestration', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e3fd3329-67a4-43e2-bfd5-ddef6e3383cc', embedding=None, metadata={'page_label': '127', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='systems, and processing engines. Common components should be\\naccessible to everyone with an appropriate use case, and teams are\\nencouraged to rely on common components already in use rather\\nthan reinventing the wheel. Common components must support\\nrobust permissions and security to enable sharing of assets among\\nteams while preventing unauthorized access.\\nCloud platforms are an ideal place to adopt common components.\\nFor example, compute and storage separation in cloud data systems\\nallows users to access a shared storage layer (most commonly\\nobject storage) using specialized tools to access and query the data\\nneeded for specific use cases.\\nChoosing common components is a balancing act. On the one hand,\\nyou need to focus on needs across the data engineering lifecycle\\nand teams, utilize common components that will be useful for\\nindividual projects, and simultaneously facilitate interoperation and\\ncollaboration. On the other hand, architects should avoid decisions\\nthat will hamper the productivity of engineers working on domain-\\nspecific problems by forcing them into one-size-fits-all technology\\nsolutions. Chapter 4 provides additional details.\\nPrinciple 2: Plan for Failure\\nEverything fails, all the time.\\n—Werner Vogels\\nModern hardware is highly robust and durable. Even so, any\\nhardware component will fail, given enough time. To build highly\\nrobust data systems, you must consider failures in your designs.\\nHere are a few key terms for evaluating failure scenarios; we\\ndescribe these in greater detail in this chapter and throughout the\\nbook:\\nAvailability', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fa31d5b2-4965-4e09-a0ad-5444f6948b4b', embedding=None, metadata={'page_label': '128', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The percentage of time an IT service or component is in an\\noperable state.\\nReliability\\nThe system’s probability of meeting defined standards in\\nperforming its intended function during a specified interval.\\nRecovery time objective\\nThe maximum acceptable time for a service or system outage.\\nThe recovery time objective (RTO) is generally set by determining\\nthe business impact of an outage. An RTO of one day might be\\nfine for an internal reporting system. A website outage of just five\\nminutes could have a significant adverse business impact on an\\nonline retailer.\\nRecovery point objective\\nThe acceptable state after recovery. In data systems, data is\\noften lost during an outage. In this setting, the recovery point\\nobjective (RPO) refers to the maximum acceptable data loss.\\nEngineers need to consider acceptable reliability, availability, RTO,\\nand RPO in designing for failure. These will guide their architecture\\ndecisions as they assess possible failure scenarios.\\nPrinciple 3: Architect for Scalability\\nScalability in data systems encompasses two main capabilities. First,\\nscalable systems can scale up to handle significant quantities of\\ndata. We might need to spin up a large cluster to train a model on a\\npetabyte of customer data or scale out a streaming ingestion system', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='66787ea2-a6f0-4724-a4b5-d04ec9236732', embedding=None, metadata={'page_label': '129', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='to handle a transient load spike. Our ability to scale up allows us to\\nhandle extreme loads temporarily. Second, scalable systems can\\nscale down. Once the load spike ebbs, we should automatically\\nremove capacity to cut costs. (This is related to principle 9.) An\\nelastic system can scale dynamically in response to load, ideally in\\nan automated fashion.\\nSome scalable systems can also scale to zero: they shut down\\ncompletely when not in use. Once the large model-training job\\ncompletes, we can delete the cluster. Many serverless systems (e.g.,\\nserverless functions and serverless online analytical processing, or\\nOLAP, databases) can automatically scale to zero.\\nNote that deploying inappropriate scaling strategies can result in\\novercomplicated systems and high costs. A straightforward relational\\ndatabase with one failover node may be appropriate for an\\napplication instead of a complex cluster arrangement. Measure your\\ncurrent load, approximate load spikes, and estimate load over the\\nnext several years to determine if your database architecture is\\nappropriate. If your startup grows much faster than anticipated, this\\ngrowth should also lead to more available resources to rearchitect\\nfor scalability.\\nPrinciple 4: Architecture Is Leadership\\nData architects are responsible for technology decisions and\\narchitecture descriptions and disseminating these choices through\\neffective leadership and training. Data architects should be highly\\ntechnically competent but delegate most individual contributor work\\nto others. Strong leadership skills combined with high technical\\ncompetence are rare and extremely valuable. The best data\\narchitects take this duality seriously.\\nNote that leadership does not imply a command-and-control\\napproach to technology. It was not uncommon in the past for\\narchitects to choose one proprietary database technology and force', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='47a40466-5363-4b93-9e4c-3b7bbc73b4c2', embedding=None, metadata={'page_label': '130', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='every team to house their data there. We oppose this approach\\nbecause it can significantly hinder current data projects. Cloud\\nenvironments allow architects to balance common component\\nchoices with flexibility that enables innovation within projects.\\nReturning to the notion of technical leadership, Martin Fowler\\ndescribes a specific archetype of an ideal software architect, well\\nembodied in his colleague Dave Rice:\\nIn many ways, the most important activity of Architectus Oryzus is\\nto mentor the development team, to raise their level so they can\\ntake on more complex issues. Improving the development team’s\\nability gives an architect much greater leverage than being the\\nsole decision-maker and thus running the risk of being an\\narchitectural bottleneck.\\nAn ideal data architect manifests similar characteristics. They\\npossess the technical skills of a data engineer but no longer practice\\ndata engineering day to day; they mentor current data engineers,\\nmake careful technology choices in consultation with their\\norganization, and disseminate expertise through training and\\nleadership. They train engineers in best practices and bring the\\ncompany’s engineering resources together to pursue common goals\\nin both technology and business.\\nAs a data engineer, you should practice architecture leadership and\\nseek mentorship from architects. Eventually, you may well occupy\\nthe architect role yourself.\\nPrinciple 5: Always Be Architecting\\nWe borrow this principle directly from Google Cloud’s Five Principles\\nfor Cloud-Native Architecture. Data architects don’t serve in their role\\nsimply to maintain the existing state; instead, they constantly design\\nnew and exciting things in response to changes in business and\\ntechnology. Per the EABOK, an architect’s job is to develop deep\\nknowledge of the baseline architecture (current state), develop a\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a391a798-6704-4a60-8651-7ac3f655d8c2', embedding=None, metadata={'page_label': '131', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='target architecture, and map out a sequencing plan to determine\\npriorities and the order of architecture changes.\\nWe add that modern architecture should not be command-and-\\ncontrol or waterfall but collaborative and agile. The data architect\\nmaintains a target architecture and sequencing plans that change\\nover time. The target architecture becomes a moving target,\\nadjusted in response to business and technology changes internally\\nand worldwide. The sequencing plan determines immediate priorities\\nfor delivery.\\nPrinciple 6: Build Loosely Coupled Systems\\nWhen the architecture of the system is designed to enable teams\\nto test, deploy, and change systems without dependencies on\\nother teams, teams require little communication to get work done.\\nIn other words, both the architecture and the teams are loosely\\ncoupled.\\n—Google DevOps tech Architecture Guide\\nIn 2002, Bezos wrote an email to Amazon employees that became\\nknown as the Bezos API Mandate:\\n1. All teams will henceforth expose their data and functionality\\nthrough service interfaces.\\n2. Teams must communicate with each other through these\\ninterfaces.\\n3. There will be no other form of interprocess communication\\nallowed: no direct linking, no direct reads of another team’s data\\nstore, no shared-memory model, no back-doors whatsoever.\\nThe only communication allowed is via service interface calls\\nover the network.\\n4. It doesn’t matter what technology they use. HTTP, Corba,\\nPubsub, custom protocols—doesn’t matter.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a4d07338-7df0-43af-8a55-927d7ea2ca0b', embedding=None, metadata={'page_label': '132', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5. All service interfaces, without exception, must be designed from\\nthe ground up to be externalizable. That is to say, the team must\\nplan and design to be able to expose the interface to developers\\nin the outside world. No exceptions.\\nThe advent of Bezos’s API Mandate is widely viewed as a watershed\\nmoment for Amazon. Putting data and services behind APIs enabled\\nthe loose coupling and eventually resulted in AWS as we know it\\nnow. Google’s pursuit of loose coupling allowed it to grow its\\nsystems to an extraordinary scale.\\nFor software architecture, a loosely coupled system has the following\\nproperties:\\n1. Systems are broken into many small components.\\n2. These systems interface with other services through abstraction\\nlayers, such as a messaging bus or an API. These abstraction\\nlayers hide and protect internal details of the service, such as a\\ndatabase backend or internal classes and method calls.\\n3. As a consequence of property 2, internal changes to a system\\ncomponent don’t require changes in other parts. Details of code\\nupdates are hidden behind stable APIs. Each piece can evolve\\nand improve separately.\\n4. As a consequence of property 3, there is no waterfall, global\\nrelease cycle for the whole system. Instead, each component is\\nupdated separately as changes and improvements are made.\\nNotice that we are talking about technical systems. We need to think\\nbigger. Let’s translate these technical characteristics into\\norganizational characteristics:\\n1. Many small teams engineer a large, complex system. Each\\nteam is tasked with engineering, maintaining, and improving\\nsome system components.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bb76749c-67cc-414b-9044-eaf50a0db975', embedding=None, metadata={'page_label': '133', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2. These teams publish the abstract details of their components to\\nother teams via API definitions, message schemas, etc. Teams\\nneed not concern themselves with other teams’ components;\\nthey simply use the published API or message specifications to\\ncall these components. They iterate their part to improve their\\nperformance and capabilities over time. They might also publish\\nnew capabilities as they are added or request new stuff from\\nother teams. Again, the latter happens without teams needing to\\nworry about the internal technical details of the requested\\nfeatures. Teams work together through loosely coupled\\ncommunication.\\n3. As a consequence of characteristic 2, each team can rapidly\\nevolve and improve its component independently of the work of\\nother teams.\\n4. Specifically, characteristic 3 implies that teams can release\\nupdates to their components with minimal downtime. Teams\\nrelease continuously during regular working hours to make code\\nchanges and test them.\\nLoose coupling of both technology and human systems will allow\\nyour data engineering teams to more efficiently collaborate with one\\nanother and with other parts of the company. This principle also\\ndirectly facilitates principle 7.\\nPrinciple 7: Make Reversible Decisions\\nThe data landscape is changing rapidly. Today’s hot technology or\\nstack is tomorrow’s afterthought. Popular opinion shifts quickly. You\\nshould aim for reversible decisions, as these tend to simplify your\\narchitecture and keep it agile.\\nAs Fowler wrote, “One of an architect’s most important tasks is to\\nremove architecture by finding ways to eliminate irreversibility in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4132395e-6c14-4fc2-9dec-56c7980d3153', embedding=None, metadata={'page_label': '134', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='software designs.” What was true when Fowler wrote this in 2003 is\\njust as accurate today.\\nAs we said previously, Bezos refers to reversible decisions as “two-\\nway doors.” As he says, “If you walk through and don’t like what you\\nsee on the other side, you can’t get back to before. We can call\\nthese Type 1 decisions. But most decisions aren’t like that—they are\\nchangeable, reversible—they’re two-way doors.” Aim for two-way\\ndoors whenever possible.\\nGiven the pace of change—and the decoupling/modularization of\\ntechnologies across your data architecture—always strive to pick the\\nbest-of-breed solutions that work for today. Also, be prepared to\\nupgrade or adopt better practices as the landscape evolves.\\nPrinciple 8: Prioritize Security\\nEvery data engineer must assume responsibility for the security of\\nthe systems they build and maintain. We focus now on two main\\nideas: zero-trust security and the shared responsibility security\\nmodel. These align closely to a cloud-native architecture.\\nHardened-perimeter and zero-trust security models\\nTo define zero-trust security, it’s helpful to start by understanding the\\ntraditional hard-perimeter security model and its limitations, as\\ndetailed in Google Cloud’s Five Principles:\\nTraditional architectures place a lot of faith in perimeter security,\\ncrudely a hardened network perimeter with “trusted things” inside\\nand “untrusted things” outside. Unfortunately, this approach has\\nalways been vulnerable to insider attacks, as well as external\\nthreats such as spear phishing.\\nThe 1996 film Mission Impossible presents a perfect example of the\\nhard-perimeter security model and its limitations. In the movie, the\\nCIA hosts highly sensitive data on a storage system inside a room\\nwith extremely tight physical security. Ethan Hunt infiltrates CIA\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='855e81e4-bf54-4169-b547-02a513b093b5', embedding=None, metadata={'page_label': '135', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='headquarters and exploits a human target to gain physical access to\\nthe storage system. Once inside the secure room, he can exfiltrate\\ndata with relative ease.\\nFor at least a decade, alarming media reports have made us aware\\nof the growing menace of security breaches that exploit human\\ntargets inside hardened organizational security perimeters. Even as\\nemployees work on highly secure corporate networks, they remain\\nconnected to the outside world through email and mobile devices.\\nExternal threats effectively become internal threats.\\nIn a cloud-native environment, the notion of a hardened perimeter\\nerodes further. All assets are connected to the outside world to some\\ndegree. While virtual private cloud (VPC) networks can be defined\\nwith no external connectivity, the API control plane that engineers\\nuse to define these networks still faces the internet.\\nThe shared responsibility model\\nAmazon emphasizes the shared responsibility model, which divides\\nsecurity into the security of the cloud and security in the cloud. AWS\\nis responsible for the security of the cloud:\\nAWS is responsible for protecting the infrastructure that runs AWS\\nservices in the AWS Cloud. AWS also provides you with services\\nthat you can use securely.\\nAWS users are responsible for security in the cloud:\\nYour responsibility is determined by the AWS service that you use.\\nYou are also responsible for other factors including the sensitivity\\nof your data, your organization’s requirements, and applicable\\nlaws and regulations.\\nIn general, all cloud providers operate on this shared responsibility\\nmodel. They secure their services according to published\\nspecifications. Still, it is ultimately the user’s responsibility to design', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a917d01b-d2d5-4d20-a49d-754113157a30', embedding=None, metadata={'page_label': '136', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='a security model for their applications and data and leverage cloud\\ncapabilities to realize this model.\\nData engineers as security engineers\\nIn the corporate world today, a command-and-control approach to\\nsecurity is quite common, wherein security and networking teams\\nmanage perimeters and general security practices. The cloud\\npushes this responsibility out to engineers who are not explicitly in\\nsecurity roles. Because of this responsibility, in conjunction with\\nmore general erosion of the hard security perimeter, all data\\nengineers should consider themselves security engineers.\\nFailure to assume these new implicit responsibilities can lead to dire\\nconsequences. Numerous data breaches have resulted from the\\nsimple error of configuring Amazon S3 buckets with public access.\\nThose who handle data must assume that they are ultimately\\nresponsible for securing it.\\nPrinciple 9: Embrace FinOps\\nLet’s start by considering a couple of definitions of FinOps. First, the\\nFinOps Foundation offers this:\\nFinOps is an evolving cloud financial management discipline and\\ncultural practice that enables organizations to get maximum\\nbusiness value by helping engineering, finance, technology, and\\nbusiness teams to collaborate on data-driven spending decisions.\\nIn addition, J.R. Sorment and Mike Fuller provide the following\\ndefinition in Cloud FinOps (O’Reilly):\\nThe term “FinOps” typically refers to the emerging professional\\nmovement that advocates a collaborative working relationship\\nbetween DevOps and Finance, resulting in an iterative, data-\\ndriven management of infrastructure spending (i.e., lowering the\\nunit economics of cloud) while simultaneously increasing the cost\\nefficiency and, ultimately, the profitability of the cloud environment.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='58b6a30e-9b51-4d75-a172-49bf1a5a473c', embedding=None, metadata={'page_label': '137', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The cost structure of data has evolved dramatically during the cloud\\nera. In an on-premises setting, data systems are generally acquired\\nwith a capital expenditure (described more in Chapter 4) for a new\\nsystem every few years in an on-premises setting. Responsible\\nparties have to balance their budget against desired compute and\\nstorage capacity. Overbuying entails wasted money, while\\nunderbuying means hampering future data projects and driving\\nsignificant personnel time to control system load and data size;\\nunderbuying may require faster technology refresh cycles, with\\nassociated extra costs.\\nIn the cloud era, most data systems are pay-as-you-go and readily\\nscalable. Systems can run on a cost per query model, cost per\\nprocessing capacity model, or another variant of a pay-as-you-go\\nmodel. This approach can be far more efficient than the capital\\nexpenditure approach. It is now possible to scale up for high\\nperformance, and then scale down to save money. However, the\\npay-as-you-go approach makes spending far more dynamic. The\\nnew challenge for data leaders is to manage budgets, priorities, and\\nefficiency.\\nCloud tooling necessitates a set of processes for managing spending\\nand resources. In the past, data engineers thought in terms of\\nperformance engineering—maximizing the performance for data\\nprocesses on a fixed set of resources and buying adequate\\nresources for future needs. With FinOps, engineers need to learn to\\nthink about the cost structures of cloud systems. For example, what\\nis the appropriate mix of AWS spot instances when running a\\ndistributed cluster? What is the most appropriate approach for\\nrunning a sizable daily job in terms of cost-effectiveness and\\nperformance? When should the company switch from a pay-per-\\nquery model to reserved capacity?\\nFinOps evolves the operational monitoring model to monitor\\nspending on an ongoing basis. Rather than simply monitor requests\\nand CPU utilization for a web server, FinOps might monitor the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3192e4e2-e6af-40e8-b95f-94113cf10931', embedding=None, metadata={'page_label': '138', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ongoing cost of serverless functions handling traffic, as well as\\nspikes in spending trigger alerts. Just as systems are designed to fail\\ngracefully in excessive traffic, companies may consider adopting\\nhard limits for spending, with graceful failure modes in response to\\nspending spikes.\\nOps teams should also think in terms of cost attacks. Just as a\\ndistributed denial-of-service (DDoS) attack can block access to a\\nweb server, many companies have discovered to their chagrin that\\nexcessive downloads from S3 buckets can drive spending through\\nthe roof and threaten a small startup with bankruptcy. When sharing\\ndata publicly, data teams can address these issues by setting\\nrequester-pays policies, or simply monitoring for excessive data\\naccess spending and quickly removing access if spending begins to\\nrise to unacceptable levels.\\nAs of this writing, FinOps is a recently formalized practice. The\\nFinOps Foundation was started only in 2019. However, we highly\\nrecommend you start thinking about FinOps early, before you\\nencounter high cloud bills. Start your journey with the FinOps\\nFoundation and O’Reilly’s Cloud FinOps. We also suggest that data\\nengineers involve themselves in the community process of creating\\nFinOps practices for data engineering— in such a new practice area,\\na good deal of territory is yet to be mapped out.\\nNow that you have a high-level understanding of good data\\narchitecture principles, let’s dive a bit deeper into the major concepts\\nyou’ll need to design and build good data architecture.\\nMajor Architecture Concepts\\nIf you follow the current trends in data, it seems like new types of\\ndata tools and architectures are arriving on the scene every week.\\nAmidst this flurry of activity, we must not lose sight of the main goal\\nof all of these architectures: to take data and transform it into\\nsomething useful for downstream consumption.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c777d7eb-ac1f-405f-849d-991590230d6f', embedding=None, metadata={'page_label': '139', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Domains and Services\\nA sphere of knowledge, influence, or activity. The subject area to\\nwhich the user applies a program is the domain of the software.\\n—Eric Evans\\nBefore diving into the components of the architecture, let’s briefly\\ncover two terms you’ll see come up very often: domain and services.\\nA domain is the real-world subject area for which you’re architecting.\\nA service is a set of functionality whose goal is to accomplish a task.\\nFor example, you might have a sales order-processing service\\nwhose task is to process orders as they are created. The sales\\norder-processing service’s only job is to process orders; it doesn’t\\nprovide other functionality, such as inventory management or\\nupdating user profiles.\\nA domain can contain multiple services. For example, you might\\nhave a sales domain with three services: orders, invoicing, and\\nproducts. Each service has particular tasks that support the sales\\ndomain. Other domains may also share services (Figure 3-3). In this\\ncase, the accounting domain is responsible for basic accounting\\nfunctions: invoicing, payroll, and accounts receivable (AR). Notice\\nthe accounting domain shares the invoice service with the sales\\ndomain since a sale generates an invoice, and accounting must\\nkeep track of invoices to ensure that payment is received. Sales and\\naccounting own their respective domains.\\nFigure 3-3. Two domains (sales and accounting) share a common service\\n(invoices), and sales and accounting own their respective domains', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='877bdfab-3b94-411b-8859-2eab4167adb2', embedding=None, metadata={'page_label': '140', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='When thinking about what constitutes a domain, focus on what the\\ndomain represents in the real world and work backward. In the\\npreceding example, the sales domain should represent what\\nhappens with the sales function in your company. When architecting\\nthe sales domain, avoid cookie-cutter copying and pasting from what\\nother companies do. Your company’s sales function likely has unique\\naspects that require specific services to make it work the way your\\nsales team expects.\\nIdentify what should go in the domain. When determining what the\\ndomain should encompass and what services to include, the best\\nadvice is to simply go and talk with users and stakeholders, listen to\\nwhat they’re saying, and build the services that will help them do\\ntheir job. Avoid the classic trap of architecting in a vacuum.\\nDistributed Systems, Scalability, and Designing\\nfor Failure\\nThe discussion in this section is related to our second and third\\nprinciples of data engineering architecture discussed previously: plan\\nfor failure and architect for scalability. As data engineers, we’re\\ninterested in four closely related characteristics of data systems\\n(availability and reliability were mentioned previously, but we\\nreiterate them here for completeness):\\nScalability\\nAllows us to increase the capacity of a system to improve\\nperformance and handle the demand. For example, we might\\nwant to scale a system to handle a high rate of queries or\\nprocess a huge data set.\\nElasticity', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8a795b53-a96c-4b27-b492-adbb7091e960', embedding=None, metadata={'page_label': '141', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The ability of a scalable system to scale dynamically; a highly\\nelastic system can automatically scale up and down based on the\\ncurrent workload. Scaling up is critical as demand increases,\\nwhile scaling down saves money in a cloud environment. Modern\\nsystems sometimes scale to zero, meaning they can\\nautomatically shut down when idle.\\nAvailability\\nThe percentage of time an IT service or component is in an\\noperable state.\\nReliability\\nThe system’s probability of meeting defined standards in\\nperforming its intended function during a specified interval.\\nTIP\\nSee PagerDuty’s “Why Are Availability and Reliability Crucial?” web\\npage for definitions and background on availability and reliability.\\nHow are these characteristics related? If a system fails to meet\\nperformance requirements during a specified interval, it may become\\nunresponsive. Thus low reliability can lead to low availability. On the\\nother hand, dynamic scaling helps ensure adequate performance\\nwithout manual intervention from engineers—elasticity improves\\nreliability.\\nScalability can be realized in a variety of ways. For your services and\\ndomains, does a single machine handle everything? A single\\nmachine can be scaled vertically; you can increase resources (CPU,\\ndisk, memory, I/O). But there are hard limits to possible resources on', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ee01498-68e8-4c77-931d-5613e8667ab9', embedding=None, metadata={'page_label': '142', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='a single machine. Also, what happens if this machine dies? Given\\nenough time, some components will eventually fail. What’s your plan\\nfor backup and failover? Single machines generally can’t offer high\\navailability and reliability.\\nWe utilize a distributed system to realize higher overall scaling\\ncapacity and increased availability and reliability. Horizontal scaling\\nallows you to add more machines to satisfy load and resource\\nrequirements (Figure 3-4). Common horizontally scaled systems\\nhave a leader node that acts as the main point of contact for the\\ninstantiation, progress, and completion of workloads. When a\\nworkload is started, the leader node distributes tasks to the worker\\nnodes within its system, completing the tasks and returning the\\nresults to the leader node. Typical modern distributed architectures\\nalso build in redundancy. Data is replicated so that if a machine dies,\\nthe other machines can pick up where the missing server left off; the\\ncluster may add more machines to restore capacity.\\nFigure 3-4. A simple horizontal distributed system utilizing a leader-follower\\narchitecture, with one leader node and three worker nodes\\nDistributed systems are widespread in the various data technologies\\nyou’ll use across your architecture. Almost every cloud data\\nwarehouse object storage system you use has some notion of\\ndistribution under the hood. Management details of the distributed\\nsystem are typically abstracted away, allowing you to focus on high-\\nlevel architecture instead of low-level plumbing. However, we highly\\nrecommend that you learn more about distributed systems because', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='190ebd86-a743-4177-8776-1dadc6ac7ef3', embedding=None, metadata={'page_label': '143', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='these details can be extremely helpful in understanding and\\nimproving the performance of your pipelines; Martin Kleppmann’s\\nDesigning Data-Intensive Applications (O’Reilly) is an excellent\\nresource.\\nTight Versus Loose Coupling: Tiers, Monoliths,\\nand Microservices\\nWhen designing a data architecture, you choose how much\\ninterdependence you want to include within your various domains,\\nservices, and resources. On one end of the spectrum, you can\\nchoose to have extremely centralized dependencies and workflows.\\nEvery part of a domain and service is vitally dependent upon every\\nother domain and service. This pattern is known as tightly coupled.\\nOn the other end of the spectrum, you have decentralized domains\\nand services that do not have strict dependence on each other, in a\\npattern known as loose coupling. In a loosely coupled scenario, it’s\\neasy for decentralized teams to build systems whose data may not\\nbe usable by their peers. Be sure to assign common standards,\\nownership, responsibility, and accountability to the teams owning\\ntheir respective domains and services. Designing “good” data\\narchitecture relies on trade-offs between the tight and loose coupling\\nof domains and services.\\nIt’s worth noting that many of the ideas in this section originate in\\nsoftware development. We’ll try to retain the context of these big\\nideas’ original intent and spirit—keeping them agnostic of data—\\nwhile later explaining some differences you should be aware of when\\napplying these concepts to data specifically.\\nArchitecture tiers\\nAs you develop your architecture, it helps to be aware of architecture\\ntiers. Your architecture has layers—data, application, business logic,\\npresentation, and so forth—and you need to know how to decouple', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='70f2fe89-8e32-419e-b2cd-3eebfc0fbe1e', embedding=None, metadata={'page_label': '144', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='these layers. Because tight coupling of modalities presents obvious\\nvulnerabilities, keep in mind how you structure the layers of your\\narchitecture to achieve maximum reliability and flexibility. Let’s look\\nat single-tier and multitier architecture.\\nSingle tier\\nIn a single-tier architecture, your database and application are tightly\\ncoupled, residing on a single server (Figure 3-5). This server could\\nbe your laptop or a single virtual machine (VM) in the cloud. The\\ntightly coupled nature means if the server, the database, or the\\napplication fails, the entire architecture fails. While single-tier\\narchitectures are good for prototyping and development, they are not\\nadvised for production environments because of the obvious failure\\nrisks.\\nFigure 3-5. Single-tier architecture\\nEven when single-tier architectures build in redundancy (for\\nexample, a failover replica), they present significant limitations in\\nother ways. For instance, it is often impractical (and not advisable) to\\nrun analytics queries against production application databases.\\nDoing so risks overwhelming the database and causing the\\napplication to become unavailable. A single-tier architecture is fine\\nfor testing systems on a local machine but is not advised for\\nproduction uses.\\nMultitier', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5539c48a-3661-4e90-b51b-eb13e0877a4a', embedding=None, metadata={'page_label': '145', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The challenges of a tightly coupled single-tier architecture are solved\\nby decoupling the data and application. A multitier (also known as n-\\ntier) architecture is composed of separate layers: data, application,\\nbusiness logic, presentation, etc. These layers are bottom-up and\\nhierarchical, meaning the lower layer isn’t necessarily dependent on\\nthe upper layers; the upper layers depend on the lower layers. The\\nnotion is to separate data from the application, and application from\\nthe presentation.\\nA common multitier architecture is a three-tier architecture, a widely\\nused client-server design. A three-tier architecture consists of data,\\napplication logic, and presentation tiers (Figure 3-6). Each tier is\\nisolated from the other, allowing for separation of concerns. With a\\nthree-tier architecture, you’re free to use whatever technologies you\\nprefer within each tier without the need to be monolithically focused.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b5ee84bf-700c-4679-9b70-fe2553ae9c7b', embedding=None, metadata={'page_label': '146', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3-6. A three-tier architecture\\nWe’ve seen many single-tier architectures in production. Single-tier\\narchitectures offer simplicity but also severe limitations. Eventually,\\nan organization or application outgrows this arrangement; it works\\nwell until it doesn’t. For instance, in a single-tier architecture, the\\ndata and logic layers share and compete for resources (disk, CPU,\\nand memory) in ways that are simply avoided in a multitier\\narchitecture. Resources are spread across various tiers. Data\\nengineers should use tiers to evaluate their layered architecture and\\nthe way dependencies are handled. Again, start simple and bake in\\nevolution to additional tiers as your architecture becomes more\\ncomplex.\\nIn a multitier architecture, you need to consider separating your\\nlayers and the way resources are shared within layers when working', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42857e91-2f5e-4616-80ee-5353d7fa4f3d', embedding=None, metadata={'page_label': '147', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='with a distributed system. Distributed systems under the hood power\\nmany technologies you’ll encounter across the data engineering\\nlifecycle. First, think about whether you want resource contention\\nwith your nodes. If not, exercise a shared-nothing architecture: a\\nsingle node handles each request, meaning other nodes do not\\nshare resources such as memory, disk, or CPU with this node or with\\neach other. Data and resources are isolated to the node.\\nAlternatively, various nodes can handle multiple requests and share\\nresources but at the risk of resource contention. Another\\nconsideration is whether nodes should share the same disk and\\nmemory accessible by all nodes. This is called a shared disk\\narchitecture and is common when you want shared resources if a\\nrandom node failure occurs.\\nMonoliths\\nThe general notion of a monolith includes as much as possible under\\none roof; in its most extreme version, a monolith consists of a single\\ncodebase running on a single machine that provides both the\\napplication logic and user interface.\\nCoupling within monoliths can be viewed in two ways: technical\\ncoupling and domain coupling. Technical coupling refers to\\narchitectural tiers, while domain coupling refers to the way domains\\nare coupled together. A monolith has varying degrees of coupling\\namong technologies and domains. You could have an application\\nwith various layers decoupled in a multitier architecture but still share\\nmultiple domains. Or, you could have a single-tier architecture\\nserving a single domain.\\nThe tight coupling of a monolith implies a lack of modularity of its\\ncomponents. Swapping out or upgrading components in a monolith\\nis often an exercise in trading one pain for another. Because of the\\ntightly coupled nature, reusing components across the architecture is\\ndifficult or impossible. When evaluating how to improve a monolithic\\narchitecture, it’s often a game of whack-a-mole: one component is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a09fdec4-8849-4205-968d-bab796a72420', embedding=None, metadata={'page_label': '148', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='improved, often at the expense of unknown consequences with other\\nareas of the monolith.\\nData teams will often ignore solving the growing complexity of their\\nmonolith, letting it devolve into a big ball of mud.\\nChapter 4 provides a more extensive discussion comparing\\nmonoliths to distributed technologies. We also discuss the distributed\\nmonolith, a strange hybrid that emerges when engineers build\\ndistributed systems with excessive tight coupling.\\nMicroservices\\nCompared with the attributes of a monolith—interwoven services,\\ncentralization, and tight coupling among services—microservices are\\nthe polar opposite. Microservices architecture comprises separate,\\ndecentralized, and loosely coupled services. Each service has a\\nspecific function and is decoupled from other services operating\\nwithin its domain. If one service temporarily goes down, it won’t\\naffect the ability of other services to continue functioning.\\nA question that comes up often is how to convert your monolith into\\nmany microservices (Figure 3-7). This completely depends on how\\ncomplex your monolith is and how much effort it will be to start\\nextracting services out of it. It’s entirely possible that your monolith\\ncannot be broken apart, in which case, you’ll want to start creating a\\nnew parallel architecture that has the services decoupled in a\\nmicroservices-friendly manner. We don’t suggest an entire refactor\\nbut instead break out services. The monolith didn’t arrive overnight\\nand is a technology issue as an organizational one. Be sure you get\\nbuy-in from stakeholders of the monolith if you plan to break it apart.\\nIf you’d like to learn more about breaking apart a monolith, we\\nsuggest reading the fantastic, pragmatic guide Software\\nArchitecture: The Hard Parts by Neal Ford et al. (O’Reilly).\\nConsiderations for data architecture', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02454e23-d990-4676-9f2b-5daf6e4dd3b9', embedding=None, metadata={'page_label': '149', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As we mentioned at the start of this section, the concepts of tight\\nversus loose coupling stem from software development, with some\\nof these concepts dating back over 20 years. Though architectural\\npractices in data are now adopting those from software\\ndevelopment, it’s still common to see very monolithic, tightly coupled\\ndata architectures. Some of this is due to the nature of existing data\\ntechnologies and the way they integrate.\\nFor example, data pipelines might consume data from many sources\\ningested into a central data warehouse. The central data warehouse\\nis inherently monolithic. A move toward a microservices equivalent\\nwith a data warehouse is to decouple the workflow with domain-\\nspecific data pipelines connecting to corresponding domain-specific\\ndata warehouses. For example, the sales data pipeline connects to\\nthe sales-specific data warehouse, and the inventory and product\\ndomains follow a similar pattern.\\nFigure 3-7. An extremely monolithic architecture runs all functionality inside a\\nsingle codebase, potentially colocating a database on the same host server\\nRather than dogmatically preach microservices over monoliths\\n(among other arguments), we suggest you pragmatically use loose\\ncoupling as an ideal, while recognizing the state and limitations of\\nthe data technologies you’re using within your data architecture.\\nIncorporate reversible technology choices that allow for modularity\\nand loose coupling whenever possible.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8729168e-1a0c-440e-a264-e6475485005e', embedding=None, metadata={'page_label': '150', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As you can see in Figure 3-7, you separate the components of your\\narchitecture into different layers of concern in a vertical fashion.\\nWhile a multitier architecture solves the technical challenges of\\ndecoupling shared resources, it does not address the complexity of\\nsharing domains. Along the lines of single versus multitiered\\narchitecture, you should also consider how you separate the\\ndomains of your data architecture. For example, your analyst team\\nmight rely on data from sales and inventory. The sales and inventory\\ndomains are different and should be viewed as separate.\\nOne approach to this problem is centralization: a single team is\\nresponsible for gathering data from all domains and reconciling it for\\nconsumption across the organization. (This is a common approach in\\ntraditional data warehousing.) Another approach is the data mesh.\\nWith the data mesh, each software team is responsible for preparing\\nits data for consumption across the rest of the organization. We’ll say\\nmore about the data mesh later in this chapter.\\nOur advice: monoliths aren’t necessarily bad, and it might make\\nsense to start with one under certain conditions. Sometimes you\\nneed to move fast, and it’s much simpler to start with a monolith.\\nJust be prepared to break it into smaller pieces eventually; don’t get\\ntoo comfortable.\\nUser Access: Single Versus Multitenant\\nAs a data engineer, you have to make decisions about sharing\\nsystems across multiple teams, organizations, and customers. In\\nsome sense, all cloud services are multitenant, although this\\nmultitenancy occurs at various grains. For example, a cloud compute\\ninstance is usually on a shared server, but the VM itself provides\\nsome degree of isolation. Object storage is a multitenant system, but\\ncloud vendors guarantee security and isolation so long as customers\\nconfigure their permissions correctly.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5019899-a2d4-4b06-b3c8-ac22d05ff447', embedding=None, metadata={'page_label': '151', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Engineers frequently need to make decisions about multitenancy at\\na much smaller scale. For example, do multiple departments in a\\nlarge company share the same data warehouse? Does the\\norganization share data for multiple large customers within the same\\ntable?\\nWe have two factors to consider in multitenancy: performance and\\nsecurity. With multiple large tenants within a cloud system, will the\\nsystem support consistent performance for all tenants, or will there\\nbe a noisy neighbor problem? (That is, will high usage from one\\ntenant degrade performance for other tenants?) Regarding security,\\ndata from different tenants must be properly isolated. When a\\ncompany has multiple external customer tenants, these tenants\\nshould not be aware of one another, and engineers must prevent\\ndata leakage. Strategies for data isolation vary by system. For\\ninstance, it is often perfectly acceptable to use multitenant tables and\\nisolate data through views. However, you must make certain that\\nthese views cannot leak data. Read vendor or project documentation\\nto understand appropriate strategies and risks.\\nEvent-Driven Architecture\\nYour business is rarely static. Things often happen in your business,\\nsuch as getting a new customer, a new order from a customer, or an\\norder for a product or service. These are all examples of events that\\nare broadly defined as something that happened, typically a change\\nin the state of something. For example, a new order might be\\ncreated by a customer, or a customer might later make an update to\\nthis order.\\nAn event-driven workflow (Figure 3-8) encompasses the ability to\\ncreate, update, and asynchronously move events across various\\nparts of the data engineering lifecycle. This workflow boils down to\\nthree main areas: event production, routing, and consumption. An\\nevent must be produced and routed to something that consumes it', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99d84597-316d-4ed1-bf40-7b1cf399f880', embedding=None, metadata={'page_label': '152', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='without tightly coupled dependencies among the producer, event\\nrouter, and consumer.\\nFigure 3-8. In an event-driven workflow, an event is produced, routed, and then\\nconsumed\\nAn event-driven architecture (Figure 3-9) embraces the event-driven\\nworkflow and uses this to communicate across various services. The\\nadvantage of an event-driven architecture is that it distributes the\\nstate of an event across multiple services. This is helpful if a service\\ngoes offline, a node fails in a distributed system, or you’d like\\nmultiple consumers or services to access the same events. Anytime\\nyou have loosely coupled services, this is a candidate for event-\\ndriven architecture. Many of the examples we describe later in this\\nchapter incorporate some form of event-driven architecture.\\nFigure 3-9. In an event-driven architecture, events are passed between loosely\\ncoupled services\\nYou’ll learn more about event-driven streaming and messaging\\nsystems in Chapter 5.\\nBrownfield Versus Greenfield Projects', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='11c8339e-7632-4eb9-80ba-61aaeae6af60', embedding=None, metadata={'page_label': '153', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Before you design your data architecture project, you need to know\\nwhether you’re starting with a clean slate or redesigning an existing\\narchitecture. Each type of project requires assessing trade-offs,\\nalbeit with different considerations and approaches. Projects roughly\\nfall into two buckets: brownfield and greenfield.\\nBrownfield projects\\nBrownfield projects often involve refactoring and reorganizing an\\nexisting architecture and are constrained by the choices of the\\npresent and past. Because a key part of architecture is change\\nmanagement, you must figure out a way around these limitations\\nand design a path forward to achieve your new business and\\ntechnical objectives. Brownfield projects require a thorough\\nunderstanding of the legacy architecture and the interplay of various\\nold and new technologies. All too often, it’s easy to criticize a prior\\nteam’s work and decisions, but it is far better to dig deep, ask\\nquestions, and understand why decisions were made. Empathy and\\ncontext go a long way in helping you diagnose problems with the\\nexisting architecture, identify opportunities, and recognize pitfalls.\\nYou’ll need to introduce your new architecture and technologies and\\ndeprecate the old stuff at some point. Let’s look at a couple of\\npopular approaches. Many teams jump headfirst into an all-at-once\\nor big-bang overhaul of the old architecture, often figuring out\\ndeprecation as they go. Though popular, we don’t advise this\\napproach because of the associated risks and lack of a plan. This\\npath often leads to disaster, with many irreversible and costly\\ndecisions. Your job is to make reversible, high-ROI decisions.\\nA popular alternative to a direct rewrite is the strangler pattern: new\\nsystems slowly and incrementally replace a legacy architecture’s\\ncomponents. Eventually, the legacy architecture is completely\\nreplaced. The attraction to the strangler pattern is its targeted and\\nsurgical approach of deprecating one piece of a system at a time.\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5da17050-9092-4820-a0f0-4e836eed2621', embedding=None, metadata={'page_label': '154', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This allows for flexible and reversible decisions while assessing the\\nimpact of the deprecation on dependent systems.\\nIt’s important to note that deprecation might be “ivory tower” advice\\nand not practical or achievable. Eradicating legacy technology or\\narchitecture might be impossible if you’re at a large organization.\\nSomeone, somewhere, is using these legacy components. As\\nsomeone once said, “Legacy is a condescending way to describe\\nsomething that makes money.”\\nIf you can deprecate, understand there are numerous ways to\\ndeprecate your old architecture. It is critical to demonstrate value on\\nthe new platform by gradually increasing its maturity to show\\nevidence of success and then follow an exit plan to shut down old\\nsystems.\\nGreenfield projects\\nOn the opposite end of the spectrum, a greenfield project allows you\\nto pioneer a fresh start, unconstrained by the history or legacy of a\\nprior architecture. Greenfield projects tend to be easier than\\nbrownfield projects, and many data architects and engineers find\\nthem more fun! You have the opportunity to try the newest and\\ncoolest tools and architectural patterns. What could be more\\nexciting?\\nYou should watch out for some things before getting too carried\\naway. We see teams get overly exuberant with shiny object\\nsyndrome. They feel compelled to reach for the latest and greatest\\ntechnology fad without understanding how it will impact the value of\\nthe project. There’s also a temptation to do resume-driven\\ndevelopment, stacking up impressive new technologies without\\nprioritizing the project’s ultimate goals. Always prioritize\\nrequirements over building something cool.\\nWhether you’re working on a brownfield or greenfield project, always\\nfocus on the tenets of “good” data architecture. Assess trade-offs,\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d8f771a1-c4e0-4eea-937b-8c903ba6b0f1', embedding=None, metadata={'page_label': '155', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='make flexible and reversible decisions, and strive for positive ROI.\\nNow, we’ll look at examples and types of architectures—some\\nestablished for decades (the data warehouse), some brand-new (the\\ndata lakehouse), and some that quickly came and went but still\\ninfluence current architecture patterns (Lambda architecture).\\nExamples and Types of Data Architecture\\nBecause data architecture is an abstract discipline, it helps to reason\\nby example. In this section, we outline prominent examples and\\ntypes of data architecture that are popular today. Though this set of\\nexamples is by no means exhaustive, the intention is to expose you\\nto some of the most common data architecture patterns and to get\\nyou thinking about the requisite flexibility and trade-off analysis\\nneeded when designing a good architecture for your use case.\\nData Warehouse\\nA data warehouse is a central data hub used for reporting and\\nanalysis. Data in a data warehouse is typically highly formatted and\\nstructured for analytics use cases. It’s among the oldest and most\\nwell-established data architectures.\\nIn 1990, Bill Inmon originated the notion of the data warehouse,\\nwhich he described as “a subject-oriented, integrated, nonvolatile,\\nand time-variant collection of data in support of management’s\\ndecisions.” Though technical aspects of the data warehouse have\\nevolved significantly, we feel this original definition still holds its\\nweight today.\\nIn the past, data warehouses were widely used at enterprises with\\nsignificant budgets (often in the millions of dollars) to acquire data\\nsystems and pay internal teams to provide ongoing support to\\nmaintain the data warehouse. This was expensive and labor-\\nintensive. Since then, the scalable, pay-as-you-go model has made\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2b02cef9-acb2-4bff-a321-5419738e0e93', embedding=None, metadata={'page_label': '156', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='cloud data warehouses accessible even to tiny companies. Because\\na third-party provider manages the data warehouse infrastructure,\\ncompanies can do a lot more with fewer people, even as the\\ncomplexity of their data grows.\\nIt’s worth noting two types of data warehouse architecture:\\norganizational and technical. The organizational data warehouse\\narchitecture organizes data associated with certain business team\\nstructures and processes. The technical data warehouse\\narchitecture reflects the technical nature of the data warehouse,\\nsuch as MPP. A company can have a data warehouse without an\\nMPP system or run an MPP system that is not organized as a data\\nwarehouse. However, the technical and organizational architectures\\nhave existed in a virtuous cycle and are frequently identified with\\neach other.\\nThe organizational data warehouse architecture has two main\\ncharacteristics:\\nSeparates analytics processes (OLAP) from production databases\\n(online transaction processing)\\nThis separation is critical as businesses grow. Moving data into a\\nseparate physical system directs load away from production\\nsystems and improves analytics performance.\\nCentralizes and organizes data\\nTraditionally, a data warehouse pulls data from application\\nsystems by using ETL. The extract phase pulls data from source\\nsystems. The transformation phase cleans and standardizes\\ndata, organizing and imposing business logic in a highly modeled\\nform. (Chapter 8 covers transformations and data models.) The\\nload phase pushes data into the data warehouse target database\\nsystem. Data is loaded into multiple data marts that serve the\\nanalytical needs for specific lines or business and departments.\\nFigure 3-10 shows the general workflow. The data warehouse', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf2a2337-f31a-454b-9d55-029da9cf9e6f', embedding=None, metadata={'page_label': '157', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and ETL go hand in hand with specific business structures,\\nincluding DBA and ETL developer teams that implement the\\ndirection of business leaders to ensure that data for reporting and\\nanalytics corresponds to business processes. An ETL system is\\nnot a prerequisite for a data warehouse, as you will learn when\\nwe discuss another load and transformation pattern, ELT.\\nFigure 3-10. Basic data warehouse with ETL\\nRegarding the technical data warehouse architecture, the first MPP\\nsystems in the late 1970s became popular in the 1980s. MPPs\\nsupport essentially the same SQL semantics used in relational\\napplication databases. Still, they are optimized to scan massive\\namounts of data in parallel and thus allow high-performance\\naggregation and statistical calculations. In recent years, MPP\\nsystems have increasingly shifted from a row-based to a columnar\\narchitecture to facilitate even larger data and queries, especially in\\ncloud data warehouses. MPPs are indispensable for running\\nperformant queries for large enterprises as data and reporting needs\\ngrow.\\nOne variation on ETL is ELT. With the ELT data warehouse\\narchitecture, data gets moved more or less directly from production\\nsystems into a staging area in the data warehouse. Staging in this\\nsetting indicates that the data is in a raw form. Rather than using an\\nexternal system, transformations are handled directly in the data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='95d64819-02fe-49e6-a4cf-2b1366e39cf2', embedding=None, metadata={'page_label': '158', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='warehouse. The intention is to take advantage of the massive\\ncomputational power of cloud data warehouses and data processing\\ntools. Data is processed in batches, and transformed output is\\nwritten into tables and views for analytics. Figure 3-11 shows the\\ngeneral process. ELT is also popular in a streaming arrangement, as\\nevents are streamed from a CDC process, stored in a staging area,\\nand then subsequently transformed within the data warehouse.\\nFigure 3-11. ELT—extract, load, and transform\\nA second version of ELT was popularized during big data growth in\\nthe Hadoop ecosystem. This is transform-on-read ELT, which we\\ndiscuss in “Data Lake”.\\nThe cloud data warehouse\\nCloud data warehouses represent a significant evolution of the on-\\npremises data warehouse architecture and have thus led to\\nsignificant changes to the organizational architecture. Amazon\\nRedshift kicked off the cloud data warehouse revolution. Instead of\\nneeding to appropriately size an MPP system for the next several\\nyears and sign a multimillion-dollar contract to procure the system,\\ncompanies had the option of spinning up a Redshift cluster on\\ndemand, scaling it up over time as data and analytics demand grew.\\nThey could even spin up new Redshift clusters on demand to serve\\nspecific workloads and quickly delete clusters when they were no\\nlonger needed.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c996491-0505-467c-82b8-ce3662451573', embedding=None, metadata={'page_label': '159', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Google BigQuery, Snowflake, and other competitors popularized the\\nidea of separating compute from storage. In this architecture, data is\\nhoused in object storage, allowing virtually limitless storage. This\\nalso gives users the option to spin up computing power on demand,\\nproviding ad hoc big data capabilities without the long-term cost of\\nthousands of nodes.\\nCloud data warehouses expand the capabilities of MPP systems to\\ncover many big data use cases that required a Hadoop cluster in the\\nvery recent past. They can readily process petabytes of data in a\\nsingle query. They typically support data structures that allow the\\nstorage of tens of megabytes of raw text data per row or extremely\\nrich and complex JSON documents. As cloud data warehouses (and\\ndata lakes) mature, the line between the data warehouse and the\\ndata lake will continue to blur.\\nSo significant is the impact of the new capabilities offered by cloud\\ndata warehouses that we might consider jettisoning the term data\\nwarehouse altogether. Instead, these services are evolving into a\\nnew data platform with much broader capabilities than those offered\\nby a traditional MPP system.\\nData marts\\nA data mart is a more refined subset of a warehouse designed to\\nserve analytics and reporting, focused on a single suborganization,\\ndepartment, or line of business; every department has its own data\\nmart, specific to its needs. This is in contrast to the full data\\nwarehouse that serves the broader organization or business.\\nData marts exist for two reasons. First, a data mart makes data more\\neasily accessible to analysts and report developers. Second, data\\nmarts provide an additional stage of transformation beyond that\\nprovided by the initial ETL or ELT pipelines. This can significantly\\nimprove performance if reports or analytics queries require complex\\njoins and aggregations of data, especially when the raw data is\\nlarge. Transform processes can populate the data mart with joined', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd0e4cd4-5c60-438c-becf-1bd4b5542818', embedding=None, metadata={'page_label': '160', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and aggregated data to improve performance for live queries.\\nFigure 3-12 shows the general workflow. We discuss data marts, and\\nmodeling data for data marts, in Chapter 8.\\nFigure 3-12. ETL or ELT plus data marts\\nData Lake\\nAmong the most popular architectures that appeared during the big\\ndata era is the data lake. Instead of imposing tight structural\\nlimitations on data, why not simply dump all of your data—structured\\nand unstructured—into a central location? The data lake promised to\\nbe a democratizing force, liberating the business to drink from a\\nfountain of limitless data. The first-generation data lake, “data lake\\n1.0,” made solid contributions but generally failed to deliver on its\\npromise.\\nData lake 1.0 started with HDFS. As the cloud grew in popularity,\\nthese data lakes moved to cloud-based object storage, with\\nextremely cheap storage costs and virtually limitless storage\\ncapacity. Instead of relying on a monolithic data warehouse where\\nstorage and compute are tightly coupled, the data lake allows an\\nimmense amount of data of any size and type to be stored. When\\nthis data needs to be queried or transformed, you have access to\\nnearly unlimited computing power by spinning up a cluster on\\ndemand, and you can pick your favorite data-processing technology\\nfor the task at hand—MapReduce, Spark, Ray, Presto, Hive, etc.\\nDespite the promise and hype, data lake 1.0 had serious\\nshortcomings. The data lake became a dumping ground; terms such', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5b5d66f-9918-4be6-8c30-ef586538f4c1', embedding=None, metadata={'page_label': '161', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='as data swamp, dark data, and WORN were coined as once-\\npromising data projects failed. Data grew to unmanageable sizes,\\nwith little in the way of schema management, data cataloging, and\\ndiscovery tools. In addition, the original data lake concept was\\nessentially write-only, creating huge headaches with the arrival of\\nregulations such as GDPR that required targeted deletion of user\\nrecords.\\nProcessing data was also challenging. Relatively banal data\\ntransformations such as joins were a huge headache to code as\\nMapReduce jobs. Later frameworks such as Pig and Hive somewhat\\nimproved the situation for data processing but did little to address the\\nbasic problems of data management. Simple data manipulation\\nlanguage (DML) operations common in SQL—deleting or updating\\nrows—were painful to implement, generally achieved by creating\\nentirely new tables. While big data engineers radiated a particular\\ndisdain for their counterparts in data warehousing, the latter could\\npoint out that data warehouses provided basic data management\\ncapabilities out of the box, and that SQL was an efficient tool for\\nwriting complex, performant queries and transformations.\\nData lake 1.0 also failed to deliver on another core promise of the big\\ndata movement. Open source software in the Apache ecosystem\\nwas touted as a means to avoid multimillion-dollar contracts for\\nproprietary MPP systems. Cheap, off-the-shelf hardware would\\nreplace custom vendor solutions. In reality, big data costs ballooned\\nas the complexities of managing Hadoop clusters forced companies\\nto hire large teams of engineers at high salaries. Companies often\\nchose to purchase licensed, customized versions of Hadoop from\\nvendors to avoid the exposed wires and sharp edges of the raw\\nApache codebase and acquire a set of scaffolding tools to make\\nHadoop more user-friendly. Even companies that avoided managing\\nHadoop clusters using cloud storage had to spend big on talent to\\nwrite MapReduce jobs.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4da423ab-d594-45df-8c42-e2e6e456f20c', embedding=None, metadata={'page_label': '162', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We should be careful not to understate the utility and power of first-\\ngeneration data lakes. Many organizations found significant value in\\ndata lakes—especially huge, heavily data-focused Silicon Valley tech\\ncompanies like Netflix and Facebook. These companies had the\\nresources to build successful data practices and create their custom\\nHadoop-based tools and enhancements. But for many organizations,\\ndata lakes turned into an internal superfund site of waste,\\ndisappointment, and spiraling costs.\\nConvergence, Next-Generation Data Lakes, and\\nthe Data Platform\\nIn response to the limitations of first-generation data lakes, various\\nplayers have sought to enhance the concept to fully realize its\\npromise. For example, Databricks introduced the notion of a data\\nlakehouse. The lakehouse incorporates the controls, data\\nmanagement, and data structures found in a data warehouse while\\nstill housing data in object storage and supporting a variety of query\\nand transformation engines. In particular, the data lakehouse\\nsupports atomicity, consistency, isolation, and durability (ACID)\\ntransactions, a big departure from the original data lake, where you\\nsimply pour in data and never update or delete it. The term data\\nlakehouse suggests a convergence between data lakes and data\\nwarehouses.\\nThe technical architecture of cloud data warehouses has evolved to\\nbe very similar to a data lake architecture. Cloud data warehouses\\nseparate compute from storage, support petabyte-scale queries,\\nstore unstructured text and semistructured objects, and integrate\\nwith advanced processing technologies such as Spark or Beam.\\nWe believe that the trend of convergence will only continue. The\\ndata lake and the data warehouse will still exist as different\\narchitectures. In practice, their capabilities will converge so that few\\nusers will notice a boundary between them in their day-to-day work.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3a53f331-90cd-4388-9202-2538cb6606bf', embedding=None, metadata={'page_label': '163', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We now see several vendors offering data platforms that combine\\ndata lake and data warehouse capabilities. From our perspective,\\nAWS, Azure, Google Cloud, Snowflake, and Databricks are class\\nleaders, each offering a constellation of tightly integrated tools for\\nworking with data, running the gamut from relational to completely\\nunstructured. Instead of choosing between a data lake or data\\nwarehouse architecture, future data engineers will have the option to\\nchoose a converged data platform based on a variety of factors,\\nincluding vendor, ecosystem, and relative openness.\\nModern Data Stack\\nThe modern data stack (Figure 3-13) is currently a trendy analytics\\narchitecture that highlights the type of abstraction we expect to see\\nmore widely used over the next several years. Whereas past data\\nstacks relied on expensive, monolithic toolsets, the main objective of\\nthe modern data stack is to use cloud-based, plug-and-play, easy-to-\\nuse, off-the-shelf components to create a modular and cost-effective\\ndata architecture. These components include data pipelines,\\nstorage, transformation, data management/governance, monitoring,\\nvisualization, and exploration. The domain is still in flux, and the\\nspecific tools are changing and evolving rapidly, but the core aim will\\nremain the same: to reduce complexity and increase modularization.\\nNote that the notion of a modern data stack integrates nicely with the\\nconverged data platform idea from the previous section.\\nFigure 3-13. Basic components of the modern data stack\\nKey outcomes of the modern data stack are self-service (analytics\\nand pipelines), agile data management, and using open source tools\\nor simple proprietary tools with clear pricing structures. Community is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='007af060-aa22-4960-89b9-867c4d12df08', embedding=None, metadata={'page_label': '164', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='a central aspect of the modern data stack as well. Unlike products of\\nthe past that had releases and roadmaps largely hidden from users,\\nprojects and companies operating in the modern data stack space\\ntypically have strong user bases and active communities that\\nparticipate in the development by using the product early, suggesting\\nfeatures, and submitting pull requests to improve the code.\\nRegardless of where “modern” goes (we share our ideas in\\nChapter 11), we think the key concept of plug-and-play modularity\\nwith easy-to-understand pricing and implementation is the way of the\\nfuture. Especially in analytics engineering, the modern data stack is\\nand will continue to be the default choice of data architecture.\\nThroughout the book, the architecture we reference contains pieces\\nof the modern data stack, such as cloud-based and plug-and-play\\nmodular components.\\nLambda Architecture\\nIn the “old days” (the early to mid-2010s), the popularity of working\\nwith streaming data exploded with the emergence of Kafka as a\\nhighly scalable message queue and frameworks such as Apache\\nStorm and Samza for streaming/real-time analytics. These\\ntechnologies allowed companies to perform new types of analytics\\nand modeling on large amounts of data, user aggregation and\\nranking, and product recommendations. Data engineers needed to\\nfigure out how to reconcile batch and streaming data into a single\\narchitecture. The Lambda architecture was one of the early popular\\nresponses to this problem.\\nIn a Lambda architecture (Figure 3-14), you have systems operating\\nindependently of each other—batch, streaming, and serving. The\\nsource system is ideally immutable and append-only, sending data to\\ntwo destinations for processing: stream, and batch. In-stream\\nprocessing intends to serve the data with the lowest possible latency\\nin a “speed” layer, usually a NoSQL database. In the batch layer,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd2383a9-b4b5-46ed-9036-1e45996f587d', embedding=None, metadata={'page_label': '165', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data is processed and transformed in a system such as a data\\nwarehouse, creating precomputed and aggregated views of the data.\\nThe serving layer provides a combined view by aggregating query\\nresults from the two layers.\\nFigure 3-14. Lambda architecture\\nLambda architecture has its share of challenges and criticisms.\\nManaging multiple systems with different codebases is as difficult as\\nit sounds, creating error-prone systems with code and data that are\\nextremely difficult to reconcile.\\nWe mention Lambda architecture because it still gets attention and is\\npopular in search-engine results for data architecture. Lambda isn’t\\nour first recommendation if you’re trying to combine streaming and\\nbatch data for analytics. Technology and practices have moved on.\\nNext, let’s look at a reaction to Lambda architecture, the Kappa\\narchitecture.\\nKappa Architecture\\nAs a response to the shortcomings of Lambda architecture, Jay\\nKreps proposed an alternative called Kappa architecture (Figure 3-\\n15).  The central thesis is this: why not just use a stream-\\nprocessing platform as the backbone for all data handling—\\ningestion, storage, and serving? This facilitates a true event-based\\narchitecture. Real-time and batch processing can be applied\\nseamlessly to the same data by reading the live event stream\\ndirectly and replaying large chunks of data for batch processing.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7fbc206-0cf9-419d-90f3-8ad6b6037ed3', embedding=None, metadata={'page_label': '166', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3-15. Kappa architecture\\nThough the original Kappa architecture article came out in 2014, we\\nhaven’t seen it widely adopted. There may be a couple of reasons\\nfor this. First, streaming itself is still a bit of a mystery for many\\ncompanies. Second, Kappa architecture turns out to be complicated\\nand expensive in practice. While some streaming systems can scale\\nto huge data volumes, they are complex and expensive; batch\\nstorage and processing remain much more efficient and cost-\\neffective for enormous historical datasets.\\nThe Dataflow Model and Unified Batch and\\nStreaming\\nBoth Lambda and Kappa sought to address limitations of the\\nHadoop ecosystem of the 2010s by trying to duct-tape together\\ncomplicated tools that were likely not natural fits in the first place.\\nThe central challenge of unifying batch and streaming data\\nremained, and Lambda and Kappa both provided inspiration and\\ngroundwork for continued progress in this pursuit.\\nOne of the central problems of managing batch and stream\\nprocessing is unifying multiple code paths. While the Kappa\\narchitecture relies on a unified queuing and storage layer, one still\\nhas to confront using different tools for collecting real-time statistics\\nor running batch aggregation jobs. Today, engineers seek to solve\\nthis in several ways. Google made its mark by developing the\\nDataflow model and the Apache Beam framework that implements\\nthis model.\\nThe core idea in the Dataflow model is to view all data as events, as\\nthe aggregation is performed over various types of windows.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea44c21a-b6f1-45c4-a25f-6e88b7986fd5', embedding=None, metadata={'page_label': '167', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ongoing real-time event streams are unbounded data. Data batches\\nare simply bounded event streams, and the boundaries provide a\\nnatural window. Engineers can choose from various windows for\\nreal-time aggregation, such as sliding or tumbling. Real-time and\\nbatch processing happens in the same system using nearly identical\\ncode.\\nThe philosophy of “batch as a special case of streaming” is now\\nmore pervasive. Various frameworks such as Flink and Spark have\\nadopted a similar approach.\\nArchitecture for IoT\\nThe Internet of Things (IoT) is the distributed collection of devices,\\naka things—computers, sensors, mobile devices, smart home\\ndevices, and anything else with an internet connection. Rather than\\ngenerating data from direct human input (think data entry from a\\nkeyboard), IoT data is generated from devices that collect data\\nperiodically or continuously from the surrounding environment and\\ntransmit it to a destination. IoT devices are often low-powered and\\noperate in low-resource/low bandwidth environments.\\nWhile the concept of IoT devices dates back at least a few decades,\\nthe smartphone revolution created a massive IoT swarm virtually\\novernight. Since then, numerous new IoT categories have emerged,\\nsuch as smart thermostats, car entertainment systems, smart TVs,\\nand smart speakers. The IoT has evolved from a futurist fantasy to a\\nmassive data engineering domain. We expect IoT to become one of\\nthe dominant ways data is generated and consumed, and this\\nsection goes a bit deeper than the others you’ve read.\\nHaving a cursory understanding of IoT architecture will help you\\nunderstand broader data architecture trends. Let’s briefly look at\\nsome IoT architecture concepts.\\nDevices', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f0d52e12-5515-4cdc-96c5-8ee33c7fc5b3', embedding=None, metadata={'page_label': '168', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Devices (also known as things) are the physical hardware connected\\nto the internet, sensing the environment around them and collecting\\nand transmitting data to a downstream destination. These devices\\nmight be used in consumer applications like a doorbell camera,\\nsmartwatch, or thermostat. The device might be an AI-powered\\ncamera that monitors an assembly line for defective components, a\\nGPS tracker to record vehicle locations, or a Raspberry Pi\\nprogrammed to download the latest tweets and brew your coffee.\\nAny device capable of collecting data from its environment is an IoT\\ndevice.\\nDevices should be minimally capable of collecting and transmitting\\ndata. However, the device might also crunch data or run ML on the\\ndata it collects before sending it downstream—edge computing and\\nedge machine learning, respectively.\\nA data engineer doesn’t necessarily need to know the inner details of\\nIoT devices but should know what the device does, the data it\\ncollects, any edge computations or ML it runs before transmitting the\\ndata, and how often it sends data. It also helps to know the\\nconsequences of a device or internet outage, environmental or other\\nexternal factors affecting data collection, and how these may impact\\nthe downstream collection of data from the device.\\nInterfacing with devices\\nA device isn’t beneficial unless you can get its data. This section\\ncovers some of the key components necessary to interface with IoT\\ndevices in the wild.\\nIoT gateway\\nAn IoT gateway is a hub for connecting devices and securely routing\\ndevices to the appropriate destinations on the internet. While you\\ncan connect a device directly to the internet without an IoT gateway,\\nthe gateway allows devices to connect using extremely little power. It', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ee6bfa86-67b2-444e-b9e2-e8858f0261b0', embedding=None, metadata={'page_label': '169', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='acts as a way station for data retention and manages an internet\\nconnection to the final data destination.\\nNew low-power WiFi standards are designed to make IoT gateways\\nless critical in the future, but these are just rolling out now. Typically,\\na swarm of devices will utilize many IoT gateways, one at each\\nphysical location where devices are present (Figure 3-16).\\nFigure 3-16. A device swarm (circles), IoT gateways, and message queue with\\nmessages (rectangles within the queue)\\nIngestion\\nIngestion begins with an IoT gateway, as discussed previously. From\\nthere, events and measurements can flow into an event ingestion\\narchitecture.\\nOf course, other patterns are possible. For instance, the gateway\\nmay accumulate data and upload it in batches for later analytics\\nprocessing. In remote physical environments, gateways may not\\nhave connectivity to a network much of the time. They may upload\\nall data only when they are brought into the range of a cellular or\\nWiFi network. The point is that the diversity of IoT systems and\\nenvironments presents complications—e.g., late-arriving data, data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4ec8dc22-34e5-4b6f-88a6-0eb4105fcb4e', embedding=None, metadata={'page_label': '170', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='structure and schema disparities, data corruption, and connection\\ndisruption—that engineers must account for in their architectures\\nand downstream analytics.\\nStorage\\nStorage requirements will depend a great deal on the latency\\nrequirement for the IoT devices in the system. For example, for\\nremote sensors collecting scientific data for analysis at a later time,\\nbatch object storage may be perfectly acceptable. However, near\\nreal-time responses may be expected from a system backend that\\nconstantly analyzes data in a home monitoring and automation\\nsolution. In this case, a message queue or time-series database is\\nmore appropriate. We discuss storage systems in more detail in\\nChapter 6.\\nServing\\nServing patterns are incredibly diverse. In a batch scientific\\napplication, data might be analyzed using a cloud data warehouse\\nand then served in a report. Data will be presented and served in\\nnumerous ways in a home-monitoring application. Data will be\\nanalyzed in the near time using a stream-processing engine or\\nqueries in a time-series database to look for critical events such as a\\nfire, electrical outage, or break-in. Detection of an anomaly will\\ntrigger alerts to the homeowner, the fire department, or other entity.\\nA batch analytics component also exists—for example, a monthly\\nreport on the state of the home.\\nOne significant serving pattern for IoT looks like reverse ETL\\n(Figure 3-17), although we tend not to use this term in the IoT\\ncontext. Think of this scenario: data from sensors on manufacturing\\ndevices is collected and analyzed. The results of these\\nmeasurements are processed to look for optimizations that will allow\\nequipment to operate more efficiently. Data is sent back to\\nreconfigure the devices and optimize them.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ceb7b428-2d30-4539-a684-35f030dea455', embedding=None, metadata={'page_label': '171', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3-17. IoT serving pattern for downstream use cases\\nScratching the surface of the IoT\\nIoT scenarios are incredibly complex, and IoT architecture and\\nsystems are also less familiar to data engineers who may have spent\\ntheir careers working with business data. We hope that this\\nintroduction will encourage interested data engineers to learn more\\nabout this fascinating and rapidly evolving specialization.\\nData Mesh\\nThe data mesh is a recent response to sprawling monolithic data\\nplatforms, such as centralized data lakes and data warehouses, and\\n“the great divide of data,” wherein the landscape is divided between\\noperational data and analytical data. The data mesh attempts to\\ninvert the challenges of centralized data architecture, taking the\\nconcepts of domain-driven design (commonly used in software\\narchitectures) and applying them to data architecture. Because the\\ndata mesh has captured much recent attention, you should be aware\\nof it.\\nA big part of the data mesh is decentralization, as Zhamak Dehghani\\nnoted in her groundbreaking article on the topic:\\nIn order to decentralize the monolithic data platform, we need to\\nreverse how we think about data, its locality, and ownership.\\nInstead of flowing the data from domains into a centrally owned\\ndata lake or platform, domains need to host and serve their\\ndomain datasets in an easily consumable way.\\nDehghani later identified four key components of the data mesh:\\n11\\n12\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='06264599-351c-4c7d-b45d-31e98045d0e9', embedding=None, metadata={'page_label': '172', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Domain-oriented decentralized data ownership and architecture\\nData as a product\\nSelf-serve data infrastructure as a platform\\nFederated computational governance\\nFigure 3-18 shows a simplified version of a data mesh architecture,\\nwith the three domains interoperating.\\nFigure 3-18. Simplified example of a data mesh architecture. Source: From Data\\nMesh, by Zhamak Dehghani. Copyright © 2022 Zhamak Dehghani. Published by\\nO’Reilly Media, Inc. Used with permission.\\nYou can learn more about data mesh in Dehghani’s book Data Mesh\\n(O’Reilly).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0688bee6-f9fa-4580-807c-5b887d527705', embedding=None, metadata={'page_label': '173', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Other Data Architecture Examples\\nData architectures have countless other variations, such as data\\nfabric, data hub, scaled architecture, metadata-first architecture,\\nevent-driven architecture, live data stack (Chapter 11), and many\\nmore. And new architectures will continue to emerge as practices\\nconsolidate and mature, and tooling simplifies and improves. We’ve\\nfocused on a handful of the most critical data architecture patterns\\nthat are extremely well established, evolving rapidly, or both.\\nAs a data engineer, pay attention to how new architectures may help\\nyour organization. Stay abreast of new developments by cultivating a\\nhigh-level awareness of the data engineering ecosystem\\ndevelopments. Be open-minded and don’t get emotionally attached\\nto one approach. Once you’ve identified potential value, deepen your\\nlearning and make concrete decisions. When done right, minor\\ntweaks—or major overhauls—in your data architecture can positively\\nimpact the business.\\nWho’s Involved with Designing a Data\\nArchitecture?\\nData architecture isn’t designed in a vacuum. Bigger companies may\\nstill employ data architects, but those architects will need to be\\nheavily in tune and current with the state of technology and data.\\nGone are the days of ivory tower data architecture. In the past,\\narchitecture was largely orthogonal to engineering. We expect this\\ndistinction will disappear as data engineering, and engineering in\\ngeneral, quickly evolves, becoming more agile, with less separation\\nbetween engineering and architecture.\\nIdeally, a data engineer will work alongside a dedicated data\\narchitect. However, if a company is small or low in its level of data\\nmaturity, a data engineer might do double duty as an architect.\\nBecause data architecture is an undercurrent of the data engineering', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='76edbc42-5cae-47fa-9719-91618ddfe2ca', embedding=None, metadata={'page_label': '174', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='lifecycle, a data engineer should understand “good” architecture and\\nthe various types of data architecture.\\nWhen designing architecture, you’ll work alongside business\\nstakeholders to evaluate trade-offs. What are the trade-offs inherent\\nin adopting a cloud data warehouse versus a data lake? What are\\nthe trade-offs of various cloud platforms? When might a unified\\nbatch/streaming framework (Beam, Flink) be an appropriate choice?\\nStudying these choices in the abstract will prepare you to make\\nconcrete, valuable decisions.\\nConclusion\\nYou’ve learned how data architecture fits into the data engineering\\nlifecycle and what makes for “good” data architecture, and you’ve\\nseen several examples of data architectures. Because architecture is\\nsuch a key foundation for success, we encourage you to invest the\\ntime to study it deeply and understand the trade-offs inherent in any\\narchitecture. You will be prepared to map out architecture that\\ncorresponds to your organization’s unique requirements.\\nNext up, let’s look at some approaches to choosing the right\\ntechnologies to be used in data architecture and across the data\\nengineering lifecycle.\\nAdditional Resources\\n“Separating Utility from Value Add” by Ross Pettit\\n“Tactics vs. Strategy: SOA and the Tarpit of Irrelevancy” by Neal\\nFord\\nThe Information Management Body of Knowledge website\\n“The Modern Data Stack: Past, Present, and Future” by Tristan\\nHandy', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1edff252-4301-40e2-a443-80625e33bad7', embedding=None, metadata={'page_label': '175', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“Potemkin Data Science” by Michael Correll\\n“A Comparison of Data Processing Frameworks” by Ludovik\\nSantos\\n“Modern CI Is Too Complex and Misdirected” by Gregory Szorc\\n“Questioning the Lambda Architecture” by Jay Kreps\\n“End-to-End Serverless ETL Orchestration in AWS: A Guide” by\\nRittika Jindal\\n“A Brief Introduction to Two Data Processing Architectures—\\nLambda and Kappa for Big Data” by Iman Samizadeh\\n“How to Beat the Cap Theorem” by Nathan Marz\\n“The Log: What Every Software Engineer Should Know About\\nReal-Time Data’s Unifying Abstraction” by Jay Kreps\\n“Run Your Data Team Like a Product Team” by Emilie Schario\\nand Taylor A. Murphy\\n“Data as a Product vs. Data as a Service” by Justin Gage\\nMartin Fowler articles:\\n“EagerReadDerivation”\\n“AnemicDomainModel”\\n“DomainDrivenDesign”\\n“Event Sourcing”\\n“ReportingDatabase”\\n“BoundedContext”\\n“Focusing on Events”\\n“Polyglot Persistence”', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e5c5c9bc-8f9a-4e9b-b18b-326cf0702947', embedding=None, metadata={'page_label': '176', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“UtilityVsStrategicDichotomy”\\nData applications:\\n“Introducing Dagster: An Open Source Python Library for\\nBuilding Data Applications” by Nick Schrock\\n“Down with Pipeline Debt: Introducing Great Expectations,” by\\nthe Great Expectations project\\n“Functional Data Engineering: A Modern Paradigm for Batch\\nData Processing” by Maxime Beauchemin\\n“What Is the Open Data Ecosystem and Why It’s Here to Stay”\\nby Casber Wang\\n“Staying Ahead of Data Debt” by Etai Mizrahi\\n“Choosing Open Wisely” by Benoit Dageville et al.\\n“The Ultimate Data Observability Checklist” by Molly Vorwerck\\n“Moving Beyond Batch vs. Streaming” by David Yaffe\\n“Disasters I’ve Seen in a Microservices World” by Joao Alves\\nNaming conventions:\\n“240 Tables and No Documentation?” by Alexey Makhotkin\\n“Data Warehouse Architecture: Overview” by Roelant Vos\\n“The Design and Implementation of Modern Column-Oriented\\nDatabase Systems” by Daniel Abadi et al.\\n“Column-oriented DBMS” Wikipedia page\\n“The Data Dichotomy: Rethinking the Way We Treat Data and\\nServices” by Ben Stopford\\n“The Curse of the Data Lake Monster” by Kiran Prakash and\\nLucy Chambers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='284ab6ac-2e74-4d90-a0f9-2312fac26b3f', embedding=None, metadata={'page_label': '177', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“Software Infrastructure 2.0: A Wishlist” by Erik Bernhardsson\\n“Data Team Platform” by GitLab Data\\n“Falling Back in Love with Data Pipelines” by Sean Knapp\\nBuild versus buy fallacy:\\n“What’s Wrong with MLOps?” by Laszlo Sragner\\n“Zachman Framework” Wikipedia page\\n“How TOGAF Defines Enterprise Architecture (EA)” by Avancier\\nLimited\\nEABOK Draft, edited by Paula Hagan\\n“What Is Data Architecture? A Framework for Managing Data”\\nby Thor Olavsrud\\n“Google Cloud Architecture Framework” Google Cloud\\nArchitecture web page\\nMicrosoft’s “Azure Architecture Center”\\n“Five Principles for Cloud-Native Architecture: What It Is and\\nHow to Master It” by Tom Grey\\n“Choosing the Right Architecture for Global Data Distribution”\\nGoogle Cloud Architecture web page\\n“Principled Data Engineering, Part I: Architectural Overview” by\\nHussein Danish\\n“A Personal Implementation of Modern Data Architecture:\\nGetting Strava Data into Google Cloud Platform” by Matthew\\nReeve\\n“The Cost of Cloud, a Trillion Dollar Paradox” by Sarah Wang\\nand Martin Casado', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e4e7128f-cd81-46d6-b04b-b24be71db6c3', embedding=None, metadata={'page_label': '178', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Architecture: A Primer for the Data Scientist by W.H.\\nInmon et al. (Academic Press)\\n“Enterprise Architecture” Gartner Glossary definition\\nEABOK website\\nTOGAF framework website\\n“Defining Architecture” ISO/IEC/IEEE 42010 web page\\n“The Six Principles of Modern Data Architecture” by Joshua\\nKlahr\\n“The Rise of the Metadata Lake” by Prukalpa\\n“The Top 5 Data Trends for CDOs to Watch Out for in 2021” by\\nPrukalpa\\n“Test Data Quality at Scale with Deequ” by Dustin Lange et al.\\n“What the Heck Is Data Mesh” by Chris Riccomini\\n“Reliable Microservices Data Exchange with the Outbox Pattern”\\nby Gunnar Morling\\n“Who Needs an Architect” by Martin Fowler\\n“Enterprise Architecture’s Role in Building a Data-Driven\\nOrganization” by Ashutosh Gupta\\n“How to Build a Data Architecture to Drive Innovation—Today\\nand Tomorrow” by Antonio Castro et al.\\n“Data Warehouse Architecture” tutorial at Javatpoint\\nSnowflake’s “What Is Data Warehouse Architecture” web page\\n“Three-Tier Architecture” by IBM Education\\n“The Building Blocks of a Modern Data Platform” by Prukalpa\\n“Data Architecture: Complex vs. Complicated” by Dave Wells', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0261bbff-1aad-4244-a545-22594c95ecb3', embedding=None, metadata={'page_label': '179', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“Data Fabric Defined” by James Serra\\n“Data Fabric Architecture Is Key to Modernizing Data\\nManagement and Integration” by Ashutosh Gupta\\n“Unified Analytics: Where Batch and Streaming Come Together;\\nSQL and Beyond” Apache Flink Roadmap\\n“What Is a Data Lakehouse?” by Ben Lorica et al.\\n“Big Data Architectures” Azure documentation\\n“Microsoft Azure IoT Reference Architecture” documentation\\n1  Jeff Haden, “Amazon Founder Jeff Bezos: This Is How Successful People\\nMake Such Smart Decisions,” Inc., December 3, 2018, https://oreil.ly/QwIm0.\\n2  Martin Fowler, “Who Needs an Architect?” IEEE Software, July/August\\n2003, https://oreil.ly/wAMmZ.\\n3  “The Bezos API Mandate: Amazon’s Manifesto for Externalization,” Nordic\\nAPIs, January 19, 2021, https://oreil.ly/vIs8m.\\n4  Fowler, “Who Needs an Architect?”\\n5  Ericka Chickowski, “Leaky Buckets: 10 Worst Amazon S3 Breaches,”\\nBitdefender Business Insights blog, Jan 24, 2018, https://oreil.ly/pFEFO.\\n6  “FinOps Foundation Soars to 300 Members and Introduces New Partner\\nTiers for Cloud Service Providers and Vendors,” Business Wire, June 17,\\n2019, https://oreil.ly/XcwYO.\\n7  Martin Fowler, “StranglerFigApplication,” June 29, 2004,\\nhttps://oreil.ly/PmqxB.\\n8  Mike Loukides, “Resume Driven Development,” O’Reilly Radar, October 13,\\n2004, https://oreil.ly/BUHa8.\\n9  H.W. Inmon, Building the Data Warehouse (Hoboken: Wiley, 2005).\\n10  Jay Kreps, “Questioning the Lambda Architecture,” O’Reilly, July 2, 2014,\\nhttps://oreil.ly/wWR3n.\\n11  Martin Fowler, “Data Mesh Principles and Logical Architecture,”\\nMartinFowler.com, December 3, 2020, https://oreil.ly/ezWE7.\\n12  Zhamak Dehghani, “How to Move Beyond a Monolithic Data Lake to a\\nDistributed Data Mesh,” MartinFowler.com, May 20, 2019,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a548b4ff-5d65-40de-860e-03e0ee847cac', embedding=None, metadata={'page_label': '180', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='https://oreil.ly/SqMe8.\\n13  Zhamak Dehghani, “Data Mesh Principles,” MartinFowler.com, December 3,\\n2020, https://oreil.ly/RymDi.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b9b80568-4c8d-4f41-830b-914130b01288', embedding=None, metadata={'page_label': '181', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 4. Choosing\\nTechnologies Across the Data\\nEngineering Lifecycle\\nData engineering nowadays suffers from an embarrassment of\\nriches. We have no shortage of technologies to solve various types\\nof data problems. Data technologies are available as turnkey\\nofferings consumable in almost every way—open source, managed\\nopen source, proprietary software, proprietary service, and more.\\nHowever, it’s easy to get caught up in chasing bleeding-edge\\ntechnology while losing sight of the core purpose of data\\nengineering: designing robust and reliable systems to carry data\\nthrough the full lifecycle and serve it according to the needs of end\\nusers. Just as structural engineers carefully choose technologies\\nand materials to realize an architect’s vision for a building, data\\nengineers are tasked with making appropriate technology choices to\\nshepherd data through the lifecycle to serve data applications and\\nusers.\\nChapter 3 discussed “good” data architecture and why it matters. We\\nnow explain how to choose the right technologies to serve this\\narchitecture. Data engineers must choose good technologies to\\nmake the best possible data product. We feel the criteria to choose a\\ngood data technology is simple: does it add value to a data product\\nand the broader business?\\nA lot of people confuse architecture and tools. Architecture is\\nstrategic; tools are tactical. We sometimes hear, “Our data\\narchitecture are tools X, Y, and Z.” This is the wrong way to think\\nabout architecture. Architecture is the top-level design, roadmap,\\nand blueprint of data systems that satisfy the strategic aims for the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7df39fc0-1711-44f3-86c0-569221f9d74e', embedding=None, metadata={'page_label': '182', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='business. Architecture is the what, why, and when. Tools are used to\\nmake the architecture a reality; tools are the how.\\nWe often see teams going “off the rails” and choosing technologies\\nbefore mapping out an architecture. The reasons vary: shiny object\\nsyndrome, resume-driven development, and a lack of expertise in\\narchitecture. In practice, this prioritization of technology often means\\nthey cobble together a kind of Dr. Suess fantasy machine rather than\\na true data architecture. We strongly advise against choosing\\ntechnology before getting your architecture right. Architecture first,\\ntechnology second.\\nThis chapter discusses our tactical plan for making technology\\nchoices once we have a strategic architecture blueprint. The\\nfollowing are some considerations for choosing data technologies\\nacross the data engineering lifecycle:\\nTeam size and capabilities\\nSpeed to market\\nInteroperability\\nCost optimization and business value\\nToday versus the future: immutable versus transitory\\ntechnologies\\nLocation (cloud, on prem, hybrid cloud, multicloud)\\nBuild versus buy\\nMonolith versus modular\\nServerless versus servers\\nOptimization, performance and the benchmark wars.\\nThe undercurrents of the data engineering lifecycle', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89a5e29e-4e70-4fff-be9d-6220708eaf93', embedding=None, metadata={'page_label': '183', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Team Size and Capabilities\\nThe first thing you need to assess is your team’s size and its\\ncapabilities with technology. Are you on a small team (perhaps a\\nteam of one) of people who are expected to wear many hats, or is\\nthe team large enough that people work in specialized roles? Will a\\nhandful of people be responsible for multiple stages of the data\\nengineering lifecycle, or do people cover particular niches? Your\\nteam’s size will influence the types of technologies you adopt.\\nThere is a continuum of simple to complex technologies, and a\\nteam’s size roughly determines the amount of bandwidth your team\\ncan dedicate to complex solutions. We sometimes see small data\\nteams read blog posts about a new cutting-edge technology at a\\ngiant tech company and then try to emulate these same extremely\\ncomplex technologies and practices. We call this cargo-cult\\nengineering, and it’s generally a big mistake that consumes a lot of\\nvaluable time and money, often with little to nothing to show in\\nreturn. Especially for small teams or teams with weaker technical\\nchops, use as many managed and SaaS tools as possible, and\\ndedicate your limited bandwidth to solving the complex problems that\\ndirectly add value to the business.\\nTake an inventory of your team’s skills. Do people lean toward low-\\ncode tools, or do they favor code-first approaches? Are people\\nstrong in certain languages like Java, Python, or Go? Technologies\\nare available to cater to every preference on the low-code to code-\\nheavy spectrum. Again, we suggest sticking with technologies and\\nworkflows with which the team is familiar. We’ve seen data teams\\ninvest a lot of time in learning the shiny new data framework, only to\\nnever use it in production. Learning new technologies, languages,\\nand tools is a considerable time investment, so make these\\ninvestments wisely.\\nSpeed to Market', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2eca0b96-4e7b-4a56-a077-57b1e3d3b7e7', embedding=None, metadata={'page_label': '184', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In technology, speed to market wins. This means choosing the right\\ntechnologies that help you deliver features and data faster while\\nmaintaining high-quality standards and security. It also means\\nworking in a tight feedback loop of launching, learning, iterating, and\\nmaking improvements.\\nPerfect is the enemy of good. Some data teams will deliberate on\\ntechnology choices for months or years without reaching any\\ndecisions. Slow decisions and output are the kiss of death to data\\nteams. We’ve seen more than a few data teams dissolve for moving\\ntoo slow and failing to deliver the value they were hired to produce.\\nDeliver value early and often. As we’ve mentioned, use what works.\\nYour team members will likely get better leverage with tools they\\nalready know. Avoid undifferentiated heavy lifting that engages your\\nteam in unnecessarily complex work that adds little to no value.\\nChoose tools that help you move quickly, reliably, safely, and\\nsecurely.\\nInteroperability\\nRarely will you use only one technology or system. When choosing a\\ntechnology or system, you’ll need to ensure that it interacts and\\noperates with other technologies. Interoperability describes how\\nvarious technologies or systems connect, exchange information, and\\ninteract.\\nLet’s say you’re evaluating two technologies, A and B. How easily\\ndoes technology A integrate with technology B when thinking about\\ninteroperability? This is often a spectrum of difficulty, ranging from\\nseamless to time-intensive. Is seamless integration already baked\\ninto each product, making setup a breeze? Or do you need to do a\\nlot of manual configuration to integrate these technologies?\\nOften, vendors and open source projects will target specific\\nplatforms and systems to interoperate. Most data ingestion and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='136400c0-8ee7-4b04-9e74-69a807c3f010', embedding=None, metadata={'page_label': '185', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='visualization tools have built-in integrations with popular data\\nwarehouses and data lakes. Furthermore, popular data-ingestion\\ntools will integrate with common APIs and services, such as CRMs,\\naccounting software, and more.\\nSometimes standards are in place for interoperability. Almost all\\ndatabases allow connections via Java Database Connectivity\\n(JDBC) or Open Database Connectivity (ODBC), meaning that you\\ncan easily connect to a database by using these standards. In other\\ncases, interoperability occurs in the absence of standards.\\nRepresentational state transfer (REST) is not truly a standard for\\nAPIs; every REST API has its quirks. In these cases, it’s up to the\\nvendor or open source software (OSS) project to ensure smooth\\nintegration with other technologies and systems.\\nAlways be aware of how simple it will be to connect your various\\ntechnologies across the data engineering lifecycle. As mentioned in\\nother chapters, we suggest designing for modularity and giving\\nyourself the ability to easily swap out technologies as new practices\\nand alternatives become available.\\nCost Optimization and Business Value\\nIn a perfect world, you’d get to experiment with all the latest, coolest\\ntechnologies without considering cost, time investment, or value\\nadded to the business. In reality, budgets and time are finite, and the\\ncost is a major constraint for choosing the right data architectures\\nand technologies. Your organization expects a positive ROI from\\nyour data projects, so you must understand the basic costs you can\\ncontrol. Technology is a major cost driver, so your technology\\nchoices and management strategies will significantly impact your\\nbudget. We look at costs through three main lenses: total cost of\\nownership, opportunity cost, and FinOps.\\nTotal Cost of Ownership', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='75835d79-1fdf-42d3-9015-c0e34aea1b44', embedding=None, metadata={'page_label': '186', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Total cost of ownership (TCO) is the total estimated cost of an\\ninitiative, including the direct and indirect costs of products and\\nservices utilized. Direct costs can be directly attributed to an\\ninitiative. Examples are the salaries of a team working on the\\ninitiative or the AWS bill for all services consumed. Indirect costs,\\nalso known as overhead, are independent of the initiative and must\\nbe paid regardless of where they’re attributed.\\nApart from direct and indirect costs, how something is purchased\\nimpacts the way costs are accounted for. Expenses fall into two big\\ngroups: capital expenses and operational expenses.\\nCapital expenses, also known as capex, require an up-front\\ninvestment. Payment is required today. Before the cloud existed,\\ncompanies would typically purchase hardware and software up front\\nthrough large acquisition contracts. In addition, significant\\ninvestments were required to host hardware in server rooms, data\\ncenters, and colocation facilities. These up-front investments—\\ncommonly hundreds of thousands to millions of dollars or more—\\nwould be treated as assets and slowly depreciate over time. From a\\nbudget perspective, capital was required to fund the entire purchase.\\nThis is capex, a significant capital outlay with a long-term plan to\\nachieve a positive ROI on the effort and expense put forth.\\nOperational expenses, also known as opex, are the opposite of\\ncapex in certain respects. Opex is gradual and spread out over time.\\nWhereas capex is long-term focused, opex is short-term. Opex can\\nbe pay-as-you-go or similar and allows a lot of flexibility. Opex is\\ncloser to a direct cost, making it easier to attribute to a data project.\\nUntil recently, opex wasn’t an option for large data projects. Data\\nsystems often required multimillion-dollar contracts. This has\\nchanged with the advent of the cloud, as data platform services allow\\nengineers to pay on a consumption-based model. In general, opex\\nallows for a far greater ability for engineering teams to choose their\\nsoftware and hardware. Cloud-based services let data engineers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f5269c31-30cf-49a8-8db6-c4fb1d3b2cd2', embedding=None, metadata={'page_label': '187', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='iterate quickly with various software and technology configurations,\\noften inexpensively.\\nData engineers need to be pragmatic about flexibility. The data\\nlandscape is changing too quickly to invest in long-term hardware\\nthat inevitably goes stale, can’t easily scale, and potentially hampers\\na data engineer’s flexibility to try new things. Given the upside for\\nflexibility and low initial costs, we urge data engineers to take an\\nopex-first approach centered on the cloud and flexible, pay-as-you-\\ngo technologies.\\nTotal Opportunity Cost of Ownership\\nAny choice inherently excludes other possibilities. Total opportunity\\ncost of ownership (TOCO) is the cost of lost opportunities that we\\nincur in choosing a technology, an architecture, or a process. Note\\nthat ownership in this setting doesn’t require long-term purchases of\\nhardware or licenses. Even in a cloud environment, we effectively\\nown a technology, a stack, or a pipeline once it becomes a core part\\nof our production data processes and is difficult to move away from.\\nData engineers often fail to evaluate TOCO when undertaking a new\\nproject; in our opinion, this is a massive blind spot.\\nIf you choose data stack A, you’ve chosen the benefits of data stack\\nA over all other options, effectively excluding data stacks B, C, and\\nD. You’re committed to data stack A and everything it entails—the\\nteam to support it, training, setup, and maintenance. What happens\\nif data stack A was a poor choice? What happens when data stack A\\nbecomes obsolete? Can you still move to data stack B?\\nHow quickly and cheaply can you move to something newer and\\nbetter? This is a critical question in the data space, where new\\ntechnologies and products seem to appear at an ever-faster rate.\\nDoes the expertise you’ve built up on data stack A translate to the\\nnext wave? Or are you able to swap out components of data stack A\\nand buy yourself some time and options?\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a94c7e50-045f-4dfe-92d4-ce41d29f1728', embedding=None, metadata={'page_label': '188', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The first step to minimizing opportunity cost is evaluating it with eyes\\nwide open. We’ve seen countless data teams get stuck with\\ntechnologies that seemed good at the time and are either not flexible\\nfor future growth or simply obsolete. Inflexible data technologies are\\na lot like bear traps. They’re easy to get into and extremely painful to\\nescape.\\nFinOps\\nWe already touched on FinOps in “Principle 9: Embrace FinOps”. As\\nwe’ve discussed, typical cloud spending is inherently opex:\\ncompanies pay for services to run critical data processes rather than\\nmaking up-front purchases and clawing back value over time. The\\ngoal of FinOps is to fully operationalize financial accountability and\\nbusiness value by applying the DevOps-like practices of monitoring\\nand dynamically adjusting systems.\\nIn this chapter, we want to emphasize one thing about FinOps that is\\nwell embodied in this quote:\\nIf it seems that FinOps is about saving money, then think again.\\nFinOps is about making money. Cloud spend can drive more\\nrevenue, signal customer base growth, enable more product and\\nfeature release velocity, or even help shut down a data center.\\nIn our setting of data engineering, the ability to iterate quickly and\\nscale dynamically is invaluable for creating business value. This is\\none the major motivations for shifting data workloads to the cloud.\\nToday Versus the Future: Immutable Versus\\nTransitory Technologies\\nIn an exciting domain like data engineering, it’s all too easy to focus\\non a rapidly evolving future while ignoring the concrete needs of the\\npresent. The intention to build a better future is noble but often leads\\nto overarchitecting and overengineering. Tooling chosen for the\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ad2b91ff-db4c-4aa5-b177-01216f143db5', embedding=None, metadata={'page_label': '189', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='future may be stale and out-of-date when this future arrives; the\\nfuture frequently looks little like what we envisioned years before.\\nAs many life coaches would tell you, focus on the present. You\\nshould choose the best technology for the moment and near future,\\nbut in a way that supports future unknowns and evolution. Ask\\nyourself: where are you today, and what are your goals for the\\nfuture? Your answers to these questions should inform your\\ndecisions about your architecture and thus the technologies used\\nwithin that architecture. This is done by understanding what is likely\\nto change and what tends to stay the same.\\nWe have two classes of tools to consider: immutable and transitory.\\nImmutable technologies might be components that underpin the\\ncloud or languages and paradigms that have stood the test of time.\\nIn the cloud, examples of immutable technologies are object storage,\\nnetworking, servers, and security. Object storage such as Amazon\\nS3 and Azure Blob Storage will be around from today until the end of\\nthe decade, and probably much longer. Storing your data in object\\nstorage is a wise choice. Object storage continues to improve in\\nvarious ways and constantly offers new options, but your data will be\\nsafe and usable in object storage regardless of the rapid evolution of\\ntechnology.\\nFor languages, SQL and bash have been around for many decades,\\nand we don’t see them disappearing anytime soon. Immutable\\ntechnologies benefit from the Lindy effect: the longer a technology\\nhas been established, the longer it will be used. Think of the power\\ngrid, relational databases, the C programming language, or the x86\\nprocessor architecture. We suggest applying the Lindy effect as a\\nlitmus test to determine whether a technology is potentially\\nimmutable.\\nTransitory technologies are those that come and go. The typical\\ntrajectory begins with a lot of hype, followed by meteoric growth in\\npopularity, then a slow descent into obscurity. The JavaScript', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cd56cd36-e78b-4868-a870-ed1c062f139c', embedding=None, metadata={'page_label': '190', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='frontend landscape is a classic example. How many JavaScript\\nfrontend frameworks have come and gone between 2010 and 2020?\\nBackbone.js, Ember.js, and Knockout were popular in the early\\n2010s, AngularJS in the mid-2010s, and React and Vue.js have\\nmassive mindshare today. What’s the popular frontend framework\\nthree years from now? Who knows.\\nNew well-funded entrants and open source projects arrive on the\\ndata front every day. Every vendor will say their product will change\\nthe industry and “make the world a better place”. Most of these\\ncompanies and projects don’t get long-term traction and fade slowly\\ninto obscurity. Top VCs are making big-money bets, knowing that\\nmost of their data-tooling investments will fail. How can you possibly\\nknow which technologies to invest in for your data architecture? It’s\\nhard. Just consider the number of technologies in Matt Turck’s\\n(in)famous depictions of the ML, AI, and data (MAD) landscape that\\nwe introduced in Chapter 1 (Figure 4-1).\\nFigure 4-1. Matt Turck’s 2021 MAD data landscape\\nEven relatively successful technologies often fade into obscurity\\nquickly, after a few years of rapid adoption, a victim of their success.\\nFor instance, in the early 2010s, Hive was met with rapid uptake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d05654bc-9d7d-4e78-b1d0-46d3e58d12a3', embedding=None, metadata={'page_label': '191', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='because it allowed both analysts and engineers to query massive\\ndatasets without coding complex MapReduce jobs by hand. Inspired\\nby the success of Hive but wishing to improve on its shortcomings,\\nengineers developed Presto and other technologies. Hive now\\nappears primarily in legacy deployments.\\nOur Advice\\nGiven the rapid pace of tooling and best-practice changes, we\\nsuggest evaluating tools every two years (Figure 4-2). Whenever\\npossible, find the immutable technologies along the data engineering\\nlifecycle, and use those as your base. Build transitory tools around\\nthe immutables.\\nFigure 4-2. Use a two-year time horizon to reevaluate your technology choices\\nGiven the reasonable probability of failure for many data\\ntechnologies, you need to consider how easy it is to transition from a\\nchosen technology. What are the barriers to leaving? As mentioned\\npreviously in our discussion about opportunity cost, avoid “bear\\ntraps.” Go into a new technology with eyes wide open, knowing the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ff471e08-a35e-4df2-9298-7b22117ec7c2', embedding=None, metadata={'page_label': '192', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='project might get abandoned, the company may not be viable, or the\\ntechnology simply isn’t a good fit any longer.\\nLocation\\nCompanies now have numerous options when deciding where to run\\ntheir technology stacks. A slow shift toward the cloud culminates in a\\nveritable stampede of companies spinning up workloads on AWS,\\nAzure, and Google Cloud Platform (GCP). In the last decade, many\\nCTOs have come to view their decisions around technology hosting\\nas having existential significance for their organizations. If they move\\ntoo slowly, they risk being left behind by their more agile competition;\\non the other hand, a poorly planned cloud migration could lead to\\ntechnological failure and catastrophic costs.\\nLet’s look at the principal places to run your technology stack: on\\npremises, the cloud, hybrid cloud, and multicloud.\\nOn Premises\\nWhile new startups are increasingly born in the cloud, on-premises\\nsystems are still the default for established companies. Essentially,\\nthese companies own their hardware, which may live in data centers\\nthey own or in leased colocation space. In either case, companies\\nare operationally responsible for their hardware and the software that\\nruns on it. If hardware fails, they have to repair or replace it. They\\nalso have to manage upgrade cycles every few years as new,\\nupdated hardware is released and older hardware ages and\\nbecomes less reliable. They must ensure that they have enough\\nhardware to handle peaks; for an online retailer, this means hosting\\nenough capacity to handle the load spikes of Black Friday. For data\\nengineers in charge of on-premises systems, this means buying\\nlarge-enough systems to allow good performance for peak load and\\nlarge jobs without overbuying and overspending.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b730f89-d826-4465-8873-4cec15b80a95', embedding=None, metadata={'page_label': '193', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the one hand, established companies have established\\noperational practices that have served them well. Suppose a\\ncompany that relies on information technology has been in business\\nfor some time. This means it has managed to juggle the cost and\\npersonnel requirements of running its hardware, managing software\\nenvironments, deploying code from dev teams, and running\\ndatabases and big data systems.\\nOn the other hand, established companies see their younger, more\\nagile competition scaling rapidly and taking advantage of cloud-\\nmanaged services. They also see established competitors making\\nforays into the cloud, allowing them to temporarily scale up\\nenormous computing power for massive data jobs or the Black\\nFriday shopping spike.\\nCompanies in competitive sectors generally don’t have the option to\\nstand still. Competition is fierce, and there’s always the threat of\\nbeing “disrupted” by more agile competition, backed by a large pile\\nof venture capital dollars. Every company must keep its existing\\nsystems running efficiently while deciding what moves to make next.\\nThis could involve adopting newer DevOps practices, such as\\ncontainers, Kubernetes, microservices, and continuous deployment\\nwhile keeping their hardware running on premises. It could involve a\\ncomplete migration to the cloud, as discussed next.\\nCloud\\nThe cloud flips the on-premises model on its head. Instead of\\npurchasing hardware, you simply rent hardware and managed\\nservices from a cloud provider (such as AWS, Azure, or Google\\nCloud). These resources can often be reserved on an extremely\\nshort-term basis; VMs spin up in less than a minute, and subsequent\\nusage is billed in per-second increments. This allows cloud users to\\ndynamically scale resources that were inconceivable with on-\\npremises servers.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b6af4d88-abe7-40da-bb2e-26f6317d07e3', embedding=None, metadata={'page_label': '194', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In a cloud environment, engineers can quickly launch projects and\\nexperiment without worrying about long lead time hardware planning.\\nThey can begin running servers as soon as their code is ready to\\ndeploy. This makes the cloud model extremely appealing to startups\\nthat are tight on budget and time.\\nThe early cloud era was dominated by infrastructure as a service\\n(IaaS) offerings—products such as VMs and virtual disks that are\\nessentially rented slices of hardware. Slowly, we’ve seen a shift\\ntoward platform as a service (PaaS), while SaaS products continue\\nto grow at a rapid clip.\\nPaaS includes IaaS products but adds more sophisticated managed\\nservices to support applications. Examples are managed databases\\nsuch as Amazon Relational Database Service (RDS) and Google\\nCloud SQL, managed streaming platforms such as Amazon Kinesis\\nand Simple Queue Service (SQS), and managed Kubernetes such\\nas Google Kubernetes Engine (GKE) and Azure Kubernetes Service\\n(AKS). PaaS services allow engineers to ignore the operational\\ndetails of managing individual machines and deploying frameworks\\nacross distributed systems. They provide turnkey access to complex,\\nautoscaling systems with minimal operational overhead.\\nSaaS offerings move one additional step up the ladder of\\nabstraction. SaaS typically provides a fully functioning enterprise\\nsoftware platform with little operational management. Examples of\\nSaaS include Salesforce, Google Workspace, Microsoft 365, Zoom,\\nand Fivetran. Both the major public clouds and third parties offer\\nSaaS platforms. SaaS covers a whole spectrum of enterprise\\ndomains, including video conferencing, data management, ad tech,\\noffice applications, and CRM systems.\\nThis chapter also discusses serverless, increasingly important in\\nPaaS and SaaS offerings. Serverless products generally offer\\nautomated scaling from zero to extremely high usage rates. They are\\nbilled on a pay-as-you-go basis and allow engineers to operate', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b0de8ae-eadf-4afc-af95-f7212f383443', embedding=None, metadata={'page_label': '195', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='without operational awareness of underlying servers. Many people\\nquibble with the term serverless; after all, the code must run\\nsomewhere. In practice, serverless usually means many invisible\\nservers.\\nCloud services have become increasingly appealing to established\\nbusinesses with existing data centers and IT infrastructure. Dynamic,\\nseamless scaling is extremely valuable to businesses that deal with\\nseasonality (e.g., retail businesses coping with Black Friday load)\\nand web traffic load spikes. The advent of COVID-19 in 2020 was a\\nmajor driver of cloud adoption, as companies recognized the value of\\nrapidly scaling up data processes to gain insights in a highly\\nuncertain business climate; businesses also had to cope with\\nsubstantially increased load due to a spike in online shopping, web\\napp usage, and remote work.\\nBefore we discuss the nuances of choosing technologies in the\\ncloud, let’s first discuss why migration to the cloud requires a\\ndramatic shift in thinking, specifically on the pricing front; this is\\nclosely related to FinOps, introduced in “FinOps”. Enterprises that\\nmigrate to the cloud often make major deployment errors by not\\nappropriately adapting their practices to the cloud pricing model.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d805026e-e9ab-4468-88c5-aace02493b88', embedding=None, metadata={'page_label': '196', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A BRIEF DETOUR ON CLOUD ECONOMICS\\nTo understand how to use cloud services efficiently through cloud\\nnative architecture, you need to know how clouds make money.\\nThis is an extremely complex concept and one on which cloud\\nproviders offer little transparency. Consider this sidebar a starting\\npoint for your research, discovery, and process development.\\nCloud Services and Credit Default Swaps\\nLet’s go on a little tangent about credit default swaps. Don’t\\nworry, this will make sense in a bit. Recall that credit default\\nswaps rose to infamy after the 2007 global financial crisis. A\\ncredit default swap was a mechanism for selling different tiers of\\nrisk attached to an asset (i.e., a mortgage.) It is not our intention\\nto present this idea in any detail, but rather to offer an analogy\\nwherein many cloud services are similar to financial derivatives;\\ncloud providers not only slice hardware assets into small pieces\\nthrough virtualization, but also sell these pieces with varying\\ntechnical characteristics and risks attached. While providers are\\nextremely tight-lipped about details of their internal systems,\\nthere are massive opportunities for optimization and scaling by\\nunderstanding cloud pricing and exchanging notes with other\\nusers.\\nLook at the example of archival cloud storage. At the time of this\\nwriting, GCP openly admits that its archival class storage runs on\\nthe same clusters as standard cloud storage, yet the price per\\ngigabyte per month of archival storage is roughly 1/17 that of\\nstandard storage. How is this possible?\\nHere’s our educated guess. When purchasing cloud storage,\\neach disk in a storage cluster has three assets that cloud\\nproviders and consumers use. First, it has a certain storage\\ncapacity—say, 10 TB. Second, it supports a certain number of\\ninput/output operations (IOPs) per second—say, 100. Third,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99ecdbd8-c448-4720-aeb3-1e8d9e26e54a', embedding=None, metadata={'page_label': '197', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='disks support a certain maximum bandwidth, the maximum read\\nspeed for optimally organized files. A magnetic drive might be\\ncapable of reading at 200 MB/s.\\nAny of these limits (IOPs, storage capacity, bandwidth) is a\\npotential bottleneck for a cloud provider. For instance, the cloud\\nprovider might have a disk storing 3 TB of data but hitting\\nmaximum IOPs. An alternative to leaving the remaining 7 TB\\nempty is to sell the empty space without selling IOPs. Or, more\\nspecifically, sell cheap storage space and expensive IOPs to\\ndiscourage reads.\\nMuch like traders of financial derivatives, cloud vendors also deal\\nin risk. In the case of archival storage, vendors are selling a type\\nof insurance, but one that pays out for the insurer rather than the\\npolicy buyer in the event of a catastrophe. While data storage\\ncosts per month are extremely cheap, I risk paying a high price if\\nI ever need to retrieve data. But this is a price that I will happily\\npay in a true emergency.\\nSimilar considerations apply to nearly any cloud service. While\\non-premises servers are essentially sold as commodity\\nhardware, the cost model in the cloud is more subtle. Rather than\\njust charging for CPU cores, memory, and features, cloud\\nvendors monetize characteristics such as durability, reliability,\\nlongevity, and predictability; a variety of compute platforms\\ndiscount their offerings for workloads that are ephemeral or can\\nbe arbitrarily interrupted when capacity is needed elsewhere.\\nCloud ≠ On Premises\\nThis heading may seem like a silly tautology, but the belief that\\ncloud services are just like familiar on-premises servers is a\\nwidespread cognitive error that plagues cloud migrations and\\nleads to horrifying bills. This demonstrates a broader issue in\\ntech that we refer to as the curse of familiarity. Many new\\ntechnology products are intentionally designed to look like', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5191c572-6f5d-4ce6-b53b-06d906fe8f78', embedding=None, metadata={'page_label': '198', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='something familiar to facilitate ease of use and accelerate\\nadoption. But, any new technology product has subtleties and\\nwrinkles that users must learn to identify, accommodate, and\\noptimize.\\nMoving on-premises servers one by one to VMs in the cloud—\\nknown as simple lift and shift—is a perfectly reasonable strategy\\nfor the initial phase of cloud migration, especially when a\\ncompany is facing some kind of financial cliff, such as the need\\nto sign a significant new lease or hardware contract if existing\\nhardware is not shut down. However, companies that leave their\\ncloud assets in this initial state are in for a rude shock. On a\\ndirect comparison basis, long-running servers in the cloud are\\nsignificantly more expensive than their on-premises counterparts.\\nThe key to finding value in the cloud is understanding and\\noptimizing the cloud pricing model. Rather than deploying a set\\nof long-running servers capable of handling full peak load, use\\nautoscaling to allow workloads to scale down to minimal\\ninfrastructure when loads are light, and up to massive clusters\\nduring peak times. To realize discounts through more ephemeral,\\nless durable workloads, use reserved or spot instances, or use\\nserverless functions in place of servers.\\nWe often think of this optimization as leading to lower costs, but\\nwe should also strive to increase business value by exploiting the\\ndynamic nature of the cloud. Data engineers can create new\\nvalue in the cloud by accomplishing things that were impossible\\nin their on-premises environment. For example, it is possible to\\nquickly spin up massive compute clusters to run complex\\ntransformations at scales that were unaffordable for on-premises\\nhardware.\\nData Gravity\\nIn addition to basic errors such as following on-premises\\noperational practices in the cloud, data engineers need to watch\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d8e88bf5-3405-4954-bdf3-62171796f5ba', embedding=None, metadata={'page_label': '199', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='out for other aspects of cloud pricing and incentives that\\nfrequently catch users unawares.\\nVendors want to lock you into their offerings. Getting data onto\\nthe platform is cheap or free on most cloud platforms, but getting\\ndata out can be extremely expensive. Be aware of data egress\\nfees and their long-term impacts on your business before getting\\nblindsided by a large bill. Data gravity is real: once data lands in\\na cloud, the cost to extract it and migrate processes can be very\\nhigh.\\nHybrid Cloud\\nAs more established businesses migrate into the cloud, the hybrid\\ncloud model is growing in importance. Virtually no business can\\nmigrate all of its workloads overnight. The hybrid cloud model\\nassumes that an organization will indefinitely maintain some\\nworkloads outside the cloud.\\nThere are many reasons to consider a hybrid cloud model.\\nOrganizations may believe that they have achieved operational\\nexcellence in certain areas, such as their application stack and\\nassociated hardware. Thus, they may migrate only to specific\\nworkloads where they see immediate benefits in the cloud\\nenvironment. For example, an on-premises Spark stack is migrated\\nto ephemeral cloud clusters, reducing the operational burden of\\nmanaging software and hardware for the data engineering team and\\nallowing rapid scaling for large data jobs.\\nThis pattern of putting analytics in the cloud is beautiful because\\ndata flows primarily in one direction, minimizing data egress costs\\n(Figure 4-3). That is, on-premises applications generate event data\\nthat can be pushed to the cloud essentially for free. The bulk of data\\nremains in the cloud where it is analyzed, while smaller amounts of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fcef2de7-d845-44dc-a446-1961e1cc517d', embedding=None, metadata={'page_label': '200', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data are pushed back to on premises for deploying models to\\napplications, reverse ETL, etc.\\nFigure 4-3. A hybrid cloud data flow model minimizing egress costs\\nA new generation of managed hybrid cloud service offerings also\\nallows customers to locate cloud-managed servers in their data\\ncenters. This gives users the ability to incorporate the best features\\nin each cloud alongside on-premises infrastructure.\\nMulticloud\\nMulticloud simply refers to deploying workloads to multiple public\\nclouds. Companies may have several motivations for multicloud\\ndeployments. SaaS platforms often wish to offer services close to\\nexisting customer cloud workloads. Snowflake and Databricks\\nprovide their SaaS offerings across multiple clouds for this reason.\\nThis is especially critical for data-intensive applications, where\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3557353-004d-4efb-99d8-76848cc30874', embedding=None, metadata={'page_label': '201', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='network latency and bandwidth limitations hamper performance, and\\ndata egress costs can be prohibitive.\\nAnother common motivation for employing a multicloud approach is\\nto take advantage of the best services across several clouds. For\\nexample, a company might want to handle its Google Ads and\\nAnalytics data on Google Cloud and deploy Kubernetes through\\nGKE. And the company might also adopt Azure specifically for\\nMicrosoft workloads. Also, the company may like AWS because it\\nhas several best-in-class services (e.g., AWS Lambda) and enjoys\\nhuge mindshare, making it relatively easy to hire AWS-proficient\\nengineers. Any mix of various cloud provider services is possible.\\nGiven the intense competition among the major cloud providers,\\nexpect them to offer more best-of-breed services, making multicloud\\nmore compelling.\\nA multicloud methodology has several disadvantages. As we just\\nmentioned, data egress costs and networking bottlenecks are\\ncritical. Going multicloud can introduce significant complexity.\\nCompanies must now manage a dizzying array of services across\\nseveral clouds; cross-cloud integration and security present a\\nconsiderable challenge; multicloud networking can be diabolically\\ncomplicated.\\nA new generation of “cloud of clouds” services aims to facilitate\\nmulticloud with reduced complexity by offering services across\\nclouds and seamlessly replicating data between clouds or managing\\nworkloads on several clouds through a single pane of glass. To cite\\none example, a Snowflake account runs in a single cloud region, but\\ncustomers can readily spin up other accounts in GCP, AWS, or\\nAzure. Snowflake provides simple scheduled data replication\\nbetween these various cloud accounts. The Snowflake interface is\\nessentially the same in all of these accounts, removing the training\\nburden of switching between cloud-native data services.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='294e861a-4bee-4bab-86b0-74a8c6c85c1b', embedding=None, metadata={'page_label': '202', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The “cloud of clouds” space is evolving quickly; within a few years of\\nthis book’s publication, many more of these services will be\\navailable. Data engineers and architects would do well to maintain\\nawareness of this quickly changing landscape.\\nDecentralized: Blockchain and the Edge\\nThough not widely used now, it’s worth briefly mentioning a new\\ntrend that might become popular over the next decade: decentralized\\ncomputing. Whereas today’s applications mainly run on premises\\nand in the cloud, the rise of blockchain, Web 3.0, and edge\\ncomputing may invert this paradigm. For the moment, decentralized\\nplatforms have proven extremely popular but have not had a\\nsignificant impact in the data space; even so, keeping an eye on\\nthese platforms is worthwhile as you assess technology decisions.\\nOur Advice\\nFrom our perspective, we are still at the beginning of the transition to\\nthe cloud. Thus the evidence and arguments around workload\\nplacement and migration are in flux. The cloud itself is changing, with\\na shift from the IaaS model built around Amazon EC2 that drove the\\nearly growth of AWS, toward more managed service offerings such\\nas AWS Glue, Google BigQuery, and Snowflake.\\nWe’ve also seen the emergence of new workload placement\\nabstractions. On-premises services are becoming more cloud-like\\nand abstracted. Hybrid cloud services allow customers to run fully\\nmanaged services within their walls while facilitating tight integration\\nbetween local and remote environments. Further, the “cloud of\\nclouds” is beginning to take shape, fueled by third-party services and\\npublic cloud vendors.\\nChoose technologies for the present, but look toward the future', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0b84e24d-5a52-4846-9cb1-7ea713386ea3', embedding=None, metadata={'page_label': '203', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As we mentioned in “Today Versus the Future: Immutable Versus\\nTransitory Technologies”, you need to keep one eye on the present\\nwhile planning for unknowns. Right now is a tough time to plan\\nworkload placements and migrations. Because of the fast pace of\\ncompetition and change in the cloud industry, the decision space will\\nlook very different in five to ten years. It is tempting to take into\\naccount every possible future architecture permutation.\\nWe believe that it is critical to avoid this endless trap of analysis.\\nInstead, plan for the present. Choose the best technologies for your\\ncurrent needs and concrete plans for the near future. Choose your\\ndeployment platform based on real business needs while focusing\\non simplicity and flexibility.\\nIn particular, don’t choose a complex multicloud or hybrid-cloud\\nstrategy unless there’s a compelling reason. Do you need to serve\\ndata near customers on multiple clouds? Do industry regulations\\nrequire you to house certain data in your data centers? Do you have\\na compelling technology need for specific services on two different\\nclouds? Choose a single-cloud deployment strategy if these\\nscenarios don’t apply to you.\\nOn the other hand, have an escape plan. As we’ve emphasized\\nbefore, every technology—even open source software—comes with\\nsome degree of lock-in. A single-cloud strategy has significant\\nadvantages of simplicity and integration but comes with significant\\nlock-in attached. In this instance, we’re talking about mental\\nflexibility, the flexibility to evaluate the current state of the world and\\nimagine alternatives. Ideally, your escape plan will remain locked\\nbehind glass, but preparing this plan will help you to make better\\ndecisions in the present and give you a way out if things go wrong in\\nthe future.\\nCloud Repatriation Arguments', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e3cf2dc7-9ede-4ec9-9908-b41c3427401c', embedding=None, metadata={'page_label': '204', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As we wrote this book, Sarah Wang and Martin Casado published\\n“The Cost of Cloud, A Trillion Dollar Paradox”, an article that\\ngenerated significant sound and fury in the tech space. Readers\\nwidely interpreted the article as a call for the repatriation of cloud\\nworkloads to on-premises servers. They make a somewhat more\\nsubtle argument that companies should expend significant resources\\nto control cloud spending and should consider repatriation as a\\npossible option.\\nWe want to take a moment to dissect one part of their discussion.\\nWang and Casado cite Dropbox’s repatriation of significant\\nworkloads from AWS to Dropbox-owned servers as a case study for\\ncompanies considering similar repatriation moves.\\nYou are not Dropbox, nor are you Cloudflare\\nWe believe that this case study is frequently used without\\nappropriate context and is a compelling example of the false\\nequivalence logical fallacy. Dropbox provides particular services\\nwhere ownership of hardware and data centers can offer a\\ncompetitive advantage. Companies should not rely excessively on\\nDropbox’s example when assessing cloud and on-premises\\ndeployment options.\\nFirst, it’s important to understand that Dropbox stores enormous\\namounts of data. The company is tight-lipped about exactly how\\nmuch data it hosts, but says it is many exabytes and continues to\\ngrow.\\nSecond, Dropbox handles a vast amount of network traffic. We know\\nthat its bandwidth consumption in 2017 was significant enough for\\nthe company to add “hundreds of gigabits of internet connectivity\\nwith transit providers (regional and global ISPs), and hundreds of\\nnew peering partners (where we exchange traffic directly rather than\\nthrough an ISP).” The data egress costs would be extremely high in\\na public cloud environment.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4c481f34-191c-4787-800b-cbeb1bf8a90e', embedding=None, metadata={'page_label': '205', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Third, Dropbox is essentially a cloud storage vendor, but one with a\\nhighly specialized storage product that combines object and block\\nstorage characteristics. Dropbox’s core competence is a differential\\nfile-update system that can efficiently synchronize actively edited\\nfiles among users while minimizing network and CPU usage. The\\nproduct is not a good fit for object storage, block storage, or other\\nstandard cloud offerings. Dropbox has instead benefited from\\nbuilding a custom, highly integrated software and hardware stack.\\nFourth, while Dropbox moved its core product to its hardware, it\\ncontinued building out other AWS workloads. This allows Dropbox to\\nfocus on building one highly tuned cloud service at an extraordinary\\nscale rather than trying to replace multiple services. Dropbox can\\nfocus on its core competence in cloud storage and data\\nsynchronization while offloading software and hardware\\nmanagement in other areas, such as data analytics.\\nOther frequently cited success stories that companies have built\\noutside the cloud include Backblaze and Cloudflare, but these offer\\nsimilar lessons. Backblaze began life as a personal cloud data\\nbackup product but has since begun to offer B2, an object storage\\nservice similar to Amazon S3. Backblaze currently stores over an\\nexabyte of data. Cloudflare claims to provide services for over 25\\nmillion internet properties, with points of presence in over 200 cities\\nand 51 terabits per second (Tbps) of total network capacity.\\nNetflix offers yet another useful example. The company is famous for\\nrunning its tech stack on AWS, but this is only partially true. Netflix\\ndoes run video transcoding on AWS, accounting for roughly 70% of\\nits compute needs in 2017. Netflix also runs its application backend\\nand data analytics on AWS. However, rather than using the AWS\\ncontent distribution network, Netflix has built a custom CDN in\\ncollaboration with internet service providers, utilizing a highly\\nspecialized combination of software and hardware. For a company\\nthat consumes a substantial slice of all internet traffic, building out\\n6\\n7\\n8\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c295fb22-a81f-4819-bed9-2129a99674a7', embedding=None, metadata={'page_label': '206', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='this critical infrastructure allowed it to deliver high-quality video to a\\nhuge customer base cost-effectively.\\nThese case studies suggest that it makes sense for companies to\\nmanage their own hardware and network connections in particular\\ncircumstances. The biggest modern success stories of companies\\nbuilding and maintaining hardware involve extraordinary scale\\n(exabytes of data, terabits per second of bandwidth, etc.) and limited\\nuse cases where companies can realize a competitive advantage by\\nengineering highly integrated hardware and software stacks. In\\naddition, all of these companies consume massive network\\nbandwidth, suggesting that data egress charges would be a major\\ncost if they chose to operate fully from a public cloud.\\nConsider continuing to run workloads on premises or repatriating\\ncloud workloads if you run a truly cloud-scale service. What is cloud\\nscale? You might be at cloud scale if you are storing an exabyte of\\ndata or handling terabits per second of traffic to and from the\\ninternet. (Achieving a terabit per second of internal network traffic is\\nfairly easy.) In addition, consider owning your servers if data egress\\ncosts are a major factor for your business. To give a concrete\\nexample of cloud scale workloads that could benefit from\\nrepatriation, Apple might gain a significant financial and performance\\nadvantage by migrating iCloud storage to its own servers.\\nBuild Versus Buy\\nBuild versus buy is an age-old debate in technology. The argument\\nfor building is that you have end-to-end control over the solution and\\nare not at the mercy of a vendor or open source community. The\\nargument supporting buying comes down to resource constraints\\nand expertise; do you have the expertise to build a better solution\\nthan something already available? Either decision comes down to\\nTCO, TOCO, and whether the solution provides a competitive\\nadvantage to your organization.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2ccf0236-af40-4ac7-87b0-7936e4bd2921', embedding=None, metadata={'page_label': '207', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='If you’ve caught on to a theme in the book so far, it’s that we suggest\\ninvesting in building and customizing when doing so will provide a\\ncompetitive advantage for your business. Otherwise, stand on the\\nshoulders of giants and use what’s already available in the market.\\nGiven the number of open source and paid services—both of which\\nmay have communities of volunteers or highly paid teams of\\namazing engineers—you’re foolish to build everything yourself.\\nAs we often ask, “When you need new tires for your car, do you get\\nthe raw materials, build the tires from scratch, and install them\\nyourself?” Like most people, you’re probably buying tires and having\\nsomeone install them. The same argument applies to build versus\\nbuy. We’ve seen teams that have built their databases from scratch.\\nA simple open source RDBMS would have served their needs much\\nbetter upon closer inspection. Imagine the amount of time and\\nmoney invested in this homegrown database. Talk about low ROI for\\nTCO and opportunity cost.\\nThis is where the distinction between the type A and type B data\\nengineer comes in handy. As we pointed out earlier, type A and type\\nB roles are often embodied in the same engineer, especially in a\\nsmall organization. Whenever possible, lean toward type A behavior;\\navoid undifferentiated heavy lifting and embrace abstraction. Use\\nopen source frameworks, or if this is too much trouble, look at buying\\na suitable managed or proprietary solution. Plenty of great modular\\nservices are available to choose from in either case.\\nThe shifting reality of how companies adopt software is worth\\nmentioning. Whereas in the past, IT used to make most of the\\nsoftware purchase and adoption decisions in a top-down manner,\\nthese days, the trend is for bottom-up software adoption in a\\ncompany, driven by developers, data engineers, data scientists, and\\nother technical roles. Technology adoption within companies is\\nbecoming an organic, continuous process.\\nLet’s look at some options for open source and proprietary solutions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5fde2cf4-eea2-4c6a-851d-a603111d0b51', embedding=None, metadata={'page_label': '208', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Open Source Software\\nOpen source software (OSS) is a software distribution model in\\nwhich software, and the underlying codebase, is made available for\\ngeneral use, typically under specific licensing terms. Often OSS is\\ncreated and maintained by a distributed team of collaborators. OSS\\nis free to use, change, and distribute most of the time, but with\\nspecific caveats. For example, many licenses require that the source\\ncode of open source–derived software be included when the\\nsoftware is distributed.\\nThe motivations for creating and maintaining OSS vary. Sometimes\\nOSS is organic, springing from the mind of an individual or a small\\nteam that creates a novel solution and chooses to release it into the\\nwild for public use. Other times, a company may make a specific tool\\nor technology available to the public under an OSS license.\\nOSS has two main flavors: community managed and commercial\\nOSS.\\nCommunity-managed OSS\\nOSS projects succeed with a strong community and vibrant user\\nbase. Community-managed OSS is a prevalent path for OSS\\nprojects. The community opens up high rates of innovations and\\ncontributions from developers worldwide with popular OSS projects.\\nThe following are factors to consider with a community-managed\\nOSS project:\\nMindshare\\nAvoid adopting OSS projects that don’t have traction and\\npopularity. Look at the number of GitHub stars, forks, and commit\\nvolume and recency. Another thing to pay attention to is\\ncommunity activity on related chat groups and forums. Does the\\nproject have a strong sense of community? A strong community', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='93de9a94-c20b-41d2-9e63-da920bb7187c', embedding=None, metadata={'page_label': '209', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='creates a virtuous cycle of strong adoption. It also means that\\nyou’ll have an easier time getting technical assistance and finding\\ntalent qualified to work with the framework.\\nMaturity\\nHow long has the project been around, how active is it today, and\\nhow usable are people finding it in production? A project’s\\nmaturity indicates that people find it useful and are willing to\\nincorporate it into their production workflows.\\nTroubleshooting\\nHow will you have to handle problems if they arise? Are you on\\nyour own to troubleshoot issues, or can the community help you\\nsolve your problem?\\nProject management\\nLook at Git issues and the way they’re addressed. Are they\\naddressed quickly? If so, what’s the process to submit an issue\\nand get it resolved?\\nTeam\\nIs a company sponsoring the OSS project? Who are the core\\ncontributors?\\nDeveloper relations and community management\\nWhat is the project doing to encourage uptake and adoption? Is\\nthere a vibrant Slack community that provides encouragement\\nand support?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f73023f8-4abc-4238-8c24-4783e68fe0d1', embedding=None, metadata={'page_label': '210', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contributing\\nDoes the project encourage and accept pull requests?\\nRoadmap\\nIs there a project roadmap? If so, is it clear and transparent?\\nSelf-hosting and maintenance\\nDo you have the resources to host and maintain the OSS\\nsolution? If so, what’s the TCO and TOCO versus buying a\\nmanaged service from the OSS vendor?\\nGiving back to the community\\nIf you like the project and are actively using it, consider investing\\nin it. You can contribute to the codebase, help fix issues, and give\\nadvice in the community forums and chats. If the project allows\\ndonations, consider making one. Many OSS projects are\\nessentially community-service projects, and the maintainers often\\nhave full-time jobs in addition to helping with the OSS project.\\nSadly, it’s often a labor of love that doesn’t afford the maintainer a\\nliving wage. If you can afford to donate, please do so.\\nCommercial OSS\\nSometimes OSS has some drawbacks. Namely, you have to host\\nand maintain the solution in your environment. This may be trivial or\\nextremely complicated and cumbersome, depending on the OSS\\napplication. Commercial vendors try to solve this management\\nheadache by hosting and managing the OSS solution for you,\\ntypically as a cloud SaaS offering. Examples of such vendors include', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f87e5b58-fe40-4ecb-84a6-0aadf770db76', embedding=None, metadata={'page_label': '211', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Databricks (Spark), Confluent (Kafka), DBT Labs (dbt), and there are\\nmany, many others.\\nThis model is called commercial OSS (COSS). Typically, a vendor\\nwill offer the “core” of the OSS for free while charging for\\nenhancements, curated code distributions, or fully managed\\nservices.\\nA vendor is often affiliated with the community OSS project. As an\\nOSS project becomes more popular, the maintainers may create a\\nseparate business for a managed version of the OSS. This typically\\nbecomes a cloud SaaS platform built around a managed version of\\nthe open source code. This is a widespread trend: an OSS project\\nbecomes popular, an affiliated company raises truckloads of venture\\ncapital (VC) money to commercialize the OSS project, and the\\ncompany scales as a fast-moving rocket ship.\\nAt this point, the data engineer has two options. You can continue\\nusing the community-managed OSS version, which you need to\\ncontinue maintaining on your own (updates, server/container\\nmaintenance, pull requests for bug fixes, etc.). Or, you can pay the\\nvendor and let it take care of the administrative management of the\\nCOSS product.\\nThe following are factors to consider with a commercial OSS project:\\nValue\\nIs the vendor offering a better value than if you managed the\\nOSS technology yourself? Some vendors will add many bells and\\nwhistles to their managed offerings that aren’t available in the\\ncommunity OSS version. Are these additions compelling to you?\\nDelivery model\\nHow do you access the service? Is the product available via\\ndownload, API, or web/mobile UI? Be sure you can easily access', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c669e096-052d-46cd-b17d-d39820c34082', embedding=None, metadata={'page_label': '212', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the initial version and subsequent releases.\\nSupport\\nSupport cannot be understated, and it’s often opaque to the\\nbuyer. What is the support model for the product, and is there an\\nextra cost for support? Frequently, vendors will sell support for an\\nadditional fee. Be sure you clearly understand the costs of\\nobtaining support. Also, understand what is covered in support,\\nand what is not covered. Anything that’s not covered by support\\nwill be your responsibility to own and manage.\\nReleases and bug fixes\\nIs the vendor transparent about the release schedule,\\nimprovements, and bug fixes? Are these updates easily available\\nto you?\\nSales cycle and pricing\\nOften a vendor will offer on-demand pricing, especially for a\\nSaaS product, and offer you a discount if you commit to an\\nextended agreement. Be sure to understand the trade-offs of\\npaying as you go versus paying up front. Is it worth paying a lump\\nsum, or is your money better spent elsewhere?\\nCompany finances\\nIs the company viable? If the company has raised VC funds, you\\ncan check their funding on sites like Crunchbase. How much\\nrunway does the company have, and will it still be in business in\\na couple of years?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e62c14a4-81bb-4584-8efa-7f6801f6acef', embedding=None, metadata={'page_label': '213', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Logos versus revenue\\nIs the company focused on growing the number of customers\\n(logos), or is it trying to grow revenue? You may be surprised by\\nthe number of companies primarily concerned with growing their\\ncustomer count, GitHub stars, or Slack channel membership\\nwithout the revenue to establish sound finances.\\nCommunity support\\nIs the company truly supporting the community version of the\\nOSS project? How much is the company contributing to the\\ncommunity OSS codebase? Controversies have arisen with\\ncertain vendors co-opting OSS projects and subsequently\\nproviding little value back to the community. How likely will the\\nproduct remain viable as a community-supported open source if\\nthe company shuts down?\\nNote also that clouds offer their own managed open source\\nproducts. If a cloud vendor sees traction with a particular product or\\nproject, expect that vendor to offer its version. This can range from\\nsimple examples (open source Linux offered on VMs) to extremely\\ncomplex managed services (fully managed Kafka). The motivation\\nfor these offerings is simple: clouds make their money through\\nconsumption. More offerings in a cloud ecosystem mean a greater\\nchance of “stickiness” and increased customer spending.\\nProprietary Walled Gardens\\nWhile OSS is ubiquitous, a big market also exists for non-OSS\\ntechnologies. Some of the biggest companies in the data industry', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a866bcf-090d-440a-bc8c-c58f88db94df', embedding=None, metadata={'page_label': '214', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='sell closed source products. Let’s look at two major types of\\nproprietary walled gardens, independent companies and cloud-\\nplatform offerings.\\nIndependent offerings\\nThe data-tool landscape has seen exponential growth over the last\\nseveral years. Every day, new independent offerings arise for data\\ntools. With the ability to raise funds from VCs flush with capital, these\\ndata companies can scale and hire great engineering, sales, and\\nmarketing teams. This presents a situation where users have some\\ngreat product choices in the marketplace while having to wade\\nthrough endless sales and marketing clutter.\\nOften a company selling a data tool will not release it as OSS,\\ninstead offering a proprietary solution. Although you won’t have the\\ntransparency of a pure OSS solution, a proprietary independent\\nsolution can work quite well, especially as a fully managed service in\\nthe cloud.\\nThe following are things to consider with an independent offering:\\nInteroperability\\nMake sure that the tool interoperates with other tools you’ve\\nchosen (OSS, other independents, cloud offerings, etc.)\\nInteroperability is key, so make sure you can try it before you buy.\\nMindshare and market share\\nIs the solution popular? Does it command a presence in the\\nmarketplace? Does it enjoy positive customer reviews?\\nDocumentation and support\\nProblems and questions will inevitably arise. Is it clear how to\\nsolve your problem, either through documentation or support?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae3dd002-c6f0-4132-bdca-a236ac107a93', embedding=None, metadata={'page_label': '215', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pricing\\nIs the pricing understandable? Map out low-, medium-, and high-\\nprobability usage scenarios, with respective costs. Are you able\\nto negotiate a contract, along with a discount? Is it worth it? How\\nmuch flexibility do you lose if you sign a contract, both in\\nnegotiation and the ability to try new options? Are you able to\\nobtain contractual commitments on future pricing?\\nLongevity\\nWill the company survive long enough for you to get value from\\nits product? If the company has raised money, search around for\\nits funding situation. Look at user reviews. Ask friends and post\\nquestions on social networks about other users’ experiences with\\nthe product. Make sure you know what you’re getting into.\\nCloud platform proprietary service offerings\\nCloud vendors develop and sell their proprietary services for storage,\\ndatabases, and more. Many of these solutions are internal tools\\nused by respective sibling companies. For example, Amazon created\\nthe database DynamoDB to overcome the limitations of traditional\\nrelational databases and handle the large amounts of user and order\\ndata as Amazon.com grew into a behemoth. Amazon later offered\\nthe DynamoDB service solely on AWS; it’s now a top-rated product\\nused by companies of all sizes and maturity levels. Cloud vendors\\nwill often bundle their products to work well together. Each cloud can\\ncreate stickiness with its user base by creating a strong integrated\\necosystem.\\nThe following are factors to consider with a proprietary cloud\\noffering:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7e2a3abf-4569-45ad-b4a4-a509a4040d3c', embedding=None, metadata={'page_label': '216', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Performance versus price comparisons\\nIs the cloud offering substantially better than an independent or\\nOSS version? What’s the TCO of choosing a cloud’s offering?\\nPurchase considerations\\nOn-demand pricing can be expensive. Can you lower your cost\\nby purchasing reserved capacity or entering into a long-term\\ncommitment agreement?\\nOur Advice\\nBuild versus buy comes back to knowing your competitive advantage\\nand where it makes sense to invest resources toward customization.\\nIn general, we favor OSS and COSS by default, which frees you to\\nfocus on improving those areas where these options are insufficient.\\nFocus on a few areas where building something will add significant\\nvalue or reduce friction substantially.\\nDon’t treat internal operational overhead as a sunk cost. There’s\\nexcellent value in upskilling your existing data team to build\\nsophisticated systems on managed platforms rather than babysitting\\non-premises servers. In addition, think about how a company makes\\nmoney, especially its sales and customer experience teams, which\\nwill generally indicate how you’re treated during the sales cycle and\\nwhen you’re a paying customer.\\nFinally, who is responsible for the budget at your company? How\\ndoes this person decide the projects and technologies that get\\nfunded? Before making the business case for COSS or managed\\nservices, does it make sense to try to use OSS first? The last thing\\nyou want is for your technology choice to be stuck in limbo while\\nwaiting for budget approval. As the old saying goes, time kills deals.\\nIn your case, more time spent in limbo means a higher likelihood', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2ceb1a87-2bbc-4a11-9013-98fb3812f3e4', embedding=None, metadata={'page_label': '217', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='your budget approval will die. Know beforehand who controls the\\nbudget and what will successfully get approved.\\nMonolith Versus Modular\\nMonoliths versus modular systems is another longtime debate in the\\nsoftware architecture space. Monolithic systems are self-contained,\\noften performing multiple functions under a single system. The\\nmonolith camp favors the simplicity of having everything in one\\nplace. It’s easier to reason about a single entity, and you can move\\nfaster because there are fewer moving parts. The modular camp\\nleans toward decoupled, best-of-breed technologies performing\\ntasks at which they are uniquely great. Especially given the rate of\\nchange in products in the data world, the argument is you should aim\\nfor interoperability among an ever-changing array of solutions.\\nWhat approach should you take in your data engineering stack?\\nLet’s explore the trade-offs.\\nMonolith\\nThe monolith (Figure 4-4) has been a technology mainstay for\\ndecades. The old days of waterfall meant that software releases\\nwere huge, tightly coupled, and moved at a slow cadence. Large\\nteams worked together to deliver a single working codebase.\\nMonolithic data systems continue to this day, with older software\\nvendors such as Informatica and open source frameworks such as\\nSpark.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='72b85185-74b8-4cc4-a569-e7fadec79aae', embedding=None, metadata={'page_label': '218', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 4-4. The monolith tightly couples its services\\nThe pros of the monolith are it’s easy to reason about, and it requires\\na lower cognitive burden and context switching since everything is\\nself-contained. Instead of dealing with dozens of technologies, you\\ndeal with “one” technology and typically one principal programming\\nlanguage. Monoliths are an excellent option if you want simplicity in\\nreasoning about your architecture and processes.\\nOf course, the monolith has cons. For one thing, monoliths are\\nbrittle. Because of the vast number of moving parts, updates and\\nreleases take longer and tend to bake in “the kitchen sink.” If the\\nsystem has a bug—hopefully, the software’s been thoroughly tested\\nbefore release!—it can harm the entire system.\\nUser-induced problems also happen with monoliths. For example,\\nwe saw a monolithic ETL pipeline that took 48 hours to run. If\\nanything broke anywhere in the pipeline, the entire process had to\\nrestart. Meanwhile, anxious business users were waiting for their\\nreports, which were already two days late by default. Breakages\\nwere common enough that the monolithic system was eventually\\nthrown out.\\nMultitenancy in a monolithic system can also be a significant\\nproblem. It can be challenging to isolate the workloads of multiple', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b15c0a86-858f-402a-a259-84dc9460f616', embedding=None, metadata={'page_label': '219', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='users. In an on-prem data warehouse, one user-defined function\\nmight consume enough CPU to slow the system for other users.\\nConflicts between dependencies and resource contention are\\nfrequent sources of headaches.\\nAnother con of monoliths is that switching to a new system will be\\npainful if the vendor or open source project dies. Because all of your\\nprocesses are contained in the monolith, extracting yourself out of\\nthat system, and onto a new platform, will be costly in both time and\\nmoney.\\nModularity\\nModularity (Figure 4-5) is an old concept in software engineering, but\\nmodular distributed systems truly came into vogue with the rise of\\nmicroservices. Instead of relying on a massive monolith to handle\\nyour needs, why not break apart systems and processes into their\\nself-contained areas of concern? Microservices can communicate\\nvia APIs, allowing developers to focus on their domains while making\\ntheir applications accessible to other microservices. This is the trend\\nin software engineering and is increasingly seen in modern data\\nsystems.\\nFigure 4-5. With modularity, each service is decoupled from another\\nMajor tech companies have been key drivers in the microservices\\nmovement. The famous Bezos API mandate decreases coupling\\nbetween applications, allowing refactoring and decomposition. Bezos\\nalso imposed the two-pizza rule (no team should be so large that two\\npizzas can’t feed the whole group). Effectively, this means that a\\nteam will have at most five members. This cap also limits the\\ncomplexity of a team’s domain of responsibility—in particular, the\\ncodebase that it can manage. Whereas an extensive monolithic', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd94fb28-1273-4fb4-9583-fcbabed71d47', embedding=None, metadata={'page_label': '220', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='application might entail a group of one hundred people, dividing\\ndevelopers into small groups of five requires that this application be\\nbroken into small, manageable, loosely coupled pieces.\\nIn a modular microservice environment, components are swappable,\\nand it’s possible to create a polyglot (multiprogramming language)\\napplication; a Java service can replace a service written in Python.\\nService customers need worry only about the technical specifications\\nof the service API, not behind-the-scenes details of implementation.\\nData-processing technologies have shifted toward a modular model\\nby providing strong support for interoperability. Data is stored in\\nobject storage in a standard format such as Parquet in data lakes\\nand lakehouses. Any processing tool that supports the format can\\nread the data and write processed results back into the lake for\\nprocessing by another tool. Cloud data warehouses support\\ninteroperation with object storage through import/export using\\nstandard formats and external tables—i.e., queries run directly on\\ndata in a data lake.\\nNew technologies arrive on the scene at a dizzying rate in today’s\\ndata ecosystem, and most get stale and outmoded quickly. Rinse\\nand repeat. The ability to swap out tools as technology changes is\\ninvaluable. We view data modularity as a more powerful paradigm\\nthan monolithic data engineering. Modularity allows engineers to\\nchoose the best technology for each job or step along the pipeline.\\nThe cons of modularity are that there’s more to reason about.\\nInstead of handling a single system of concern, now you potentially\\nhave countless systems to understand and operate. Interoperability\\nis a potential headache; hopefully, these systems all play nicely\\ntogether.\\nThis very problem led us to break out orchestration as a separate\\nundercurrent instead of placing it under data management.\\nOrchestration is also important for monolithic data architectures;\\nwitness the success of tools like BMC Software’s Control-M in the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9ce5fe0c-343f-4776-9ec5-cd27bf146671', embedding=None, metadata={'page_label': '221', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='traditional data warehousing space. But orchestrating five or ten\\ntools is dramatically more complex than orchestrating one.\\nOrchestration becomes the glue that binds data stack modules\\ntogether.\\nThe Distributed Monolith Pattern\\nThe distributed monolith pattern is a distributed architecture that still\\nsuffers from many of the limitations of monolithic architecture. The\\nbasic idea is that one runs a distributed system with different\\nservices to perform different tasks. Still, services and nodes share a\\ncommon set of dependencies or a common codebase.\\nOne standard example is a traditional Hadoop cluster. A Hadoop\\ncluster can simultaneously host several frameworks, such as Hive,\\nPig, or Spark. The cluster also has many internal dependencies. In\\naddition, the cluster runs core Hadoop components: Hadoop\\ncommon libraries, HDFS, YARN, and Java. In practice, a cluster\\noften has one version of each component installed.\\nA standard on-prem Hadoop system entails managing a common\\nenvironment that works for all users and all jobs. Managing upgrades\\nand installations is a significant challenge. Forcing jobs to upgrade\\ndependencies risks breaking them; maintaining two versions of a\\nframework entails extra complexity.\\nSome modern Python-based orchestration technologies also suffer\\nfrom this problem. While they utilize a highly decoupled and\\nasynchronous architecture, every service runs the same codebase\\nwith the same dependencies. Any executor can execute any task, so\\na client library for a single task run in one DAG must be installed on\\nthe whole cluster. Orchestrating many tools entails installing client\\nlibraries for a host of APIs. Dependency conflicts are a constant\\nproblem.\\nOne solution to the problems of the distributed monolith is ephemeral\\ninfrastructure in a cloud setting. Each job gets its own temporary', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e63bd29e-1766-4639-aae6-d3b89ca1e1ec', embedding=None, metadata={'page_label': '222', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='server or cluster installed with dependencies. Each cluster remains\\nhighly monolithic, but separating jobs dramatically reduces conflicts.\\nFor example, this pattern is now quite common for Spark with\\nservices like Amazon EMR and Google Cloud Dataproc.\\nA second solution is to properly decompose the distributed monolith\\ninto multiple software environments using containers. We have more\\nto say on containers in “Serverless Versus Servers”.\\nOur Advice\\nWhile monoliths are attractive because of ease of understanding and\\nreduced complexity, this comes at a high cost. The cost is the\\npotential loss of flexibility, opportunity cost, and high-friction\\ndevelopment cycles.\\nHere are some things to consider when evaluating monoliths versus\\nmodular options:\\nInteroperability\\nArchitect for sharing and interoperability.\\nAvoiding the “bear trap”\\nSomething that is easy to get into might be painful or impossible\\nto escape.\\nFlexibility\\nThings are moving so fast in the data space right now.\\nCommitting to a monolith reduces flexibility and reversible\\ndecisions.\\nServerless Versus Servers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ebc942cd-b8d8-4190-9243-a4c1194c3e26', embedding=None, metadata={'page_label': '223', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A big trend for cloud providers is serverless, allowing developers and\\ndata engineers to run applications without managing servers behind\\nthe scenes. Serverless provides a quick time to value for the right\\nuse cases. For other cases, it might not be a good fit. Let’s look at\\nhow to evaluate whether serverless is right for you.\\nServerless\\nThough serverless has been around for quite some time, the\\nserverless trend kicked off in full force with AWS Lambda in 2014.\\nWith the promise of executing small chunks of code on an as-\\nneeded basis without having to manage a server, serverless\\nexploded in popularity. The main reasons for its popularity are cost\\nand convenience. Instead of paying the cost of a server, why not just\\npay when your code is evoked?\\nServerless has many flavors. Though function as a service (FaaS) is\\nwildly popular, serverless systems predate the advent of AWS\\nLambda. Google Cloud’s BigQuery is serverless in that data\\nengineers don’t need to manage backend infrastructure, and the\\nsystem scales to zero and scales up automatically to handle large\\nqueries. Just load data into the system and start querying. You pay\\nfor the amount of data your query consumes and a small cost to\\nstore your data. This payment model—paying for consumption and\\nstorage—is becoming more prevalent.\\nWhen does serverless make sense? As with many other cloud\\nservices, it depends; and data engineers would do well to\\nunderstand the details of cloud pricing to predict when serverless\\ndeployments will become expensive. Looking specifically at the case\\nof AWS Lambda, various engineers have found hacks to run batch\\nworkloads at meager costs. On the other hand, serverless\\nfunctions suffer from an inherent overhead inefficiency. Handling one\\nevent per function call at a high event rate can be catastrophically\\nexpensive.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9cf593e-fec1-4863-96eb-ca5ab8c4c354', embedding=None, metadata={'page_label': '224', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As with other areas of ops, it’s critical to monitor and model. Monitor\\nto determine cost per event in a real-world environment, and model\\nusing this cost per event to determine overall costs as event rates\\ngrow. Modeling should also include worst-case scenarios—what\\nhappens if my site gets hit by a bot swarm or DDoS attack?\\nContainers\\nIn conjunction with serverless and microservices, containers are one\\nof the most powerful trending operational technologies as of this\\nwriting. Containers play a role in both serverless and microservices.\\nContainers are often referred to as lightweight virtual machines.\\nWhereas a traditional VM wraps up an entire operating system, a\\ncontainer packages an isolated user space (such as a filesystem and\\na few processes); many such containers can coexist on a single host\\noperating system. This provides some of the principal benefits of\\nvirtualization (i.e., dependency and code isolation) without the\\noverhead of carrying around an entire operating system kernel.\\nA single hardware node can host numerous containers with fine-\\ngrained resource allocations. At the time of this writing, containers\\ncontinue to grow in popularity, along with Kubernetes, a container\\nmanagement system. Serverless environments typically run on\\ncontainers behind the scenes. Indeed, Kubernetes is a kind of\\nserverless environment because it allows developers and ops teams\\nto deploy microservices without worrying about the details of the\\nmachines where they are deployed.\\nContainers provide a partial solution to problems of the distributed\\nmonolith mentioned earlier in this chapter. For example, Hadoop now\\nsupports containers, allowing each job to have its own isolated\\ndependencies.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6c7b4fca-de94-407f-9004-7f7a582c9bbc', embedding=None, metadata={'page_label': '225', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='WARNING\\nContainer clusters do not provide the same security and isolation that\\nfull VMs offer. Container escape—broadly, a class of exploits whereby\\ncode in a container gains privileges outside the container at the OS\\nlevel—is common enough to be considered a risk for multitenancy.\\nWhile Amazon EC2 is a truly multitenant environment with VMs from\\nmany customers hosted on the same hardware, a Kubernetes cluster\\nshould host code only within an environment of mutual trust (e.g., inside\\nthe walls of a single company). In addition, code review processes and\\nvulnerability scanning are critical to ensure that a developer doesn’t\\nintroduce a security hole.\\nVarious flavors of container platforms add additional serverless\\nfeatures. Containerized function platforms run containers as\\nephemeral units triggered by events rather than persistent\\nservices. This gives users the simplicity of AWS Lambda with the\\nfull flexibility of a container environment instead of the highly\\nrestrictive Lambda runtime. And services such as AWS Fargate and\\nGoogle App Engine run containers without managing a compute\\ncluster required for Kubernetes. These services also fully isolate\\ncontainers, preventing the security issues associated with\\nmultitenancy.\\nAbstraction will continue working its way across the data stack.\\nConsider the impact of Kubernetes on cluster management. While\\nyou can manage your Kubernetes cluster—and many engineering\\nteams do so—even Kubernetes is widely available as a managed\\nservice.\\nWhen Infrastructure Makes Sense\\nWhy would you want to run your own servers instead of using\\nserverless? There are a few reasons. Cost is a big factor. Serverless\\nmakes less sense when the usage and cost exceed the ongoing cost\\nof running and maintaining a server (Figure 4-6). However, at a\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6c150e16-6be3-41e0-b656-135229d627c1', embedding=None, metadata={'page_label': '226', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='certain scale, the economic benefits of serverless may diminish, and\\nrunning servers becomes more attractive.\\nFigure 4-6. Cost of serverless versus utilizing a server\\nCustomization, power, and control are other major reasons to favor\\nservers over serverless. Some serverless frameworks can be\\nunderpowered or limited for certain use cases. Here are some things\\nto consider when using servers, particularly in the cloud, where\\nserver resources are ephemeral:\\nExpect servers to fail.\\nServer failure will happen. Avoid using a “special snowflake”\\nserver that is overly customized and brittle, as this introduces a\\nglaring vulnerability in your architecture. Instead, treat servers as\\nephemeral resources that you can create as needed and then\\ndelete. If your application requires specific code to be installed on\\nthe server, use a boot script or build an image. Deploy code to\\nthe server through a CI/CD pipeline.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0629fe55-c52b-4b95-bf7e-6749fed7f465', embedding=None, metadata={'page_label': '227', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Use clusters and autoscaling.\\nTake advantage of the cloud’s ability to grow and shrink compute\\nresources on demand. As your application increases its usage,\\ncluster your application servers, and use autoscaling capabilities\\nto automatically horizontally scale your application as demand\\ngrows.\\nTreat your infrastructure as code.\\nAutomation doesn’t apply to just servers and should extend to\\nyour infrastructure whenever possible. Deploy your infrastructure\\n(servers or otherwise) using deployment managers such as\\nTerraform, AWS CloudFormation, and Google Cloud Deployment\\nManager.\\nUse containers.\\nFor more sophisticated or heavy-duty workloads with complex\\ninstalled dependencies, consider using containers on either a\\nsingle server or Kubernetes.\\nOur Advice\\nHere are some key considerations to help you determine whether\\nserverless is right for you:\\nWorkload size and complexity\\nServerless works best for simple, discrete tasks and workloads.\\nIt’s not as suitable if you have many moving parts or require a lot\\nof compute or memory horsepower. In that case, consider using', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='449840fd-65e7-4428-9a5b-6590924ba576', embedding=None, metadata={'page_label': '228', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='containers and a container workflow orchestration framework like\\nKubernetes.\\nExecution frequency and duration\\nHow many requests per second will your serverless application\\nprocess? How long will each request take to process? Cloud\\nserverless platforms have limits on execution frequency,\\nconcurrency, and duration. If your application can’t function neatly\\nwithin these limits, it is time to consider a container-oriented\\napproach.\\nRequests and networking\\nServerless platforms often utilize some form of simplified\\nnetworking and don’t support all cloud virtual networking features,\\nsuch as VPCs and firewalls.\\nLanguage\\nWhat language do you typically use? If it’s not one of the officially\\nsupported languages supported by the serverless platform, you\\nshould consider containers instead.\\nRuntime limitations\\nServerless platforms don’t give you complete operating system\\nabstractions. Instead, you’re limited to a specific runtime image.\\nCost\\nServerless functions are incredibly convenient but potentially\\nexpensive. When your serverless function processes only a few', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d8141b2-a30d-4c33-bd93-d4dca01f1d74', embedding=None, metadata={'page_label': '229', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='events, your costs are low; costs rise rapidly as the event count\\nincreases. This scenario is a frequent source of surprise cloud\\nbills.\\nIn the end, abstraction tends to win. We suggest looking at using\\nserverless first and then servers—with containers and orchestration\\nif possible—once you’ve outgrown serverless options.\\nOptimization, Performance, and the Benchmark\\nWars\\nImagine that you are a billionaire shopping for new transportation.\\nYou’ve narrowed your choice to two options:\\n787 Business Jet\\nRange: 9,945 nautical miles (with 25 passengers)\\nMaximum speed: 0.90 Mach\\nCruise speed: 0.85 Mach\\nFuel capacity: 101,323 kilograms\\nMaximum takeoff weight: 227,930 kilograms\\nMaximum thrust: 128,000 pounds\\nTesla Model S Plaid\\nRange: 560 kilometers\\nMaximum speed: 322 kilometers/hour\\n0–-100 kilometers/hour: 2.1 seconds\\nBattery capacity: 100 kilowatt hours', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4683743-160c-4019-bdeb-e6ad84338cfd', embedding=None, metadata={'page_label': '230', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Nurburgring lap time: 7 minutes, 30.9 seconds\\nHorsepower: 1020\\nTorque: 1050 lb-ft\\nWhich of these options offers better performance? You don’t have to\\nknow much about cars or aircraft to recognize that this is an idiotic\\ncomparison. One option is a wide-body private jet designed for\\nintercontinental operation, while the other is an electric supercar.\\nWe see such apples-to-oranges comparisons made all the time in\\nthe database space. Benchmarks either compare databases that are\\noptimized for completely different use cases, or use test scenarios\\nthat bear no resemblance to real-world needs.\\nRecently, we saw a new round of benchmark wars flare up among\\nmajor vendors in the data space. We applaud benchmarks and are\\nglad to see many database vendors finally dropping DeWitt clauses\\nfrom their customer contracts. Even so, let the buyer beware: the\\ndata space is full of nonsensical benchmarks. Here are a few\\ncommon tricks used to place a thumb on the benchmark scale.\\nBig Data...for the 1990s\\nProducts that claim to support “big data” at petabyte scale will often\\nuse benchmark datasets small enough to easily fit in the storage on\\nyour smartphone. For systems that rely on caching layers to deliver\\nperformance, test datasets fully reside in solid-state drive (SSD) or\\nmemory, and benchmarks can show ultra-high performance by\\nrepeatedly querying the same data. A small test dataset also\\nminimizes RAM and SSD costs when comparing pricing.\\nTo benchmark for real-world use cases, you must simulate\\nanticipated real-world data and query size. Evaluate query\\nperformance and resource costs based on a detailed evaluation of\\nyour needs.\\n13\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dff53e89-8a8b-46ab-9d30-973a4cc6fb63', embedding=None, metadata={'page_label': '231', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Nonsensical Cost Comparisons\\nNonsensical cost comparisons are a standard trick when analyzing a\\nprice/performance or TCO. For instance, many MPP systems can’t\\nbe readily created and deleted even when they reside in a cloud\\nenvironment; these systems run for years on end once they’ve been\\nconfigured. Other databases support a dynamic compute model and\\ncharge either per query or per second of use. Comparing ephemeral\\nand non-ephemeral systems on a cost-per-second basis is\\nnonsensical, but we see this all the time in benchmarks.\\nAsymmetric Optimization\\nThe deceit of asymmetric optimization appears in many guises, but\\nhere’s one example. Often a vendor will compare a row-based MPP\\nsystem against a columnar database by using a benchmark that runs\\ncomplex join queries on highly normalized data. The normalized data\\nmodel is optimal for the row-based system, but the columnar system\\nwould realize its full potential only with some schema changes. To\\nmake matters worse, vendors juice their systems with an extra shot\\nof join optimization (e.g., pre-indexing joins) without applying\\ncomparable tuning in the competing database (e.g., putting joins in a\\nmaterialized view).\\nCaveat Emptor\\nAs with all things in data technology, let the buyer beware. Do your\\nhomework before blindly relying on vendor benchmarks to evaluate\\nand choose technology.\\nUndercurrents and Their Impacts on Choosing\\nTechnologies', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a4963ac-f2c2-4bde-aa6e-4978cfae45f9', embedding=None, metadata={'page_label': '232', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As seen in this chapter, a data engineer has a lot to consider when\\nevaluating technologies. Whatever technology you choose, be sure\\nto understand how it supports the undercurrents of the data\\nengineering lifecycle. Let’s briefly review them again.\\nData Management\\nData management is a broad area, and concerning technologies, it\\nisn’t always apparent whether a technology adopts data\\nmanagement as a principal concern. For example, behind the\\nscenes, a third-party vendor may use data management best\\npractices—regulatory compliance, security, privacy, data quality, and\\ngovernance—but hide these details behind a limited UI layer. In this\\ncase, while evaluating the product, it helps to ask the company about\\nits data management practices. Here are some sample questions\\nyou should ask:\\nHow are you protecting data against breaches, both from the\\noutside and within?\\nWhat is your product’s compliance with GDPR, CCPA, and other\\ndata privacy regulations?\\nDo you allow me to host my data to comply with these\\nregulations?\\nHow do you ensure data quality and that I’m viewing the correct\\ndata in your solution?\\nThere are many other questions to ask, and these are just a few of\\nthe ways to think about data management as it relates to choosing\\nthe right technologies. These same questions should also apply to\\nthe OSS solutions you’re considering.\\nDataOps', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0167fdf9-1871-4f30-80ca-7a9a03eb5475', embedding=None, metadata={'page_label': '233', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Problems will happen. They just will. A server or database may die, a\\ncloud’s region may have an outage, you might deploy buggy code,\\nbad data might be introduced into your data warehouse, and other\\nunforeseen problems may occur.\\nWhen evaluating a new technology, how much control do you have\\nover deploying new code, how will you be alerted if there’s a\\nproblem, and how will you respond when there’s a problem? The\\nanswer largely depends on the type of technology you’re\\nconsidering. If the technology is OSS, you’re likely responsible for\\nsetting up monitoring, hosting, and code deployment. How will you\\nhandle issues? What’s your incident response?\\nMuch of the operations are outside your control if you’re using a\\nmanaged offering. Consider the vendor’s SLA, the way they alert you\\nto issues, and whether they’re transparent about how they’re\\naddressing the case, including providing an ETA to a fix.\\nData Architecture\\nAs discussed in Chapter 3, good data architecture means assessing\\ntrade-offs and choosing the best tools for the job while keeping your\\ndecisions reversible. With the data landscape morphing at warp\\nspeed, the best tool for the job is a moving target. The main goals\\nare to avoid unnecessary lock-in, ensure interoperability across the\\ndata stack, and produce high ROI. Choose your technologies\\naccordingly.\\nOrchestration Example: Airflow\\nThroughout most of this chapter, we have actively avoided\\ndiscussing any particular technology too extensively. We make an\\nexception for orchestration because the space is currently dominated\\nby one open source technology, Apache Airflow.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc31cb9b-17c0-4be2-b3c9-5053cf7853a7', embedding=None, metadata={'page_label': '234', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Maxime Beauchemin kicked off the Airflow project at Airbnb in 2014.\\nAirflow was developed from the beginning as a noncommercial open\\nsource project. The framework quickly grew significant mindshare\\noutside Airbnb, becoming an Apache Incubator project in 2016 and a\\nfull Apache-sponsored project in 2019.\\nAirflow enjoys many advantages, largely because of its dominant\\nposition in the open source marketplace. First, the Airflow open\\nsource project is extremely active, with a high rate of commits and a\\nquick response time for bugs and security issues, and the project\\nrecently released Airflow 2, a major refactor of the codebase.\\nSecond, Airflow enjoys massive mindshare. Airflow has a vibrant,\\nactive community on many communications platforms, including\\nSlack, Stack Overflow, and GitHub. Users can easily find answers to\\nquestions and problems. Third, Airflow is available commercially as a\\nmanaged service or software distribution through many vendors,\\nincluding GCP, AWS, and Astronomer.io.\\nAirflow also has some downsides. Airflow relies on a few core\\nnonscalable components (the scheduler and backend database) that\\ncan become bottlenecks for performance, scale, and reliability; the\\nscalable parts of Airflow still follow a distributed monolith pattern.\\n(See “Monolith Versus Modular”.) Finally, Airflow lacks support for\\nmany data-native constructs, such as schema management, lineage,\\nand cataloging; and it is challenging to develop and test Airflow\\nworkflows.\\nWe do not attempt an exhaustive discussion of Airflow alternatives\\nhere but just mention a couple of the key orchestration contenders at\\nthe time of writing. Prefect and Dagster aim to solve some of the\\nproblems discussed previously by rethinking components of the\\nAirflow architecture. Will there be other orchestration frameworks\\nand technologies not discussed here? Plan on it.\\nWe highly recommend that anyone choosing an orchestration\\ntechnology study the options discussed here. They should also', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1d5652fd-34d4-4a6f-9a27-5ddba3ba05b6', embedding=None, metadata={'page_label': '235', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='acquaint themselves with activity in the space, as new developments\\nwill certainly occur by the time you read this.\\nSoftware Engineering\\nAs a data engineer, you should strive for simplification and\\nabstraction across the data stack. Buy or use prebuilt open source\\nsolutions whenever possible. Eliminating undifferentiated heavy\\nlifting should be your big goal. Focus your resources—custom\\ncoding and tooling—on areas that give you a solid competitive\\nadvantage. For example, is hand-coding a database connection\\nbetween your production database and your cloud data warehouse a\\ncompetitive advantage for you? Probably not. This is very much a\\nsolved problem. Pick an off-the-shelf solution (open source or\\nmanaged SaaS) instead. The world doesn’t need the millionth +1\\ndatabase-to-cloud data warehouse connector.\\nOn the other hand, why do customers buy from you? Your business\\nlikely has something special about the way it does things. Maybe it’s\\na particular algorithm that powers your fintech platform. By\\nabstracting away a lot of the redundant workflows and processes,\\nyou can continue chipping away, refining, and customizing the things\\nthat move the needle for the business.\\nConclusion\\nChoosing the right technologies is no easy task, especially when\\nnew technologies and patterns emerge daily. Today is possibly the\\nmost confusing time in history for evaluating and selecting\\ntechnologies. Choosing technologies is a balance of use case, cost,\\nbuild versus buy, and modularization. Always approach technology\\nthe same way as architecture: assess trade-offs and aim for\\nreversible decisions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d993e4c4-4614-4775-a5fb-115dfa0fc984', embedding=None, metadata={'page_label': '236', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1  For more details, see “Total Opportunity Cost of Ownership” by Joseph Reis\\nin 97 Things Every Data Engineer Should Know (O’Reilly).\\n2  J.R. Storment and Mike Fuller, Cloud FinOps (Sebastopol, CA: O’Reilly,\\n2019), 6, https://oreil.ly/RvRvX.\\n3  This is a major point of emphasis in Storment and Fuller, Cloud FinOps.\\n4  Examples include Google Cloud Anthos and AWS Outposts.\\n5  Raghav Bhargava, “Evolution of Dropbox’s Edge Network,” Dropbox.Tech,\\nJune 19, 2017, https://oreil.ly/RAwPf.\\n6  Akhil Gupta, “Scaling to Exabytes and Beyond,” Dropbox.Tech, March 14,\\n2016, https://oreil.ly/5XPKv.\\n7  “Dropbox Migrates 34 PB of Data to an Amazon S3 Data Lake for\\nAnalytics,” AWS website, 2020, https://oreil.ly/wpVoM.\\n8  Todd Hoff, “The Eternal Cost Savings of Netflix’s Internal Spot Market,” High\\nScalability, December 4, 2017, https://oreil.ly/LLoFt.\\n9  Todd Spangler, “Netflix Bandwidth Consumption Eclipsed by Web Media\\nStreaming Applications,” Variety, September 10, 2019, https://oreil.ly/tTm3k.\\n10  Amir Efrati and Kevin McLaughlin, “Apple’s Spending on Google Cloud\\nStorage on Track to Soar 50% This Year,” The Information, June 29, 2021,\\nhttps://oreil.ly/OlFyR.\\n11  Evan Sangaline, “Running FFmpeg on AWS Lambda for 1.9% the Cost of\\nAWS Elastic Transcoder,” Intoli blog, May 2, 2018, https://oreil.ly/myzOv.\\n12  Examples include OpenFaaS, Knative, and Google Cloud Run.\\n13  Justin Olsson and Reynold Xin, “Eliminating the Anti-competitive DeWitt\\nClause for Database Benchmarking,” Databricks, November 8, 2021,\\nhttps://oreil.ly/3iFOE.\\n14  For a classic of the genre, see William McKnight and Jake Dolezal, “Data\\nWarehouse in the Cloud Benchmark,” GigaOm, February 7, 2019,\\nhttps://oreil.ly/QjCmA.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eee88aa8-a250-4083-812f-b4483d581f99', embedding=None, metadata={'page_label': '237', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Part II. The Data Engineering\\nLifecycle in Depth', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='16353298-9fd8-4e10-863b-3416f576b6f0', embedding=None, metadata={'page_label': '238', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 5. Data Generation in\\nSource Systems\\nWelcome to the first stage of the data engineering lifecycle: data\\ngeneration in source systems. As we described earlier, the job of a\\ndata engineer is to take data from source systems, do something\\nwith it, and make it helpful in serving downstream use cases. But\\nbefore you get raw data, you must understand where the data exists,\\nhow it is generated, and its characteristics and quirks.\\nThis chapter covers some popular operational source system\\npatterns and the significant types of source systems. Many source\\nsystems exist for data generation, and we’re not exhaustively\\ncovering them all. We’ll consider the data these systems generate\\nand things you should consider when working with source systems.\\nWe also discuss how the undercurrents of data engineering apply to\\nthis first phase of the data engineering lifecycle (Figure 5-1).\\nFigure 5-1. Source systems generate the data for the rest of the data engineering\\nlifecycle', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='276dcbe3-4990-4d78-aed3-4f03dc3b6aa5', embedding=None, metadata={'page_label': '239', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As data proliferates, especially with the rise of data sharing\\n(discussed next), we expect that a data engineer’s role will shift\\nheavily toward understanding the interplay between data sources\\nand destinations. The basic plumbing tasks of data engineering—\\nmoving data from A to B—will simplify dramatically. On the other\\nhand, it will remain critical to understand the nature of data as it’s\\ncreated in source systems.\\nSources of Data: How Is Data Created?\\nAs you learn about the various underlying operational patterns of the\\nsystems that generate data, it’s essential to understand how data is\\ncreated. Data is an unorganized, context-less collection of facts and\\nfigures. It can be created in many ways, both analog and digital.\\nAnalog data creation occurs in the real world, such as vocal speech,\\nsign language, writing on paper, or playing an instrument. This\\nanalog data is often transient; how often have you had a verbal\\nconversation whose contents are lost to the ether after the\\nconversation ends?\\nDigital data is either created by converting analog data to digital form\\nor is the native product of a digital system. An example of analog to\\ndigital is a mobile texting app that converts analog speech into digital\\ntext. An example of digital data creation is a credit card transaction\\non an ecommerce platform. A customer places an order, the\\ntransaction is charged to their credit card, and the information for the\\ntransaction is saved to various databases.\\nWe’ll utilize a few common examples in this chapter, such as data\\ncreated when interacting with a website or mobile application. But in\\ntruth, data is everywhere in the world around us. We capture data\\nfrom IoT devices, credit card terminals, telescope sensors, stock\\ntrades, and more.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d580068a-4bb1-49a9-b976-ba657cf2579e', embedding=None, metadata={'page_label': '240', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Get familiar with your source system and how it generates data. Put\\nin the effort to read the source system documentation and\\nunderstand its patterns and quirks. If your source system is an\\nRDBMS, learn how it operates (writes, commits, queries, etc.); learn\\nthe ins and outs of the source system that might affect your ability to\\ningest from it.\\nSource Systems: Main Ideas\\nSource systems produce data in various ways. This section\\ndiscusses the main ideas you’ll frequently encounter as you work\\nwith source systems.\\nFiles and Unstructured Data\\nA file is a sequence of bytes, typically stored on a disk. Applications\\noften write data to files. Files may store local parameters, events,\\nlogs, images, and audio.\\nIn addition, files are a universal medium of data exchange. As much\\nas data engineers wish that they could get data programmatically,\\nmuch of the world still sends and receives files. For example, if\\nyou’re getting data from a government agency, there’s an excellent\\nchance you’ll download the data as an Excel or CSV file or receive\\nthe file in an email.\\nThe main types of source file formats you’ll run into as a data\\nengineer—files that originate either manually or as an output from a\\nsource system process—are Excel, CSV, TXT, JSON, and XML.\\nThese files have their quirks and can be structured (Excel, CSV),\\nsemistructured (JSON, XML, CSV), or unstructured (TXT, CSV).\\nAlthough you’ll use certain formats heavily as a data engineer (such\\nas Parquet, ORC, and Avro), we’ll cover these later and put the\\nspotlight here on source system files. Chapter 6 covers the technical\\ndetails of files.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='15da37c3-9945-40dd-9eff-e839f80c430c', embedding=None, metadata={'page_label': '241', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='APIs\\nApplication programming interfaces (APIs) are a standard way of\\nexchanging data between systems. In theory, APIs simplify the data\\ningestion task for data engineers. In practice, many APIs still expose\\na good deal of data complexity for engineers to manage. Even with\\nthe rise of various services and frameworks, and services for\\nautomating API data ingestion, data engineers must often invest a\\ngood deal of energy into maintaining custom API connections. We\\ndiscuss APIs in greater detail later in this chapter.\\nApplication Databases (OLTP systems)\\nAn application database stores the state of an application. A\\nstandard example is a database that stores account balances for\\nbank accounts. As customer transactions and payments happen, the\\napplication updates bank account balances.\\nTypically, an application database is an online transaction processing\\n(OLTP) system—a database that reads and writes individual data\\nrecords at a high rate. OLTP systems are often referred to as\\ntransactional databases, but this does not necessarily imply that the\\nsystem in question supports atomic transactions.\\nMore generally, OLTP databases support low latency and high\\nconcurrency. An RDBMS database can select or update a row in\\nless than a millisecond (not accounting for network latency) and\\nhandle thousands of reads and writes per second. A document\\ndatabase cluster can manage even higher document commit rates at\\nthe expense of potential inconsistency. Some graph databases can\\nalso handle transactional use cases.\\nFundamentally, OLTP databases work well as application backends\\nwhen thousands or even millions of users might be interacting with\\nthe application simultaneously, updating and writing data\\nconcurrently. OLTP systems are less suited to use cases driven by', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97b6e2be-db21-4e0b-95fb-833559747d3d', embedding=None, metadata={'page_label': '242', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='analytics at scale, where a single query must scan a vast amount of\\ndata.\\nACID\\nSupport for atomic transactions is one of a critical set of database\\ncharacteristics known together as ACID (as you may recall from\\nChapter 3, this stands for atomicity, consistency, isolation, durability).\\nConsistency means that any database read will return the last written\\nversion of the retrieved item. Isolation entails that if two updates are\\nin flight concurrently for the same thing, the end database state will\\nbe consistent with the sequential execution of these updates in the\\norder they were submitted. Durability indicates that committed data\\nwill never be lost, even in the event of power loss.\\nNote that ACID characteristics are not required to support application\\nbackends, and relaxing these constraints can be a considerable\\nboon to performance and scale. However, ACID characteristics\\nguarantee that the database will maintain a consistent picture of the\\nworld, dramatically simplifying the app developer’s task.\\nAll engineers (data or otherwise) must understand operating with\\nand without ACID. For instance, to improve performance, some\\ndistributed databases use relaxed consistency constraints, such as\\neventual consistency, to improve performance. Understanding the\\nconsistency model you’re working with helps you prevent disasters.\\nAtomic transactions\\nAn atomic transaction is a set of several changes that are committed\\nas a unit. In the example in Figure 5-2, a traditional banking\\napplication running on an RDBMS executes a SQL statement that\\nchecks two account balances, one in Account A (the source) and\\nanother in Account B (the destination). Money is then moved from\\nAccount A to Account B if sufficient funds are in Account A. The\\nentire transaction should run with updates to both account balances', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='439feb26-6a07-4960-9937-080d3b41e829', embedding=None, metadata={'page_label': '243', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='or fail without updating either account balance. That is, the whole\\noperation should happen as a transaction.\\nFigure 5-2. Example of an atomic transaction: a bank account transfer using OLTP\\nOLTP and analytics\\nOften, small companies run analytics directly on an OLTP. This\\npattern works in the short term but is ultimately not scalable. At some\\npoint, running analytical queries on OLTP runs into performance\\nissues due to structural limitations of OLTP or resource contention\\nwith competing transactional workloads. Data engineers must\\nunderstand the inner workings of OLTP and application backends to\\nset up appropriate integrations with analytics systems without\\ndegrading production application performance.\\nAs companies offer more analytics capabilities in SaaS applications,\\nthe need for hybrid capabilities—quick updates with combined\\nanalytics capabilities—has created new challenges for data\\nengineers. We’ll use the term data application to refer to applications\\nthat hybridize transactional and analytics workloads.\\nOnline Analytical Processing System\\nIn contrast to an OLTP system, an online analytical processing\\n(OLAP) system is built to run large analytics queries and is typically\\ninefficient at handling lookups of individual records. For example,\\nmodern column databases are optimized to scan large volumes of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='974c4d7a-11fa-4659-a62e-96078493a89f', embedding=None, metadata={'page_label': '244', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data, dispensing with indexes to improve scalability and scan\\nperformance. Any query typically involves scanning a minimal data\\nblock, often 100 MB or more in size. Trying to look up thousands of\\nindividual items per second in such a system will bring it to its knees\\nunless it is combined with a caching layer designed for this use case.\\nNote that we’re using the term OLAP to refer to any database\\nsystem that supports high-scale interactive analytics queries; we are\\nnot limiting ourselves to systems that support OLAP cubes\\n(multidimensional arrays of data). The online part of OLAP implies\\nthat the system constantly listens for incoming queries, making\\nOLAP systems suitable for interactive analytics.\\nAlthough this chapter covers source systems, OLAPs are typically\\nstorage and query systems for analytics. Why are we talking about\\nthem in our chapter on source systems? In practical use cases,\\nengineers often need to read data from an OLAP system. For\\nexample, a data warehouse might serve data used to train an ML\\nmodel. Or, an OLAP system might serve a reverse ETL workflow,\\nwhere derived data in an analytics system is sent back to a source\\nsystem, such as a CRM, SaaS platform, or transactional application.\\nChange Data Capture\\nChange data capture (CDC) is a method for extracting each change\\nevent (insert, update, delete) that occurs in a database. CDC is\\nfrequently leveraged to replicate between databases in near real\\ntime or create an event stream for downstream processing.\\nCDC is handled differently depending on the database technology.\\nRelational databases often generate an event log stored directly on\\nthe database server that can be processed to create a stream. (See\\n“Database Logs”.) Many cloud NoSQL databases can send a log or\\nevent stream to a target storage location.\\nLogs', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='79442dbd-b8d3-4e0f-af77-a6f6ae3a6863', embedding=None, metadata={'page_label': '245', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A log captures information about events that occur in systems. For\\nexample, a log may capture traffic and usage patterns on a web\\nserver. Your desktop computer’s operating system (Windows,\\nmacOS, Linux) logs events as the system boots and when\\napplications start or crash, for example.\\nLogs are a rich data source, potentially valuable for downstream\\ndata analysis, ML, and automation. Here are a few familiar sources\\nof logs:\\nOperating systems\\nApplications\\nServers\\nContainers\\nNetworks\\nIoT devices\\nAll logs track events and event metadata. At a minimum, a log\\nshould capture who, what, and when:\\nWho\\nThe human, system, or service account associated with the event\\n(e.g., a web browser user agent or a user ID)\\nWhat happened\\nThe event and related metadata\\nWhen\\nThe timestamp of the event\\nLog encoding', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f35770ed-6701-4394-b972-78de1f032921', embedding=None, metadata={'page_label': '246', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Logs are encoded in a few ways:\\nBinary-encoded logs\\nThese encode data in a custom compact format for space\\nefficiency and fast I/O. Database logs, discussed in “Database\\nLogs”, are a standard example.\\nSemistructured logs\\nThese are encoded as text in an object serialization format\\n(JSON, more often than not). Semistructured logs are machine-\\nreadable and portable. However, they are much less efficient\\nthan binary logs. And though they are nominally machine-\\nreadable, extracting value from them often requires significant\\ncustom code.\\nPlain-text (unstructured) logs\\nThese essentially store the console output from software. As\\nsuch, no general-purpose standards exist. These logs can\\nprovide helpful information for data scientists and ML engineers,\\nthough extracting useful information from the raw text data might\\nbe complicated.\\nLog resolution\\nLogs are created at various resolutions and log levels. The log\\nresolution refers to the amount of event data captured in a log. For\\nexample, database logs capture enough information from database\\nevents to allow reconstructing the database state at any point in\\ntime.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d83b9aa9-29d7-44ad-9082-330cc3b464e9', embedding=None, metadata={'page_label': '247', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the other hand, capturing all data changes in logs for a big data\\nsystem often isn’t practical. Instead, these logs may note only that a\\nparticular type of commit event has occurred. The log level refers to\\nthe conditions required to record a log entry, specifically concerning\\nerrors and debugging. Software is often configurable to log every\\nevent or to log only errors, for example.\\nLog latency: Batch or real time\\nBatch logs are often written continuously to a file. Individual log\\nentries can be written to a messaging system such as Kafka or\\nPulsar for real-time applications.\\nDatabase Logs\\nDatabase logs are essential enough that they deserve more detailed\\ncoverage. Write-ahead logs—typically, binary files stored in a\\nspecific database-native format—play a crucial role in database\\nguarantees and recoverability. The database server receives write\\nand update requests to a database table (see Figure 5-3), storing\\neach operation in the log before acknowledging the request. The\\nacknowledgment comes with a log associated guarantee: even if the\\nserver fails, it can recover its state on reboot by completing the\\nunfinished work from the logs.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f696a6b-7475-4a9e-a9c9-3cfcbc47c12d', embedding=None, metadata={'page_label': '248', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 5-3. Log operations on a database table\\nDatabase logs are extremely useful in data engineering, especially\\nfor CDC to generate event streams from database changes.\\nCRUD\\nCRUD, which stands for create, read, update, and delete, is a\\ntransactional pattern commonly used in programming and represents\\nthe four basic operations of persistent storage. CRUD is the most\\ncommon pattern for storing application state in a database. A basic\\ntenet of CRUD is that data must be created before being used. After\\nthe data has been created, the data can be read and updated.\\nFinally, the data may need to be destroyed. CRUD guarantees these\\nfour operations will occur on data, regardless of its storage.\\nCRUD is a widely used pattern in software applications, and you’ll\\ncommonly find CRUD used in APIs and databases. For example, a\\nweb application will make heavy use of CRUD for RESTful HTTP\\nrequests and storing and retrieving data from a database.\\nAs with any database, we can use snapshot-based extraction to get\\ndata from a database where our application applies CRUD', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='72c810cd-412a-4ea5-9174-6e971c44c393', embedding=None, metadata={'page_label': '249', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='operations. On the other hand, event extraction with CDC gives us a\\ncomplete history of operations and potentially allows for near real-\\ntime analytics.\\nInsert-Only\\nThe insert-only pattern retains history directly in a table containing\\ndata. Rather than updating records, new records get inserted with a\\ntimestamp indicating when they were created (Table 5-1). For\\nexample, suppose you have a table of customer addresses.\\nFollowing a CRUD pattern, you would simply update the record if the\\ncustomer changed their address. With the insert-only pattern, a new\\naddress record is inserted with the same customer ID. To read the\\ncurrent customer address by customer ID, you would look up the\\nlatest record under that ID.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90fbfb6d-2c1b-4536-a73d-1b50d6cf119f', embedding=None, metadata={'page_label': '250', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='T\\na\\nb\\nl\\ne\\n \\n5\\n-\\n1\\n. \\nA\\nn\\n \\ni\\nn\\ns\\ne\\nr\\nt\\n-\\no\\nn\\nl\\ny \\np\\na\\nt\\nt\\ne\\nr\\nn\\n \\np\\nr', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12fcc692-7851-4113-915a-0e7d1ef818c4', embedding=None, metadata={'page_label': '251', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='o\\nd\\nu\\nc\\ne\\ns \\nm\\nu\\nlt\\ni\\np\\nl\\ne\\n \\nv\\ne\\nr\\ns\\ni\\no\\nn\\ns \\no\\nf \\na\\n \\nr\\ne\\nc\\no\\nr\\nd\\nRecord ID Value Timestamp', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56b29e6b-ef1f-43dd-a3b5-d98a5d6e5271', embedding=None, metadata={'page_label': '252', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 40 2021-09-19T00:10:23+00:00\\n1 51 2021-09-30T00:12:00+00:00\\nIn a sense, the insert-only pattern maintains a database log directly\\nin the table itself, making it especially useful if the application needs\\naccess to history. For example, the insert-only pattern would work\\nwell for a banking application designed to present customer address\\nhistory.\\nA separate analytics insert-only pattern is often used with regular\\nCRUD application tables. In the insert-only ETL pattern, data\\npipelines insert a new record in the target analytics table anytime an\\nupdate occurs in the CRUD table.\\nInsert-only has a couple of disadvantages. First, tables can grow\\nquite large, especially if data frequently changes, since each change\\nis inserted into the table. Sometimes records are purged based on a\\nrecord sunset date or a maximum number of record versions to keep\\ntable size reasonable. The second disadvantage is that record\\nlookups incur extra overhead because looking up the current state\\ninvolves running MAX(created_timestamp). If hundreds or\\nthousands of records are under a single ID, this lookup operation is\\nexpensive to run.\\nMessages and Streams\\nRelated to event-driven architecture, two terms that you’ll often see\\nused interchangeably are message queue and streaming platform,\\nbut a subtle but essential difference exists between the two. Defining\\nand contrasting these terms is worthwhile since they encompass\\nmany big ideas related to source systems and practices, and\\ntechnologies spanning the entire data engineering lifecycle.\\nA message is raw data communicated across two or more systems\\n(Figure 5-4). For example, we have System 1 and System 2, where', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e840f7f2-e6f0-4cbb-9f35-4cf4689e739a', embedding=None, metadata={'page_label': '253', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='System 1 sends a message to System 2. These systems could be\\ndifferent microservices, a server sending a message to a serverless\\nfunction, etc.. A message is typically sent through a message queue\\nfrom a publisher to a consumer, and once the message is delivered,\\nit is removed from the queue.\\nFigure 5-4. A message passed between two systems\\nMessages are discrete and singular signals in an event-driven\\nsystem. For example, an IoT device might send a message with the\\nlatest temperature reading to a message queue. This message is\\nthen ingested by a service that determines whether the furnace\\nshould be turned on or off. This service sends a message to a\\nfurnace controller that takes the appropriate action. Once the\\nmessage is received, and the action is taken, the message is\\nremoved from the message queue.\\nBy contrast, a stream is an append-only log of event records.\\n(Streams are ingested and stored in event-streaming platforms,\\nwhich we discuss at greater length in “Messages and Streams”.) As\\nevents occur, they are accumulated in an ordered sequence\\n(Figure 5-5); a timestamp or an ID might order events. (Note that\\nevents aren’t always delivered in exact order because of the\\nsubtleties of distributed systems.)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9284990c-9beb-4670-bf7f-8c39a5a2ebf2', embedding=None, metadata={'page_label': '254', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 5-5. A stream, which is an ordered append-only log of records\\nYou’ll use streams when you care about what happened over many\\nevents. Because of the append-only nature of streams, records in a\\nstream are persisted over a long retention window—often weeks or\\nmonths—allowing for complex operations on records such as\\naggregations on multiple records or the ability to rewind to a point in\\ntime within the stream.\\nIt’s worth noting that systems that process streams can process\\nmessages, and streaming platforms are frequently used for message\\npassing. We often accumulate messages in streams when we want\\nto perform message analytics. In our IoT example, the temperature\\nreadings that trigger the furnace to turn on or off might also be later\\nanalyzed to determine temperature trends and statistics.\\nTypes of Time\\nWhile time is an essential consideration for all data ingestion, it\\nbecomes that much more critical and subtle in the context of\\nstreaming, where we view data as continuous and expect to\\nconsume it shortly after it is produced. Let’s look at the key types of\\ntime you’ll run into when ingesting data: the time that the event is\\ngenerated, when it’s ingested and processed, and how long\\nprocessing took (Figure 5-6).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e190a6f1-58b0-4361-ba44-348d3d07dd76', embedding=None, metadata={'page_label': '255', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 5-6. Event, ingestion, process, and processing time\\nEvent time indicates when an event is generated in a source system,\\nincluding the timestamp of the original event itself. An undetermined\\ntime lag will occur upon event creation, before the event is ingested\\nand processed downstream. Always include timestamps for each\\nphase through which an event travels. Log events as they occur and\\nat each stage of time—when they’re created, ingested, and\\nprocessed. Use these timestamp logs to accurately track the\\nmovement of your data through your data pipelines.\\nAfter data is created, it is ingested somewhere. Ingestion time\\nindicates when an event is ingested from source systems into a\\nmessage queue, cache, memory, object storage, a database, or any\\nplace else that data is stored (see Chapter 6). After ingestion, data\\nmay be processed immediately; or within minutes, hours, or days; or\\nsimply persist in storage indefinitely.\\nProcess time occurs after ingestion time, when the data is processed\\n(typically, a transformation). Processing time is how long the data\\ntook to process, measured in seconds, minutes, hours, etc.\\nYou’ll want to record these various times, preferably in an automated\\nway. Set up monitoring along your data workflows to capture when\\nevents occur, when they’re ingested and processed, and how long it\\ntook to process events.\\nSource System Practical Details', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9b2411df-2a73-47f3-8a7e-b20a49a740c0', embedding=None, metadata={'page_label': '256', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This section discusses the practical details of interacting with\\nmodern source systems. We’ll dig into the details of commonly\\nencountered databases, APIs, and other aspects. This information\\nwill have a shorter shelf life than the main ideas discussed\\npreviously; popular API frameworks, databases, and other details will\\ncontinue to change rapidly.\\nNevertheless, these details are critical knowledge for working data\\nengineers. We suggest that you study this information as baseline\\nknowledge but read extensively to stay abreast of ongoing\\ndevelopments.\\nDatabases\\nIn this section, we’ll look at common source system database\\ntechnologies that you’ll encounter as a data engineer and high-level\\nconsiderations for working with these systems. There are as many\\ntypes of databases as there are use cases for data.\\nMajor considerations for understanding database technologies\\nHere, we introduce major ideas that occur across a variety of\\ndatabase technologies, including those that back software\\napplications and those that support analytics use cases:\\nDatabase management system\\nA database system used to store and serve data. Abbreviated as\\nDBMS, it consists of a storage engine, query optimizer, disaster\\nrecovery, and other key components for managing the database\\nsystem.\\nLookups\\nHow does the database find and retrieve data? Indexes can help\\nspeed up lookups, but not all databases have indexes. Know', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d05782d-30be-4342-b3b8-466911d8f288', embedding=None, metadata={'page_label': '257', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='whether your database uses indexes; if so, what are the best\\npatterns for designing and maintaining them? Understand how to\\nleverage for efficient extraction. It also helps to have a basic\\nknowledge of the major types of indexes, including B-tree and\\nlog-structured merge-trees (LSM).\\nQuery optimizer\\nDoes the database utilize an optimizer? What are its\\ncharacteristics?\\nScaling and distribution\\nDoes the database scale with demand? What scaling strategy\\ndoes it deploy? Does it scale horizontally (more database nodes)\\nor vertically (more resources on a single machine)?\\nModeling patterns\\nWhat modeling patterns work best with the database (e.g., data\\nnormalization or wide tables)? (See Chapter 8 for our discussion\\nof data modeling.)\\nCRUD\\nHow is data queried, created, updated, and deleted in the\\ndatabase? Every type of database handles CRUD operations\\ndifferently.\\nConsistency\\nIs the database fully consistent, or does it support a relaxed\\nconsistency model (e.g., eventual consistency)? Does the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ba053cff-af38-4808-a92d-69337d788a66', embedding=None, metadata={'page_label': '258', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='database support optional consistency modes for reads and\\nwrites (e.g., strongly consistent reads)?\\nWe divide databases into relational and nonrelational categories. In\\ntruth, the nonrelational category is far more diverse, but relational\\ndatabases still occupy significant space in application backends.\\nRelational databases\\nA relational database management system (RDBMS) is one of the\\nmost common application backends. Relational databases were\\ndeveloped at IBM in the 1970s and popularized by Oracle in the\\n1980s. The growth of the internet saw the rise of the LAMP stack\\n(Linux, Apache web server, MySQL, PHP) and an explosion of\\nvendor and open source RDBMS options. Even with the rise of\\nNoSQL databases (described in the following section), relational\\ndatabases have remained extremely popular.\\nData is stored in a table of relations (rows), and each relation\\ncontains multiple fields (columns); see Figure 5-7. Note that we use\\nthe terms column and field interchangeably throughout this book.\\nEach relation in the table has the same schema (a sequence of\\ncolumns with assigned static types such as string, integer, or float).\\nRows are typically stored as a contiguous sequence of bytes on disk.\\nFigure 5-7. RDBMS stores and retrieves data at a row level', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e93704e9-5cff-44b0-8d2e-9628bd825769', embedding=None, metadata={'page_label': '259', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Tables are indexed by a primary key, a unique field for each row in\\nthe table. The indexing strategy for the primary key is closely\\nconnected with the layout of the table on disk.\\nTables can also have various foreign keys—fields with values\\nconnected with the values of primary keys in other tables, facilitating\\njoins, and allowing for complex schemas that spread data across\\nmultiple tables. In particular, it is possible to design a normalized\\nschema. Normalization is a strategy for ensuring that data in records\\nis not duplicated in multiple places, thus avoiding the need to update\\nstates in multiple locations at once and preventing inconsistencies\\n(see Chapter 8).\\nRDBMS systems are typically ACID compliant. Combining a\\nnormalized schema, ACID compliance, and support for high\\ntransaction rates makes relational database systems ideal for storing\\nrapidly changing application states. The challenge for data engineers\\nis to determine how to capture state information over time.\\nA full discussion of the theory, history, and technology of RDBMS is\\nbeyond the scope of this book. We encourage you to study RDBMS\\nsystems, relational algebra, and strategies for normalization because\\nthey’re widespread, and you’ll encounter them frequently. See\\n“Additional Resources” for suggested books.\\nNonrelational databases: NoSQL\\nWhile relational databases are terrific for many use cases, they’re\\nnot a one-size-fits-all solution. We often see that people start with a\\nrelational database under the impression it’s a universal appliance\\nand shoehorn in a ton of use cases and workloads. As data and\\nquery requirements morph, the relational database collapses under\\nits weight. At that point, you’ll want to use a database that’s\\nappropriate for the specific workload under pressure. Enter\\nnonrelational or NoSQL databases. NoSQL, which stands for not\\nonly SQL, refers to a whole class of databases that abandon the\\nrelational paradigm.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e11a0358-d282-49fd-b1f3-fe0979f9bf43', embedding=None, metadata={'page_label': '260', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the one hand, dropping relational constraints can improve\\nperformance, scalability, and schema flexibility. But as always in\\narchitecture, trade-offs exist. NoSQL databases also typically\\nabandon various RDBMS characteristics, such as strong\\nconsistency, joins, or a fixed schema.\\nA big theme of this book is that data innovation is constant. Let’s\\ntake a quick look at the history of NoSQL, as it’s helpful to gain a\\nperspective on why and how data innovations impact your work as a\\ndata engineer. In the early 2000s, tech companies such as Google\\nand Amazon began to outgrow their relational databases and\\npioneered new distributed, nonrelational databases to scale their\\nweb platforms.\\nWhile the term NoSQL first appeared in 1998, the modern version\\nwas coined by Eric Evans in the 2000s. He tells the story in a 2009\\nblog post:\\nI’ve spent the last couple of days at nosqleast and one of the hot\\ntopics here is the name “nosql.” Understandably, there are a lot of\\npeople who worry that the name is Bad, that it sends an\\ninappropriate or inaccurate message. While I make no claims to\\nthe idea, I do have to accept some blame for what it is now being\\ncalled. How’s that? Johan Oskarsson was organizing the first\\nmeetup and asked the question “What’s a good name?” on IRC; it\\nwas one of three or four suggestions that I spouted off in the span\\nof like 45 seconds, without thinking.\\nMy regret, however, isn’t about what the name says; it’s about\\nwhat it doesn’t. When Johan originally had the idea for the first\\nmeetup, he seemed to be thinking Big Data and linearly scalable\\ndistributed systems, but the name is so vague that it opened the\\ndoor to talk submissions for literally anything that stored data, and\\nwasn’t an RDBMS.\\nNoSQL remains vague in 2022, but it’s been widely adopted to\\ndescribe a universe of “new school” databases, alternatives to\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1fbe6f0c-0e6f-40f4-a0f5-1879a11672fe', embedding=None, metadata={'page_label': '261', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='relational databases.\\nThere are numerous flavors of NoSQL database designed for almost\\nany imaginable use case. Because there are far too many NoSQL\\ndatabases to cover exhaustively in this section, we consider the\\nfollowing database types: key-value, document, wide-column, graph,\\nsearch, and time series. These databases are all wildly popular and\\nenjoy widespread adoption. A data engineer should understand\\nthese types of databases, including usage considerations, the\\nstructure of the data they store, and how to leverage each in the data\\nengineering lifecycle.\\nKey-value stores\\nA key-value database is a nonrelational database that retrieves\\nrecords using a key that uniquely identifies each record. This is\\nsimilar to hash map or dictionary data structures presented in many\\nprogramming languages but potentially more scalable. Key-value\\nstores encompass several NoSQL database types—for example,\\ndocument stores and wide column databases (discussed next).\\nDifferent types of key-value databases offer a variety of performance\\ncharacteristics to serve various application needs. For example, in-\\nmemory key-value databases are popular for caching session data\\nfor web and mobile applications, where ultra-fast lookup and high\\nconcurrency are required. Storage in these systems is typically\\ntemporary; if the database shuts down, the data disappears. Such\\ncaches can reduce pressure on the main application database and\\nserve speedy responses.\\nOf course, key-value stores can also serve applications requiring\\nhigh-durability persistence. An ecommerce application may need to\\nsave and update massive amounts of event state changes for a user\\nand their orders. A user logs into the ecommerce application, clicks\\naround various screens, adds items to a shopping cart, and then\\nchecks out. Each event must be durably stored for retrieval. Key-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1dec4944-efe9-490a-b8ee-55a2bd3b1f87', embedding=None, metadata={'page_label': '262', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='value stores often persist data to disk and across multiple nodes to\\nsupport such use cases.\\nDocument stores\\nAs mentioned previously, a document store is a specialized key-\\nvalue store. In this context, a document is a nested object; we can\\nusually think of each document as a JSON object for practical\\npurposes. Documents are stored in collections and retrieved by key.\\nA collection is roughly equivalent to a table in a relational database\\n(see Table 5-2).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d05c1f0d-aea5-403f-a508-0f5d294e7610', embedding=None, metadata={'page_label': '263', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='T\\na\\nb\\nl\\ne \\n5\\n-\\n2\\n. \\nC\\no\\nm\\np\\na\\nri\\ns\\no\\nn \\no\\nf \\nR\\nD\\nB\\nM\\nS\\n \\na\\nn\\nd \\nd\\no\\nc\\nu\\nm', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='67cffa27-21f2-4ea9-a927-9ad9c60c3a05', embedding=None, metadata={'page_label': '264', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='e\\nn\\nt \\nt\\ne\\nr\\nm\\ni\\nn\\no\\nl\\no\\ng\\ny\\nRDBMS Document database\\nTable Collection\\nRow Document, items, entity\\nOne key difference between relational databases and document\\nstores is that the latter does not support joins. This means that data\\ncannot be easily normalized, i.e., split across multiple tables.\\n(Applications can still join manually. Code can look up a document,\\nextract a property, and then retrieve another document.) Ideally, all\\nrelated data can be stored in the same document.\\nIn many cases, the same data must be stored in multiple documents\\nspread across numerous collections; software engineers must be\\ncareful to update a property everywhere it is stored. (Many document\\nstores support a notion of transactions to facilitate this.)\\nDocument databases generally embrace all the flexibility of JSON\\nand don’t enforce schema or types; this is a blessing and a curse.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c67c1b2-c23c-42b7-a11e-7c4a06cb2815', embedding=None, metadata={'page_label': '265', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the one hand, this allows the schema to be highly flexible and\\nexpressive. The schema can also evolve as an application grows.\\nOn the flip side, we’ve seen document databases become absolute\\nnightmares to manage and query. If developers are not careful in\\nmanaging schema evolution, data may become inconsistent and\\nbloated over time. Schema evolution can also break downstream\\ningestion and cause headaches for data engineers if it’s not\\ncommunicated in a timely fashion (before deployment).\\nThe following is an example of data that is stored in a collection\\ncalled users. The collection key is the id. We also have a name\\n(along with first and last as child elements) and an array of the\\nperson’s favorite bands within each document:\\nusers:[\\n    {\\n     id: 1234\\n        name:\\n            first: \"Joe\"\\n            last: \"Reis\"\\n        favorite_bands: [\"AC/DC\", \"Slayer\", \"WuTang Clan\", \\n\"Action Bronson\"]\\n     },\\n    {\\n     id: 1235\\n        name:\\n            first: \"Matt\"\\n            last: \"Housley\"\\n        favorite_bands: [\"Dave Matthews Band\", \"Creed\", \\n\"Nickelback\"]\\n    }\\n]\\nTo query the data in this example, you can retrieve records by key.\\nNote that most document databases also support the creation of\\nindexes and lookup tables to allow retrieval of documents by specific\\nproperties. This is often invaluable in application development when\\nyou need to search for documents in various ways. For example, you\\ncould set an index on name.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='54cea4e0-3622-4a86-920a-dac51e0fe3cc', embedding=None, metadata={'page_label': '266', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Another critical technical detail for data engineers is that document\\nstores are generally not ACID compliant, unlike relational databases.\\nTechnical expertise in a particular document store is essential to\\nunderstanding performance, tuning, configuration, related effects on\\nwrites, consistency, durability, etc. For example, many document\\nstores are eventually consistent. Allowing data distribution across a\\ncluster is a boon for scaling and performance but can lead to\\ncatastrophes when engineers and developers don’t understand the\\nimplications.\\nTo run analytics on document stores, engineers generally must run a\\nfull scan to extract all data from a collection or employ a CDC\\nstrategy to send events to a target stream. The full scan approach\\ncan have both performance and cost implications. The scan often\\nslows the database as it runs, and many serverless cloud offerings\\ncharge a significant fee for each full scan. In document databases,\\nit’s often helpful to create an index to help speed up queries. We\\ndiscuss indexes and query patterns in Chapter 8.\\nWide-column\\nA wide-column database is optimized for storing massive amounts of\\ndata with high transaction rates and extremely low latency. These\\ndatabases can scale to extremely high write rates and vast amounts\\nof data. Specifically, wide-column databases can support petabytes\\nof data, millions of requests per second, and sub-10ms latency.\\nThese characteristics have made wide-column databases popular in\\necommerce, fintech, ad tech, IoT, and real-time personalization\\napplications. Data engineers must be aware of the operational\\ncharacteristics of the wide-column databases they work with to set\\nup a suitable configuration, design the schema, and choose an\\nappropriate row key to optimize performance and avoid common\\noperational issues.\\nThese databases support rapid scans of massive amounts of data,\\nbut they do not support complex queries. They have only a single\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='805a9d5e-bec3-4a5a-960f-8f1a6b60087b', embedding=None, metadata={'page_label': '267', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='index (the row key) for lookups. Data engineers must generally\\nextract data and send it to a secondary analytics system to run\\ncomplex queries to deal with these limitations. This can be\\naccomplished by running large scans for the extraction or employing\\nCDC to capture an event stream.\\nGraph databases\\nGraph databases explicitly store data with a mathematical graph\\nstructure (as a set of nodes and edges). Neo4j has proven\\nextremely popular, while Amazon, Oracle, and other vendors offer\\ntheir graph database products. Roughly speaking, graph databases\\nare a good fit when you want to analyze the connectivity between\\nelements.\\nFor example, you could use a document database to store one\\ndocument for each user describing their properties. You could add\\nan array element for connections that contains directly connected\\nusers’ IDs in a social media context. It’s pretty easy to determine the\\nnumber of direct connections a user has, but suppose you want to\\nknow how many users can be reached by traversing two direct\\nconnections. You could answer this question by writing complex\\ncode, but each query would run slowly and consume significant\\nresources. The document store is simply not optimized for this use\\ncase.\\nGraph databases are designed for precisely this type of query. Their\\ndata structures allow for queries based on the connectivity between\\nelements; graph databases are indicated when we care about\\nunderstanding complex traversals between elements. In the parlance\\nof graphs, we store nodes (users in the preceding example) and\\nedges (connections between users). Graph databases support rich\\ndata models for both nodes and edges. Depending on the underlying\\ngraph database engine, graph databases utilize specialized query\\nlanguages such as SPARQL, Resource Description Framework\\n(RDF), Graph Query Language (GQL), and Cypher.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='568d1a27-3e5b-4469-b3d4-ba9c1ec554be', embedding=None, metadata={'page_label': '268', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As an example of a graph, consider a network of four users. User 1\\nfollows User 2, who follows User 3 and User 4; User 3 also follows\\nUser 4 (Figure 5-8).\\nFigure 5-8. A social network graph\\nWe anticipate that graph database applications will grow dramatically\\noutside of tech companies; market analyses also predict rapid\\ngrowth. Of course, graph databases are beneficial from an\\noperational perspective and support the kinds of complex social\\nrelationships critical to modern applications. Graph structures are\\nalso fascinating from the perspective of data science and ML,\\npotentially revealing deep insights into human interactions and\\nbehavior.\\nThis introduces unique challenges for data engineers who may be\\nmore accustomed to dealing with structured relations, documents, or\\nunstructured data. Engineers must choose whether to do the\\nfollowing:\\nMap source system graph data into one of their existing\\npreferred paradigms\\nAnalyze graph data within the source system itself\\nAdopt graph-specific analytics tools\\nGraph data can be reencoded into rows in a relational database,\\nwhich may be a suitable solution depending on the analytics use\\ncase. Transactional graph databases are also designed for analytics,\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='57e24a68-7492-4849-950b-c47795dbc07a', embedding=None, metadata={'page_label': '269', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='although large queries may overload production systems.\\nContemporary cloud-based graph databases support read-heavy\\ngraph analytics on massive quantities of data.\\nSearch\\nA search database is a nonrelational database used to search your\\ndata’s complex and straightforward semantic and structural\\ncharacteristics. Two prominent use cases exist for a search\\ndatabase: text search and log analysis. Let’s cover each of these\\nseparately.\\nText search involves searching a body of text for keywords or\\nphrases, matching on exact, fuzzy, or semantically similar matches.\\nLog analysis is typically used for anomaly detection, real-time\\nmonitoring, security analytics, and operational analytics. Queries can\\nbe optimized and sped up with the use of indexes.\\nDepending on the type of company you work at, you may use search\\ndatabases either regularly or not at all. Regardless, it’s good to be\\naware they exist in case you come across them in the wild. Search\\ndatabases are popular for fast search and retrieval and can be found\\nin various applications; an ecommerce site may power its product\\nsearch using a search database. As a data engineer, you might be\\nexpected to bring data from a search database (such as\\nElasticsearch, Apache Solr or Lucene, or Algolia) into downstream\\nKPI reports or something similar.\\nTime series\\nA time series is a series of values organized by time. For example,\\nstock prices might move as trades are executed throughout the day,\\nor a weather sensor will take atmospheric temperatures every\\nminute. Any events that are recorded over time—either regularly or\\nsporadically—are time-series data. A time-series database is\\noptimized for retrieving and statistical processing of time-series data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d992648-7443-475d-a7fa-6314dcc78832', embedding=None, metadata={'page_label': '270', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='While time-series data such as orders, shipments, logs, and so forth\\nhave been stored in relational databases for ages, these data sizes\\nand volumes were often tiny. As data grew faster and bigger, new\\nspecial-purpose databases were needed. Time-series databases\\naddress the needs of growing, high-velocity data volumes from IoT,\\nevent and application logs, ad tech, and fintech, among many other\\nuse cases. Often these workloads are write-heavy. As a result, time-\\nseries databases often utilize memory buffering to support fast writes\\nand reads.\\nWe should distinguish between measurement and event-based data,\\ncommon in time-series databases. Measurement data is generated\\nregularly, such as temperature or air-quality sensors. Event-based\\ndata is irregular and created every time an event occurs—for\\ninstance, when a motion sensor detects movement.\\nThe schema for a time series typically contains a timestamp and a\\nsmall set of fields. Because the data is time-dependent, the data is\\nordered by the timestamp. This makes time-series databases\\nsuitable for operational analytics but not great for BI use cases. Joins\\nare not common, though some quasi time-series databases such as\\nApache Druid support joins. Many time-series databases are\\navailable, both as open source and paid options.\\nAPIs\\nAPIs are now a standard and pervasive way of exchanging data in\\nthe cloud, for SaaS platforms, and between internal company\\nsystems. Many types of API interfaces exist across the web, but we\\nare principally interested in those built around HTTP, the most\\npopular type on the web and in the cloud.\\nREST\\nWe’ll first talk about REST, currently the dominant API paradigm. As\\nnoted in Chapter 4, REST stands for representational state transfer.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='af546e6e-35a5-4ffc-981a-7c3c93ff697b', embedding=None, metadata={'page_label': '271', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This set of practices and philosophies for building HTTP web APIs\\nwas laid out by Roy Fielding in 2000 in a PhD dissertation. REST is\\nbuilt around HTTP verbs, such as GET and PUT; in practice, modern\\nREST uses only a handful of the verb mappings outlined in the\\noriginal dissertation.\\nOne of the principal ideas of REST is that interactions are stateless.\\nUnlike in a Linux terminal session, there is no notion of a session\\nwith associated state variables such as a working directory; each\\nREST call is independent. REST calls can change the system’s\\nstate, but these changes are global, applying to the full system rather\\nthan a current session.\\nCritics point out that REST is in no way a full specification. REST\\nstipulates basic properties of interactions, but developers utilizing an\\nAPI must gain a significant amount of domain knowledge to build\\napplications or pull data effectively.\\nWe see great variation in levels of API abstraction. In some cases,\\nAPIs are merely a thin wrapper over internals that provides the\\nminimum functionality required to protect the system from user\\nrequests. In other examples, a REST data API is a masterpiece of\\nengineering that prepares data for analytics applications and\\nsupports advanced reporting.\\nA couple of developments have simplified setting up data-ingestion\\npipelines from REST APIs. First, data providers frequently supply\\nclient libraries in various languages, especially in Python. Client\\nlibraries remove much of the boilerplate labor of building API\\ninteraction code. Client libraries handle critical details such as\\nauthentication and map fundamental methods into accessible\\nclasses.\\nSecond, various services and open source libraries have emerged to\\ninteract with APIs and manage data synchronization. Many SaaS\\nand open source vendors provide off-the-shelf connectors for\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1be3efa5-08b8-40eb-a804-65fd461940c1', embedding=None, metadata={'page_label': '272', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='common APIs. Platforms also simplify the process of building custom\\nconnectors as required.\\nThere are numerous data APIs without client libraries or out-of-the-\\nbox connector support. As we emphasize throughout the book,\\nengineers would do well to reduce undifferentiated heavy lifting by\\nusing off-the-shelf tools. However, low-level plumbing tasks still\\nconsume many resources. At virtually any large company, data\\nengineers will need to deal with the problem of writing and\\nmaintaining custom code to pull data from APIs, which requires\\nunderstanding the structure of the data as provided, developing\\nappropriate data-extraction code, and determining a suitable data\\nsynchronization strategy.\\nGraphQL\\nGraphQL was created at Facebook as a query language for\\napplication data and an alternative to generic REST APIs. Whereas\\nREST APIs generally restrict your queries to a specific data model,\\nGraphQL opens up the possibility of retrieving multiple data models\\nin a single request. This allows for more flexible and expressive\\nqueries than with REST. GraphQL is built around JSON and returns\\ndata in a shape resembling the JSON query.\\nThere’s something of a holy war between REST and GraphQL, with\\nsome engineering teams partisans of one or the other and some\\nusing both. In reality, engineers will encounter both as they interact\\nwith source systems.\\nWebhooks\\nWebhooks are a simple event-based data-transmission pattern. The\\ndata source can be an application backend, a web page, or a mobile\\napp. When specified events happen in the source system, this\\ntriggers a call to an HTTP endpoint hosted by the data consumer.\\nNotice that the connection goes from the source system to the data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1d82f00d-17d9-44bd-a3fc-1d51ede3e800', embedding=None, metadata={'page_label': '273', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='sink, the opposite of typical APIs. For this reason, webhooks are\\noften called reverse APIs.\\nThe endpoint can do various things with the POST event data,\\npotentially triggering a downstream process or storing the data for\\nfuture use. For analytics purposes, we’re interested in collecting\\nthese events. Engineers commonly use message queues to ingest\\ndata at high velocity and volume. We will talk about message queues\\nand event streams later in this chapter.\\nRPC and gRPC\\nA remote procedure call (RPC) is commonly used in distributed\\ncomputing. It allows you to run a procedure on a remote system.\\ngRPC is a remote procedure call library developed internally at\\nGoogle in 2015 and later released as an open standard. Its use at\\nGoogle alone would be enough to merit inclusion in our discussion.\\nMany Google services, such as Google Ads and GCP, offer gRPC\\nAPIs. gRPC is built around the Protocol Buffers open data\\nserialization standard, also developed by Google.\\ngRPC emphasizes the efficient bidirectional exchange of data over\\nHTTP/2. Efficiency refers to aspects such as CPU utilization, power\\nconsumption, battery life, and bandwidth. Like GraphQL, gRPC\\nimposes much more specific technical standards than REST, thus\\nallowing the use of common client libraries and allowing engineers to\\ndevelop a skill set that will apply to any gRPC interaction code.\\nData Sharing\\nThe core concept of cloud data sharing is that a multitenant system\\nsupports security policies for sharing data among tenants.\\nConcretely, any public cloud object storage system with a fine-\\ngrained permission system can be a platform for data sharing.\\nPopular cloud data-warehouse platforms also support data-sharing\\ncapabilities. Of course, data can also be shared through download or', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='54d0cffb-0057-4b8d-b358-8395c365d693', embedding=None, metadata={'page_label': '274', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='exchange over email, but a multitenant system makes the process\\nmuch easier.\\nMany modern sharing platforms (especially cloud data warehouses)\\nsupport row, column, and sensitive data filtering. Data sharing also\\nstreamlines the notion of the data marketplace, available on several\\npopular clouds and data platforms. Data marketplaces provide a\\ncentralized location for data commerce, where data providers can\\nadvertise their offerings and sell them without worrying about the\\ndetails of managing network access to data systems.\\nData sharing can also streamline data pipelines within an\\norganization. Data sharing allows units of an organization to manage\\ntheir data and selectively share it with other units while still allowing\\nindividual units to manage their compute and query costs separately,\\nfacilitating data decentralization. This facilitates decentralized data\\nmanagement patterns such as data mesh.\\nData sharing and data mesh align closely with our philosophy of\\ncommon architecture components. Choose common components\\n(Chapter 3) that allow the simple and efficient interchange of data\\nand expertise rather than embracing the most exciting and\\nsophisticated technology.\\nThird-Party Data Sources\\nThe consumerization of technology means every company is\\nessentially now a technology company. The consequence is that\\nthese companies—and increasingly government agencies—want to\\nmake their data available to their customers and users, either as part\\nof their service or as a separate subscription. For example, the\\nUnited States Bureau of Labor Statistics publishes various statistics\\nabout the US labor market. The National Aeronautics and Space\\nAdministration (NASA) publishes various data from its research\\ninitiatives. Facebook shares data with businesses that advertise on\\nits platform.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2c864110-c3d3-4463-8dd6-644b6ad6f430', embedding=None, metadata={'page_label': '275', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Why would companies want to make their data available? Data is\\nsticky, and a flywheel is created by allowing users to integrate and\\nextend their application into a user’s application. Greater user\\nadoption and usage means more data, which means users can\\nintegrate more data into their applications and data systems. The\\nside effect is there are now almost infinite sources of third-party data.\\nDirect third-party data access is commonly done via APIs, through\\ndata sharing on a cloud platform, or through data download. APIs\\noften provide deep integration capabilities, allowing customers to pull\\nand push data. For example, many CRMs offer APIs that their users\\ncan integrate into their systems and applications. We see a common\\nworkflow to get data from a CRM, blend the CRM data through the\\ncustomer scoring model, and then use reverse ETL to send that data\\nback into CRM for salespeople to contact better-qualified leads.\\nMessage Queues and Event-Streaming Platforms\\nEvent-driven architectures are pervasive in software applications and\\nare poised to grow their popularity even further. First, message\\nqueues and event-streaming platforms—critical layers in event-\\ndriven architectures—are easier to set up and manage in a cloud\\nenvironment. Second, the rise of data apps—applications that\\ndirectly integrate real-time analytics—are growing from strength to\\nstrength. Event-driven architectures are ideal in this setting because\\nevents can both trigger work in the application and feed near real-\\ntime analytics.\\nPlease note that streaming data (in this case, messages and\\nstreams) cuts across many data engineering lifecycle stages. Unlike\\nan RDBMS, which is often directly attached to an application, the\\nlines of streaming data are sometimes less clear-cut. These systems\\nare used as source systems, but they will often cut across the data\\nengineering lifecycle because of their transient nature. For example,\\nyou can use an event-streaming platform for message passing in an', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='75080bdf-d616-48b6-93ba-4a0277694442', embedding=None, metadata={'page_label': '276', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='event-driven application, a source system. The same event-\\nstreaming platform can be used in the ingestion and transformation\\nstage to process data for real-time analytics.\\nAs source systems, message queues and event-streaming platforms\\nare used in numerous ways, from routing messages between\\nmicroservices ingesting millions of events per second of event data\\nfrom web, mobile, and IoT applications. Let’s look at message\\nqueues and event-streaming platforms a bit more closely.\\nMessage queues\\nA message queue is a mechanism to asynchronously send data\\n(usually as small individual messages, in the kilobytes) between\\ndiscrete systems using a publish and subscribe model. Data is\\npublished to a message queue and is delivered to one or more\\nsubscribers (Figure 5-9). The subscriber acknowledges receipt of the\\nmessage, removing it from the queue.\\nFigure 5-9. A simple message queue\\nMessage queues allow applications and systems to be decoupled\\nfrom each other and are widely used in microservices architectures.\\nThe message queue buffers messages to handle transient load\\nspikes and makes messages durable through a distributed\\narchitecture with replication.\\nMessage queues are a critical ingredient for decoupled\\nmicroservices and event-driven architectures. Some things to keep\\nin mind with message queues are frequency of delivery, message\\nordering, and scalability.\\nMessage ordering and delivery', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='65f203a6-7c56-45cc-b908-3a364c785b64', embedding=None, metadata={'page_label': '277', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The order in which messages are created, sent, and received can\\nsignificantly impact downstream subscribers. In general, order in\\ndistributed messaging queues is a tricky problem. Message queues\\noften apply a fuzzy notion of order and first in, first out (FIFO). Strict\\nFIFO means that if message A is ingested before message B,\\nmessage A will always be delivered before message B. In practice,\\nmessages might be published and received out of order, especially\\nin highly distributed message systems.\\nFor example, Amazon SQS standard queues make the best effort to\\npreserve message order. SQS also offers FIFO queues, which offer\\nmuch stronger guarantees at the cost of extra overhead.\\nIn general, don’t assume that your messages will be delivered in\\norder unless your message queue technology guarantees it. You\\ntypically need to design for out-of-order message delivery.\\nDelivery frequency\\nMessages can be sent exactly once or at least once. If a message is\\nsent exactly once, then after the subscriber acknowledges the\\nmessage, the message disappears and won’t be delivered again.\\nMessages sent at least once can be consumed by multiple\\nsubscribers or by the same subscriber more than once. This is great\\nwhen duplications or redundancy don’t matter.\\nIdeally, systems should be idempotent. In an idempotent system, the\\noutcome of processing a message once is identical to the outcome\\nof processing it multiple times. This helps to account for a variety of\\nsubtle scenarios. For example, even if our system can guarantee\\nexactly-once delivery, a consumer might fully process a message but\\nfail right before acknowledging processing. The message will\\neffectively be processed twice, but an idempotent system handles\\nthis scenario gracefully.\\nScalability', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0882bcac-0865-4f58-a4e2-1d0a1a0982c5', embedding=None, metadata={'page_label': '278', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The most popular message queues utilized in event-driven\\napplications are horizontally scalable, running across multiple\\nservers. This allows these queues to scale up and down dynamically,\\nbuffer messages when systems fall behind, and durably store\\nmessages for resilience against failure. However, this can create a\\nvariety of complications, as mentioned previously (multiple deliveries\\nand fuzzy ordering).\\nEvent-streaming platforms\\nIn some ways, an event-streaming platform is a continuation of a\\nmessage queue in that messages are passed from producers to\\nconsumers. As discussed previously in this chapter, the big\\ndifference between messages and streams is that a message queue\\nis primarily used to route messages with certain delivery guarantees.\\nIn contrast, an event-streaming platform is used to ingest and\\nprocess data in an ordered log of records. In an event-streaming\\nplatform, data is retained for a while, and it is possible to replay\\nmessages from a past point in time.\\nLet’s describe an event related to an event-streaming platform. As\\nmentioned in Chapter 3, an event is “something that happened,\\ntypically a change in the state of something.” An event has the\\nfollowing features: a key, a value, and a timestamp. Multiple key-\\nvalue timestamps might be contained in a single event. For example,\\nan event for an ecommerce order might look like this:\\nKey: \"Order # 12345\"\\nValue: \"SKU 123, purchase price of $100\"\\nTimestamp: \"2023-01-02 06:01:00\"\\nLet’s look at some of the critical characteristics of an event-\\nstreaming platform that you should be aware of as a data engineer.\\nTopics', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='86d927e3-c83f-4489-915b-614b2883fe56', embedding=None, metadata={'page_label': '279', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In an event-streaming platform, a producer streams events to a\\ntopic, a collection of related events. A topic might contain fraud\\nalerts, customer orders, or temperature readings from IoT devices,\\nfor example. A topic can have zero, one, or multiple producers and\\ncustomers on most event-streaming platforms.\\nUsing the preceding event example, a topic might be web orders.\\nAlso, let’s send this topic to a couple of consumers, such as\\nfulfillment and marketing. This is an excellent example of\\nblurred lines between analytics and an event-driven system. The\\nfulfillment subscriber will use events to trigger a fulfillment\\nprocess, while marketing runs real-time analytics to tune\\nmarketing campaigns (Figure 5-10).\\nFigure 5-10. An order-processing system generates events (small rectangles) and\\npublishes them to the web orders topic. Two subscribers—marketing and\\nfulfillment—pull events from the topic.\\nStream partitions\\nStream partitions are subdivisions of a stream into multiple streams.\\nA good analogy is a multilane freeway. Having multiple lanes allows\\nfor parallelism and higher throughput. Messages are distributed\\nacross partitions by partition key. Messages with the same partition\\nkey will always end up in the same partition.\\nIn Figure 5-11, for example, each message has a numeric ID—\\nshown inside the circle representing the message—that we use as a\\npartition key. To determine the partition, we divide by 3 and take the\\nremainder. Going from bottom to top, the partitions have remainder\\n0, 1, and 2, respectively.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9846ac7-0fca-44c8-b8c1-d163b12c838c', embedding=None, metadata={'page_label': '280', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 5-11. An incoming message stream broken into three partitions\\nSet a partition key so that messages that should be processed\\ntogether have the same partition key. For example, it is common in\\nIoT settings to want to send all messages from a particular device to\\nthe same processing server. We can achieve this by using a device\\nID as the partition key, and then setting up one server to consume\\nfrom each partition.\\nA key concern with stream partitioning is ensuring that your partition\\nkey does not generate hotspotting—a disproportionate number of\\nmessages delivered to one partition. For example, if each IoT device\\nwere known to be located in a particular US state, we might use the\\nstate as the partition key. Given a device distribution proportional to\\nstate population, the partitions containing California, Texas, Florida,\\nand New York might be overwhelmed, with other partitions relatively\\nunderutilized. Ensure that your partition key will distribute messages\\nevenly across partitions.\\nFault tolerance and resilience\\nEvent-streaming platforms are typically distributed systems, with\\nstreams stored on various nodes. If a node goes down, another node\\nreplaces it, and the stream is still accessible. This means records\\naren’t lost; you may choose to delete records, but that’s another\\nstory. This fault tolerance and resilience make streaming platforms a\\ngood choice when you need a system that can reliably produce,\\nstore, and ingest event data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc9e89c8-bdbf-4c8f-80b6-562aba033b26', embedding=None, metadata={'page_label': '281', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whom You’ll Work With\\nWhen accessing source systems, it’s essential to understand the\\npeople with whom you’ll work. In our experience, good diplomacy\\nand relationships with the stakeholders of source systems are an\\nunderrated and crucial part of successful data engineering.\\nWho are these stakeholders? Typically, you’ll deal with two\\ncategories of stakeholders: systems and data stakeholders\\n(Figure 5-12). A systems stakeholder builds and maintains the\\nsource systems; these might be software engineers, application\\ndevelopers, and third parties. Data stakeholders own and control\\naccess to the data you want, generally handled by IT, a data\\ngovernance group, or third parties. The systems and data\\nstakeholders are often different people or teams; sometimes, they\\nare the same.\\nFigure 5-12. The data engineer’s upstream stakeholders\\nYou’re often at the mercy of the stakeholder’s ability to follow correct\\nsoftware engineering, database management, and development\\npractices. Ideally, the stakeholders are doing DevOps and working in\\nan agile manner. We suggest creating a feedback loop between data\\nengineers and stakeholders of the source systems to create\\nawareness of how data is consumed and used. This is among the\\nsingle most overlooked areas where data engineers can get a lot of\\nvalue. When something happens to the upstream source data—and\\nsomething will happen, whether it’s a schema or data change, a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2647cb1f-6ca0-4193-886c-d0efb2298c91', embedding=None, metadata={'page_label': '282', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='failed server or database, or other important events—you want to\\nmake sure that you’re made aware of the impact these issues will\\nhave on your data engineering systems.\\nIt might help to have a data contract in place with your upstream\\nsource system owners. What is a data contract? James Denmore\\noffers this definition:\\nA data contract is a written agreement between the owner of a\\nsource system and the team ingesting data from that system for\\nuse in a data pipeline. The contract should state what data is\\nbeing extracted, via what method (full, incremental), how often, as\\nwell as who (person, team) are the contacts for both the source\\nsystem and the ingestion. Data contracts should be stored in a\\nwell-known and easy-to-find location such as a GitHub repo or\\ninternal documentation site. If possible, format data contracts in a\\nstandardized form so they can be integrated into the development\\nprocess or queried programmatically.\\nIn addition, consider establishing an SLA with upstream providers.\\nAn SLA provides expectations of what you can expect from the\\nsource systems you rely upon. An example of an SLA might be “data\\nfrom source systems will be reliably available and of high quality.” A\\nservice-level objective (SLO) measures performance against what\\nyou’ve agreed to in the SLA. For example, given your example SLA,\\nan SLO might be “source systems will have 99% uptime.” If a data\\ncontract or SLA/SLO seems too formal, at least verbally set\\nexpectations for source system guarantees for uptime, data quality,\\nand anything else of importance to you. Upstream owners of source\\nsystems need to understand your requirements so they can provide\\nyou with the data you need.\\nUndercurrents and Their Impact on Source\\nSystems\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='98ec8e3a-abde-42a0-8e36-2297d2e335e9', embedding=None, metadata={'page_label': '283', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Unlike other parts of the data engineering lifecycle, source systems\\nare generally outside the control of the data engineer. There’s an\\nimplicit assumption (some might call it hope) that the stakeholders\\nand owners of the source systems—and the data they produce—are\\nfollowing best practices concerning data management, DataOps\\n(and DevOps), DODD (mentioned in Chapter 2) data architecture,\\norchestration, and software engineering. The data engineer should\\nget as much upstream support as possible to ensure that the\\nundercurrents are applied when data is generated in source\\nsystems. Doing so will make the rest of the steps in the data\\nengineering lifecycle proceed a lot more smoothly.\\nHow do the undercurrents impact source systems? Let’s have a\\nlook.\\nSecurity\\nSecurity is critical, and the last thing you want is to accidentally\\ncreate a point of vulnerability in a source system. Here are some\\nareas to consider:\\nIs the source system architected so data is secure and\\nencrypted, both with data at rest and while data is transmitted?\\nDo you have to access the source system over the public\\ninternet, or are you using a virtual private network (VPN)?\\nKeep passwords, tokens, and credentials to the source system\\nsecurely locked away. For example, if you’re using Secure Shell\\n(SSH) keys, use a key manager to protect your keys; the same\\nrule applies to passwords—use a password manager or a single\\nsign-on (SSO) provider.\\nDo you trust the source system? Always be sure to trust but\\nverify that the source system is legitimate. You don’t want to be\\non the receiving end of data from a malicious actor.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d5b3e45-2fed-4284-b0f4-ad972687c89f', embedding=None, metadata={'page_label': '284', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Management\\nData management of source systems is challenging for data\\nengineers. In most cases, you will have only peripheral control—if\\nany control at all—over source systems and the data they produce.\\nTo the extent possible, you should understand the way data is\\nmanaged in source systems since this will directly influence how you\\ningest, store, and transform the data.\\nHere are some areas to consider:\\nData governance\\nAre upstream data and systems governed in a reliable, easy-to-\\nunderstand fashion? Who manages the data?\\nData quality\\nHow do you ensure data quality and integrity in upstream\\nsystems? Work with source system teams to set expectations on\\ndata and communication.\\nSchema\\nExpect that upstream schemas will change. Where possible,\\ncollaborate with source system teams to be notified of looming\\nschema changes.\\nMaster data management\\nIs the creation of upstream records controlled by a master data\\nmanagement practice or system?\\nPrivacy and ethics', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='141384a9-a7f9-4fbe-b94e-748156a364e1', embedding=None, metadata={'page_label': '285', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Do you have access to raw data, or will the data be obfuscated?\\nWhat are the implications of the source data? How long is it\\nretained? Does it shift locations based on retention policies?\\nRegulatory\\nBased upon regulations, are you supposed to access the data?\\nDataOps\\nOperational excellence—DevOps, DataOps, MLOps, XOps—should\\nextend up and down the entire stack and support the data\\nengineering and lifecycle. While this is ideal, it’s often not fully\\nrealized.\\nBecause you’re working with stakeholders who control both the\\nsource systems and the data they produce, you need to ensure that\\nyou can observe and monitor the uptime and usage of the source\\nsystems and respond when incidents occur. For example, when the\\napplication database you depend on for CDC exceeds its I/O\\ncapacity and needs to be rescaled, how will that affect your ability to\\nreceive data from this system? Will you be able to access the data,\\nor will it be unavailable until the database is rescaled? How will this\\naffect reports? In another example, if the software engineering team\\nis continuously deploying, a code change may cause unanticipated\\nfailures in the application itself. How will the failure impact your ability\\nto access the databases powering the application? Will the data be\\nup-to-date?\\nSet up a clear communication chain between data engineering and\\nthe teams supporting the source systems. Ideally, these stakeholder\\nteams have incorporated DevOps into their workflow and culture.\\nThis will go a long way to accomplishing the goals of DataOps (a\\nsibling of DevOps), to address and reduce errors quickly. As we\\nmentioned earlier, data engineers need to weave themselves into the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='07babe8d-85b5-471b-b8de-65120979536a', embedding=None, metadata={'page_label': '286', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='DevOps practices of stakeholders, and vice versa. Successful\\nDataOps works when all people are on board and focus on making\\nsystems holistically work.\\nA few DataOps considerations are as follows:\\nAutomation\\nThere’s the automation impacting the source system, such as\\ncode updates and new features. Then there’s the DataOps\\nautomation that you’ve set up for your data workflows. Does an\\nissue in the source system’s automation impact your data\\nworkflow automation? If so, consider decoupling these systems\\nso they can perform automation independently.\\nObservability\\nHow will you know when there’s an issue with a source system,\\nsuch as an outage or a data-quality issue? Set up monitoring for\\nsource system uptime (or use the monitoring created by the team\\nthat owns the source system). Set up checks to ensure that data\\nfrom the source system conforms with expectations for\\ndownstream usage. For example, is the data of good quality? Is\\nthe schema conformant? Are customer records consistent? Is\\ndata hashed as stipulated by the internal policy?\\nIncident response\\nWhat’s your plan if something bad happens? For example, how\\nwill your data pipeline behave if a source system goes offline?\\nWhat’s your plan to backfill the “lost” data once the source\\nsystem is back online?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cc5587dd-97d0-4723-bef1-c112712157dc', embedding=None, metadata={'page_label': '287', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Architecture\\nSimilar to data management, unless you’re involved in the design\\nand maintenance of the source system architecture, you’ll have little\\nimpact on the upstream source system architecture. You should also\\nunderstand how the upstream architecture is designed, and its\\nstrengths and weaknesses. Talk often with the teams responsible for\\nthe source systems to understand the factors discussed in this\\nsection and ensure that their systems can meet your expectations.\\nKnowing where the architecture performs well and where it doesn’t\\nwill impact how you design your data pipeline.\\nHere are some things to consider regarding source system\\narchitectures:\\nReliability\\nAll systems suffer from entropy at some point, and outputs will\\ndrift from what’s expected. Bugs are introduced, and random\\nglitches happen. Does the system produce predictable outputs?\\nHow often can we expect the system to fail? What’s the mean\\ntime to repair to get the system back to sufficient reliability?\\nDurability\\nEverything fails. A server might die, a cloud’s zone or region\\ncould go offline, or other issues may arise. You need to account\\nfor how an inevitable failure or outage will affect your managed\\ndata systems. How does the source system handle data loss\\nfrom hardware failures or network outages? What’s the plan for\\nhandling outages for an extended period and limiting the blast\\nradius of an outage?\\nAvailability', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='841e502b-47b6-4053-81d6-814e4e8afabf', embedding=None, metadata={'page_label': '288', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What guarantees that the source system is up, running, and\\navailable when it’s supposed to be?\\nPeople\\nWho’s in charge of the source system’s design, and how will you\\nknow if breaking changes are made in the architecture? A data\\nengineer needs to work with the teams who maintain the source\\nsystems and ensure that these systems are architected reliably.\\nCreate an SLA with the source system team to set expectations\\nabout potential system failure.\\nOrchestration\\nWhen orchestrating within your data engineering workflow, you’ll\\nprimarily be concerned with making sure your orchestration can\\naccess the source system, which requires the correct network\\naccess, authentication, and authorization.\\nHere are some things to think about concerning orchestration for\\nsource systems:\\nCadence and frequency\\nIs the data available on a fixed schedule, or can you access new\\ndata whenever you want?\\nCommon frameworks\\nDo the software and data engineers use the same container\\nmanager, such as Kubernetes? Would it make sense to integrate\\napplication and data workloads into the same Kubernetes\\ncluster? If you’re using an orchestration framework like Airflow,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4d121075-7b64-4ebc-a41c-7ba1f881a83e', embedding=None, metadata={'page_label': '289', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='does it make sense to integrate it with the upstream application\\nteam? There’s no correct answer here, but you need to balance\\nthe benefits of integration with the risks of tight coupling.\\nSoftware Engineering\\nAs the data landscape shifts to tools that simplify and automate\\naccess to source systems, you’ll likely need to write code. Here are a\\nfew considerations when writing code to access a source system:\\nNetworking\\nMake sure your code will be able to access the network where\\nthe source system resides. Also, always think about secure\\nnetworking. Are you accessing an HTTPS URL over the public\\ninternet, SSH, or a VPN?\\nAuthentication and authorization\\nDo you have the proper credentials (tokens,\\nusername/passwords) to access the source system? Where will\\nyou store these credentials so they don’t appear in your code or\\nversion control? Do you have the correct IAM roles to perform the\\ncoded tasks?\\nAccess patterns\\nHow are you accessing the data? Are you using an API, and how\\nare you handling REST/GraphQL requests, response data\\nvolumes, and pagination? If you’re accessing data via a database\\ndriver, is the driver compatible with the database you’re', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='276feb9e-35b8-4de8-a2cb-9d147f50eb55', embedding=None, metadata={'page_label': '290', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='accessing? For either access pattern, how are things like retries\\nand timeouts handled?\\nOrchestration\\nDoes your code integrate with an orchestration framework, and\\ncan it be executed as an orchestrated workflow?\\nParallelization\\nHow are you managing and scaling parallel access to source\\nsystems?\\nDeployment\\nHow are you handling the deployment of source code changes?\\nConclusion\\nSource systems and their data are vital in the data engineering\\nlifecycle. Data engineers tend to treat source systems as “someone\\nelse’s problem”—do this at your peril! Data engineers who abuse\\nsource systems may need to look for another job when production\\ngoes down.\\nIf there’s a stick, there’s also a carrot. Better collaboration with\\nsource system teams can lead to higher-quality data, more\\nsuccessful outcomes, and better data products. Create a\\nbidirectional flow of communications with your counterparts on these\\nteams; set up processes to notify of schema and application\\nchanges that affect analytics and ML. Communicate your data needs\\nproactively to assist application teams in the data engineering\\nprocess.\\nBe aware that the integration between data engineers and source\\nsystem teams is growing. One example is reverse ETL, which has', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='864d08a7-1392-4c1b-a88b-29da549d0f3a', embedding=None, metadata={'page_label': '291', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='long lived in the shadows but has recently risen into prominence. We\\nalso discussed that the event-streaming platform could serve a role\\nin event-driven architectures and analytics; a source system can also\\nbe a data engineering system. Build shared systems where it makes\\nsense to do so.\\nLook for opportunities to build user-facing data products. Talk to\\napplication teams about analytics they would like to present to their\\nusers or places where ML could improve the user experience. Make\\napplication teams stakeholders in data engineering, and find ways to\\nshare your successes.\\nNow that you understand the types of source systems and the data\\nthey generate, we’ll next look at ways to store this data.\\nAdditional Resources\\n“The What, Why, and When of Single-Table Design with\\nDynamoDB” by Alex DeBrie\\n“Test Data Quality at Scale with Deequ” by Dustin Lange et al.\\n“Modernizing Business Data Indexing” by Benjamin Douglas\\nand Mohammad Mohtasham\\nConfluent’s “Schema Evolution and Compatibility”\\ndocumentation\\n“NoSQL: What’s in a Name” by Eric Evans\\n“The Log: What Every Software Engineer Should Know About\\nReal-Time Data’s Unifying Abstraction” by ay Kreps\\nDatabase System Concepts by Abraham (Avi) Silberschatz et\\nal. (McGraw Hill)\\nDatabase Internals by Alex Petrov (O’Reilly)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca023095-8266-4baa-b328-4c2f265c1475', embedding=None, metadata={'page_label': '292', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1  Keith D. Foote, “A Brief History of Non-Relational Databases,” Dataversity,\\nJune 19, 2018, https://oreil.ly/5Ukg2.\\n2  Nemil Dalal’s excellent series on the history of MongoDB recounts some\\nharrowing tales of database abuse and its consequences for fledgling\\nstartups.\\n3  Martin Kleppmann, Designing Data-Intensive Applications (Sebastopol, CA:\\nO’Reilly, 2017), 49, https://oreil.ly/v1NhG.\\n4  Aashish Mehra, “Graph Database Market Worth $5.1 Billion by 2026:\\nExclusive Report by MarketsandMarkets,” Cision PR Newswire, July 30,\\n2021, https://oreil.ly/mGVkY.\\n5  For one example, see Michael S. Mikowski, “RESTful APIs: The Big Lie,”\\nAugust 10, 2015, https://oreil.ly/rqja3.\\n6  Martin Fowler, “How to Move Beyond a Monolithic Data Lake to a\\nDistributed Data Mesh,” MartinFowler.com, May 20, 2019,\\nhttps://oreil.ly/TEdJF.\\n7  Read Data Pipelines Pocket Reference (O’Reilly) for more information on\\nhow a data contract should be written.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e404cf94-74ff-4048-b690-43d997e11e41', embedding=None, metadata={'page_label': '293', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 6. Storage\\nStorage is the cornerstone of the data engineering lifecycle\\n(Figure 6-1) and underlies its major stages—ingestion,\\ntransformation, and serving. Data gets stored many times as it\\nmoves through the lifecycle. To paraphrase an old saying, it’s\\nstorage all the way down. Whether data is needed seconds, minutes,\\ndays, months, or years later, it must persist in storage until systems\\nare ready to consume it for further processing and transmission.\\nKnowing the use case of the data and the way you will retrieve it in\\nthe future is the first step to choosing the proper storage solutions for\\nyour data architecture.\\nFigure 6-1. Storage plays a central role in the data engineering lifecycle\\nWe also discussed storage in Chapter 5, but with a difference in\\nfocus and domain of control. Source systems are generally not\\nmaintained or controlled by data engineers. The storage that data\\nengineers handle directly, which we’ll focus on in this chapter,\\nencompasses the data engineering lifecycle stages of ingesting data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='643e73af-3991-42b5-8785-8dda6e9e4236', embedding=None, metadata={'page_label': '294', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='from source systems to serving data to deliver value with analytics,\\ndata science, etc. Many forms of storage undercut the entire data\\nengineering lifecycle in some fashion.\\nTo understand storage, we’re going to start by studying the raw\\ningredients that compose storage systems, including hard drives,\\nsolid state drives, and system memory (see Figure 6-2). It’s essential\\nto understand the basic characteristics of physical storage\\ntechnologies to assess the trade-offs inherent in any storage\\narchitecture. This section also discusses serialization and\\ncompression, key software elements of practical storage. (We defer\\na deeper technical discussion of serialization and compression to\\nAppendix A.) We also discuss caching, which is critical in assembling\\nstorage systems.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e369ad04-e16a-4497-9610-a985a67e72ab', embedding=None, metadata={'page_label': '295', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 6-2. Raw ingredients, storage systems, and storage abstractions', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4accfae-c11e-4839-a470-dde131d4b415', embedding=None, metadata={'page_label': '296', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Next, we’ll look at storage systems. In practice, we don’t directly\\naccess system memory or hard disks. These physical storage\\ncomponents exist inside servers and clusters that can ingest and\\nretrieve data using various access paradigms.\\nFinally, we’ll look at storage abstractions. Storage systems are\\nassembled into a cloud data warehouse, a data lake, etc. When\\nbuilding data pipelines, engineers choose the appropriate\\nabstractions for storing their data as it moves through the ingestion,\\ntransformation, and serving stages.\\nRaw Ingredients of Data Storage\\nStorage is so common that it’s easy to take it for granted. We’re\\noften surprised by the number of software and data engineers who\\nuse storage every day but have little idea how it works behind the\\nscenes or the trade-offs inherent in various storage media. As a\\nresult, we see storage used in some pretty...interesting ways.\\nThough current managed services potentially free data engineers\\nfrom the complexities of managing servers, data engineers still need\\nto be aware of underlying components’ essential characteristics,\\nperformance considerations, durability, and costs.\\nIn most data architectures, data frequently passes through magnetic\\nstorage, SSDs, and memory as it works its way through the various\\nprocessing phases of a data pipeline. Data storage and query\\nsystems generally follow complex recipes involving distributed\\nsystems, numerous services, and multiple hardware storage layers.\\nThese systems require the right raw ingredients to function correctly.\\nLet’s look at some of the raw ingredients of data storage: disk drives,\\nmemory, networking and CPU, serialization, compression, and\\ncaching.\\nMagnetic Disk Drive', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92d83d4f-bce6-4b45-b074-b6095ee63cd7', embedding=None, metadata={'page_label': '297', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Magnetic disks utilize spinning platters coated with a ferromagnetic\\nfilm (Figure 6-3). This film is magnetized by a read/write head during\\nwrite operations to physically encode binary data. The read/write\\nhead detects the magnetic field and outputs a bitstream during read\\noperations. Magnetic disk drives have been around for ages. Still,\\nthey form the backbone of bulk data storage systems because they\\nare significantly cheaper than SSDs per gigabyte of stored data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3550e56f-0394-45d0-aeb1-ff934f1ce34b', embedding=None, metadata={'page_label': '298', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 6-3. Magnetic disk head movement and rotation are essential in random\\naccess latency', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3d703713-a14a-4748-9ef3-d15245e3e754', embedding=None, metadata={'page_label': '299', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the one hand, these disks have seen extraordinary\\nimprovements in performance, storage density, and cost. On the\\nother hand, SSDs dramatically outperform magnetic disks on various\\nmetrics. Currently, commercial magnetic disk drives cost roughly 3\\ncents per gigabyte of capacity. (Note that we’ll frequently use the\\nabbreviations HDD and SSD to denote rotating magnetic disk and\\nsolid-state drives, respectively.)\\nIBM developed magnetic disk drive technology in the 1950s. Since\\nthen, magnetic disk capacities have grown steadily. The first\\ncommercial magnetic disk drive, the IBM 350, had a capacity of 3.75\\nmegabytes. As of this writing, magnetic drives storing 20 TB are\\ncommercially available. In fact, magnetic disks continue to see rapid\\ninnovation, with methods such as heat-assisted magnetic recording\\n(HAMR), shingled magnetic recording (SMR), and helium-filled disk\\nenclosures being used to realize ever greater storage densities. In\\nspite of the continuing improvements in drive capacity, other aspects\\nof HDD performance are hampered by physics.\\nFirst, disk transfer speed, the rate at which data can be read and\\nwritten, does not scale in proportion with disk capacity. Disk capacity\\nscales with areal density (gigabits stored per square inch), whereas\\ntransfer speed scales with linear density (bits per inch). This means\\nthat if disk capacity grows by a factor of 4, transfer speed increases\\nby only a factor of 2. Consequently, current data center drives\\nsupport maximum data transfer speeds of 200–-300 MB/s. To frame\\nthis another way, it takes more than 20 hours to read the entire\\ncontents of a 30 TB magnetic drive, assuming a transfer speed of\\n300 MB/s.\\nA second major limitation is seek time. To access data, the drive\\nmust physically relocate the read/write heads to the appropriate track\\non the disk. Third, in order to find a particular piece of data on the\\ndisk, the disk controller must wait for that data to rotate under the\\nread/write heads. This leads to rotational latency. Typical commercial\\ndrives spinning at 7,200 revolutions per minute (RPM) seek time,\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca4a21e1-6683-4e0f-a74e-9ad644814cd7', embedding=None, metadata={'page_label': '300', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and rotational latency, leads to over four milliseconds of overall\\naverage latency (time to access a selected piece of data). A fourth\\nlimitation is input/output operations per second (IOPS), critical for\\ntransactional databases. A magnetic drive ranges from 50 to 500\\nIOPS.\\nVarious tricks can improve latency and transfer speed. Using a\\nhigher rotational speed can increase transfer rate and decrease\\nrotational latency. Limiting the radius of the disk platter or writing\\ndata into only a narrow band on the disk reduces seek time.\\nHowever, none of these techniques makes magnetic drives remotely\\ncompetitive with SSDs for random access lookups. SSDs can deliver\\ndata with significantly lower latency, higher IOPS, and higher transfer\\nspeeds, partially because there is no physically rotating disk or\\nmagnetic head to wait for.\\nAs mentioned earlier, magnetic disks are still prized in data centers\\nfor their low data-storage costs. In addition, magnetic drives can\\nsustain extraordinarily high transfer rates through parallelism. This is\\nthe critical idea behind cloud object storage: data can be distributed\\nacross thousands of disks in clusters. Data-transfer rates go up\\ndramatically by reading from numerous disks simultaneously, limited\\nprimarily by network performance rather than disk transfer rate.\\nThus, network components and CPUs are also key raw ingredients\\nin storage systems, and we will return to these topics shortly.\\nSolid-State Drive\\nSolid-state drives (SSDs) store data as charges in flash memory\\ncells. SSDs eliminate the mechanical components of magnetic\\ndrives; the data is read by purely electronic means. SSDs can look\\nup random data in less than 0.1 ms (100 microseconds). In addition,\\nSSDs can scale both data-transfer speeds and IOPS by slicing\\nstorage into partitions with numerous storage controllers running in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fff8bd70-bd7b-44f4-8b3d-14c35abe040e', embedding=None, metadata={'page_label': '301', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='parallel. Commercial SSDs can support transfer speeds of many\\ngigabytes per second and tens of thousands of IOPS.\\nBecause of these exceptional performance characteristics, SSDs\\nhave revolutionized transactional databases and are the accepted\\nstandard for commercial deployments of OLTP systems. SSDs allow\\nrelational databases such as PostgreSQL, MySQL, and SQL Server\\nto handle thousands of transactions per second.\\nHowever, SSDs are not currently the default option for high-scale\\nanalytics data storage. Again, this comes down to cost. Commercial\\nSSDs typically cost 20–30 cents (USD) per gigabyte of capacity,\\nnearly 10 times the cost per capacity of a magnetic drive. Thus,\\nobject storage on magnetic disks has emerged as the leading option\\nfor large-scale data storage in data lakes and cloud data\\nwarehouses.\\nSSDs still play a significant role in OLAP systems. Some OLAP\\ndatabases leverage SSD caching to support high-performance\\nqueries on frequently accessed data. As low-latency OLAP becomes\\nmore popular, we expect SSD usage in these systems to follow suit.\\nRandom Access Memory\\nWe commonly use the terms random access memory (RAM) and\\nmemory interchangeably. Strictly speaking, magnetic drives and\\nSSDs also serve as memory that stores data for later random access\\nretrieval, but RAM has several specific characteristics:\\nIs attached to a CPU and mapped into CPU address space.\\nStores the code that CPUs execute and the data that this code\\ndirectly processes.\\nIs volatile, while magnetic drives and SSDs are nonvolatile.\\nThough they may occasionally fail and corrupt or lose data,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d6413b6-c04f-4161-b80d-6f189fc5201d', embedding=None, metadata={'page_label': '302', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='drives generally retain data when powered off. RAM loses data\\nin less than a second when it is unpowered.\\nOffers significantly higher transfer speeds and faster retrieval\\ntimes than SSD storage. DDR5 memory—the latest widely used\\nstandard for RAM—offers data retrieval latency on the order of\\n100 ns, roughly 1,000 times faster than SSD. A typical CPU can\\nsupport 100 GB/s bandwidth to attached memory and millions of\\nIOPS. (Statistics vary dramatically depending on the number of\\nmemory channels and other configuration details.)\\nIs significantly more expensive than SSD storage, at roughly\\n$10/GB (at the time of this writing).\\nIs limited in the amount of RAM attached to an individual CPU\\nand memory controller. This adds further to complexity and cost.\\nHigh-memory servers typically utilize many interconnected\\nCPUs on one board, each with a block of attached RAM.\\nIs still significantly slower than CPU cache, a type of memory\\nlocated directly on the CPU die or in the same package. Cache\\nstores frequently and recently accessed data for ultrafast\\nretrieval during processing. CPU designs incorporate several\\nlayers of cache of varying size and performance characteristics.\\nWhen we talk about system memory, we almost always mean\\ndynamic RAM, a high-density, low-cost form of memory. Dynamic\\nRAM stores data as charges in capacitors. These capacitors leak\\nover time, so the data must be frequently refreshed (read and\\nrewritten) to prevent data loss. The hardware memory controller\\nhandles these technical details; data engineers simply need to worry\\nabout bandwidth and retrieval latency characteristics. Other forms of\\nmemory, such as static RAM, are used in specialized applications\\nsuch as CPU caches.\\nCurrent CPUs virtually always employ the von Neumann\\narchitecture, with code and data stored together in the same memory', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='77b20bda-91b1-4e24-bcb6-2bf0f541a1be', embedding=None, metadata={'page_label': '303', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='space. However, CPUs typically also support the option to disable\\ncode execution in specific pages of memory for enhanced security.\\nThis feature is reminiscent of the Harvard architecture, which\\nseparates code and data.\\nRAM is used in various storage and processing systems and can be\\nused for caching, data processing, or indexes. Several databases\\ntreat RAM as a primary storage layer, allowing ultra-fast read and\\nwrite performance. In these applications, data engineers must\\nalways keep in mind the volatility of RAM. Even if data stored in\\nmemory is replicated across a cluster, a power outage that brings\\ndown several nodes could cause data loss. Architectures intended to\\ndurably store data may use battery backups and automatically dump\\nall data to disk in the event of power loss.\\nNetworking and CPU\\nWhy are we mentioning networking and CPU as raw ingredients for\\nstoring data? Increasingly, storage systems are distributed to\\nenhance performance, durability, and availability. We mentioned\\nspecifically that individual magnetic disks offer relatively low-transfer\\nperformance, but a cluster of disks parallelizes reads for significant\\nperformance scaling. While storage standards such as redundant\\narrays of independent disks (RAID) parallelize on a single server,\\ncloud object storage clusters operate at a much larger scale, with\\ndisks distributed across a network and even multiple data centers\\nand availability zones.\\nAvailability zones are a standard cloud construct consisting of\\ncompute environments with independent power, water, and other\\nresources. Multizonal storage enhances both the availability and\\ndurability of data.\\nCPUs handle the details of servicing requests, aggregating reads,\\nand distributing writes. Storage becomes a web application with an\\nAPI, backend service components, and load balancing. Network', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3deea22-6212-411b-b575-98dddadcb984', embedding=None, metadata={'page_label': '304', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='device performance and network topology are key factors in realizing\\nhigh performance.\\nData engineers need to understand how networking will affect the\\nsystems they build and use. Engineers constantly balance the\\ndurability and availability achieved by spreading out data\\ngeographically versus the performance and cost benefits of keeping\\nstorage in a small geographic area and close to data consumers or\\nwriters. Appendix B covers cloud networking and major relevant\\nideas.\\nSerialization\\nSerialization is another raw storage ingredient and a critical element\\nof database design. The decisions around serialization will inform\\nhow well queries perform across a network, CPU overhead, query\\nlatency, and more. Designing a data lake, for example, involves\\nchoosing a base storage system (e.g., Amazon S3) and standards\\nfor serialization that balance interoperability with performance\\nconsiderations.\\nWhat is serialization, exactly? Data stored in system memory by\\nsoftware is generally not in a format suitable for storage on disk or\\ntransmission over a network. Serialization is the process of flattening\\nand packing data into a standard format that a reader will be able to\\ndecode. Serialization formats provide a standard of data exchange.\\nWe might encode data in a row-based manner as an XML, JSON, or\\nCSV file and pass it to another user who can then decode it using a\\nstandard library. A serialization algorithm has logic for handling\\ntypes, imposes rules on data structure, and allows exchange\\nbetween programming languages and CPUs. The serialization\\nalgorithm also has rules for handling exceptions. For instance,\\nPython objects can contain cyclic references; the serialization\\nalgorithm might throw an error or limit nesting depth on encountering\\na cycle.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='437d7f07-2275-4d9e-9b3e-2b90dffb5810', embedding=None, metadata={'page_label': '305', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Low-level database storage is also a form of serialization. Row-\\noriented relational databases organize data as rows on disk to\\nsupport speedy lookups and in-place updates. Columnar databases\\norganize data into column files to optimize for highly efficient\\ncompression and support fast scans of large data volumes. Each\\nserialization choice comes with a set of trade-offs, and data\\nengineers tune these choices to optimize performance to\\nrequirements.\\nWe provide a more detailed catalog of common data serialization\\ntechniques and formats in Appendix A. We suggest that data\\nengineers become familiar with common serialization practices and\\nformats, especially the most popular current formats (e.g., Apache\\nParquet), hybrid serialization (e.g., Apache Hudi), and in-memory\\nserialization (e.g., Apache Arrow).\\nCompression\\nCompression is another critical component of storage engineering.\\nOn a basic level, compression makes data smaller, but compression\\nalgorithms interact with other details of storage systems in complex\\nways.\\nHighly efficient compression has three main advantages in storage\\nsystems. First, the data is smaller and thus takes up less space on\\nthe disk. Second, compression increases the practical scan speed\\nper disk. With a 10:1 compression ratio, we go from scanning 200\\nMB/s per magnetic disk to an effective rate of 2 GB/s per disk.\\nThe third advantage is in network performance. Given that a network\\nconnection between an Amazon EC2 instance and S3 provides 10\\ngigabits per second (Gbps) of bandwidth, a 10:1 compression ratio\\nincreases effective network bandwidth to 100 Gbps.\\nCompression also comes with disadvantages. Compressing and\\ndecompressing data entails extra time and resource consumption to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e30fb849-5df3-4708-97f4-f0cfb99b1218', embedding=None, metadata={'page_label': '306', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='read or write data. We undertake a more detailed discussion of\\ncompression algorithms and trade-offs in Appendix A.\\nCaching\\nWe’ve already mentioned caching in our discussion of RAM. The\\ncore idea of caching is to store frequently or recently accessed data\\nin a fast access layer. The faster the cache, the higher the cost and\\nthe less storage space available. Less frequently accessed data is\\nstored in cheaper, slower storage. Caches are critical for data\\nserving, processing, and transformation.\\nAs we analyze storage systems, it is helpful to put every type of\\nstorage we utilize inside a cache hierarchy (Table 6-1). Most\\npractical data systems rely on many cache layers assembled from\\nstorage with varying performance characteristics. This starts inside\\nCPUs; processors may deploy up to four cache tiers. We move down\\nthe hierarchy to RAM and SSDs. Cloud object storage is a lower tier\\nthat supports long-term data retention and durability while allowing\\nfor data serving and dynamic data movement in pipelines.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c4628206-f0be-4e01-aa9c-cb9c08a637b9', embedding=None, metadata={'page_label': '307', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='T\\na\\nb\\nl\\ne \\n6\\n-\\n1\\n. \\nA\\n \\nh\\ne\\nu\\nri\\ns\\nti\\nc \\nc\\na\\nc\\nh\\ne \\nh\\ni\\ne\\nr\\na\\nr\\nc\\nh\\ny \\nd\\ni', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9cb6f6e6-792f-4f8f-9f8a-15bb3456d03c', embedding=None, metadata={'page_label': '308', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='s\\np\\nl\\na\\ny\\ni\\nn\\ng \\ns\\nt\\no\\nr\\na\\ng\\ne \\nt\\ny\\np\\ne\\ns \\nw\\nit\\nh \\na\\np\\np\\nr\\no\\nx\\ni\\nm\\na\\nt\\ne \\np\\nri', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='922c0675-b4c7-4808-b35c-c0a237b630fb', embedding=None, metadata={'page_label': '309', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='c\\ni\\nn\\ng \\na\\nn\\nd \\np\\ne\\nrf\\no\\nr\\nm\\na\\nn\\nc\\ne \\nc\\nh\\na\\nr\\na\\nc\\nt\\ne\\nri\\ns\\nti\\nc\\ns\\nStorage type Data fetch latency Bandwidth Price\\nCPU cache 1 nanosecond 1 TB/s\\nRAM 0.1 100 GB/s $10/GB\\na', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3dcc42f-45a1-4e08-acb3-a08f8f315f45', embedding=None, metadata={'page_label': '310', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='microseconds\\nSSD 0.1 milliseconds 4 GB/s $0.20/GB\\nHDD 4 milliseconds 300 MB/s $0.03/GB\\nObject storage 100 \\nmilliseconds\\n3 GB/s per instance $0.02/GB per \\nmonth\\nArchival storage 12 hours Same as object storage \\nonce data is available\\n$0.004/GB per \\nmonth\\na  A microsecond is 1,000 nanoseconds, and a millisecond is 1,000 microseconds.\\nWe can think of archival storage as a reverse cache. Archival\\nstorage provides inferior access characteristics for low costs.\\nArchival storage is generally used for data backups and to meet\\ndata-retention compliance requirements. In typical scenarios, this\\ndata will be accessed only in an emergency (e.g., data in a database\\nmight be lost and need to be recovered, or a company might need to\\nlook back at historical data for legal discovery).\\nData Storage Systems\\nThis section covers the major data storage systems you’ll encounter\\nas a data engineer. Storage systems exist at a level of abstraction\\nabove raw ingredients. For example, magnetic disks are a raw\\nstorage ingredient, while major cloud object storage platforms and\\nHDFS are storage systems that utilize magnetic disks. Still higher\\nlevels of storage abstraction exist, such as data lakes and\\nlakehouses (which we cover in “Data Engineering Storage\\nAbstractions”).\\nSingle Machine Versus Distributed Storage\\nAs data storage and access patterns become more complex and\\noutgrow the usefulness of a single server, distributing data to more\\nthan one server becomes necessary. Data can be stored on multiple', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='861dfb76-0be2-4927-b26f-948d8f683fb6', embedding=None, metadata={'page_label': '311', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='servers, known as distributed storage. This is a distributed system\\nwhose purpose is to store data in a distributed fashion (Figure 6-4).\\nFigure 6-4. Single machine versus distributed storage on multiple servers\\nDistributed storage coordinates the activities of multiple servers to\\nstore, retrieve, and process data faster and at a larger scale, all\\nwhile providing redundancy in case a server becomes unavailable.\\nDistributed storage is common in architectures where you want built-\\nin redundancy and scalability for large amounts of data. For\\nexample, object storage, Apache Spark, and cloud data warehouses\\nrely on distributed storage architectures.\\nData engineers must always be aware of the consistency paradigms\\nof the distributed systems, which we’ll explore next.\\nEventual Versus Strong Consistency\\nA challenge with distributed systems is that your data is spread\\nacross multiple servers. How does this system keep the data\\nconsistent? Unfortunately, distributed systems pose a dilemma for\\nstorage and query accuracy. It takes time to replicate changes\\nacross the nodes of a system; often a balance exists between\\ngetting current data and getting “sorta” current data in a distributed\\ndatabase. Let’s look at two common consistency patterns in\\ndistributed systems: eventual and strong.\\nWe’ve covered ACID compliance throughout this book, starting in\\nChapter 5. Another acronym is BASE, which stands for basically\\navailable, soft-state, eventual consistency. Think of it as the opposite\\nof ACID. BASE is the basis of eventual consistency. Let’s briefly\\nexplore its components:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c10c9075-9370-4038-867e-a3225bfd4243', embedding=None, metadata={'page_label': '312', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Basically available\\nConsistency is not guaranteed, but efforts at database reads and\\nwrites are made on a best-effort basis, meaning consistent data\\nis available most of the time.\\nSoft-state\\nThe state of the transaction is fuzzy, and it’s uncertain whether\\nthe transaction is committed or uncommitted.\\nEventual consistency\\nAt some point, reading data will return consistent values.\\nIf reading data in an eventually consistent system is unreliable, why\\nuse it? Eventual consistency is a common trade-off in large-scale,\\ndistributed systems. If you want to scale horizontally (across multiple\\nnodes) to process data in high volumes, then eventually, consistency\\nis often the price you’ll pay. Eventual consistency allows you to\\nretrieve data quickly without verifying that you have the latest version\\nacross all nodes.\\nThe opposite of eventual consistency is strong consistency. With\\nstrong consistency, the distributed database ensures that writes to\\nany node are first distributed with a consensus and that any reads\\nagainst the database return consistent values. You’ll use strong\\nconsistency when you can tolerate higher query latency and require\\ncorrect data every time you read from the database.\\nGenerally, data engineers make decisions about consistency in three\\nplaces. First, the database technology itself sets the stage for a\\ncertain level of consistency. Second, configuration parameters for\\nthe database will have an impact on consistency. Third, databases\\noften support some consistency configuration at an individual query', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='94f96f89-4cf1-4804-8921-80228e0a880f', embedding=None, metadata={'page_label': '313', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='level. For example, DynamoDB supports eventually consistent reads\\nand strongly consistent reads. Strongly consistent reads are slower\\nand consume more resources, so it is best to use them sparingly, but\\nthey are available when consistency is required.\\nYou should understand how your database handles consistency.\\nAgain, data engineers are tasked with understanding technology\\ndeeply and using it to solve problems appropriately. A data engineer\\nmight need to negotiate consistency requirements with other\\ntechnical and business stakeholders. Note that this is both a\\ntechnology and organizational problem; ensure that you have\\ngathered requirements from your stakeholders and choose your\\ntechnologies appropriately.\\nFile Storage\\nWe deal with files every day, but the notion of a file is somewhat\\nsubtle. A file is a data entity with specific read, write, and reference\\ncharacteristics used by software and operating systems. We define a\\nfile to have the following characteristics:\\nFinite length\\nA file is a finite-length stream of bytes.\\nAppend operations\\nWe can append bytes to the file up to the limits of the host\\nstorage system.\\nRandom access\\nWe can read from any location in the file or write updates to any\\nlocation.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d20ef940-5da8-4754-b81f-062d3ddeba24', embedding=None, metadata={'page_label': '314', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Object storage behaves much like file storage but with key\\ndifferences. While we set the stage for object storage by discussing\\nfile storage first, object storage is arguably much more important for\\nthe type of data engineering you’ll do today. We will forward-\\nreference the object storage discussion extensively over the next few\\npages.\\nFile storage systems organize files into a directory tree. The\\ndirectory reference for a file might look like this:\\n/Users/matthewhousley/output.txt\\nWhen I provide this to the operating system, it starts at the root\\ndirectory /, finds Users, matthewhousley, and finally\\noutput.txt. Working from the left, each directory is contained\\ninside a parent directory, until we finally reach the file output.txt.\\nThis example uses Unix semantics, but Windows file reference\\nsemantics are similar. The filesystem stores each directory as\\nmetadata about the files and directories that it contains. This\\nmetadata consists of the name of each entity, relevant permission\\ndetails, and a pointer to the actual entity. To find a file on disk, the\\noperating system looks at the metadata at each hierarchy level and\\nfollows the pointer to the next subdirectory entity until finally reaching\\nthe file itself.\\nNote that other file-like data entities generally don’t necessarily have\\nall these properties. For example, objects in object storage support\\nonly the first characteristic, finite length, but are still extremely useful.\\nWe discuss this in “Object Storage”.\\nIn cases where file storage paradigms are necessary for a pipeline,\\nbe careful with state and try to use ephemeral environments as\\nmuch as possible. Even if you must process files on a server with an\\nattached disk, use object storage for intermediate storage between\\nprocessing steps. Try to reserve manual, low-level file processing for', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='46b13ec3-dfd7-4126-ba38-0bfdf7c102e1', embedding=None, metadata={'page_label': '315', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='one-time ingestion steps or the exploratory stages of pipeline\\ndevelopment.\\nLocal disk storage\\nThe most familiar type of file storage is an operating system–\\nmanaged filesystem on a local disk partition of SSD or magnetic\\ndisk. New Technology File System (NTFS) and ext4 are popular\\nfilesystems on Windows and Linux, respectively. The operating\\nsystem handles the details of storing directory entities, files, and\\nmetadata. Filesystems are designed to write data to allow for easy\\nrecovery in the event of power loss during a write, though any\\nunwritten data will still be lost.\\nLocal filesystems generally support full read after write consistency;\\nreading immediately after a write will return the written data.\\nOperating systems also employ various locking strategies to manage\\nconcurrent writing attempts to a file.\\nLocal disk filesystems may also support advanced features such as\\njournaling, snapshots, redundancy, the extension of the filesystem\\nacross multiple disks, full disk encryption, and compression. In\\n“Block Storage”, we also discuss RAID.\\nNetwork-attached storage\\nNetwork-attached storage (NAS) systems provide a file storage\\nsystem to clients over a network. NAS is a prevalent solution for\\nservers; they quite often ship with built-in dedicated NAS interface\\nhardware. While there are performance penalties to accessing the\\nfilesystem over a network, significant advantages to storage\\nvirtualization also exist, including redundancy and reliability, fine-\\ngrained control of resources, storage pooling across multiple disks\\nfor large virtual volumes, and file sharing across multiple machines.\\nEngineers should be aware of the consistency model provided by\\ntheir NAS solution, especially when multiple clients will potentially\\naccess the same data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d8513bc9-5610-4c2e-a988-b7179cbad725', embedding=None, metadata={'page_label': '316', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A popular alternative to NAS is a storage area network (SAN), but\\nSAN systems provide block-level access without the filesystem\\nabstraction. We cover SAN systems in “Block Storage”.\\nCloud file system services\\nCloud filesystem services provide a fully managed filesystem for use\\nwith multiple cloud VMs and applications, potentially including clients\\noutside the cloud environment. Cloud filesystems should not be\\nconfused with standard storage attached to VMs—generally, block\\nstorage with a filesystem managed by the VM operating system.\\nCloud filesystems behave much like NAS solutions, but the details of\\nnetworking, managing disk clusters, failures, and configuration are\\nfully handled by the cloud vendor.\\nFor example, Amazon Elastic File System (EFS) is an extremely\\npopular example of a cloud filesystem service. Storage is exposed\\nthrough the NFS 4 protocol, which is also used by NAS systems.\\nEFS provides automatic scaling and pay-per-storage pricing with no\\nadvanced storage reservation required. The service also provides\\nlocal read-after-write consistency (when reading from the machine\\nthat performed the write). It also offers open-after-close consistency\\nacross the full filesystem. In other words, once an application closes\\na file, subsequent readers will see changes saved to the closed file.\\nBlock Storage\\nFundamentally, block storage is the type of raw storage provided by\\nSSDs and magnetic disks. In the cloud, virtualized block storage is\\nthe standard for VMs. These block storage abstractions allow fine\\ncontrol of storage size, scalability, and data durability beyond that\\noffered by raw disks.\\nIn our earlier discussion of SSDs and magnetic disks, we mentioned\\nthat with these random-access devices, the operating system can\\nseek, read, and write any data on the disk. A block is the smallest', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0fce89d8-057d-48b7-b07a-41f2daa01c2f', embedding=None, metadata={'page_label': '317', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='addressable unit of data supported by a disk. This was often 512\\nbytes of usable data on older disks, but it has now grown to 4,096\\nbytes for most current disks, making writes less fine-grained but\\ndramatically reducing the overhead of managing blocks. Blocks\\ntypically contain extra bits for error detection/correction and other\\nmetadata.\\nBlocks on magnetic disks are geometrically arranged on a physical\\nplatter. Two blocks on the same track can be read without moving\\nthe head, while reading two blocks on separate tracks requires a\\nseek. Seek time can occur between blocks on an SSD, but this is\\ninfinitesimal compared to the seek time for magnetic disk tracks.\\nBlock storage applications\\nTransactional database systems generally access disks at a block\\nlevel to lay out data for optimal performance. For row-oriented\\ndatabases, this originally meant that rows of data were written as\\ncontinuous streams; the situation has grown more complicated with\\nthe arrival of SSDs and their associated seek-time performance\\nimprovements, but transactional databases still rely on the high\\nrandom access performance offered by direct access to a block\\nstorage device.\\nBlock storage also remains the default option for operating system\\nboot disks on cloud VMs. The block device is formatted much as it\\nwould be directly on a physical disk, but the storage is usually\\nvirtualized. (see “Cloud virtualized block storage”.)\\nRAID\\nRAID stands for redundant array of independent disks, as noted\\npreviously. RAID simultaneously controls multiple disks to improve\\ndata durability, enhance performance, and combine capacity from\\nmultiple drives. An array can appear to the operating system as a\\nsingle block device. Many encoding and parity schemes are\\navailable, depending on the desired balance between enhanced', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='53625ba7-89b1-42d0-becb-cbe5ca5e2160', embedding=None, metadata={'page_label': '318', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='effective bandwidth and higher fault tolerance (tolerance for many\\ndisk failures).\\nStorage area network\\nStorage area network (SAN) systems provide virtualized block\\nstorage devices over a network, typically from a storage pool. SAN\\nabstraction can allow fine-grained storage scaling and enhance\\nperformance, availability, and durability. You might encounter SAN\\nsystems if you’re working with on-premises storage systems; you\\nmight also encounter a cloud version of SAN, as in the next\\nsubsection.\\nCloud virtualized block storage\\nCloud virtualized block storage solutions are similar to SAN but free\\nengineers from dealing with SAN clusters and networking details.\\nWe’ll look at Amazon Elastic Block Store (EBS) as a standard\\nexample; other public clouds have similar offerings. EBS is the\\ndefault storage for Amazon EC2 virtual machines; other cloud\\nproviders also treat virtualized object storage as a key component of\\ntheir VM offerings.\\nEBS offers several tiers of service with different performance\\ncharacteristics. Generally, EBS performance metrics are given in\\nIOPS and throughput (transfer speed). The higher performance tiers\\nof EBS storage are backed by SSD disks, while magnetic disk-\\nbacked storage offers lower IOPS but costs less per gigabyte.\\nEBS volumes store data separate from the instance host server but\\nin the same zone to support high performance and low latency\\n(Figure 6-5). This allows EBS volumes to persist when an EC2\\ninstance shuts down, when a host server fails, or even when the\\ninstance is deleted. EBS storage is suitable for applications such as\\ndatabases, where data durability is a high priority. In addition, EBS\\nreplicates all data to at least two separate host machines, protecting\\ndata if a disk fails.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0728896e-4e05-406a-845b-5456dff72e6c', embedding=None, metadata={'page_label': '319', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 6-5. EBS volumes replicate data to multiple hosts and disks for high\\ndurability and availability, but are not resilient to the failure of an availability zone\\nEBS storage virtualization also supports several advanced features.\\nFor example, EBS volumes allow instantaneous point-in-time\\nsnapshots while the drive is used. Although it still takes some time\\nfor the snapshot to be replicated to S3, EBS can effectively freeze\\nthe state of data blocks when the snapshot is taken, while allowing\\nthe client machine to continue using the disk. In addition, snapshots\\nafter the initial full backup are differential; only changed blocks are\\nwritten to S3 to minimize storage costs and backup time.\\nEBS volumes are also highly scalable. At the time of this writing,\\nsome EBS volume classes can scale up to 64 TiB, 256,000 IOPS,\\nand 4,000 MiB/s.\\nLocal instance volumes\\nCloud providers also offer block storage volumes that are physically\\nattached to the host server running a virtual machine. These storage\\nvolumes are generally very low cost (included with the price of the\\nVM in the case of Amazon’s EC2 instance store) and provide low\\nlatency and high IOPS.\\nInstance store volumes (Figure 6-6) behave essentially like a disk\\nphysically attached to a server in a data center. One key difference is\\nthat when a VM shuts down or is deleted, the contents of the locally\\nattached disk are lost, whether or not this event was caused by\\nintentional user action. This ensures that a new virtual machine\\ncannot read disk contents belonging to a different customer.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='98c5bce5-d8d5-4bb7-9494-5b5df1614a93', embedding=None, metadata={'page_label': '320', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 6-6. Instance store volumes offer high performance and low cost but do not\\nprotect data in the event of disk failure or VM shutdown\\nLocally attached disks support none of the advanced virtualization\\nfeatures offered by virtualized storage services like EBS. The locally\\nattached disk is not replicated, so a physical disk failure can lose or\\ncorrupt data even if the host VM continues running. Furthermore,\\nlocally attached volumes do not support snapshots or other backup\\nfeatures.\\nDespite these limitations, locally attached disks are extremely useful.\\nIn many cases, we use disks as a local cache and hence don’t need\\nall the advanced virtualization features of a service like EBS. For\\nexample, suppose we’re running AWS EMR on EC2 instances. We\\nmay be running an ephemeral job that consumes data from S3,\\nstores it temporarily in the distributed filesystem running across the\\ninstances, processes the data, and writes the results back to S3. The\\nEMR filesystem builds in replication and redundancy and is serving\\nas a cache rather than permanent storage. The EC2 instance store\\nis a perfectly suitable solution in this case and can enhance', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='476f8656-5c12-4778-acc4-5609e8925f29', embedding=None, metadata={'page_label': '321', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='performance since data can be read and processed locally without\\nflowing over a network (see Figure 6-7).\\nFigure 6-7. Instance store volumes can be used as a processing cache in an\\nephemeral Hadoop cluster\\nWe recommend that engineers think about locally attached storage\\nin worst-case scenarios. What are the consequences of a local disk\\nfailure? Of an accidental VM or cluster shutdown? Of a zonal or\\nregional cloud outage? If none of these scenarios will have\\ncatastrophic consequences when data on locally attached volumes\\nis lost, local storage may be a cost-effective and performant option.\\nIn addition, simple mitigation strategies (periodic checkpoint backups\\nto S3) can prevent data loss.\\nObject Storage\\nObject storage contains objects of all shapes and sizes (Figure 6-8).\\nThe term object storage is somewhat confusing because object has\\nseveral meanings in computer science. In this context, we’re talking\\nabout a specialized file-like construct. It could be any type of file—\\nTXT, CSV, JSON, images, videos, or audio.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='06f8f2c0-9fb7-4866-be4e-219a70852412', embedding=None, metadata={'page_label': '322', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 6-8. Object storage contains immutable objects of all shapes and sizes.\\nUnlike files on a local disk, objects cannot be modified in place.\\nObject stores have grown in importance and popularity with the rise\\nof big data and the cloud. Amazon S3, Azure Blob Storage, and\\nGoogle Cloud Storage (GCS) are widely used object stores. In\\naddition, many cloud data warehouses (and a growing number of\\ndatabases) utilize object storage as their storage layer, and cloud\\ndata lakes generally sit on object stores.\\nAlthough many on-premises object storage systems can be installed\\non server clusters, we’ll focus mostly on fully managed cloud object\\nstores. From an operational perspective, one of the most attractive\\ncharacteristics of cloud object storage is that it is straightforward to\\nmanage and use. Object storage was arguably one of the first', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6114fa61-a1b9-46c1-8d8b-3f0a3af39fb0', embedding=None, metadata={'page_label': '323', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“serverless” services; engineers don’t need to consider the\\ncharacteristics of underlying server clusters or disks.\\nAn object store is a key-value store for immutable data objects. We\\nlose much of the writing flexibility we expect with file storage on a\\nlocal disk in an object store. Objects don’t support random writes or\\nappend operations; instead, they are written once as a stream of\\nbytes. After this initial write, objects become immutable. To change\\ndata in an object or append data to it, we must rewrite the full object.\\nObject stores generally support random reads through range\\nrequests, but these lookups may perform much worse than random\\nreads from data stored on an SSD.\\nFor a software developer used to leveraging local random access file\\nstorage, the characteristics of objects might seem like constraints,\\nbut less is more; object stores don’t need to support locks or change\\nsynchronization, allowing data storage across massive disk clusters.\\nObject stores support extremely performant parallel stream writes\\nand reads across many disks, and this parallelism is hidden from\\nengineers, who can simply deal with the stream rather than\\ncommunicating with individual disks. In a cloud environment, write\\nspeed scales with the number of streams being written up to quota\\nlimits set by the vendor. Read bandwidth can scale with the number\\nof parallel requests, the number of virtual machines employed to\\nread data, and the number of CPU cores. These characteristics\\nmake object storage ideal for serving high-volume web traffic or\\ndelivering data to highly parallel distributed query engines.\\nTypical cloud object stores save data in several availability zones,\\ndramatically reducing the odds that storage will go fully offline or be\\nlost in an unrecoverable way. This durability and availability are built\\ninto the cost; cloud storage vendors offer other storage classes at\\ndiscounted prices in exchange for reduced durability or availability.\\nWe’ll discuss this in “Storage classes and tiers”.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc4b852f-b7b1-46c3-a638-cac013a8f9c8', embedding=None, metadata={'page_label': '324', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cloud object storage is a key ingredient in separating compute and\\nstorage, allowing engineers to process data with ephemeral clusters\\nand scale these clusters up and down on demand. This is a key\\nfactor in making big data available to smaller organizations that can’t\\nafford to own hardware for data jobs that they’ll run only occasionally.\\nSome major tech companies will continue to run permanent Hadoop\\nclusters on their hardware. Still, the general trend is that most\\norganizations will move data processing to the cloud, using an object\\nstore as essential storage and serving layer while processing data\\non ephemeral clusters.\\nIn object storage, available storage space is also highly scalable, an\\nideal characteristic for big data systems. Storage space is\\nconstrained by the number of disks the storage provider owns, but\\nthese providers handle exabytes of data. In a cloud environment,\\navailable storage space is virtually limitless; in practice, the primary\\nlimit on storage space for public cloud customers is budget. From a\\npractical standpoint, engineers can quickly store massive quantities\\nof data for projects without planning months in advance for\\nnecessary servers and disks.\\nObject stores for data engineering applications\\nFrom the standpoint of data engineering, object stores provide\\nexcellent performance for large batch reads and batch writes. This\\ncorresponds well to the use case for massive OLAP systems. A bit of\\ndata engineering folklore says that object stores are not good for\\nupdates, but this is only partially true. Object stores are an inferior fit\\nfor transactional workloads with many small updates every second;\\nthese use cases are much better served by transactional databases\\nor block storage systems. Object stores work well for a low rate of\\nupdate operations, where each operation updates a large volume of\\ndata.\\nObject stores are now the gold standard of storage for data lakes. In\\nthe early days of data lakes, write once, read many (WORM) was the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='40d89ce4-8db5-4a48-baad-62962e564ff8', embedding=None, metadata={'page_label': '325', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='operational standard, but this had more to do with the complexities of\\nmanaging data versions and files than the limitations of HDFS and\\nobject stores. Since then, systems such as Apache Hudi and Delta\\nLake have emerged to manage this complexity, and privacy\\nregulations such as GDPR and CCPA have made deletion and\\nupdate capabilities imperative. Update management for object\\nstorage is the central idea behind the data lakehouse concept, which\\nwe introduced in Chapter 3.\\nObject storage is an ideal repository for unstructured data in any\\nformat beyond these structured data applications. Object storage\\ncan house any binary data with no constraints on type or structure\\nand frequently plays a role in ML pipelines for raw text, images,\\nvideo, and audio.\\nObject lookup\\nAs we mentioned, object stores are key-value stores. What does this\\nmean for engineers? It’s critical to understand that, unlike file stores,\\nobject stores do not utilize a directory tree to find objects. The object\\nstore uses a top-level logical container (a bucket in S3 and GCS)\\nand references objects by key. A simple example in S3 might look\\nlike this:\\nS3://oreilly-data-engineering-book/data-example.json\\nIn this case, S3://oreilly-data-engineering-book/ is the\\nbucket name, and data-example.json is the key pointing to a\\nparticular object. S3 bucket names must be unique across all of\\nAWS. Keys are unique within a bucket. Although cloud object stores\\nmay appear to support directory tree semantics, no true directory\\nhierarchy exists. We might store an object with the following full path:\\nS3://oreilly-data-engineering-book/project-\\ndata/11/23/2021/data.txt', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='352d574e-8db4-46db-a753-e655639d14be', embedding=None, metadata={'page_label': '326', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the surface, this looks like subdirectories you might find in a\\nregular file folder system: project-data, 11, 23, and 2021. Many\\ncloud console interfaces allow users to view the objects inside a\\n“directory,” and cloud command-line tools often support Unix-style\\ncommands such as ls inside an object store directory. However,\\nbehind the scenes, the object system does not traverse a directory\\ntree to reach the object. Instead, it simply sees a key (project-\\ndata/11/23/2021/data.txt) that happens to match directory\\nsemantics. This might seem like a minor technical detail, but\\nengineers need to understand that certain “directory”-level\\noperations are costly in an object store. To run aws ls\\nS3://oreilly-data-engineering-book/project-\\ndata/11/ the object store must filter keys on the key prefix\\nproject-data/11/. If the bucket contains millions of objects, this\\noperation might take some time, even if the “subdirectory” houses\\nonly a few objects.\\nObject consistency and versioning\\nAs mentioned, object stores don’t support in-place updates or\\nappends as a general rule. We write a new object under the same\\nkey to update an object. When data engineers utilize updates in data\\nprocesses, they must be aware of the consistency model for the\\nobject store they’re using. Object stores may be eventually\\nconsistent or strongly consistent. For example, until recently, S3 was\\neventually consistent; after a new version of an object was written\\nunder the same key, the object store might sometimes return the old\\nversion of the object. The eventual part of eventual consistency\\nmeans that after enough time has passed, the storage cluster\\nreaches a state such that only the latest version of the object will be\\nreturned. This contrasts with the strong consistency model we\\nexpect of local disks attached to a server: reading after a write will\\nreturn the most recently written data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a0be1f9-eb42-4358-af96-391ade86117b', embedding=None, metadata={'page_label': '327', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='It might be desirable to impose strong consistency on an object store\\nfor various reasons, and standard methods are used to achieve this.\\nOne approach is to add a strongly consistent database (e.g.,\\nPostgreSQL) to the mix. Writing an object is now a two-step process:\\n1. Write the object.\\n2. Write the returned metadata for the object version to the\\nstrongly consistent database.\\nThe version metadata (an object hash or an object timestamp) can\\nuniquely identify an object version in conjunction with the object key.\\nTo read an object, a reader undertakes the following steps:\\n1. Fetch the latest object metadata from the strongly consistent\\ndatabase.\\n2. Query object metadata using the object key. Read the object\\ndata if it matches the metadata fetched from the consistent\\ndatabase.\\n3. If the object metadata does not match, repeat step 2 until the\\nlatest version of the object is returned.\\nA practical implementation has exceptions and edge cases to\\nconsider, such as when the object gets rewritten during this querying\\nprocess. These steps can be managed behind an API so that an\\nobject reader sees a strongly consistent object store at the cost of\\nhigher latency for object access.\\nObject versioning is closely related to object consistency. When we\\nrewrite an object under an existing key in an object store, we’re\\nessentially writing a brand-new object, setting references from the\\nexisting key to the object, and deleting the old object references.\\nUpdating all references across the cluster takes time, hence the\\npotential for stale reads. Eventually, the storage cluster garbage\\ncollector deallocates the space dedicated to the dereferenced data,\\nrecycling disk capacity for use by new objects.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='69217d15-4208-49aa-92ab-b631edd11113', embedding=None, metadata={'page_label': '328', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='With object versioning turned on, we add additional metadata to the\\nobject that stipulates a version. While the default key reference gets\\nupdated to point to the new object, we retain other pointers to\\nprevious versions. We also maintain a version list so that clients can\\nget a list of all object versions, and then pull a specific version.\\nBecause old versions of the object are still referenced, they aren’t\\ncleaned up by the garbage collector.\\nIf we reference an object with a version, the consistency issue with\\nsome object storage systems disappears: the key and version\\nmetadata together form a unique reference to a particular, immutable\\ndata object. We will always get the same object back when we use\\nthis pair, provided that we haven’t deleted it. The consistency issue\\nstill exists when a client requests the “default” or “latest” version of\\nan object.\\nThe principal overhead that engineers need to consider with object\\nversioning is the cost of storage. Historical versions of objects\\ngenerally have the same associated storage costs as current\\nversions. Object version costs may be nearly insignificant or\\ncatastrophically expensive, depending on various factors. The data\\nsize is an issue, as is update frequency; more object versions can\\nlead to significantly larger data size. Keep in mind that we’re talking\\nabout brute-force object versioning. Object storage systems\\ngenerally store full object data for each version, not differential\\nsnapshots.\\nEngineers also have the option of deploying storage lifecycle\\npolicies. Lifecycle policies allow automatic deletion of old object\\nversions when certain conditions are met (e.g., when an object\\nversion reaches a certain age or many newer versions exist). Cloud\\nvendors also offer various archival data tiers at heavily discounted\\nprices, and the archival process can be managed using lifecycle\\npolicies.\\nStorage classes and tiers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae7c70b0-4037-4da5-b91a-af143b8b968a', embedding=None, metadata={'page_label': '329', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cloud vendors now offer storage classes that discount data storage\\npricing in exchange for reduced access or reduced durability. We use\\nthe term reduced access here because many of these storage tiers\\nstill make data highly available, but with high retrieval costs in\\nexchange for reduced storage costs.\\nLet’s look at a couple of examples in S3 since Amazon is a\\nbenchmark for cloud service standards. The S3 Standard-Infrequent\\nAccess storage class discounts monthly storage costs for increased\\ndata retrieval costs. (See “A Brief Detour on Cloud Economics” for a\\ntheoretical discussion of the economics of cloud storage tiers.)\\nAmazon also offers the Amazon S3 One Zone-Infrequent Access\\ntier, replicating only to a single zone. Projected availability drops\\nfrom 99.9% to 99.5% to account for the possibility of a zonal outage.\\nAmazon still claims extremely high data durability, with the caveat\\nthat data will be lost if an availability zone is destroyed.\\nFurther down the tiers of reduced access are the archival tiers in S3\\nGlacier. S3 Glacier promises a dramatic reduction in long-term\\nstorage costs for much higher access costs. Users have various\\nretrieval speed options, from minutes to hours, with higher retrieval\\ncosts for faster access. For example, at the time of this writing, S3\\nGlacier Deep Archive discounts storage costs even further; Amazon\\nadvertises that storage costs start at $1 per terabyte per month. In\\nexchange, data restoration takes 12 hours. In addition, this storage\\nclass is designed for data that will be stored from 7–10 years and be\\naccessed only one to two times per year.\\nBe aware of how you plan to utilize archival storage, as it’s easy to\\nget into and often costly to access data, especially if you need it\\nmore often than expected. See Chapter 4 for a more extensive\\ndiscussion of archival storage economics.\\nObject store–backed filesystems\\nObject store synchronization solutions have become increasingly\\npopular. Tools like s3fs and Amazon S3 File Gateway allow users to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='df52cf1d-29cd-4c5f-897b-80fc214f5aeb', embedding=None, metadata={'page_label': '330', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='mount an S3 bucket as local storage. Users of these tools should be\\naware of the characteristics of writes to the filesystem and how these\\nwill interact with the characteristics and pricing of object storage. File\\nGateway, for example, handles changes to files fairly efficiently by\\ncombining portions of objects into a new object using the advanced\\ncapabilities of S3. However, high-speed transactional writing will\\noverwhelm the update capabilities of an object store. Mounting\\nobject storage as a local filesystem works well for files that are\\nupdated infrequently.\\nCache and Memory-Based Storage Systems\\nAs discussed in “Raw Ingredients of Data Storage”, RAM offers\\nexcellent latency and transfer speeds. However, traditional RAM is\\nextremely vulnerable to data loss because a power outage lasting\\neven a second can erase data. RAM-based storage systems are\\ngenerally focused on caching applications, presenting data for quick\\naccess and high bandwidth. Data should generally be written to a\\nmore durable medium for retention purposes.\\nThese ultra-fast cache systems are useful when data engineers\\nneed to serve data with ultra-fast retrieval latency.\\nExample: Memcached and lightweight object caching\\nMemcached is a key-value store designed for caching database\\nquery results, API call responses, and more. Memcached uses\\nsimple data structures, supporting either string or integer types.\\nMemcached can deliver results with very low latency while also\\ntaking the load off backend systems.\\nExample: Redis, memory caching with optional persistence\\nLike Memcached, Redis is a key-value store, but it supports\\nsomewhat more complex data types (such as lists or sets) Redis\\nalso builds in multiple persistence mechanisms, including\\nsnapshotting and journaling. With a typical configuration, Redis', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eb70408e-4b90-47a9-8b3c-056aa2efcd3f', embedding=None, metadata={'page_label': '331', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='writes data roughly every two seconds. Redis is thus suitable for\\nextremely high-performance applications but can tolerate a small\\namount of data loss.\\nThe Hadoop Distributed File System\\nThe Hadoop Distributed File System is based on Google File System\\n(GFS) and was initially engineered to process data with the\\nMapReduce programming model. Hadoop is similar to object\\nstorage, but with a key difference: Hadoop combines compute and\\nstorage on the same nodes, where object stores typically have\\nlimited support for internal processing.\\nHadoop breaks large files into blocks, chunks of data less than a few\\nhundred megabytes in size. The filesystem is managed by the\\nNameNode, which maintains directories, file metadata, and a\\ndetailed catalog describing the location of file blocks in the cluster. In\\na typical configuration, each block of data is replicated to three\\nnodes. This increases both the durability and availability of data. If a\\ndisk or node fails, the replication factor for some file blocks will fall\\nbelow 3. The NameNode will instruct other nodes to replicate these\\nfile blocks so that they again reach the correct replication factor.\\nThus, the probability of losing data is very low, barring a correlated\\nfailure (e.g., an asteroid hitting the data center).\\nHadoop is not simply a storage system. Hadoop combines compute\\nresources with storage nodes to allow in-place data processing. This\\nwas originally achieved using the MapReduce programming model,\\nwhich we discuss in Chapter 8.\\nHadoop is dead. Long live Hadoop!\\nWe often see claims that Hadoop is dead. This is only partially true.\\nHadoop is no longer a hot, bleeding-edge technology. Many Hadoop\\necosystem tools such as Apache Pig are now on life support and\\nprimarily used to run legacy jobs. The pure MapReduce', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c844839-0fb8-4d47-a2f8-51c8e8b4ff5a', embedding=None, metadata={'page_label': '332', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='programming model has fallen by the wayside. HDFS remains widely\\nused in various applications and organizations.\\nHadoop still appears in many legacy installations. Many\\norganizations that adopted Hadoop during the peak of the big data\\ncraze have no immediate plans to migrate to newer technologies.\\nThis is a good choice for companies that run massive (thousand-\\nnode) Hadoop clusters and have the resources to maintain on-\\npremises systems effectively. Smaller companies may want to\\nreconsider the cost overhead and scale limitations of running a small\\nHadoop cluster against migrating to cloud solutions.\\nIn addition, HDFS is a key ingredient of many current big data\\nengines, such as Amazon EMR. In fact, Apache Spark is still\\ncommonly run on HDFS clusters. We discuss this in more detail in\\n“Separation of Compute from Storage”.\\nStreaming Storage\\nStreaming data has different storage requirements than\\nnonstreaming data. In the case of message queues, stored data is\\ntemporal and expected to disappear after a certain duration.\\nHowever, distributed, scalable streaming frameworks like Apache\\nKafka now allow extremely long-duration streaming data retention.\\nKafka supports indefinite data retention by pushing old, infrequently\\naccessed messages down to object storage. Kafka competitors\\n(including Amazon Kinesis, Apache Pulsar, and Google Cloud\\nPub/Sub) also support long data retention\\nClosely related to data retention in these systems is the notion of\\nreplay. Replay allows a streaming system to return a range of\\nhistorical stored data. Replay is the standard data-retrieval\\nmechanism for streaming storage systems. Replay can be used to\\nrun batch queries over a time range or to reprocess data in a\\nstreaming pipeline. Chapter 7 covers replay in more depth.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea423384-095e-435e-808a-d22400fc3673', embedding=None, metadata={'page_label': '333', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Other storage engines have emerged for real-time analytics\\napplications. In some sense, transactional databases emerged as\\nthe first real-time query engines; data becomes visible to queries as\\nsoon as it is written. However, these databases have well-known\\nscaling and locking limitations, especially for analytics queries that\\nrun across large volumes of data. While scalable versions of row-\\noriented transactional databases have overcome some of these\\nlimitations, they are still not truly optimized for analytics at scale.\\nIndexes, Partitioning, and Clustering\\nIndexes provide a map of the table for particular fields and allow\\nextremely fast lookup of individual records. Without indexes, a\\ndatabase would need to scan an entire table to find the records\\nsatisfying a WHERE condition.\\nIn most RDBMSs, indexes are used for primary table keys (allowing\\nunique identification of rows) and foreign keys (allowing joins with\\nother tables). Indexes can also be applied to other columns to serve\\nthe needs of specific applications. Using indexes, an RDBMS can\\nlook up and update thousands of rows per second.\\nWe do not cover transactional database records in depth in this\\nbook; numerous technical resources are available on this topic.\\nRather, we are interested in the evolution away from indexes in\\nanalytics-oriented storage systems and some new developments in\\nindexes for analytics use cases.\\nThe evolution from rows to columns\\nAn early data warehouse was typically built on the same type of\\nRDBMS used for transactional applications. The growing popularity\\nof MPP systems meant a shift toward parallel processing for\\nsignificant improvements in scan performance across large\\nquantities of data for analytics purposes. However, these row-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b5e563a-7420-4905-b384-1810fda3010b', embedding=None, metadata={'page_label': '334', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='oriented MPPs still used indexes to support joins and condition\\nchecking.\\nIn “Raw Ingredients of Data Storage”, we discuss columnar\\nserialization. Columnar serialization allows a database to scan only\\nthe columns required for a particular query, sometimes dramatically\\nreducing the amount of data read from the disk. In addition,\\narranging data by column packs similar values next to each other,\\nyielding high-compression ratios with minimal compression\\noverhead. This allows data to be scanned more quickly from disk\\nand over a network.\\nColumnar databases perform poorly for transactional use cases—\\ni.e., when we try to look up large numbers of individual rows\\nasynchronously. However, they perform extremely well when large\\nquantities of data must be scanned—e.g., for complex data\\ntransformations, aggregations, statistical calculations, or evaluation\\nof complex conditions on large datasets.\\nIn the past, columnar databases performed poorly on joins, so the\\nadvice for data engineers was to denormalize data, using wide\\nschemas, arrays, and nested data wherever possible. Join\\nperformance for columnar databases has improved dramatically in\\nrecent years, so while there can still be performance advantages in\\ndenormalization, this is no longer a necessity. You’ll learn more\\nabout normalization and denormalization in Chapter 8.\\nFrom indexes to partitions and clustering\\nWhile columnar databases allow for fast scan speeds, it’s still helpful\\nto reduce the amount of data scanned as much as possible. In\\naddition to scanning only data in columns relevant to a query, we can\\npartition a table into multiple subtables by splitting it on a field. It is\\nquite common in analytics and data science use cases to scan over\\na time range, so date- and time-based partitioning is extremely\\ncommon. Columnar databases generally support a variety of other\\npartition schemes as well.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f80b1bf6-d694-48d3-9264-6c93057c6c20', embedding=None, metadata={'page_label': '335', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Clusters allow finer-grained organization of data within partitions. A\\nclustering scheme applied within a columnar database sorts data by\\none or a few fields, colocating similar values. This improves\\nperformance for filtering, sorting, and joining these values.\\nExample: Snowflake micro-partitioning\\nWe mention Snowflake micro-partitioning because it’s a good\\nexample of recent developments and evolution in approaches to\\ncolumnar storage. Micro partitions are sets of rows between 50 and\\n500 megabytes in uncompressed size. Snowflake uses an\\nalgorithmic approach that attempts to cluster together similar rows.\\nThis contrasts the traditional naive approach to partitioning on a\\nsingle designated field, such as a date. Snowflake specifically looks\\nfor values that are repeated in a field across many rows. This allows\\naggressive pruning of queries based on predicates. For example, a\\nWHERE clause might stipulate the following:\\nWHERE created_date='2017-01-02'\\nIn such a query, Snowflake excludes any micro-partitions that don’t\\ninclude this date, effectively pruning this data. Snowflake also allows\\noverlapping micro-partitions, potentially partitioning on multiple fields\\nshowing significant repeats.\\nEfficient pruning is facilitated by Snowflake’s metadata database,\\nwhich stores a description of each micro-partition, including the\\nnumber of rows and value ranges for fields. At each query stage,\\nSnowflake analyzes micro-partitions to determine which ones need\\nto be scanned. Snowflake uses the term hybrid columnar storage,\\npartially referring to the fact that its tables are broken into small\\ngroups of rows, even though storage is fundamentally columnar. The\\nmetadata database plays a role similar to an index in a traditional\\nrelational database.\\n2\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5bd168be-46e9-4e02-8509-b6b37babe731', embedding=None, metadata={'page_label': '336', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Engineering Storage Abstractions\\nData engineering storage abstractions are data organization and\\nquery patterns that sit at the heart of the data engineering lifecycle\\nand are built atop the data storage systems discussed previously\\n(see Figure 6-3). We introduced many of these abstractions in\\nChapter 3, and we will revisit them here.\\nThe main types of abstractions we’ll concern ourselves with are\\nthose that support data science, analytics, and reporting use cases.\\nThese include data warehouse, data lake, data lakehouse, data\\nplatforms, and data catalogs. We won’t cover source systems, as\\nthey are discussed in Chapter 5.\\nThe storage abstraction you require as a data engineer boils down to\\na few key considerations:\\nPurpose and use case\\nYou must first identify the purpose of storing the data. What is it\\nused for?\\nUpdate patterns\\nIs the abstraction optimized for bulk updates, streaming inserts,\\nor upserts?\\nCost\\nWhat are the direct and indirect financial costs? The time to\\nvalue? The opportunity costs?\\nSeparate storage and compute\\nThe trend is toward separating storage and compute, but most\\nsystems hybridize separation and colocation. We cover this in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cdd52130-eb5c-42dc-ac78-e8de60215674', embedding=None, metadata={'page_label': '337', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“Separation of Compute from Storage” since it affects purpose,\\nspeed, and cost.\\nYou should know that the popularity of separating storage from\\ncompute means the lines between OLAP databases and data lakes\\nare increasingly blurring. Major cloud data warehouses and data\\nlakes are on a collision course. In the future, the differences between\\nthese two may be in name only since they might functionally and\\ntechnically be very similar under the hood.\\nThe Data Warehouse\\nData warehouses are a standard OLAP data architecture. As\\ndiscussed in Chapter 3, the term data warehouse refers to\\ntechnology platforms (e.g., Google BigQuery and Teradata), an\\narchitecture for data centralization, and an organizational pattern\\nwithin a company. In terms of storage trends, we’ve evolved from\\nbuilding data warehouses atop conventional transactional\\ndatabases, row-based MPP systems (e.g., Teradata and IBM\\nNetezza), and columnar MPP systems (e.g., Vertica and Teradata\\nColumnar) to cloud data warehouses and data platforms. (See our\\ndata warehousing discussion in Chapter 3 for more details on MPP\\nsystems.)\\nIn practice, cloud data warehouses are often used to organize data\\ninto a data lake, a storage area for massive amounts of unprocessed\\nraw data, as originally conceived by James Dixon. Cloud data\\nwarehouses can handle massive amounts of raw text and complex\\nJSON documents. The limitation is that cloud data warehouses\\ncannot handle truly unstructured data, such as images, video, or\\naudio, unlike a true data lake. Cloud data warehouses can be\\ncoupled with object storage to provide a complete data-lake solution.\\nThe Data Lake\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='090a9966-ad52-43fc-9677-880336a3eca9', embedding=None, metadata={'page_label': '338', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The data lake was originally conceived as a massive store where\\ndata was retained in raw, unprocessed form. Initially, data lakes were\\nbuilt primarily on Hadoop systems, where cheap storage allowed for\\nretention of massive amounts of data without the cost overhead of a\\nproprietary MPP system.\\nThe last five years have seen two major developments in the\\nevolution of data lake storage. First, a major migration toward\\nseparation of compute and storage has occurred. In practice, this\\nmeans a move away from Hadoop toward cloud object storage for\\nlong-term retention of data. Second, data engineers discovered that\\nmuch of the functionality offered by MPP systems (schema\\nmanagement; update, merge and delete capabilities) and initially\\ndismissed in the rush to data lakes was, in fact, extremely useful.\\nThis led to the notion of the data lakehouse.\\nThe Data Lakehouse\\nThe data lakehouse is an architecture that combines aspects of the\\ndata warehouse and the data lake. As it is generally conceived, the\\nlakehouse stores data in object storage just like a lake. However, the\\nlakehouse adds to this arrangement features designed to streamline\\ndata management and create an engineering experience similar to a\\ndata warehouse. This means robust table and schema support and\\nfeatures for managing incremental updates and deletes. Lakehouses\\ntypically also support table history and rollback; this is accomplished\\nby retaining old versions of files and metadata.\\nA lakehouse system is a metadata and file-management layer\\ndeployed with data management and transformation tools.\\nDatabricks has heavily promoted the lakehouse concept with Delta\\nLake, an open source storage management system.\\nWe would be remiss not to point out that the architecture of the data\\nlakehouse is similar to the architecture used by various commercial\\ndata platforms, including BigQuery and Snowflake. These systems', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d68bae71-e4c3-4339-8abf-6f7f8e6a03d2', embedding=None, metadata={'page_label': '339', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='store data in object storage and provide automated metadata\\nmanagement, table history, and update/delete capabilities. The\\ncomplexities of managing underlying files and storage are fully\\nhidden from the user.\\nThe key advantage of the data lakehouse over proprietary tools is\\ninteroperability. It’s much easier to exchange data between tools\\nwhen stored in an open file format. Reserializing data from a\\nproprietary database format incurs overhead in processing, time, and\\ncost. In a data lakehouse architecture, various tools can connect to\\nthe metadata layer and read data directly from object storage.\\nIt is important to emphasize that much of the data in a data\\nlakehouse may not have a table structure imposed. We can impose\\ndata warehouse features where we need them in a lakehouse,\\nleaving other data in a raw or even unstructured format.\\nThe data lakehouse technology is evolving rapidly. A variety of new\\ncompetitors to Delta Lake have emerged, including Apache Hudi and\\nApache Iceberg. See Appendix A for more details.\\nData Platforms\\nIncreasingly, vendors are styling their products as data platforms.\\nThese vendors have created their ecosystems of interoperable tools\\nwith tight integration into the core data storage layer. In evaluating\\nplatforms, engineers must ensure that the tools offered meet their\\nneeds. Tools not directly provided in the platform can still\\ninteroperate, with extra data overhead for data interchange.\\nPlatforms also emphasize close integration with object storage for\\nunstructured use cases, as mentioned in our discussion of cloud\\ndata warehouses.\\nAt this point, the notion of the data platform frankly has yet to be fully\\nfleshed out. However, the race is on to create a walled garden of\\ndata tools, both simplifying the work of data engineering and\\ngenerating significant vendor lock-in.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='08e0137d-c139-48c3-836e-65f3134d57cc', embedding=None, metadata={'page_label': '340', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Stream-to-Batch Storage Architecture\\nThe stream-to-batch storage architecture has many similarities to the\\nLambda architecture, though some might quibble over the technical\\ndetails. Essentially, data flowing through a topic in the streaming\\nstorage system is written out to multiple consumers.\\nSome of these consumers might be real-time processing systems\\nthat generate statistics on the stream. In addition, a batch storage\\nconsumer writes data for long-term retention and batch queries. The\\nbatch consumer could be AWS Kinesis Firehose, which can\\ngenerate S3 objects based on configurable triggers (e.g., time and\\nbatch size). Systems such as BigQuery ingest streaming data into a\\nstreaming buffer. This streaming buffer is automatically reserialized\\ninto columnar object storage. The query engine supports seamless\\nquerying of both the streaming buffer and the object data to provide\\nusers a current, nearly real-time view of the table.\\nBig Ideas and Trends in Storage\\nIn this section, we’ll discuss some big ideas in storage—key\\nconsiderations that you need to keep in mind as you build out your\\nstorage architecture. Many of these considerations are part of larger\\ntrends. For example, data catalogs fit under the trend toward\\n“enterprisey” data engineering and data management. Separation of\\ncompute from storage is now largely an accomplished fact in cloud\\ndata systems. And data sharing is an increasingly important\\nconsideration as businesses adopt data technology.\\nData Catalog\\nA data catalog is a centralized metadata store for all data across an\\norganization. Strictly speaking, a data catalog is not a top-level data\\nstorage abstraction, but it integrates with various systems and\\nabstractions. Data catalogs typically work across operational and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='54055a93-611e-4db9-9196-677a9347e043', embedding=None, metadata={'page_label': '341', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='analytics data sources, integrate data lineage and presentation of\\ndata relationships, and allow user editing of data descriptions.\\nData catalogs are often used to provide a central place where people\\ncan view their data, queries, and data storage. As a data engineer,\\nyou’ll likely be responsible for setting up and maintaining the various\\ndata integrations of data pipeline and storage systems that will\\nintegrate with the data catalog and the integrity of the data catalog\\nitself.\\nCatalog application integration\\nIdeally, data applications are designed to integrate with catalog APIs\\nto handle their metadata and updates directly. As catalogs are more\\nwidely used in an organization, it becomes easier to approach this\\nideal.\\nAutomated scanning\\nIn practice, cataloging systems typically need to rely on an\\nautomated scanning layer that collects metadata from various\\nsystems such as data lakes, data warehouses, and operational\\ndatabases. Data catalogs can collect existing metadata and may\\nalso use scanning tools to infer metadata such as key relationships\\nor the presence of sensitive data.\\nData portal and social layer\\nData catalogs also typically provide a human access layer through a\\nweb interface, where users can search for data and view data\\nrelationships. Data catalogs can be enhanced with a social layer\\noffering Wiki functionality. This allows users to provide information on\\ntheir datasets, request information from other users, and post\\nupdates as they become available.\\nData catalog use cases', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0dfc1ef3-8635-480b-8ea6-eede2d157839', embedding=None, metadata={'page_label': '342', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data catalogs have both organizational and technical use cases.\\nData catalogs make metadata easily available to systems. For\\ninstance, a data catalog is a key ingredient of the data lakehouse,\\nallowing table discoverability for queries.\\nOrganizationally, data catalogs allow business users, analysts, data\\nscientists, and engineers to search for data to answer questions.\\nData catalogs streamline cross-organizational communications and\\ncollaboration.\\nData Sharing\\nData sharing allows organizations and individuals to share specific\\ndata and carefully defined permissions with specific entities. Data\\nsharing allows data scientists to share data from a sandbox with their\\ncollaborators within an organization. Across organizations, data\\nsharing facilitates collaboration between partner businesses. For\\nexample, an ad tech company can share advertising data with its\\ncustomers.\\nA cloud multitenant environment makes interorganizational\\ncollaboration much easier. However, it also presents new security\\nchallenges. Organizations must carefully control policies that govern\\nwho can share data with whom to prevent accidental exposure or\\ndeliberate exfiltration.\\nData sharing is a core feature of many cloud data platforms. See\\nChapter 5 for a more extensive discussion of data sharing.\\nSchema\\nWhat is the expected form of the data? What is the file format? Is it\\nstructured, semistructured, or unstructured? What data types are\\nexpected? How does the data fit into a larger hierarchy? Is it\\nconnected to other data through shared keys or other relationships?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='10872de0-a9f5-4602-bceb-9d7b89f8cab3', embedding=None, metadata={'page_label': '343', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Note that schema need not be relational. Rather, data becomes\\nmore useful when we have as much information about its structure\\nand organization. For images stored in a data lake, this schema\\ninformation might explain the image format, resolution, and the way\\nthe images fit into a larger hierarchy.\\nSchema can function as a sort of Rosetta stone, instructions that tell\\nus how to read the data. Two major schema patterns exist: schema\\non write and schema on read. Schema on write is essentially the\\ntraditional data warehouse pattern: a table has an integrated\\nschema; any writes to the table must conform. To support schema on\\nwrite, a data lake must integrate a schema metastore.\\nWith schema on read, the schema is dynamically created when data\\nis written, and a reader must determine the schema when reading\\nthe data. Ideally, schema on read is implemented using file formats\\nthat implement built-in schema information, such as Parquet or\\nJSON. CSV files are notorious for schema inconsistency and are not\\nrecommended in this setting.\\nThe principal advantage of schema on write is that it enforces data\\nstandards, making data easier to consume and utilize in the future.\\nSchema on read emphasizes flexibility, allowing virtually any data to\\nbe written. This comes at the cost of greater difficulty consuming\\ndata in the future.\\nSeparation of Compute from Storage\\nA key idea we revisit throughout this book is the separation of\\ncompute from storage. This has emerged as a standard data access\\nand query pattern in today’s cloud era. Data lakes, as we discussed,\\nstore data in object stores and spin up temporary compute capacity\\nto read and process it. Most fully managed OLAP products now rely\\non object storage behind the scenes. To understand the motivations\\nfor separating compute and storage, we should first look at the\\ncolocation of compute and storage.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c6f6cdb7-a128-4110-9c79-bd07a67cf429', embedding=None, metadata={'page_label': '344', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Colocation of compute and storage\\nColocation of compute and storage has long been a standard\\nmethod to improve database performance. For transactional\\ndatabases, data colocation allows fast, low-latency disk reads and\\nhigh bandwidth. Even when we virtualize storage (e.g., using\\nAmazon EBS), data is located relatively close to the host machine.\\nThe same basic idea applies for analytics query systems running\\nacross a cluster of machines. For example, with HDFS and\\nMapReduce, the standard approach is to locate data blocks that\\nneed to be scanned in the cluster, and then push individual map jobs\\nout to these blocks. The data scan and processing for the map step\\nare strictly local. The reduce step involves shuffling data across the\\ncluster, but keeping map steps local effectively preserves more\\nbandwidth for shuffling, delivering better overall performance; map\\nsteps that filter heavily also dramatically reduce the amount of data\\nto be shuffled.\\nSeparation of compute and storage\\nIf colocation of compute and storage delivers high performance, why\\nthe shift toward separation of compute and storage? Several\\nmotivations exist.\\nEphemerality and scalability\\nIn the cloud, we’ve seen a dramatic shift toward ephemerality. In\\ngeneral, it’s cheaper to buy and host a server than to rent it from a\\ncloud provider, provided that you’re running it 24 hours a day\\nnonstop for years on end. In practice, workloads vary dramatically,\\nand significant efficiencies are realized with a pay-as-you-go model if\\nservers can scale up and down. This is true for web servers in online\\nretail, and it is also true for big data batch jobs that may run only\\nperiodically.\\nEphemeral compute resources allow engineers to spin up massive\\nclusters to complete jobs on time, and then delete clusters when', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a9f92288-caf1-4030-b925-acea11cd4470', embedding=None, metadata={'page_label': '345', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='these jobs are done. The performance benefits of temporarily\\noperating at ultra-high scale can outweigh the bandwidth limitations\\nof object storage.\\nData durability and availability\\nCloud object stores significantly mitigate the risk of data loss and\\ngenerally provide extremely high uptime (availability). For example,\\nS3 stores data across multiple zones; if a natural disaster destroys a\\nzone, data is still available from the remaining zones. Having\\nmultiple zones available also reduces the odds of a data outage. If\\nresources in one zone go down, engineers can spin up the same\\nresources in a different zone.\\nThe potential for a misconfiguration that destroys data in object\\nstorage is still somewhat scary, but simple-to-deploy mitigations are\\navailable. Copying data to multiple cloud regions reduces this risk\\nsince configuration changes are generally deployed to only one\\nregion at a time. Replicating data to multiple storage providers can\\nfurther reduce the risk.\\nHybrid separation and colocation\\nThe practical realities of separating compute from storage are more\\ncomplicated than we’ve implied. In reality, we constantly hybridize\\ncolocation and separation to realize the benefits of both approaches.\\nThis hybridization is typically done in two ways: multitier caching and\\nhybrid object storage.\\nWith multitier caching, we utilize object storage for long-term data\\nretention and access but spin up local storage to be used during\\nqueries and various stages of data pipelines. Both Google and\\nAmazon offer versions of hybrid object storage (object storage that is\\ntightly integrated with compute).\\nLet’s look at examples of how some popular processing engines\\nhybridize separation and colocation of storage and compute.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6756f033-0d08-4710-9599-407a3fdfd7f8', embedding=None, metadata={'page_label': '346', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example: AWS EMR with S3 and HDFS\\nBig data services like Amazon EMR spin up temporary HDFS\\nclusters to process data. Engineers have the option of referencing\\nboth S3 and HDFS as a filesystem. A common pattern is to stand up\\nHDFS on SSD drives, pull from S3, and save data from intermediate\\nprocessing steps on local HDFS. Doing so can realize significant\\nperformance gains over processing directly from S3. Full results are\\nwritten back to S3 once the cluster completes its steps, and the\\ncluster and HDFS are deleted. Other consumers read the output\\ndata directly from S3.\\nExample: Apache Spark\\nIn practice, Spark generally runs jobs on HDFS or some other\\nephemeral distributed filesystem to support performant storage of\\ndata between processing steps. In addition, Spark relies heavily on\\nin-memory storage of data to improve processing. The problem with\\nowning the infrastructure for running Spark is that dynamic RAM\\n(DRAM) is extremely expensive; by separating compute and storage\\nin the cloud, we can rent large quantities of memory and then\\nrelease that memory when the job completes.\\nExample: Apache Druid\\nApache Druid relies heavily on SSDs to realize high performance.\\nSince SSDs are significantly more expensive than magnetic disks,\\nDruid keeps only one copy of data in its cluster, reducing “live”\\nstorage costs by a factor of three.\\nOf course, maintaining data durability is still critical, so Druid uses an\\nobject store as its durability layer. When data is ingested, it’s\\nprocessed, serialized into compressed columns, and written to\\ncluster SSDs and object storage. In the event of node failure or\\ncluster data corruption, data can be automatically recovered to new\\nnodes. In addition, the cluster can be shut down and then fully\\nrecovered from SSD storage.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='50edc891-c573-4543-9d8b-1f252c5bd045', embedding=None, metadata={'page_label': '347', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example: Hybrid Object Storage\\nGoogle’s Colossus file storage system supports fine-grained control\\nof data block location, although this functionality is not exposed\\ndirectly to the public. BigQuery uses this feature to colocate\\ncustomer tables in a single location, allowing ultra-high bandwidth for\\nqueries in that location. We refer to this as hybrid object storage\\nbecause it combines the clean abstractions of object storage with\\nsome advantages of colocating compute and storage. Amazon also\\noffers some notion of hybrid object storage through S3 Select, a\\nfeature that allows users to filter S3 data directly in S3 clusters\\nbefore data is returned across the network.\\nWe speculate that public clouds will adopt hybrid object storage\\nmore widely to improve the performance of their offerings and make\\nmore efficient use of available network resources. Some may be\\nalready doing so without disclosing this publicly.\\nThe concept of hybrid object storage underscores that there can still\\nbe advantages to having low-level access to hardware rather than\\nrelying on someone else’s public cloud. Public cloud services do not\\nexpose low-level details of hardware and systems (e.g., data block\\nlocations for Colossus), but these details can be extremely useful in\\nperformance optimization and enhancement. See our discussion of\\ncloud economics in Chapter 4.\\nWhile we’re now seeing a mass migration of data to public clouds,\\nwe believe that many hyper-scale data service vendors that currently\\nrun on public clouds provided by other vendors may build their data\\ncenters in the future, albeit with deep network integration into public\\nclouds.\\nZero copy cloning\\nCloud-based systems based around object storage support zero\\ncopy cloning. This typically means that a new virtual copy of an\\nobject is created (e.g., a new table) without necessarily physically\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92e527d3-fc5b-4e75-89d7-5b4ac03b4c0a', embedding=None, metadata={'page_label': '348', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='copying the underlying data. Typically, new pointers are created to\\nthe raw data files, and future changes to these tables will not be\\nrecorded in the old table. For those familiar with the inner workings\\nof object-oriented languages such as Python, this type of “shallow”\\ncopying is familiar from other contexts.\\nZero copy cloning is a compelling feature, but engineers must\\nunderstand its strengths and limitations. For example, cloning an\\nobject in a data lake environment and then deleting the files in the\\noriginal object might also wipe out the new object.\\nFor fully managed object-store-based systems (e.g., Snowflake and\\nBigQuery), engineers need to be extremely familiar with the exact\\nlimits of shallow copying. Engineers have more access to underlying\\nobject storage in data lake systems such as Databricks—a blessing\\nand a curse. Data engineers should exercise great caution before\\ndeleting any raw files in the underlying object store. Databricks and\\nother data lake management technologies sometimes also support a\\nnotion of deep copying, whereby all underlying data objects are\\ncopied. This is a more expensive process, but also more robust in\\nthe event that files are unintentionally lost or deleted.\\nData Storage Lifecycle and Data Retention\\nStoring data isn’t as simple as just saving it to object storage or disk\\nand forgetting about it. You need to think about the data storage\\nlifecycle and data retention. When you think about access frequency\\nand use cases, ask, “How important is the data to downstream\\nusers, and how often do they need to access it?” This is the data\\nstorage lifecycle. Another question you should ask is, “How long\\nshould I keep this data?” Do you need to retain data indefinitely, or\\nare you fine discarding it past a certain time frame? This is data\\nretention. Let’s dive into each of these.\\nHot, warm, and cold data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='616ec0d5-7fb7-45c7-a933-55e5d89aa626', embedding=None, metadata={'page_label': '349', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Did you know that data has a temperature? Depending on how\\nfrequently data is accessed, we can roughly bucket the way it is\\nstored into three categories of persistence: hot, warm, and cold.\\nQuery access patterns differ for each dataset (Figure 6-9). Typically,\\nnewer data is queried more often than older data. Let’s look at hot,\\ncold, and warm data in that order.\\nFigure 6-9. Hot, warm, and cold data costs associated with access frequency\\nHot data\\nHot data has instant or frequent access requirements. The\\nunderlying storage for hot data is suited for fast access and reads,\\nsuch as SSD or memory. Because of the type of hardware involved\\nwith hot data, storing hot data is often the most expensive form of\\nstorage. Example use cases for hot data include retrieving product\\nrecommendations and product page results. The cost of storing hot\\ndata is the highest of these three storage tiers, but retrieval is often\\ninexpensive.\\nQuery results cache is another example of hot data. When a query is\\nrun, some query engines will persist the query results in the cache.\\nFor a limited time, when the same query is run, instead of rerunning\\nthe same query against storage, the query results cache serves the\\ncached results. This allows for much faster query response times\\nversus redundantly issuing the same query repeatedly. In upcoming\\nchapters, we cover query results caches in more detail.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dd1db8b2-66ee-49ba-a5c1-c1cc1aca0529', embedding=None, metadata={'page_label': '350', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Warm data\\nWarm data is accessed semi-regularly, say, once per month. No hard\\nand fast rules indicate how often warm data is accessed, but it’s less\\nthan hot data and more than cold data. The major cloud providers\\noffer object storage tiers that accommodate warm data. For\\nexample,S3 offers an Infrequently Accessed Tier, and Google Cloud\\nhas a similar storage tier called Nearline. Vendors give their models\\nof recommended access frequency, and engineers can also do their\\ncost modeling and monitoring. Storage of warm data is cheaper than\\nhot data, with slightly more expensive retrieval costs.\\nCold data\\nOn the other extreme, cold data is infrequently accessed data. The\\nhardware used to archive cold data is typically cheap and durable,\\nsuch as HDD, tape storage, and cloud-based archival systems. Cold\\ndata is mainly meant for long-term archival, when there’s little to no\\nintention to access the data. Though storing cold data is cheap,\\nretrieving cold data is often expensive.\\nStorage tier considerations\\nWhen considering the storage tier for your data, consider the costs\\nof each tier. If you store all of your data in hot storage, all of the data\\ncan be accessed quickly. But this comes at a tremendous price!\\nConversely, if you store all data in cold storage to save on costs,\\nyou’ll certainly lower your storage costs, but at the expense of\\nprolonged retrieval times and high retrieval costs if you need to\\naccess data. The storage price goes down from faster/higher\\nperforming storage to lower storage.\\nCold storage is popular for archiving data. Historically, cold storage\\ninvolved physical backups and often mailing this data to a third party\\nthat would archive it in a literal vault. Cold storage is increasingly\\npopular in the cloud. Every cloud vendor offers a cold data solution,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3192a00-8580-441f-8a0c-fe6af3134794', embedding=None, metadata={'page_label': '351', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and you should weigh the cost of pushing data into cold storage\\nversus the cost and time to retrieve the data.\\nData engineers need to account for spillover from hot to warm/cold\\nstorage. Memory is expensive and finite. For example, if hot data is\\nstored in memory, it can be spilled to disk when there’s too much\\nnew data to store and not enough memory. Some databases may\\nmove infrequently accessed data to warm or cold tiers, offloading the\\ndata to either HDD or object storage. The latter is increasingly more\\ncommon because of the cost-effectiveness of object storage. If\\nyou’re in the cloud and using managed services, disk spillover will\\nhappen automatically.\\nIf you’re using cloud-based object storage, create automated\\nlifecycle policies for your data. This will drastically reduce your\\nstorage costs. For example, if your data needs to be accessed only\\nonce a month, move the data to an infrequent access storage tier. If\\nyour data is 180 days old and not accessed for current queries,\\nmove it to an archival storage tier. In both cases, you can automate\\nthe migration of data away from regular object storage, and you’ll\\nsave money. That said, consider the retrieval costs—both in time\\nand money—using infrequent or archival style storage tiers. Access\\nand retrieval times and costs may vary depending on the cloud\\nprovider. Some cloud providers make it simple and cheap to migrate\\ndata into archive storage, but it is costly and slow to retrieve your\\ndata.\\nData retention\\nBack in the early days of “big data,” there was a tendency to err on\\nthe side of accumulating every piece of data possible, regardless of\\nits usefulness. The expectation was, “we might need this data in the\\nfuture.” This data hoarding inevitably became unwieldy and dirty,\\ngiving rise to data swamps, and regulatory crackdowns on data\\nretention, among other consequences and nightmares. Nowadays,\\ndata engineers need to consider data retention: what data do you', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='da17ce86-2ee7-415e-a83f-d91884286343', embedding=None, metadata={'page_label': '352', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='need to keep, and how long should you keep it? Here are some\\nthings to think about with data retention.\\nValue\\nData is an asset, so you should know the value of the data you’re\\nstoring. Of course, value is subjective and depends on what it’s\\nworth to your immediate use case and your broader organization. Is\\nthis data impossible to re-create, or can it easily be re-created by\\nquerying upstream systems? What’s the impact to downstream\\nusers if this data is available versus if it is not?\\nTime\\nThe value to downstream users also depends upon the age of the\\ndata. New data is typically more valuable and frequently accessed\\nthan older data. Technical limitations may determine how long you\\ncan store data in certain storage tiers. For example, if you store hot\\ndata in cache or memory, you’ll likely need to set a time to live (TTL),\\nso you can expire data after a certain point or persist it to warm or\\ncold storage. Otherwise, your hot storage will become full, and\\nqueries against the hot data will suffer from performance lags.\\nCompliance\\nCertain regulations (e.g., HIPAA and Payment Card Industry, or PCI)\\nmight require you to keep data for a certain time. In these situations,\\nthe data simply needs to be accessible upon request, even if the\\nlikelihood of an access request is low. Other regulations might\\nrequire you to hold data for only a limited period of time, and you’ll\\nneed to have the ability to delete specific information on time and\\nwithin compliance guidelines. You’ll need a storage and archival data\\nprocess—along with the ability to search the data—that fits the\\nretention requirements of the particular regulation with which you\\nneed to comply. Of course, you’ll want to balance compliance\\nagainst cost.\\nCost', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7bdcf943-a692-472d-bd92-67eccc824361', embedding=None, metadata={'page_label': '353', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data is an asset that (hopefully) has an ROI. On the cost side of\\nROI, an obvious storage expense is associated with data. Consider\\nthe timeline in which you need to retain data. Given our discussion\\nabout hot, warm, and cold data, implement automatic data lifecycle\\nmanagement practices and move the data to cold storage if you\\ndon’t need the data past the required retention date. Or delete data if\\nit’s truly not needed.\\nSingle-Tenant Versus Multitenant Storage\\nIn Chapter 3, we covered the trade-offs between single-tenant and\\nmultitenant architecture. To recap, with single-tenant architecture,\\neach group of tenants (e.g., individual users, groups of users,\\naccounts, or customers) gets its own dedicated set of resources\\nsuch as networking, compute, and storage. A multitenant\\narchitecture inverts this and shares these resources among groups\\nof users. Both architectures are widely used. This section looks at\\nthe implications of single-tenant and multitenant storage.\\nAdopting single-tenant storage means that every tenant gets their\\ndedicated storage. In the example in Figure 6-10, each tenant gets a\\ndatabase. No data is shared among these databases, and storage is\\ntotally isolated. An example of using single-tenant storage is that\\neach customer’s data must be stored in isolation and cannot be\\nblended with any other customer’s data. In this case, each customer\\ngets their own database.\\nFigure 6-10. In single-tenant storage, each tenant gets their own database', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0cc08c55-78c5-4d85-a38b-5552035865e5', embedding=None, metadata={'page_label': '354', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Separate data storage implies separate and independent schemas,\\nbucket structures, and everything related to storage. This means you\\nhave the liberty of designing each tenant’s storage environment to\\nbe uniform or let them evolve however they may. Schema variation\\nacross customers can be an advantage and a complication; as\\nalways, consider the trade-offs. If each tenant’s schema isn’t uniform\\nacross all tenants, this has major consequences if you need to query\\nmultiple tenants’ tables to create a unified view of all tenant data.\\nMultitenant storage allows for the storage of multiple tenants within a\\nsingle database. For example, instead of the single-tenant scenario\\nwhere customers get their own database, multiple customers may\\nreside in the same database schemas or tables in a multitenant\\ndatabase. Storing multitenant data means each tenant’s data is\\nstored in the same place (Figure 6-11).\\nFigure 6-11. In this multitenant storage, four tenants occupy the same database\\nYou need to be aware of querying both single and multitenant\\nstorage, which we cover in more detail in Chapter 8.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='456dbdc7-b0b2-464b-b9ee-7044f8a49232', embedding=None, metadata={'page_label': '355', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whom You’ll Work With\\nStorage is at the heart of data engineering infrastructure. You’ll\\ninteract with the people who own your IT infrastructure—typically,\\nDevOps, security, and cloud architects. Defining domains of\\nresponsibility between data engineering and other teams is critical.\\nDo data engineers have the authority to deploy their infrastructure in\\nan AWS account, or must another team handle these changes?\\nWork with other teams to define streamlined processes so that\\nteams can work together efficiently and quickly.\\nThe division of responsibilities for data storage will depend\\nsignificantly on the maturity of the organization involved. The data\\nengineer will likely manage the storage systems and workflow if the\\ncompany is early in its data maturity. If the company is later in its\\ndata maturity, the data engineer will probably manage a section of\\nthe storage system. This data engineer will also likely interact with\\nengineers on either side of storage—ingestion and transformation.\\nThe data engineer needs to ensure that the storage systems used by\\ndownstream users are securely available, contain high-quality data,\\nhave ample storage capacity, and perform when queries and\\ntransformations are run.\\nUndercurrents\\nThe undercurrents for storage are significant because storage is a\\ncritical hub for all stages of the data engineering lifecycle. Unlike\\nother undercurrents for which data might be in motion (ingestion) or\\nqueried and transformed, the undercurrents for storage differ\\nbecause storage is so ubiquitous.\\nSecurity\\nWhile engineers often view security as an impediment to their work,\\nthey should embrace the idea that security is a key enabler. Robust', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3739c3f8-17d4-43e4-b138-d7198699b6ec', embedding=None, metadata={'page_label': '356', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='security with fine-grained data access control allows data to be\\nshared and consumed more widely within a business. The value of\\ndata goes up significantly when this is possible.\\nAs always, exercise the principle of least privilege. Don’t give full\\ndatabase access to anyone unless required. This means most data\\nengineers don’t need full database access in practice. Also, pay\\nattention to the column, row, and cell-level access controls in your\\ndatabase. Give users only the information they need and no more.\\nData Management\\nData management is critical as we read and write data with storage\\nsystems.\\nData catalogs and metadata management\\nData is enhanced by robust metadata. Cataloging enables data\\nscientists, analysts, and ML engineers by enabling data discovery.\\nData lineage accelerates the time to track down data problems and\\nallows consumers to locate upstream raw sources. As you build out\\nyour storage systems, invest in your metadata. Integration of a data\\ndictionary with these other tools allows users to share and record\\ninstitutional knowledge robustly.\\nMetadata management also significantly enhances data governance.\\nBeyond simply enabling passive data cataloging and lineage,\\nconsider implementing analytics over these systems to get a clear,\\nactive picture of what’s happening with your data.\\nData versioning in object storage\\nMajor cloud object storage systems enable data versioning. Data\\nversioning can help with error recovery when processes fail, and\\ndata becomes corrupted. Versioning is also beneficial for tracking the\\nhistory of datasets used to build models. Just as code version control\\nallows developers to track down commits that cause bugs, data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e30ed680-4f52-4b21-8c03-0dc524f40fbb', embedding=None, metadata={'page_label': '357', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='version control can aid ML engineers in tracking changes that lead to\\nmodel performance degradation.\\nPrivacy\\nGDPR and other privacy regulations have significantly impacted\\nstorage system design. Any data with privacy implications has a\\nlifecycle that data engineers must manage. Data engineers must be\\nprepared to respond to data deletion requests and selectively\\nremove data as required. In addition, engineers can accommodate\\nprivacy and security through anonymization and masking.\\nDataOps\\nDataOps is not orthogonal to data management, and a significant\\narea of overlap exists. DataOps concerns itself with traditional\\noperational monitoring of storage systems and monitoring the data\\nitself, inseparable from metadata and quality.\\nSystems monitoring\\nData engineers must monitor storage in a variety of ways. This\\nincludes monitoring infrastructure storage components, where they\\nexist, but also monitoring object storage and other “serverless”\\nsystems. Data engineers should take the lead on FinOps (cost\\nmanagement), security monitoring, and access monitoring.\\nObserving and monitoring data\\nWhile metadata systems as we’ve described are critical, good\\nengineering must consider the entropic nature of data by actively\\nseeking to understand its characteristics and watching for major\\nchanges. Engineers can monitor data statistics, apply anomaly\\ndetection methods or simple rules, and actively test and validate for\\nlogical inconsistencies.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b93b4305-347b-49ae-ad5e-9202d860786c', embedding=None, metadata={'page_label': '358', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data Architecture\\nChapter 3 covers the basics of data architecture, as storage is the\\ncritical underbelly of the data engineering lifecycle.\\nConsider the following data architecture tips. Design for required\\nreliability and durability. Understand the upstream source systems\\nand how that data, once ingested, will be stored and accessed.\\nUnderstand the types of data models and queries that will occur\\ndownstream.\\nIf data is expected to grow, can you negotiate storage with your\\ncloud provider? Take an active approach to FinOps, and treat it as a\\ncentral part of architecture conversations. Don’t prematurely\\noptimize, but prepare for scale if business opportunities exist in\\noperating on large data volumes.\\nLean toward fully managed systems, and understand provider SLAs.\\nFully managed systems are generally far more robust and scalable\\nthan systems you have to babysit.\\nOrchestration\\nOrchestration is highly entangled with storage. Storage allows data\\nto flow through pipelines, and orchestration is the pump.\\nOrchestration also helps engineers cope with the complexity of data\\nsystems, potentially combining many storage systems and query\\nengines.\\nSoftware Engineering\\nWe can think about software engineering in the context of storage in\\ntwo ways. First, the code you write should perform well with your\\nstorage system. Make sure the code you write stores the data\\ncorrectly and doesn’t accidentally cause data, memory leaks, or\\nperformance issues. Second, define your storage infrastructure as\\ncode and use ephemeral compute resources when it’s time to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8fbe29c6-fe36-4192-ab1a-0b4432397c3a', embedding=None, metadata={'page_label': '359', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='process your data. Because storage is increasingly distinct from\\ncompute, you can automatically spin resources up and down while\\nkeeping your data in object storage. This keeps your infrastructure\\nclean and avoids coupling your storage and query layers.\\nConclusion\\nStorage is everywhere and underlays many stages of the data\\nengineering lifecycle. In this chapter, you learned about the raw\\ningredients, types, abstractions, and big ideas around storage\\nsystems. Gain deep knowledge of the inner workings and limitations\\nof the storage systems you’ll use. Know the types of data, activities,\\nand workloads appropriate for your storage.\\nAdditional Resources\\n“Column-oriented DBMS” Wikipedia page\\n“Rowise vs. Columnar Database? Theory and in Practice” by\\nMangat Rai Modi\\n“The Design and Implementation of Modern Column-Oriented\\nDatabase Systems” by Daniel Abadi et al.\\n“Diving Into Delta Lake: Schema Enforcement and Evolution” by\\nBurak Yavuz et al.\\n“Snowflake Solution Anti-Patterns: The Probable Data Scientist”\\nby John Aven\\n“The What, When, Why, and How of Incremental Loads” by Tim\\nMitchell\\nIDC’s “Data Creation and Replication Will Grow at a Faster Rate\\nthan Installed Storage Capacity, According to the IDC Global\\nDataSphere and StorageSphere Forecasts” press release', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ace4a231-5b17-47bd-bb0f-e0d5a3b74312', embedding=None, metadata={'page_label': '360', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“What Is a Vector Database?” by Bryan Turriff\\n“Hot Data vs. Cold Data: Why It Matters” by Afzaal Ahmad\\nZeeshan\\n1  Andy Klein, “Hard Disk Drive (HDD) vs. Solid-State Drive (SSD): What’s the\\nDiff?,” Backblaze blog, October 5, 2021, https://oreil.ly/XBps8.\\n2  Benoit Dageville, “The Snowflake Elastic Data Warehouse,” SIGMOD ’16:\\nProceedings of the 2016 International Conference on Management of Data\\n(June 2016): 215–226, https://oreil.ly/Tc1su.\\n3  James Dixon, “Data Lakes Revisited,” James Dixon’s Blog, September 25,\\n2014, https://oreil.ly/FH25v.\\n4  Valliappa Lakshmanan and Jordan Tigani, GoogleBig Query: The Definitive\\nGuide (Seastopol, CA: O’Reilly, 2019), page 15, https://oreil.ly/5aXXu', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6591da29-0946-4695-8fba-0e1e4c74198b', embedding=None, metadata={'page_label': '361', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 7. Ingestion\\nYou’ve learned about the various source systems you’ll likely\\nencounter as a data engineer and about ways to store data. Let’s\\nnow turn our attention to the patterns and choices that apply to\\ningesting data from various source systems. In this chapter, we\\ndiscuss data ingestion (see Figure 7-1), the key engineering\\nconsiderations for the ingestion phase, the major patterns for batch\\nand streaming ingestion, technologies you’ll encounter, whom you’ll\\nwork with as you develop your data ingestion pipeline, and how the\\nundercurrents feature in the ingestion phase.\\nFigure 7-1. To begin processing data, we must ingest it\\nWhat Is Data Ingestion?\\nData ingestion is the process of moving data from one place to\\nanother. Data ingestion implies data movement from source systems\\ninto storage in the data engineering lifecycle, with ingestion as an\\nintermediate step (Figure 7-2).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='31c31dca-9481-40ef-8bc3-c5cac0397aec', embedding=None, metadata={'page_label': '362', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 7-2. Data from system 1 is ingested into system 2\\nIt’s worth quickly contrasting data ingestion with data integration.\\nWhereas data ingestion is data movement from point A to B, data\\nintegration combines data from disparate sources into a new\\ndataset. For example, you can use data integration to combine data\\nfrom a CRM system, advertising analytics data, and web analytics to\\ncreate a user profile, which is saved to your data warehouse.\\nFurthermore, using reverse ETL, you can send this newly created\\nuser profile back to your CRM so salespeople can use the data for\\nprioritizing leads. We describe data integration more fully in\\nChapter 8, where we discuss data transformations; reverse ETL is\\ncovered in Chapter 9.\\nWe also point out that data ingestion is different from internal\\ningestion within a system. Data stored in a database is copied from\\none table to another, or data in a stream is temporarily cached. We\\nconsider this another part of the general data transformation process\\ncovered in Chapter 8.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90a70116-63ac-4761-906a-f939858cd5da', embedding=None, metadata={'page_label': '363', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='DATA PIPELINES DEFINED\\nData pipelines begin in source systems, but ingestion is the\\nstage where data engineers begin actively designing data\\npipeline activities. In the data engineering space, a good deal of\\nceremony occurs around data movement and processing\\npatterns, with established patterns such as ETL, newer patterns\\nsuch as ELT, and new names for long-established practices\\n(reverse ETL) and data sharing.\\nAll of these concepts are encompassed in the idea of a data\\npipeline. It is essential to understand the details of these various\\npatterns and know that a modern data pipeline includes all of\\nthem. As the world moves away from a traditional monolithic\\napproach with rigid constraints on data movement, and toward\\nan open ecosystem of cloud services that are assembled like\\nLEGO bricks to realize products, data engineers prioritize using\\nthe right tools to accomplish the desired outcome over adhering\\nto a narrow philosophy of data movement.\\nIn general, here’s our definition of a data pipeline:\\nA data pipeline is the combination of architecture, systems,\\nand processes that move data through the stages of the data\\nengineering lifecycle.\\nOur definition is deliberately fluid—and intentionally vague—to\\nallow data engineers to plug in whatever they need to accomplish\\nthe task at hand. A data pipeline could be a traditional ETL\\nsystem, where data is ingested from an on-premises\\ntransactional system, passed through a monolithic processor,\\nand written into a data warehouse. Or it could be a cloud-based\\ndata pipeline that pulls data from 100 sources, combines it into\\n20 wide tables, trains five other ML models, deploys them into\\nproduction, and monitors ongoing performance. A data pipeline', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='20eff18f-a1eb-41ae-b989-63c5fd506c06', embedding=None, metadata={'page_label': '364', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='should be flexible enough to fit any needs along the data\\nengineering lifecycle.\\nLet’s keep this notion of data pipelines in mind as we proceed\\nthrough this chapter.\\nKey Engineering Considerations for the Ingestion\\nPhase\\nWhen preparing to architect or build an ingestion system, here are\\nsome primary considerations and questions to ask yourself related to\\ndata ingestion:\\nWhat’s the use case for the data I’m ingesting?\\nCan I reuse this data and avoid ingesting multiple versions of\\nthe same dataset?\\nWhere is the data going? What’s the destination?\\nHow often should the data be updated from the source?\\nWhat is the expected data volume?\\nWhat format is the data in? Can downstream storage and\\ntransformation accept this format?\\nIs the source data in good shape for immediate downstream\\nuse? That is, is the data of good quality? What post-processing\\nis required to serve it? What are data-quality risks (e.g., could\\nbot traffic to a website contaminate the data)?\\nDoes the data require in-flight processing for downstream\\ningestion if the data is from a streaming source?\\nThese questions undercut batch and streaming ingestion and apply\\nto the underlying architecture you’ll create, build, and maintain.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d4edf3b-4eb6-4e53-9cd9-59d8d82605f6', embedding=None, metadata={'page_label': '365', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Regardless of how often the data is ingested, you’ll want to consider\\nthese factors when designing your ingestion architecture:\\nBounded versus unbounded\\nFrequency\\nSynchronous versus asynchronous\\nSerialization and deserialization\\nThroughput and elastic scalability\\nReliability and durability\\nPayload\\nPush versus pull versus poll patterns\\nLet’s look at each of these.\\nBounded Versus Unbounded\\nAs you might recall from Chapter 3, data comes in two forms:\\nbounded and unbounded (Figure 7-3). Unbounded data is data as it\\nexists in reality, as events happen, either sporadically or\\ncontinuously, ongoing and flowing. Bounded data is a convenient\\nway of bucketing data across some sort of boundary, such as time.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='08457bd1-dd8d-430e-9dd9-a18286c93f07', embedding=None, metadata={'page_label': '366', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 7-3. Bounded versus unbounded data\\nLet us adopt this mantra: All data is unbounded until it’s bounded.\\nLike many mantras, this one is not precisely accurate 100% of the\\ntime. The grocery list that I scribbled this afternoon is bounded data.\\nI wrote it as a stream of consciousness (unbounded data) onto a\\npiece of scrap paper, where the thoughts now exist as a list of things\\n(bounded data) I need to buy at the grocery store. However, the idea\\nis correct for practical purposes for the vast majority of data you’ll\\nhandle in a business context. For example, an online retailer will\\nprocess customer transactions 24 hours a day until the business\\nfails, the economy grinds to a halt, or the sun explodes.\\nBusiness processes have long imposed artificial bounds on data by\\ncutting discrete batches. Keep in mind the true unboundedness of\\nyour data; streaming ingestion systems are simply a tool for\\npreserving the unbounded nature of data so that subsequent steps in\\nthe lifecycle can also process it continuously.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='48b71b9c-792c-4154-84e0-a56b29ad3592', embedding=None, metadata={'page_label': '367', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Frequency\\nOne of the critical decisions that data engineers must make in\\ndesigning data-ingestion processes is the data-ingestion frequency.\\nIngestion processes can be batch, micro-batch, or real-time.\\nIngestion frequencies vary dramatically from slow to fast (Figure 7-\\n4). On the slow end, a business might ship its tax data to an\\naccounting firm once a year. On the faster side, a CDC system could\\nretrieve new log updates from a source database once a minute.\\nEven faster, a system might continuously ingest events from IoT\\nsensors and process these within seconds. Data-ingestion\\nfrequencies are often mixed in a company, depending on the use\\ncase and technologies.\\nFigure 7-4. The spectrum batch to real-time ingestion frequencies\\nWe note that “real-time” ingestion patterns are becoming\\nincreasingly common. We put “real-time” in quotation marks because\\nno ingestion system is genuinely real-time. Any database, queue or\\npipeline has inherent latency in delivering data to a target system. It\\nis more accurate to speak of near real-time, but we often use real-\\ntime for brevity. The near real-time pattern generally does away with\\nan explicit update frequency; events are processed in the pipeline\\neither one by one as they arrive or in micro-batches (i.e., batches\\nover concise time intervals). For this book, we will use real-time and\\nstreaming interchangeably.\\nEven with a streaming data-ingestion process, batch processing\\ndownstream is relatively standard. At the time of this writing, ML\\nmodels are typically trained on a batch basis, although continuous', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='54c042a1-dc98-4477-b49b-bf25cf256f05', embedding=None, metadata={'page_label': '368', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='online training is becoming more prevalent. Rarely do data engineers\\nhave the option to build a purely near real-time pipeline with no batch\\ncomponents. Instead, they choose where batch boundaries will\\noccur—i.e., the data engineering lifecycle data will be broken into\\nbatches. Once data reaches a batch process, the batch frequency\\nbecomes a bottleneck for all downstream processing.\\nIn addition, streaming systems are the best fit for many data source\\ntypes. In IoT applications, the typical pattern is for each sensor to\\nwrite events or measurements to streaming systems as they happen.\\nWhile this data can be written directly into a database, a streaming\\ningestion platform such as Amazon Kinesis or Apache Kafka is a\\nbetter fit for the application. Software applications can adopt similar\\npatterns by writing events to a message queue as they happen\\nrather than waiting for an extraction process to pull events and state\\ninformation from a backend database. This pattern works\\nexceptionally well for event-driven architectures already exchanging\\nmessages through queues. And again, streaming architectures\\ngenerally coexist with batch processing.\\nSynchronous Versus Asynchronous Ingestion\\nWith synchronous ingestion, the source, ingestion, and destination\\nhave complex dependencies and are tightly coupled. As you can see\\nin Figure 7-5, each stage of the data engineering lifecycle has\\nprocesses A, B, and C directly dependent upon one another. If\\nprocess A fails, processes B and C cannot start; if process B fails,\\nprocess C doesn’t start. This type of synchronous workflow is\\ncommon in older ETL systems, where data extracted from a source\\nsystem must then be transformed before being loaded into a data\\nwarehouse. Processes downstream of ingestion can’t start until all\\ndata in the batch has been ingested. If the ingestion or\\ntransformation process fails, the entire process must be rerun.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e052ea1d-beec-4a92-a33e-7a0c9e77c4dc', embedding=None, metadata={'page_label': '369', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 7-5. A synchronous ingestion process runs as discrete batch steps\\nHere’s a mini case study of how not to design your data pipelines. At\\none company, the transformation process itself was a series of\\ndozens of tightly coupled synchronous workflows, with the entire\\nprocess taking over 24 hours to finish. If any step of that\\ntransformation pipeline failed, the whole transformation process had\\nto be restarted from the beginning! In this instance, we saw process\\nafter process fail, and because of nonexistent or cryptic error\\nmessages, fixing the pipeline was a game of whack-a-mole that took\\nover a week to diagnose and cure. Meanwhile, the business didn’t\\nhave updated reports during that time. People weren’t happy.\\nWith asynchronous ingestion, dependencies can now operate at the\\nlevel of individual events, much as they would in a software backend\\nbuilt from microservices (Figure 7-6). Individual events become\\navailable in storage as soon as they are ingested individually. Take\\nthe example of a web application on AWS that emits events into\\nAmazon Kinesis Data Streams (here acting as a buffer). The stream\\nis read by Apache Beam, which parses and enriches events, and\\nthen forwards them to a second Kinesis stream; Kinesis Data\\nFirehose rolls up events and writes objects to Amazon S3.\\nFigure 7-6. Asynchronous processing of an event stream in AWS\\nThe big idea is that rather than relying on asynchronous processing,\\nwhere a batch process runs for each stage as the input batch closes\\nand certain time conditions are met, each stage of the asynchronous\\npipeline can process data items as they become available in parallel', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d31deeb2-8bb3-4607-8ce5-753c3e7fc2ed', embedding=None, metadata={'page_label': '370', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='across the Beam cluster. The processing rate depends on available\\nresources. The Kinesis Data Stream acts as the shock absorber,\\nmoderating the load so that event rate spikes will not overwhelm\\ndownstream processing. Events will move through the pipeline\\nquickly when the event rate is low, and any backlog has cleared.\\nNote that we could modify the scenario and use a Kinesis Data\\nStream for storage, eventually extracting events to S3 before they\\nexpire out of the stream.\\nSerialization and Deserialization\\nMoving data from source to destination involves serialization and\\ndeserialization. As a reminder, serialization means encoding the data\\nfrom a source and preparing data structures for transmission and\\nintermediate storage stages.\\nWhen ingesting data, ensure that your destination can deserialize\\nthe data it receives. We’ve seen data ingested from a source but\\nthen sitting inert and unusable in the destination because the data\\ncannot be properly deserialized. See the more extensive discussion\\nof serialization in Appendix A.\\nThroughput and Scalability\\nIn theory, your ingestion should never be a bottleneck. In practice,\\ningestion bottlenecks are pretty standard. Data throughput and\\nsystem scalability become critical as your data volumes grow and\\nrequirements change. Design your systems to scale and shrink to\\nflexibly match the desired data throughput.\\nWhere you’re ingesting data from matters a lot. If you’re receiving\\ndata as it’s generated, will the upstream system have any issues that\\nmight impact your downstream ingestion pipelines? For example,\\nsuppose a source database goes down. When it comes back online\\nand attempts to backfill the lapsed data loads, will your ingestion be\\nable to keep up with this sudden influx of backlogged data?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='43245419-b803-4bd3-936d-63c9fb75d566', embedding=None, metadata={'page_label': '371', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Another thing to consider is your ability to handle bursty data\\ningestion. Data generation rarely happens at a constant rate and\\noften ebbs and flows. Built-in buffering is required to collect events\\nduring rate spikes to prevent data from getting lost. Buffering bridges\\nthe time while the system scales and allows storage systems to\\naccommodate bursts even in a dynamically scalable system.\\nWhenever possible, use managed services that handle the\\nthroughput scaling for you. While you can manually accomplish\\nthese tasks by adding more servers, shards, or workers, often this\\nisn’t value-added work, and there’s a good chance you’ll miss\\nsomething. Much of this heavy lifting is now automated. Don’t\\nreinvent the data ingestion wheel if you don’t have to.\\nReliability and Durability\\nReliability and durability are vital in the ingestion stages of data\\npipelines. Reliability entails high uptime and proper failover for\\ningestion systems. Durability entails making sure that data isn’t lost\\nor corrupted.\\nSome data sources (e.g., IoT devices and caches) may not retain\\ndata if it is not correctly ingested. Once lost, it is gone for good. In\\nthis sense, the reliability of ingestion systems leads directly to the\\ndurability of generated data. If data is ingested, downstream\\nprocesses can theoretically run late if they break temporarily.\\nOur advice is to evaluate the risks and build an appropriate level of\\nredundancy and self-healing based on the impact and cost of losing\\ndata. Reliability and durability have both direct and indirect costs. For\\nexample, will your ingestion process continue if an AWS zone goes\\ndown? How about a whole region? How about the power grid or the\\ninternet? Of course, nothing is free. How much will this cost you?\\nYou might be able to build a highly redundant system and have a\\nteam on call 24 hours a day to handle outages. This also means\\nyour cloud and labor costs become prohibitive (direct costs), and the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8fb429aa-2534-4b99-9097-ce85d7b516da', embedding=None, metadata={'page_label': '372', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ongoing work takes a significant toll on your team (indirect costs).\\nThere’s no correct answer, and you need to evaluate the costs and\\nbenefits of your reliability and durability decisions.\\nDon’t assume that you can build a system that will reliably and\\ndurably ingest data in every possible scenario. Even the nearly\\ninfinite budget of the US federal government can’t guarantee this. In\\nmany extreme scenarios, ingesting data actually won’t matter. There\\nwill be little to ingest if the internet goes down, even if you build\\nmultiple air-gapped data centers in underground bunkers with\\nindependent power. Continually evaluate the trade-offs and costs of\\nreliability and durability.\\nPayload\\nA payload is the dataset you’re ingesting and has characteristics\\nsuch as kind, shape, size, schema and data types, and metadata.\\nLet’s look at some of these characteristics to understand why this\\nmatters.\\nKind\\nThe kind of data you handle directly impacts how it’s dealt with\\ndownstream in the data engineering lifecycle. Kind consists of type\\nand format. Data has a type—tabular, image, video, text, etc. The\\ntype directly influences the data format or the way it is expressed in\\nbytes, names, and file extensions. For example, a tabular kind of\\ndata may be in formats such as CSV or Parquet, with each of these\\nformats having different byte patterns for serialization and\\ndeserialization. Another kind of data is an image, which has a format\\nof JPG or PNG and is inherently unstructured.\\nShape\\nEvery payload has a shape that describes its dimensions. Data\\nshape is critical across the data engineering lifecycle. For instance,\\nan image’s pixel and red, green, blue (RGB) dimensions are', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9700ad0e-e8cb-4816-9a0d-5994f9fd9001', embedding=None, metadata={'page_label': '373', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='necessary for training deep learning models. As another example, if\\nyou’re trying to import a CSV file into a database table, and your\\nCSV has more columns than the database table, you’ll likely get an\\nerror during the import process. Here are some examples of the\\nshapes of various kinds of data:\\nTabular\\nThe number of rows and columns in the dataset, commonly\\nexpressed as M rows and N columns\\nSemistructured JSON\\nThe key-value pairs and nesting depth occur with subelements\\nUnstructured text\\nNumber of words, characters, or bytes in the text body\\nImages\\nThe width, height, and RGB color depth (e.g., 8 bits per pixel)\\nUncompressed audio\\nNumber of channels (e.g., two for stereo), sample depth (e.g., 16\\nbits per sample), sample rate (e.g., 48 kHz), and length (e.g.,\\n10,003 seconds)\\nSize\\nThe size of the data describes the number of bytes of a payload. A\\npayload may range in size from single bytes to terabytes and larger.\\nTo reduce the size of a payload, it may be compressed into various\\nformats such as ZIP and TAR (see the discussion of compression in\\nAppendix A).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c7ebd1d6-3cb3-4d00-8c5a-209727071e67', embedding=None, metadata={'page_label': '374', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A massive payload can also be split into chunks, which effectively\\nreduces the size of the payload into smaller subsections. When\\nloading a huge file into a cloud object storage or data warehouse,\\nthis is a common practice as the small individual files are easier to\\ntransmit over a network (especially if they’re compressed). The\\nsmaller chunked files are sent to their destination and then\\nreassembled after all data has arrived.\\nSchema and data types\\nMany data payloads have a schema, such as tabular and\\nsemistructured data. As mentioned earlier in this book, a schema\\ndescribes the fields and types of data within those fields. Other data,\\nsuch as unstructured text, images, and audio, will not have an\\nexplicit schema or data types. However, they might come with\\ntechnical file descriptions on shape, data and file format, encoding,\\nsize, etc.\\nAlthough you can connect to databases in various ways (such as file\\nexport, CDC, JDBC/ODBC), the connection is easy. The great\\nengineering challenge is understanding the underlying schema.\\nApplications organize data in various ways, and engineers need to\\nbe intimately familiar with the organization of the data and relevant\\nupdate patterns to make sense of it. The problem has been\\nsomewhat exacerbated by the popularity of object-relational mapping\\n(ORM), which automatically generates schemas based on object\\nstructure in languages such as Java or Python. Natural structures in\\nan object-oriented language often map to something messy in an\\noperational database. Data engineers may need to familiarize\\nthemselves with the class structure of application code.\\nSchema is not only for databases. As we’ve discussed, APIs present\\ntheir schema complications. Many vendor APIs have friendly\\nreporting methods that prepare data for analytics. In other cases,\\nengineers are not so lucky. The API is a thin wrapper around', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0f6056e5-d1f9-4e20-b131-896ddc0ea52d', embedding=None, metadata={'page_label': '375', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='underlying systems, requiring engineers to understand application\\ninternals to use the data.\\nMuch of the work associated with ingesting from source schemas\\nhappens in the data engineering lifecycle transformation stage,\\nwhich we discuss in Chapter 8. We’ve placed this discussion here\\nbecause data engineers need to begin studying source schemas as\\nsoon they plan to ingest data from a new source.\\nCommunication is critical for understanding source data, and\\nengineers also have the opportunity to reverse the flow of\\ncommunication and help software engineers improve data where it is\\nproduced. Later in this chapter, we’ll return to this topic in “Whom\\nYou’ll Work With”.\\nDetecting and handling schema changes in upstream and\\ndownstream systems\\nSchema changes frequently occur in source systems and are often\\nwell out of data engineers’ control. Examples of schema changes\\ninclude the following:\\nAdding a new column\\nChanging a column type\\nCreating a new table\\nRenaming a column\\nIt’s becoming increasingly common for ingestion tools to automate\\nthe detection of schema changes and even auto-update target\\ntables. Ultimately, this is something of a mixed blessing. Schema\\nchanges can still break pipelines downstream of staging and\\ningestion.\\nEngineers must still implement strategies to respond to changes\\nautomatically and alert on changes that cannot be accommodated\\nautomatically. Automation is excellent, but the analysts and data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7afdea26-3ea9-4e5f-8142-8779b76922c9', embedding=None, metadata={'page_label': '376', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='scientists who rely on this data should be informed of the schema\\nchanges that violate existing assumptions. Even if automation can\\naccommodate a change, the new schema may adversely affect the\\nperformance of reports and models. Communication between those\\nmaking schema changes and those impacted by these changes is as\\nimportant as reliable automation that checks for schema changes.\\nSchema registries\\nIn streaming data, every message has a schema, and these\\nschemas may evolve between producers and consumers. A schema\\nregistry is a metadata repository used to maintain schema and data\\ntype integrity in the face of constantly changing schemas. Schema\\nregistries can also track schema versions and history. It describes\\nthe data model for messages, allowing consistent serialization and\\ndeserialization between producers and consumers. Schema\\nregistries are used in most major data tools and clouds.\\nMetadata\\nIn addition to the apparent characteristics we’ve just covered, a\\npayload often contains metadata, which we first discussed in\\nChapter 2. Metadata is data about data. Metadata can be as critical\\nas the data itself. One of the significant limitations of the early\\napproach to the data lake—or data swamp, which could become a\\ndata superfund site—was a complete lack of attention to metadata.\\nWithout a detailed description of the data, it may be of little value.\\nWe’ve already discussed some types of metadata (e.g., schema)\\nand will address them many times throughout this chapter.\\nPush Versus Pull Versus Poll Patterns\\nWe mentioned push versus pull when we introduced the data\\nengineering lifecycle in Chapter 2. A push strategy (Figure 7-7)\\ninvolves a source system sending data to a target, while a pull\\nstrategy (Figure 7-8) entails a target reading data directly from a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a8a80ffd-68cf-459d-89f7-cacbaa39421b', embedding=None, metadata={'page_label': '377', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='source. As we mentioned in that discussion, the lines between these\\nstrategies are blurry.\\nFigure 7-7. Pushing data from source to destination\\nFigure 7-8. A destination pulling data from a source\\nAnother pattern related to pulling is polling for data (Figure 7-9).\\nPolling involves periodically checking a data source for any changes.\\nWhen changes are detected, the destination pulls the data as it\\nwould in a regular pull situation.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d1372f2e-f874-42b1-8d21-0577f1c5917b', embedding=None, metadata={'page_label': '378', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 7-9. Polling for changes in a source system\\nBatch Ingestion Considerations\\nBatch ingestion, which involves processing data in bulk, is often a\\nconvenient way to ingest data. This means that data is ingested by\\ntaking a subset of data from a source system, based either on a time\\ninterval or the size of accumulated data (Figure 7-10).\\nFigure 7-10. Time-interval batch ingestion\\nTime-interval batch ingestion is widespread in traditional business\\nETL for data warehousing. This pattern is often used to process data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='84694319-1de9-4f35-8983-3f0c2f9bb814', embedding=None, metadata={'page_label': '379', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='once a day, overnight during off-hours, to provide daily reporting, but\\nother frequencies can also be used.\\nSize-based batch ingestion (Figure 7-11) is quite common when data\\nis moved from a streaming-based system into object storage;\\nultimately, you must cut the data into discrete blocks for future\\nprocessing in a data lake. Some size-based ingestion systems can\\nbreak data into objects based on various criteria, such as the size in\\nbytes of the total number of events.\\nFigure 7-11. Size-based batch ingestion\\nSome commonly used batch ingestion patterns, which we discuss in\\nthis section, include the following:\\nSnapshot or differential extraction\\nFile-based export and ingestion\\nETL versus ELT\\nInserts, updates, and batch size\\nData migration\\nSnapshot or Differential Extraction\\nData engineers must choose whether to capture full snapshots of a\\nsource system or differential (sometimes called incremental)\\nupdates. With full snapshots, engineers grab the entire current state\\nof the source system on each update read. With the differential', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b2cd9fb7-31f6-4b94-9440-c0740b99d741', embedding=None, metadata={'page_label': '380', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='update pattern, engineers can pull only the updates and changes\\nsince the last read from the source system. While differential\\nupdates are ideal for minimizing network traffic and target storage\\nusage, full snapshot reads remain extremely common because of\\ntheir simplicity.\\nFile-Based Export and Ingestion\\nData is quite often moved between databases and systems using\\nfiles. Data is serialized into files in an exchangeable format, and\\nthese files are provided to an ingestion system. We consider file-\\nbased export to be a push-based ingestion pattern. This is because\\ndata export and preparation work is done on the source system side.\\nFile-based ingestion has several potential advantages over a direct\\ndatabase connection approach. It is often undesirable to allow direct\\naccess to backend systems for security reasons. With file-based\\ningestion, export processes are run on the data-source side, giving\\nsource system engineers complete control over what data gets\\nexported and how the data is preprocessed. Once files are done,\\nthey can be provided to the target system in various ways. Common\\nfile-exchange methods are object storage, secure file transfer\\nprotocol (SFTP), electronic data interchange (EDI), or secure copy\\n(SCP).\\nETL Versus ELT\\nChapter 3 introduced ETL and ELT, both extremely common\\ningestion, storage, and transformation patterns you’ll encounter in\\nbatch workloads. This section covers the extract (E) and the load (L)\\nparts of ETL and ELT.\\nExtract\\nExtract means getting data from a source system. While extract\\nseems to imply pulling data, it can also be push based. Extraction', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b80ea26b-0345-4bdf-85ff-47088f055295', embedding=None, metadata={'page_label': '381', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='may also require reading metadata and schema changes.\\nLoad\\nOnce data is extracted, it can either be transformed (ETL) before\\nloading it into a storage destination or simply loaded into storage for\\nfuture transformation. When loading data, you should be mindful of\\nthe type of system you’re loading, the schema of the data, and the\\nperformance impact of loading. We cover ETL and ELT in Chapter 8.\\nInserts, Updates, and Batch Size\\nBatch-oriented systems often perform poorly when users attempt to\\nperform many small-batch operations rather than a smaller number\\nof large operations. For example, while it is common to insert one\\nrow at a time in a transactional database, this is a bad pattern for\\nmany columnar databases as it forces the creation of many small,\\nsuboptimal files, and forces the system to run a high number of\\ncreate object operations. Running many small in-place update\\noperations is an even bigger problem because it causes the\\ndatabase to scan each existing column file to run the update.\\nUnderstand the appropriate update patterns for the database or data\\nstore you’re working with. Also, understand that certain technologies\\nare purpose-built for high insert rates. For example, Apache Druid\\nand Apache Pinot can handle high insert rates. SingleStore can\\nmanage hybrid workloads that combine OLAP and OLTP\\ncharacteristics. BigQuery performs poorly on a high rate of vanilla\\nSQL single-row inserts but extremely well if data is fed in through its\\nstream buffer. Know the limits and characteristics of your tools.\\nData Migration\\nMigrating data to a new database or environment is not usually\\ntrivial, and data needs to be moved in bulk. Sometimes this means\\nmoving data sizes that are hundreds of terabytes or much larger,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4aec8b3-38d9-4526-ae71-557de9df12b1', embedding=None, metadata={'page_label': '382', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='often involving the migration of specific tables and moving entire\\ndatabases and systems.\\nData migrations probably aren’t a regular occurrence as a data\\nengineer, but you should be familiar with them. As is often the case\\nfor data ingestion, schema management is a crucial consideration.\\nSuppose you’re migrating data from one database system to a\\ndifferent one (say, SQL Server to Snowflake). No matter how closely\\nthe two databases resemble each other, subtle differences almost\\nalways exist in the way they handle schema. Fortunately, it is\\ngenerally easy to test ingestion of a sample of data and find schema\\nissues before undertaking a complete table migration.\\nMost data systems perform best when data is moved in bulk rather\\nthan as individual rows or events. File or object storage is often an\\nexcellent intermediate stage for transferring data. Also, one of the\\nbiggest challenges of database migration is not the movement of the\\ndata itself but the movement of data pipeline connections from the\\nold system to the new one.\\nBe aware that many tools are available to automate various types of\\ndata migrations. Especially for large and complex migrations, we\\nsuggest looking at these options before doing this manually or\\nwriting your own migration solution.\\nMessage and Stream Ingestion Considerations\\nIngesting event data is common. This section covers issues you\\nshould consider when ingesting events, drawing on topics covered in\\nChapters 5 and 6.\\nSchema Evolution\\nSchema evolution is common when handling event data; fields may\\nbe added or removed, or value types might change (say, a string to\\nan integer). Schema evolution can have unintended impacts on your', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='314f5bfa-c175-494a-b3ca-c7a08c0c065f', embedding=None, metadata={'page_label': '383', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data pipelines and destinations. For example, an IoT device gets a\\nfirmware update that adds a new field to the event it transmits, or a\\nthird-party API introduces changes to its event payload or countless\\nother scenarios. All of these potentially impact your downstream\\ncapabilities.\\nTo alleviate issues related to schema evolution, here are a few\\nsuggestions. First, if your event-processing framework has a schema\\nregistry (discussed earlier in this chapter), use it to version your\\nschema changes. Next, a dead-letter queue (described in “Error\\nHandling and Dead-Letter Queues”) can help you investigate issues\\nwith events that are not properly handled. Finally, the low-fidelity\\nroute (and the most effective) is regularly communicating with\\nupstream stakeholders about potential schema changes and\\nproactively addressing schema changes with the teams introducing\\nthese changes instead of reacting to the receiving end of breaking\\nchanges.\\nLate-Arriving Data\\nThough you probably prefer all event data to arrive on time, event\\ndata might arrive late. A group of events might occur around the\\nsame time frame (similar event times), but some might arrive later\\nthan others (late ingestion times) because of various circumstances.\\nFor example, an IoT device might be late sending a message\\nbecause of internet latency issues. This is common when ingesting\\ndata. You should be aware of late-arriving data and the impact on\\ndownstream systems and uses. Suppose you assume that ingestion\\nor process time is the same as the event time. You may get some\\nstrange results if your reports or analysis depend on an accurate\\nportrayal of when events occur. To handle late-arriving data, you\\nneed to set a cutoff time for when late-arriving data will no longer be\\nprocessed.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4d1ffd67-770d-494b-a6d4-fafad42be05e', embedding=None, metadata={'page_label': '384', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ordering and Multiple Delivery\\nStreaming platforms are generally built out of distributed systems,\\nwhich can cause some complications. Specifically, messages may\\nbe delivered out of order and more than once (at-least-once\\ndelivery). See the event-streaming platforms discussion in Chapter 5\\nfor more details.\\nReplay\\nReplay allows readers to request a range of messages from the\\nhistory, allowing you to rewind your event history to a particular point\\nin time. Replay is a key capability in many streaming ingestion\\nplatforms and is particularly useful when you need to re-ingest and\\nreprocess data for a specific time range. For example, RabbitMQ\\ntypically deletes messages after all subscribers consume them.\\nKafka, Kinesis, and Pub/Sub all support event retention and replay.\\nTime to Live\\nHow long will you preserve your event record? A key parameter is\\nmaximum message retention time, also known as the time to live\\n(TTL). TTL is usually a configuration you’ll set for how long you want\\nevents to live before they are acknowledged and ingested. Any\\nunacknowledged event that’s not ingested after its TTL expires\\nautomatically disappears. This is helpful to reduce backpressure and\\nunnecessary event volume in your event-ingestion pipeline.\\nFind the right balance of TTL impact on our data pipeline. An\\nextremely short TTL (milliseconds or seconds) might cause most\\nmessages to disappear before processing. A very long TTL (several\\nweeks or months) will create a backlog of many unprocessed\\nmessages, resulting in long wait times.\\nLet’s look at how some popular platforms handle TTL at the time of\\nthis writing. Google Cloud Pub/Sub supports retention periods of up', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7b50cf19-59a8-460e-84b8-c6777c3a5f68', embedding=None, metadata={'page_label': '385', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='to 7 days. Amazon Kinesis Data Streams retention can be turned up\\nto 365 days. Kafka can be configured for indefinite retention, limited\\nby available disk space. (Kafka also supports the option to write\\nolder messages to cloud object storage, unlocking virtually unlimited\\nstorage space and retention.)\\nMessage Size\\nMessage size is an easily overlooked issue: you must ensure that\\nthe streaming framework in question can handle the maximum\\nexpected message size. Amazon Kinesis supports a maximum\\nmessage size of 1 MB. Kafka defaults to this maximum size but can\\nbe configured for a maximum of 20 MB or more. (Configurability may\\nvary on managed service platforms.)\\nError Handling and Dead-Letter Queues\\nSometimes events aren’t successfully ingested. Perhaps an event is\\nsent to a nonexistent topic or message queue, the message size\\nmay be too large, or the event has expired past its TTL. Events that\\ncannot be ingested need to be rerouted and stored in a separate\\nlocation called a dead-letter queue.\\nA dead-letter queue segregates problematic events from events that\\ncan be accepted by the consumer (Figure 7-12). If events are not\\nrerouted to a dead-letter queue, these erroneous events risk\\nblocking other messages from being ingested. Data engineers can\\nuse a dead-letter queue to diagnose why event ingestions errors\\noccur and solve data pipeline problems, and might be able to\\nreprocess some messages in the queue after fixing the underlying\\ncause of errors.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='468efeaa-65ba-4bc4-82f5-adc3c4df2090', embedding=None, metadata={'page_label': '386', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 7-12. “Good” events are passed to the consumer, whereas “bad” events are\\nstored in a dead-letter queue\\nConsumer Pull and Push\\nA consumer subscribing to a topic can get events in two ways: push\\nand pull. Let’s look at the ways some streaming technologies pull\\nand push data. Kafka and Kinesis support only pull subscriptions.\\nSubscribers read messages from a topic and confirm when they\\nhave been processed. In addition, to pull subscriptions, Pub/Sub and\\nRabbitMQ support push subscriptions, allowing these services to\\nwrite messages to a listener.\\nPull subscriptions are the default choice for most data engineering\\napplications, but you may want to consider push capabilities for\\nspecialized applications. Note that pull-only message ingestion\\nsystems can still push if you add an extra layer to handle this.\\nLocation\\nIt is often desirable to integrate streaming across several locations\\nfor enhanced redundancy and to consume data close to where it is\\ngenerated. As a general rule, the closer your ingestion is to where\\ndata originates, the better your bandwidth and latency. However, you\\nneed to balance this against the costs of moving data between\\nregions to run analytics on a combined dataset. As always, data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='261e2951-822c-4d99-80c4-c88e3d0f94bf', embedding=None, metadata={'page_label': '387', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='egress costs can spiral quickly. Do a careful evaluation of the trade-\\noffs as you build out your architecture.\\nWays to Ingest Data\\nNow that we’ve described some of the significant patterns underlying\\nbatch and streaming ingestion, let’s focus on ways you can ingest\\ndata. Although we will cite some common ways, keep in mind that\\nthe universe of data ingestion practices and technologies is vast and\\ngrowing daily.\\nDirect Database Connection\\nData can be pulled from databases for ingestion by querying and\\nreading over a network connection. Most commonly, this connection\\nis made using ODBC or JDBC.\\nODBC uses a driver hosted by a client accessing the database to\\ntranslate commands issued to the standard ODBC API into\\ncommands issued to the database. The database returns query\\nresults over the wire, where the driver receives them and translates\\nthem back into a standard form and read by the client. For ingestion,\\nthe application utilizing the ODBC driver is an ingestion tool. The\\ningestion tool may pull data through many small queries or a single\\nlarge query.\\nJDBC is conceptually remarkably similar to ODBC. A Java driver\\nconnects to a remote database and serves as a translation layer\\nbetween the standard JDBC API and the native network interface of\\nthe target database. It might seem strange to have a database API\\ndedicated to a single programming language, but there are strong\\nmotivations for this. The Java Virtual Machine (JVM) is standard,\\nportable across hardware architectures and operating systems, and\\nprovides the performance of compiled code through a just-in-time', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b0cac2e3-2c3c-42fd-b74f-174d9afe4df7', embedding=None, metadata={'page_label': '388', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='(JIT) compiler. The JVM is an extremely popular compiling VM for\\nrunning code in a portable manner.\\nJDBC provides extraordinary database driver portability. ODBC\\ndrivers are shipped as OS and architecture native binaries; database\\nvendors must maintain versions for each architecture/OS version\\nthat they wish to support. On the other hand, vendors can ship a\\nsingle JDBC driver that is compatible with any JVM language (e.g.,\\nJava, Scala, Clojure, or Kotlin) and JVM data framework (i.e.,\\nSpark.) JDBC has become so popular that it is also used as an\\ninterface for non-JVM languages such as Python; the Python\\necosystem provides translation tools that allow Python code to talk to\\na JDBC driver running on a local JVM.\\nJDBC and ODBC are used extensively for data ingestion from\\nrelational databases, returning to the general concept of direct\\ndatabase connections. Various enhancements are used to\\naccelerate data ingestion. Many data frameworks can parallelize\\nseveral simultaneous connections and partition queries to pull data\\nin parallel. On the other hand, nothing is free; using parallel\\nconnections also increases the load on the source database.\\nJDBC and ODBC were long the gold standards for data ingestion\\nfrom databases, but these connection standards are beginning to\\nshow their age for many data engineering applications. These\\nconnection standards struggle with nested data, and they send data\\nas rows. This means that native nested data must be re-encoded as\\nstring data to be sent over the wire, and columns from columnar\\ndatabases must be re-serialized as rows.\\nAs discussed in “File-Based Export and Ingestion”, many databases\\nnow support native file export that bypasses JDBC/ODBC and\\nexports data directly in formats such as Parquet, ORC, and Avro.\\nAlternatively, many cloud data warehouses provide direct REST\\nAPIs.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c291aa75-7e81-4d1c-8113-b947e6266830', embedding=None, metadata={'page_label': '389', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='JDBC connections should generally be integrated with other\\ningestion technologies. For example, we commonly use a reader\\nprocess to connect to a database with JDBC, write the extracted\\ndata into multiple objects, and then orchestrate ingestion into a\\ndownstream system (see Figure 7-13). The reader process can run\\nin a wholly ephemeral cloud instance or in an orchestration system.\\nFigure 7-13. An ingestion process reads from a source database using JDBC, and\\nthen writes objects into object storage. A target database (not shown) can be\\ntriggered to ingest the data with an API call from an orchestration system.\\nChange Data Capture\\nChange data capture (CDC). introduced in Chapter 2, is the process\\nof ingesting changes from a source database system. For example,\\nwe might have a source PostgreSQL system that supports an\\napplication and periodically or continuously ingests table changes for\\nanalytics.\\nNote that our discussion here is by no means exhaustive. We\\nintroduce you to common patterns but suggest that you read the\\ndocumentation on a particular database to handle the details of CDC\\nstrategies.\\nBatch-oriented CDC\\nIf the database table in question has an updated_at field\\ncontaining the last time a record was written or updated, we can\\nquery the table to find all updated rows since a specified time. We\\nset the filter timestamp based on when we last captured changed\\nrows from the tables. This process allows us to pull changes and\\ndifferentially update a target table.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9bc77412-3d56-4f56-bcf9-d462122c241b', embedding=None, metadata={'page_label': '390', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This form of batch-oriented CDC has a key limitation: while we can\\neasily determine which rows have changed since a point in time, we\\ndon’t necessarily obtain all changes that were applied to these rows.\\nConsider the example of running batch CDC on a bank account table\\nevery 24 hours. This operational table shows the current account\\nbalance for each account. When money is moved in and out of\\naccounts, the banking application runs a transaction to update the\\nbalance.\\nWhen we run a query to return all rows in the account table that\\nchanged in the last 24 hours, we’ll see records for each account that\\nrecorded a transaction. Suppose that a certain customer withdrew\\nmoney five times using a debit card in the last 24 hours. Our query\\nwill return only the last account balance recorded in the 24 hour\\nperiod; other records over the period won’t appear. This issue can be\\nmitigated by utilizing an insert-only schema, where each account\\ntransaction is recorded as a new record in the table (see “Insert-\\nOnly”).\\nContinuous CDC\\nContinuous CDC captures all table history and can support near\\nreal-time data ingestion, either for real-time database replication or\\nto feed real-time streaming analytics. Rather than running periodic\\nqueries to get a batch of table changes, continuous CDC treats each\\nwrite to the database as an event.\\nWe can capture an event stream for continuous CDC in a couple of\\nways. One of the most common approaches with a transactional\\ndatabase such as PostgreSQL is log-based CDC. The database\\nbinary log records every change to the database sequentially (see\\n“Database Logs”) A CDC tool can read this log and send the events\\nto a target, such as the Apache Kafka Debezium streaming platform.\\nSome databases support a simplified, managed CDC paradigm. For\\ninstance, many cloud-hosted databases can be configured to directly\\ntrigger a serverless function or write to an event stream every time a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='44e05842-2a4d-4be7-af51-eb674f947f15', embedding=None, metadata={'page_label': '391', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='change happens in the database. This completely frees engineers\\nfrom worrying about the details of how events are captured in the\\ndatabase and forwarded.\\nCDC and database replication\\nCDC can be used to replicate between databases: events are\\nbuffered into a stream and asynchronously written into a second\\ndatabase. However, many databases natively support a tightly\\ncoupled version of replication (synchronous replication) that keeps\\nthe replica fully in sync with the primary database. Synchronous\\nreplication typically requires that the primary database and the\\nreplica are of the same type (e.g., PostgreSQL to PostgreSQL). The\\nadvantage of synchronous replication is that the secondary database\\ncan offload work from the primary database by acting as a read\\nreplica; read queries can be redirected to the replica. The query will\\nreturn the same results that would be returned from the primary\\ndatabase.\\nRead replicas are often used in batch data ingestion patterns to\\nallow large scans to run without overloading the primary production\\ndatabase. In addition, an application can be configured to fail over to\\nthe replica if the primary database becomes unavailable. No data will\\nbe lost in the failover because the replica is entirely in sync with the\\nprimary database.\\nThe advantage of asynchronous CDC replication is a loosely\\ncoupled architecture pattern. While the replica might be slightly\\ndelayed from the primary database, this is often not a problem for\\nanalytics applications, and events can now be directed to a variety of\\ntargets; we might run CDC replication while simultaneously directing\\nevents to object storage and a streaming analytics processor.\\nCDC considerations\\nLike anything in technology, CDC is not free. CDC consumes various\\ndatabase resources, such as memory, disk bandwidth, storage, CPU', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f24f9629-e1cc-4d5b-9fa8-55ddedb2fd20', embedding=None, metadata={'page_label': '392', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='time, and network bandwidth. Engineers should work with production\\nteams and run tests before turning on CDC on production systems to\\navoid operational problems. Similar considerations apply to\\nsynchronous replication.\\nFor batch CDC, be aware that running any large batch query against\\na transactional production system can cause excessive load. Either\\nrun such queries only at off-hours or use a read replica to avoid\\nburdening the primary database.\\nAPIs\\nThe bulk of software engineering is just plumbing.\\n—Karl Hughes\\nAs we mentioned in Chapter 5, APIs are a data source that\\ncontinues to grow in importance and popularity. A typical\\norganization may have hundreds of external data sources such as\\nSaaS platforms or partner companies. The hard reality is that no\\nproper standard exists for data exchange over APIs. Data engineers\\ncan spend a significant amount of time reading documentation,\\ncommunicating with external data owners, and writing and\\nmaintaining API connection code.\\nThree trends are slowly changing this situation. First, many vendors\\nprovide API client libraries for various programming languages that\\nremove much of the complexity of API access.\\nSecond, numerous data connector platforms are available now as\\nSaaS, open source, or managed open source. These platforms\\nprovide turnkey data connectivity to many data sources; they offer\\nframeworks for writing custom connectors for unsupported data\\nsources. See “Managed Data Connectors”.\\nThe third trend is the emergence of data sharing (discussed in\\nChapter 5)—i.e., the ability to exchange data through a standard\\nplatform such as BigQuery, Snowflake, Redshift, or S3. Once data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='497defae-1732-4768-9685-5ba327b63671', embedding=None, metadata={'page_label': '393', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='lands on one of these platforms, it is straightforward to store it,\\nprocess it, or move it somewhere else. Data sharing has had a large\\nand rapid impact in the data engineering space.\\nDon’t reinvent the wheel when data sharing is not an option and\\ndirect API access is necessary. While a managed service might look\\nlike an expensive option, consider the value of your time and the\\nopportunity cost of building API connectors when you could be\\nspending your time on higher-value work.\\nIn addition, many managed services now support building custom\\nAPI connectors. This may provide API technical specifications in a\\nstandard format or writing connector code that runs in a serverless\\nfunction framework (e.g., AWS Lambda) while letting the managed\\nservice handle the details of scheduling and synchronization. Again,\\nthese services can be a huge time-saver for engineers, both for\\ndevelopment and ongoing maintenance.\\nReserve your custom connection work for APIs that aren’t well\\nsupported by existing frameworks; you will find that there are still\\nplenty of these to work on. Handling custom API connections has\\ntwo main aspects: software development and ops. Follow software\\ndevelopment best practices; you should use version control,\\ncontinuous delivery, and automated testing. In addition to following\\nDevOps best practices, consider an orchestration framework, which\\ncan dramatically streamline the operational burden of data ingestion.\\nMessage Queues and Event-Streaming Platforms\\nMessage queues and event-streaming platforms are widespread\\nways to ingest real-time data from web and mobile applications, IoT\\nsensors, and smart devices. As real-time data becomes more\\nubiquitous, you’ll often find yourself either introducing or retrofitting\\nways to handle real-time data in your ingestion workflows. As such,\\nit’s essential to know how to ingest real-time data. Popular real-time\\ndata ingestion includes message queues or event-streaming', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='509e4102-a31b-4f3d-9a6b-4bb1a5967cd1', embedding=None, metadata={'page_label': '394', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='platforms, which we covered in Chapter 5. Though these are both\\nsource systems, they also act as ways to ingest data. In both cases,\\nyou consume events from the publisher you subscribe to.\\nRecall the differences between messages and streams. A message\\nis handled at the individual event level and is meant to be transient.\\nOnce a message is consumed, it is acknowledged and removed\\nfrom the queue. On the other hand, a stream ingests events into an\\nordered log. The log persists for as long as you wish, allowing events\\nto be queried over various ranges, aggregated, and combined with\\nother streams to create new transformations published to\\ndownstream consumers. In Figure 7-14, we have two producers\\n(producers 1 and 2) sending events to two consumers (consumers 1\\nand 2). These events are combined into a new dataset and sent to a\\nproducer for downstream consumption.\\nFigure 7-14. Two datasets are produced and consumed (producers 1 and 2), and\\nthen combined, with the combined data published to a new producer (producer 3)\\nThe last point is an essential difference between batch and\\nstreaming ingestion. Whereas batch usually involves static workflows\\n(ingest data, store it, transform it, and serve it), messages and\\nstreams are fluid. Ingestion can be nonlinear, with data being\\npublished, consumed, republished, and re-consumed. When\\ndesigning your real-time ingestion workflows, keep in mind how data\\nwill flow.\\nAnother consideration is the throughput of your real-time data\\npipelines. Messages and events should flow with as little latency as', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ec715f1-77b7-4cd9-ace2-f4d9546908fb', embedding=None, metadata={'page_label': '395', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='possible, meaning you should provision adequate partition (or shard)\\nbandwidth and throughput. Provide sufficient memory, disk, and CPU\\nresources for event processing, and if you’re managing your real-\\ntime pipelines, incorporate autoscaling to handle spikes and save\\nmoney as load decreases. For these reasons, managing your\\nstreaming platform can entail significant overhead. Consider\\nmanaged services for your real-time ingestion pipelines, and focus\\nyour attention on ways to get value from your real-time data.\\nManaged Data Connectors\\nThese days, if you’re considering writing a data ingestion connector\\nto a database or API, ask yourself: has this already been created?\\nFurthermore, is there a service that will manage the nitty-gritty\\ndetails of this connection for me? “APIs” mentions the popularity of\\nmanaged data connector platforms and frameworks. These tools aim\\nto provide a standard set of connectors available out of the box to\\nspare data engineers building complicated plumbing to connect to a\\nparticular source. Instead of creating and managing a data\\nconnector, you outsource this service to a third party.\\nGenerally, options in the space allow users to set a target and\\nsource, ingest in various ways (e.g., CDC, replication, truncate and\\nreload), set permissions and credentials, configure an update\\nfrequency, and begin syncing data. The vendor or cloud behind the\\nscenes fully manages and monitors data syncs. If data\\nsynchronization fails, you’ll receive an alert with logged information\\non the cause of the error.\\nWe suggest using managed connector platforms instead of creating\\nand managing your connectors. Vendors and OSS projects each\\ntypically have hundreds of prebuilt connector options and can easily\\ncreate custom connectors. The creation and management of data\\nconnectors is largely undifferentiated heavy lifting these days and\\nshould be outsourced whenever possible.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa7357a4-85d4-4b84-a18c-30e6c7ad6ec8', embedding=None, metadata={'page_label': '396', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Moving Data with Object Storage\\nObject storage is a multitenant system in public clouds, and it\\nsupports storing massive amounts of data. This makes object\\nstorage ideal for moving data in and out of data lakes, between\\nteams, and transferring data between organizations. You can even\\nprovide short-term access to an object with a signed URL, giving a\\nuser temporary permission.\\nIn our view, object storage is the most optimal and secure way to\\nhandle file exchange. Public cloud storage implements the latest\\nsecurity standards, has a robust track record of scalability and\\nreliability, accepts files of arbitrary types and sizes, and provides\\nhigh-performance data movement. We discussed object storage\\nmuch more extensively in Chapter 6.\\nEDI\\nAnother practical reality for data engineers is electronic data\\ninterchange (EDI). The term is vague enough to refer to any data\\nmovement method. It usually refers to somewhat archaic means of\\nfile exchange, such as by email or flash drive. Data engineers will\\nfind that some data sources do not support more modern means of\\ndata transport, often because of archaic IT systems or human\\nprocess limitations.\\nEngineers can at least enhance EDI through automation. For\\nexample, they can set up a cloud-based email server that saves files\\nonto company object storage as soon as they are received. This can\\ntrigger orchestration processes to ingest and process data. This is\\nmuch more robust than an employee downloading the attached file\\nand manually uploading it to an internal system, which we still\\nfrequently see.\\nDatabases and File Export', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dd7e34bf-e392-47b8-b48c-3e51eac4f385', embedding=None, metadata={'page_label': '397', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Engineers should be aware of how the source database systems\\nhandle file export. Export involves large data scans that significantly\\nload the database for many transactional systems. Source system\\nengineers must assess when these scans can be run without\\naffecting application performance and might opt for a strategy to\\nmitigate the load. Export queries can be broken into smaller exports\\nby querying over key ranges or one partition at a time. Alternatively,\\na read replica can reduce load. Read replicas are especially\\nappropriate if exports happen many times a day and coincide with a\\nhigh source system load.\\nMajor cloud data warehouses are highly optimized for direct file\\nexport. For example, Snowflake, BigQuery, Redshift, and others\\nsupport direct export to object storage in various formats.\\nPractical Issues with Common File Formats\\nEngineers should also be aware of the file formats to export. CSV is\\nstill ubiquitous and highly error prone at the time of this writing.\\nNamely, CSV’s default delimiter is also one of the most familiar\\ncharacters in the English language—the comma! But it gets worse.\\nCSV is by no means a uniform format. Engineers must stipulate the\\ndelimiter, quote characters, and escaping to appropriately handle the\\nexport of string data. CSV also doesn’t natively encode schema\\ninformation or directly support nested structures. CSV file encoding\\nand schema information must be configured in the target system to\\nensure appropriate ingestion. Autodetection is a convenience feature\\nprovided in many cloud environments but is inappropriate for\\nproduction ingestion. As a best practice, engineers should record\\nCSV encoding and schema details in file metadata.\\nMore robust and expressive export formats include Parquet, Avro,\\nArrow, and ORC or JSON. These formats natively encode schema\\ninformation and handle arbitrary string data with no particular\\nintervention. Many of them also handle nested data structures', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a1b1dcc7-f82e-4265-b930-6475176bb3a0', embedding=None, metadata={'page_label': '398', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='natively so that JSON fields are stored using internal nested\\nstructures rather than simple strings. For columnar databases,\\ncolumnar formats (Parquet, Arrow, ORC) allow more efficient data\\nexport because columns can be directly transcoded between\\nformats. These formats are also generally more optimized for query\\nengines. The Arrow file format is designed to map data directly into\\nprocessing engine memory, providing high performance in data lake\\nenvironments.\\nThe disadvantage of these newer formats is that many of them are\\nnot natively supported by source systems. Data engineers are often\\nforced to work with CSV data and then build robust exception\\nhandling and error detection to ensure data quality on ingestion. See\\nAppendix A for a more extensive discussion of file formats.\\nShell\\nThe shell is an interface by which you may execute commands to\\ningest data. The shell can be used to script workflows for virtually\\nany software tool, and shell scripting is still used extensively in\\ningestion processes. A shell script might read data from a database,\\nreserialize it into a different file format, upload it to object storage,\\nand trigger an ingestion process in a target database. While storing\\ndata on a single instance or server is not highly scalable, many of\\nour data sources are not particularly large, and such approaches\\nwork just fine.\\nIn addition, cloud vendors generally provide robust CLI-based tools.\\nIt is possible to run complex ingestion processes simply by issuing\\ncommands to the AWS CLI. As ingestion processes grow more\\ncomplicated and the SLA grows more stringent, engineers should\\nconsider moving to a proper orchestration system.\\nSSH', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='71c6aa54-9019-4f34-a295-4e09bdbcb74b', embedding=None, metadata={'page_label': '399', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='SSH is not an ingestion strategy but a protocol used with other\\ningestion strategies. We use SSH in a few ways. First, SSH can be\\nused for file transfer with SCP, as mentioned earlier. Second, SSH\\ntunnels are used to allow secure, isolated connections to databases.\\nApplication databases should never be directly exposed on the\\ninternet. Instead, engineers can set up a bastion host—i.e., an\\nintermediate host instance that can connect to the database in\\nquestion. This host machine is exposed on the internet, although\\nlocked down for minimal access from only specified IP addresses to\\nspecified ports. To connect to the database, a remote machine first\\nopens an SSH tunnel connection to the bastion host, and then\\nconnects from the host machine to the database.\\nSFTP and SCP\\nAccessing and sending data both from secure FTP (SFTP) and\\nsecure copy (SCP) are techniques you should be familiar with, even\\nif data engineers do not typically use these regularly (IT or\\nsecurity/secOps will handle this).\\nEngineers rightfully cringe at the mention of SFTP (occasionally, we\\neven hear instances of FTP being used in production). Regardless,\\nSFTP is still a practical reality for many businesses. They work with\\npartner businesses that consume or provide data using SFTP, and\\nare unwilling to rely on other standards. To avoid data leaks, security\\nanalysis is critical in these situations.\\nSCP is a file-exchange protocol that runs over an SSH connection\\nrelated to SSH. SCP can be a secure file-transfer option if it is\\nconfigured correctly. Again, adding additional network access control\\n(defense in depth) to enhance SCP security is highly recommended.\\nWebhooks', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='39bb7699-e974-4600-9a2b-92e524d5eb23', embedding=None, metadata={'page_label': '400', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Webhooks, as we discussed in Chapter 5, are often referred to as\\nreverse APIs. For a typical REST data API, the data provider gives\\nengineers API specifications that they use to write their data\\ningestion code. The code makes requests and receives data in\\nresponses.\\nWith a webhook (Figure 7-15), the data provider defines an API\\nrequest specification, but the data provider makes API calls rather\\nthan receiving them; it’s the data consumer’s responsibility to provide\\nan API endpoint for the provider to call. The consumer is responsible\\nfor ingesting each request and handling data aggregation, storage,\\nand processing.\\nFigure 7-15. A basic webhook ingestion architecture built from cloud services\\nWebhook-based data ingestion architectures can be brittle, difficult\\nto maintain, and inefficient. Using appropriate off-the-shelf tools, data\\nengineers can build more robust webhook architectures with lower\\nmaintenance and infrastructure costs. For example, a webhook\\npattern in AWS might use a serverless function framework (Lambda)\\nto receive incoming events, a managed event-streaming platform to\\nstore and buffer messages (Kinesis), a stream-processing\\nframework to handle real-time analytics (Flink), and an object store\\nfor long-term storage (S3).\\nYou’ll notice that this architecture does much more than simply\\ningest the data. This underscores ingestion’s entanglement with the\\nother stages of the data engineering lifecycle; it is often impossible to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c80c14dc-9013-4e36-8c69-48941a56e7f2', embedding=None, metadata={'page_label': '401', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='define your ingestion architecture without making decisions about\\nstorage and processing.\\nWeb Interface\\nWeb interfaces for data access remain a practical reality for data\\nengineers. We frequently run into situations where not all data and\\nfunctionality in a SaaS platform is exposed through automated\\ninterfaces such as APIs and file drops. Instead, someone must\\nmanually access a web interface, generate a report, and download a\\nfile to a local machine. This has obvious drawbacks, such as people\\nforgetting to run the report or having their laptop die. Where possible,\\nchoose tools and workflows that allow for automated access to data.\\nWeb Scraping\\nWeb scraping automatically extracts data from web pages, often by\\ncombing the web page’s various HTML elements. You might scrape\\necommerce sites to extract product pricing information or scrape\\nmultiple news sites for your news aggregator. Web scraping is\\nwidespread, and you may encounter it as a data engineer. It’s also a\\nmurky area where ethical and legal lines are blurry.\\nHere is some top-level advice to be aware of before undertaking any\\nweb-scraping project. First, ask yourself if you should be web\\nscraping or if data is available from a third party. If your decision is to\\nweb scrape, be a good citizen. Don’t inadvertently create a denial-of-\\nservice (DoS) attack, and don’t get your IP address blocked.\\nUnderstand how much traffic you generate and pace your web-\\ncrawling activities appropriately. Just because you can spin up\\nthousands of simultaneous Lambda functions to scrape doesn’t\\nmean you should; excessive web scraping could lead to the\\ndisabling of your AWS account.\\nSecond, be aware of the legal implications of your activities. Again,\\ngenerating DoS attacks can entail legal consequences. Actions that', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a682ebea-2afc-4091-bca8-c9d89371347c', embedding=None, metadata={'page_label': '402', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='violate terms of service may cause headaches for your employer or\\nyou personally.\\nThird, web pages constantly change their HTML element structure,\\nmaking it tricky to keep your web scraper updated. Ask yourself, is\\nthe headache of maintaining these systems worth the effort?\\nWeb scraping has interesting implications for the data engineering\\nlifecycle processing stage; engineers should think about various\\nfactors at the beginning of a web-scraping project. What do you\\nintend to do with the data? Are you just pulling required fields from\\nthe scraped HTML by using Python code and then writing these\\nvalues to a database? Do you intend to maintain the complete HTML\\ncode of the scraped websites and process this data using a\\nframework like Spark? These decisions may lead to very different\\narchitectures downstream of ingestion.\\nTransfer Appliances for Data Migration\\nFor massive data (100 TB or more), transferring data directly over\\nthe internet may be a slow and costly process. At this scale, the\\nfastest, most efficient way to move data is not over the wire but by\\ntruck. Cloud vendors offer the ability to send your data via a physical\\n“box of hard drives.” Simply order a storage device, called a transfer\\nappliance, load your data from your servers, and then send it back to\\nthe cloud vendor, which will upload your data.\\nThe suggestion is to consider using a transfer appliance if your data\\nsize hovers around 100 TB. On the extreme end, AWS even offers\\nSnowmobile, a transfer appliance sent to you in a semitrailer!\\nSnowmobile is intended to lift and shift an entire data center, in\\nwhich data sizes are in the petabytes or greater.\\nTransfer appliances are handy for creating hybrid-cloud or multicloud\\nsetups. For example, Amazon’s data transfer appliance (AWS\\nSnowball) supports import and export. To migrate into a second\\ncloud, users can export their data into a Snowball device, and then', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0434683a-421a-431a-b18a-f68bfd28bc2b', embedding=None, metadata={'page_label': '403', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='import it into a second transfer appliance to move data into GCP or\\nAzure. This might sound awkward, but even when it’s feasible to\\npush data over the internet between clouds, data egress fees make\\nthis a costly proposition. Physical transfer appliances are a cheaper\\nalternative when the data volumes are significant.\\nRemember that transfer appliances and data migration services are\\none-time data ingestion events and are not suggested for ongoing\\nworkloads. Suppose you have workloads requiring constant data\\nmovement in either a hybrid or multicloud scenario. In that case,\\nyour data sizes are presumably batching or streaming much smaller\\ndata sizes on an ongoing basis.\\nData Sharing\\nData sharing is growing as a popular option for consuming data (see\\nChapters 5 and 6.) Data providers will offer datasets to third-party\\nsubscribers, either for free or at a cost. These datasets are often\\nshared in a read-only fashion, meaning you can integrate these\\ndatasets with your own data (and other third-party datasets), but you\\ndo not own the shared dataset. In the strict sense, this isn’t\\ningestion, where you get physical possession of the dataset. If the\\ndata provider decides to remove your access to a dataset, you’ll no\\nlonger have access to it.\\nMany cloud platforms offer data sharing, allowing you to share your\\ndata and consume data from various providers. Some of these\\nplatforms also provide data marketplaces where companies and\\norganizations can offer their data for sale.\\nWhom You’ll Work With\\nData ingestion sits at several organizational boundaries. In\\ndeveloping and managing data ingestion pipelines, data engineers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='611b4771-253b-44b6-bbcc-ca2a9e40aa60', embedding=None, metadata={'page_label': '404', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='will work with both people and systems sitting upstream (data\\nproducers) and downstream (data consumers).\\nUpstream Stakeholders\\nA significant disconnect often exists between those responsible for\\ngenerating data—typically, software engineers—and the data\\nengineers who will prepare this data for analytics and data science.\\nSoftware engineers and data engineers usually sit in separate\\norganizational silos; if they think about data engineers, they typically\\nsee them simply as downstream consumers of the data exhaust from\\ntheir application, not as stakeholders.\\nWe see this current state of affairs as a problem and a significant\\nopportunity. Data engineers can improve the quality of their data by\\ninviting software engineers to be stakeholders in data engineering\\noutcomes. The vast majority of software engineers are well aware of\\nthe value of analytics and data science but don’t necessarily have\\naligned incentives to contribute to data engineering efforts directly.\\nSimply improving communication is a significant first step. Often\\nsoftware engineers have already identified potentially valuable data\\nfor downstream consumption. Opening a communication channel\\nencourages software engineers to get data into shape for consumers\\nand communicate about data changes to prevent pipeline\\nregressions.\\nBeyond communication, data engineers can highlight the\\ncontributions of software engineers to team members, executives,\\nand especially product managers. Involving product managers in the\\noutcome and treating downstream data processed as part of a\\nproduct encourages them to allocate scarce software development\\nto collaboration with data engineers. Ideally, software engineers can\\nwork partially as extensions of the data engineering team; this allows\\nthem to collaborate on various projects, such as creating an event-\\ndriven architecture to enable real-time analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42889015-8126-4f1c-9467-f6a0fd7c63c1', embedding=None, metadata={'page_label': '405', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Downstream Stakeholders\\nWho is the ultimate customer for data ingestion? Data engineers\\nfocus on data practitioners and technology leaders such as data\\nscientists, analysts, and chief technical officers. They would do well\\nalso to remember their broader circle of business stakeholders such\\nas marketing directors, vice presidents over the supply chain, and\\nCEOs.\\nToo often, we see data engineers pursuing sophisticated projects\\n(e.g., real-time streaming buses or complex data systems) while\\ndigital marketing managers next door are left downloading Google\\nAds reports manually. View data engineering as a business, and\\nrecognize who your customers are. Often basic automation of\\ningestion processes has significant value, especially for departments\\nlike marketing that control massive budgets and sit at the heart of\\nrevenue for the business. Basic ingestion work may seem tedious,\\nbut delivering value to these core parts of the company will open up\\nmore budget and more exciting long-term data engineering\\nopportunities.\\nData engineers can also invite more executive participation in this\\ncollaborative process. For a good reason, data-driven culture is quite\\nfashionable in business leadership circles. Still, it is up to data\\nengineers and other data practitioners to provide executives with\\nguidance on the best structure for a data-driven business. This\\nmeans communicating the value of lowering barriers between data\\nproducers and data engineers while supporting executives in\\nbreaking down silos and setting up incentives to lead to a more\\nunified data-driven culture.\\nOnce again, communication is the watchword. Honest\\ncommunication early and often with stakeholders will go a long way\\nto ensure that your data ingestion adds value.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9463771-41e4-4cce-bd31-660f7052b530', embedding=None, metadata={'page_label': '406', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Undercurrents\\nVirtually all the undercurrents touch the ingestion phase, but we’ll\\nemphasize the most salient ones here.\\nSecurity\\nMoving data introduces security vulnerabilities because you have to\\ntransfer data between locations. The last thing you want is to capture\\nor compromise the data while moving.\\nConsider where the data lives and where it is going. Data that needs\\nto move within your VPC should use secure endpoints and never\\nleave the confines of the VPC. Use a VPN or a dedicated private\\nconnection if you need to send data between the cloud and an on-\\npremises network. This might cost money, but the security is a good\\ninvestment. If your data traverses the public internet, ensure that the\\ntransmission is encrypted. It is always a good practice to encrypt\\ndata over the wire.\\nData Management\\nNaturally, data management begins at data ingestion. This is the\\nstarting point for lineage and data cataloging; from this point on, data\\nengineers need to think about master data management, ethics,\\nprivacy, and compliance.\\nSchema changes\\nSchema changes (such as adding, changing, or removing columns\\nin a database table) remain, from our perspective, an unsettled issue\\nin data management. The traditional approach is a careful\\ncommand-and-control review process. Working with clients at large\\nenterprises, we have been quoted lead times of six months for the\\naddition of a single field. This is an unacceptable impediment to\\nagility.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='05e737ee-c190-4363-8982-55396d78a789', embedding=None, metadata={'page_label': '407', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On the opposite end of the spectrum, any schema change in the\\nsource triggers target tables to be re-created with the new schema.\\nThis solves schema problems at the ingestion stage but can still\\nbreak downstream pipelines and destination storage systems.\\nOne possible solution, which we, the authors, have meditated on for\\na while, is an approach pioneered by Git version control. When Linus\\nTorvalds was developing Git, many of his choices were inspired by\\nthe limitations of Concurrent Versions System (CVS). CVS is\\ncompletely centralized; it supports only one current official version of\\nthe code, stored on a central project server. To make Git a truly\\ndistributed system, Torvalds used the notion of a tree; each\\ndeveloper could maintain their processed branch of the code and\\nthen merge to or from other branches.\\nA few years ago, such an approach to data was unthinkable. On-\\npremises MPP systems are typically operated at close to maximum\\nstorage capacity. However, storage is cheap in big data and cloud\\ndata warehouse environments. One may quite easily maintain\\nmultiple versions of a table with different schemas and even different\\nupstream transformations. Teams can support various “development”\\nversions of a table by using orchestration tools such as Airflow;\\nschema changes, upstream transformation, and code changes can\\nappear in development tables before official changes to the main\\ntable.\\nData ethics, privacy, and compliance\\nClients often ask for our advice on encrypting sensitive data in\\ndatabases, which generally leads us to ask a fundamental question:\\ndo you need the sensitive data you’re trying to encrypt? As it turns\\nout, this question often gets overlooked when creating requirements\\nand solving problems.\\nData engineers should always train themselves to ask this question\\nwhen setting up ingestion pipelines. They will inevitably encounter\\nsensitive data; the natural tendency is to ingest it and forward it to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f13dc446-9971-4d31-a7a1-ee45b5da99a7', embedding=None, metadata={'page_label': '408', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the next step in the pipeline. But if this data is not needed, why\\ncollect it at all? Why not simply drop sensitive fields before data is\\nstored? Data cannot leak if it is never collected.\\nWhere it is truly necessary to keep track of sensitive identities, it is\\ncommon practice to apply tokenization to anonymize identities in\\nmodel training and analytics. But engineers should look at where this\\ntokenization is used. If possible, hash data at ingestion time.\\nData engineers cannot avoid working with highly sensitive data in\\nsome cases. Some analytics systems must present identifiable,\\nsensitive information. Engineers must act under the highest ethical\\nstandards whenever they handle sensitive data. In addition, they can\\nput in place a variety of practices to reduce the direct handling of\\nsensitive data. Aim as much as possible for touchless production\\nwhere sensitive data is involved. This means that engineers develop\\nand test code on simulated or cleansed data in development and\\nstaging environments but automated code deployments to\\nproduction.\\nTouchless production is an ideal that engineers should strive for, but\\nsituations inevitably arise that cannot be fully solved in development\\nand staging environments. Some bugs may not be reproducible\\nwithout looking at the live data that is triggering a regression. For\\nthese cases, put a broken-glass process in place: require at least\\ntwo people to approve access to sensitive data in the production\\nenvironment. This access should be tightly scoped to a particular\\nissue and come with an expiration date.\\nOur last bit of advice on sensitive data: be wary of naive\\ntechnological solutions to human problems. Both encryption and\\ntokenization are often treated like privacy magic bullets. Most cloud-\\nbased storage systems and nearly all databases encrypt data at rest\\nand in motion by default. Generally, we don’t see encryption\\nproblems but data access problems. Is the solution to apply an extra\\nlayer of encryption to a single field or to control access to that field?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6630b306-b34d-4f67-a5cb-3bc753806c86', embedding=None, metadata={'page_label': '409', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='After all, one must still tightly manage access to the encryption key.\\nLegitimate use cases exist for single-field encryption, but watch out\\nfor ritualistic encryption.\\nOn the tokenization front, use common sense and assess data\\naccess scenarios. If someone had the email of one of your\\ncustomers, could they easily hash the email and find the customer in\\nyour data? Thoughtlessly hashing data without salting and other\\nstrategies may not protect privacy as well as you think.\\nDataOps\\nReliable data pipelines are the cornerstone of the data engineering\\nlifecycle. When they fail, all downstream dependencies come to a\\nscreeching halt. Data warehouses and data lakes aren’t replenished\\nwith fresh data, and data scientists and analysts can’t effectively do\\ntheir jobs; the business is forced to fly blind.\\nEnsuring that your data pipelines are properly monitored is a crucial\\nstep toward reliability and effective incident response. If there’s one\\nstage in the data engineering lifecycle where monitoring is critical,\\nit’s in the ingestion stage. Weak or nonexistent monitoring means the\\npipelines may or may not be working. Referring back to our earlier\\ndiscussion on time, be sure to track the various aspects of time—\\nevent creation, ingestion, process, and processing times. Your data\\npipelines should predictably process data in batches or streams.\\nWe’ve seen countless examples of reports and ML models\\ngenerated from stale data. In one extreme case, an ingestion\\npipeline failure wasn’t detected for six months. (One might question\\nthe concrete utility of the data in this instance, but that’s another\\nmatter.) This was very much avoidable through proper monitoring.\\nWhat should you monitor? Uptime, latency, and data volumes\\nprocessed are good places to start. If an ingestion job fails, how will\\nyou respond? In general, build monitoring into your pipelines from\\nthe beginning rather than waiting for deployment.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02cbd0af-f0ce-4e0d-af62-8283fc82fd48', embedding=None, metadata={'page_label': '410', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Monitoring is key, and knowledge of the behavior of the upstream\\nsystems you depend on and how they generate data. You should be\\naware of the number of events generated per time interval you’re\\nconcerned with (events/minute, events/second, and so on) and the\\naverage size of each event. Your data pipeline should handle both\\nthe frequency and size of the events you’re ingesting.\\nThis also applies to third-party services. In the case of these\\nservices, what you’ve gained in terms of lean operational efficiencies\\n(reduced headcount) is replaced by systems you depend on being\\noutside of your control. If you’re using a third-party service (cloud,\\ndata integration service, etc.), how will you be alerted if there’s an\\noutage? What’s your response plan if a service you depend on\\nsuddenly goes offline?\\nSadly, no universal response plan exists for third-party failures. If you\\ncan fail over to other servers, preferably in another zone or region,\\ndefinitely set this up.\\nIf your data ingestion processes are built internally, do you have the\\nproper testing and deployment automation to ensure that the code\\nfunctions in production? And if the code is buggy or fails, can you roll\\nit back to a working version?\\nData-quality tests\\nWe often refer to data as a silent killer. If quality, valid data is the\\nfoundation of success in today’s businesses, using bad data to make\\ndecisions is much worse than having no data. Bad data has caused\\nuntold damage to businesses; these data disasters are sometimes\\ncalled datastrophes.\\nData is entropic; it often changes in unexpected ways without\\nwarning. One of the inherent differences between DevOps and\\nDataOps is that we expect software regressions only when we\\ndeploy changes, while data often presents regressions\\nindependently because of events outside our control.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='10465e56-0047-40ba-8e8f-64e019fab7a7', embedding=None, metadata={'page_label': '411', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='DevOps engineers are typically able to detect problems by using\\nbinary conditions. Has the request failure rate breached a certain\\nthreshold? How about response latency? In the data space,\\nregressions often manifest as subtle statistical distortions. Is a\\nchange in search-term statistics a result of customer behavior? Of a\\nspike in bot traffic that has escaped the net? Of a site test tool\\ndeployed in some other part of the company?\\nLike system failures in DevOps, some data regressions are\\nimmediately visible. For example, in the early 2000s, Google\\nprovided search terms to websites when users arrived from search.\\nIn 2011, Google began withholding this information in some cases to\\nprotect user privacy better. Analysts quickly saw “not provided”\\nbubbling to the tops of their reports.\\nThe truly dangerous data regressions are silent and can come from\\ninside or outside a business. Application developers may change the\\nmeaning of database fields without adequately communicating with\\ndata teams. Changes to data from third-party sources may go\\nunnoticed. In the best-case scenario, reports break in obvious ways.\\nOften business metrics are distorted unbeknownst to decision\\nmakers.\\nWhenever possible, work with software engineers to fix data-quality\\nissues at the source. It’s surprising how many data-quality issues\\ncan be handled by respecting basic best practices in software\\nengineering, such as logs to capture the history of data changes,\\nchecks (nulls, etc.), and exception handling (try, catch, etc.).\\nTraditional data testing tools are generally built on simple binary\\nlogic. Are nulls appearing in a non-nullable field? Are new,\\nunexpected items showing up in a categorical column? Statistical\\ndata testing is a new realm, but one that is likely to grow dramatically\\nin the next five years.\\nOrchestration\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ec956b31-bdc1-4365-90e5-9abddcd8168b', embedding=None, metadata={'page_label': '412', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ingestion generally sits at the beginning of a large and complex data\\ngraph; since ingestion is the first stage of the data engineering\\nlifecycle, ingested data will flow into many more data processing\\nsteps, and data from many sources will commingle in complex ways.\\nAs we’ve emphasized throughout this book, orchestration is a crucial\\nprocess for coordinating these steps.\\nOrganizations in an early stage of data maturity may choose to\\ndeploy ingestion processes as simple scheduled cron jobs. However,\\nit is crucial to recognize that this approach is brittle and can slow the\\nvelocity of data engineering deployment and development.\\nAs data pipeline complexity grows, true orchestration is necessary.\\nBy true orchestration, we mean a system capable of scheduling\\ncomplete task graphs rather than individual tasks. An orchestration\\ncan start each ingestion task at the appropriate scheduled time.\\nDownstream processing and transform steps begin as ingestion\\ntasks are completed. Further downstream, processing steps lead to\\nadditional processing steps.\\nSoftware Engineering\\nThe ingestion stage of the data engineering lifecycle is engineering\\nintensive. This stage sits at the edge of the data engineering domain\\nand often interfaces with external systems, where software and data\\nengineers have to build a variety of custom plumbing.\\nBehind the scenes, ingestion is incredibly complicated, often with\\nteams operating open source frameworks like Kafka or Pulsar, or\\nsome of the biggest tech companies running their own forked or\\nhomegrown ingestion solutions. As discussed in this chapter,\\nmanaged data connectors have simplified the ingestion process,\\nsuch as Fivetran, Matillion, and Airbyte. Data engineers should take\\nadvantage of the best available tools—primarily, managed tools and\\nservices that do a lot of the heavy lifting for you—and develop high\\nsoftware development competency in areas where it matters. It pays', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21cfb4b1-afa8-4a53-a84e-92001b3cc3fe', embedding=None, metadata={'page_label': '413', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='to use proper version control and code review processes and\\nimplement appropriate tests even for any ingestion-related code.\\nWhen writing software, your code needs to be decoupled. Avoid\\nwriting monolithic systems with tight dependencies on the source or\\ndestination systems.\\nConclusion\\nIn your work as a data engineer, ingestion will likely consume a\\nsignificant part of your energy and effort. At the heart, ingestion is\\nplumbing, connecting pipes to other pipes, ensuring that data flows\\nconsistently and securely to its destination. At times, the minutiae of\\ningestion may feel tedious, but the exciting data applications (e.g.,\\nanalytics and ML) cannot happen without it.\\nAs we’ve emphasized, we’re also in the midst of a sea change,\\nmoving from batch toward streaming data pipelines. This is an\\nopportunity for data engineers to discover interesting applications for\\nstreaming data, communicate these to the business, and deploy\\nexciting new technologies.\\nAdditional Resources\\nGoogle Cloud’s “Streaming Pipelines” web page\\nMicrosoft’s “Snapshot Window (Azure Stream Analytics)”\\ndocumentation\\nAirbyte’s “Connections and Sync Modes” web page\\n1  Andy Petrella, “Datastrophes,” Medium, March 1, 2021,\\nhttps://oreil.ly/h6FRW.\\n2  Danny Sullivan, “Dark Google: One Year Since Search Terms Went ‘Not\\nProvided,’” MarTech, October 19, 2012, https://oreil.ly/Fp8ta', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='14e3f7d6-f7dc-4100-aba3-876575db2c6b', embedding=None, metadata={'page_label': '414', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 8. Queries, Modeling, andTransformation\\nUp to this point, the stages of the data engineering lifecycle have primarily been about passing datafrom one place to another or storing it. In this chapter, you’ll learn how to make data useful. Byunderstanding queries, modeling, and transformations (see Figure 8-1), you’ll have the tools to turn rawdata ingredients into something consumable by downstream stakeholders.\\nFigure 8-1. Transformations allow us to create value from data\\nWe’ll first discuss queries and the significant patterns underlying them. Second, we will look at the majordata modeling patterns you can use to introduce business logic into your data. Then, we’ll covertransformations, which take the logic of your data models and the results of queries and make themuseful for more straightforward downstream consumption. Finally, we’ll cover whom you’ll work with andthe undercurrents as they relate to this chapter.\\nA variety of techniques can be used to query, model, and transform data in SQL and NoSQL databases.This section focuses on queries made to an OLAP system, such as a data warehouse or data lake.Although many languages exist for querying, for the sake of convenience and familiarity, throughoutmost of this chapter, we’ll focus heavily on SQL, the most popular and universal query language. Mostof the concepts for OLAP databases and SQL will translate to other types of databases and querylanguages. This chapter assumes you have an understanding of the SQL language, and relatedconcepts like primary and foreign keys. If these ideas are unfamiliar to you, countless resources areavailable to help you get started.\\nA note on the terms used in this chapter. For convenience, we’ll use the term database as a shorthandfor a query engine and the storage it’s querying; this could be a cloud data warehouse or Apache Sparkquerying data stored in S3. We assume the database has a storage engine that organizes the dataunder the hood. This extends to file-based queries (loading a CSV file into a Python notebook) andqueries against file formats such as Parquet.\\nAlso, note that this chapter focuses mainly on the query, modeling patterns, and transformations relatedto structured and semistructured data, which data engineers use often. Many of the practices discussedcan also be applied to working with unstructured data such as images, video, and raw text.\\nBefore we get into modeling and transforming data, let’s look at queries—what they are, how they work,considerations for improving query performance, and queries on streaming data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='399ed809-1a25-4a52-a33a-2f2e124e79c4', embedding=None, metadata={'page_label': '415', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Queries\\nQueries are a fundamental part of data engineering, data science, and analysis. Before you learn aboutthe underlying patterns and technologies for transformations, you need to understand what queries are,how they work on various data, and techniques for improving query performance.\\nThis section primarily concerns itself with queries on tabular and semistructured data. As a dataengineer, you’ll most frequently query and transform these data types. Before we get into morecomplicated topics about queries and transformations, let’s start by answering a pretty simple question:what is a query?\\nWhat Is a Query?\\nWe often run into people who know how to write SQL but are unfamiliar with how a query works underthe hood. Some of this introductory material on queries will be familiar to experienced data engineers;feel free to skip ahead if this applies to you.\\nA query allows you to retrieve and act on data. Recall our conversation in Chapter 5 about CRUD.When a query retrieves data, it is issuing a request to read a pattern of records. This is the R (read) inCRUD. You might issue a query that gets all records from a table foo, such as SELECT * FROM foo.Or, you might apply a predicate (logical condition) to filter your data by retrieving only records where theid is 1, using the SQL query SELECT * FROM foo WHERE id=1.\\nMany databases allow you to create, update, and delete data. These are the CUD in CRUD; your querywill either create, mutate, or destroy existing records. Let’s review some other common acronyms you’llrun into when working with query languages.\\nData definition language\\nAt a high level, you first need to create the database objects before adding data. You’ll use datadefinition language (DDL) commands to perform operations on database objects, such as the databaseitself, schemas, tables, or users; DDL defines the state of objects in your database.\\nData engineers use common SQL DDL expressions: CREATE, DROP, and UPDATE. For example, youcan create a database by using the DDL expression CREATE DATABASE bar. After that, you can alsocreate new tables (CREATE table bar_table) or delete a table (DROP table bar_table).\\nData manipulation language\\nAfter using DDL to define database objects, you need to add and alter data within these objects, whichis the primary purpose of data manipulation language (DML). Some common DML commands you’ll useas a data engineer are as follows:\\nSELECT INSERT UPDATE DELETE COPY MERGE\\nFor example, you can INSERT new records into a database table, UPDATE existing ones, and SELECTspecific records.\\nData control language\\nYou most likely want to limit access to database objects and finely control who has access to what. Datacontrol language (DCL) allows you to control access to the database objects or the data by using SQLcommands such as GRANT, DENY, and REVOKE.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c723cbc8-b07d-4f91-b38e-c7567540d23b', embedding=None, metadata={'page_label': '416', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Let’s walk through a brief example using DCL commands. A new data scientist named Sarah joins yourcompany, and she needs read-only access to a database called data_science_db. You give Sarahaccess to this database by using the following DCL command:\\nGRANT SELECT ON data_science_db TO user_name Sarah;\\nIt’s a hot job market, and Sarah has worked at the company for only a few months before gettingpoached by a big tech company. So long, Sarah! Being a security-minded data engineer, you removeSarah’s ability to read from the database:\\nREVOKE SELECT ON data_science_db TO user_name Sarah;\\nAccess-control requests and issues are common, and understanding DCL will help you resolveproblems if you or a team member can’t access the data they need, as well as prevent access to datathey don’t need.\\nTCL\\nTCL stands for transaction control language. As the name suggests, TCL supports commands thatcontrol the details of transactions. With TCL, we can define commit checkpoints, conditions whenactions will be rolled back, and more. Two common TCL commands include COMMIT and ROLLBACK.\\nThe Life of a Query\\nHow does a query work, and what happens when a query is executed? Let’s cover the high-level basicsof query execution (Figure 8-2), using an example of a typical SQL query executing in a database.\\nFigure 8-2. The life of a SQL query in a database\\nWhile running a query might seem simple—write code, run it, and get results—a lot is going on underthe hood. When you execute a SQL query, here’s a summary of what happens:\\n1. The database engine compiles the SQL, parsing the code to check for proper semantics andensuring that the database objects referenced exist and that the current user has the appropriateaccess to these objects.\\n2. The SQL code is converted into bytecode. This bytecode expresses the steps that must beexecuted on the database engine in an efficient, machine-readable format.\\n3. The database’s query optimizer analyzes the bytecode to determine how to execute the query,reordering and refactoring steps to use available resources as efficiently as possible.\\n4. The query is executed, and results are produced.\\nThe Query Optimizer\\nQueries can have wildly different execution times, depending on how they’re executed. A queryoptimizer’s job is to optimize query performance and minimize costs by breaking the query intoappropriate steps in an efficient order. The optimizer will assess joins, indexes, data scan size, andother factors. The query optimizer attempts to execute the query in the least expensive manner.\\nQuery optimizers are fundamental to how your query will perform. Every database is different andexecutes queries in ways that are obviously and subtly different from each other. You won’t directly workwith a query optimizer, but understanding some of its functionality will help you write more performant', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c8cff9f-db38-4927-9341-0824883412bd', embedding=None, metadata={'page_label': '417', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='queries. You’ll need to know how to analyze a query’s performance, using things like an explain plan orquery analysis, described in the following section.\\nImproving Query Performance\\nIn data engineering, you’ll inevitably encounter poorly performing queries. Knowing how to identify andfix these queries is invaluable. Don’t fight your database. Learn to work with its strengths and augmentits weaknesses. This section shows various ways to improve your query performance.\\nOptimize your JOIN strategy and schema\\nA single dataset (such as a table or file) is rarely useful on its own; we create value by combining it withother datasets. Joins are one of the most common means of combining datasets and creating newones. We assume that you’re familiar with the significant types of joins (e.g., inner, outer, left, cross) andthe types of join relationships (e.g., one to one, one to many, many to one, and many to many).\\nJoins are critical in data engineering and are well supported and performant in many databases. Evencolumnar databases, which in the past had a reputation for slow join performance, now generally offerexcellent performance.\\nA common technique for improving query performance is to pre-join data. If you find that analyticsqueries are joining the same data repeatedly, it often makes sense to join the data in advance and havequeries read from the pre-joined version of the data so that you’re not repeating computationallyintensive work. This may mean changing the schema and relaxing normalization conditions to widentables and utilize newer data structures (such as arrays or structs) for replacing frequently joined entityrelationships. Another strategy is maintaining a more normalized schema but pre-joining tables for themost common analytics and data science use cases. We can simply create pre-joined tables and trainusers to utilize these or join inside materialized views (see “Materialized Views, Federation, and QueryVirtualization”).\\nNext, consider the details and complexity of your join conditions. Complex join logic may consumesignificant computational resources. We can improve performance for complex joins in a few ways.\\nMany row-oriented databases allow you to index a result computed from a row. For instance,PostgreSQL allows you to create an index on a string field converted to lowercase; when the optimizerencounters a query where the lower() function appears inside a predicate, it can apply the index. Youcan also create a new derived column for joining, though you will need to train users to join on thiscolumn.\\nROW EXPLOSION\\nAn obscure but frustrating problem is row explosion. This occurs when we have a large number ofmany-to-many matches, either because of repetition in join keys or as a consequence of join logic.Suppose the join key in table A has the value this repeated five times, and the join key in table Bcontains this same value repeated 10 times. This leads to a cross-join of these rows: every thisrow from table A paired with every this row from table B. This creates 5 × 10 = 50 rows in theoutput. Now suppose that many other repeats are in the join key. Row explosion often generatesenough rows to consume a massive quantity of database resources or even cause a query to fail.\\nIt is also essential to know how your query optimizer handles joins. Some databases can reorderjoins and predicates, while others cannot. A row explosion in an early query stage may cause thequery to fail, even though a later predicate should correctly remove many of the repeats in theoutput. Predicate reordering can significantly reduce the computational resources required by aquery.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b789a89e-f49a-4a5f-8b3a-6e28cefe673f', embedding=None, metadata={'page_label': '418', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Finally, use common table expressions (CTEs) instead of nested subqueries or temporary tables. CTEsallow users to compose complex queries together in a readable fashion, helping you understand theflow of your query. The importance of readability for complex queries cannot be understated.\\nIn many cases, CTEs will also deliver better performance than a script that creates intermediate tables;if you have to create intermediate tables, consider creating temporary tables. If you’d like to learn moreabout CTEs, a quick web search will yield plenty of helpful information.\\nUse the explain plan and understand your query’s performance\\nAs you learned in the preceding section, the database’s query optimizer influences the execution of aquery. The query optimizer’s explain plan will show you how the query optimizer determined its optimumlowest-cost query, the database objects used (tables, indexes, cache, etc.), and various resourceconsumption and performance statistics in each query stage. Some databases provide a visualrepresentation of query stages. In contrast, others make the explain plan available via SQL with theEXPLAIN command, which displays the sequence of steps the database will take to execute the query.\\nIn addition to using EXPLAIN to understand how your query will run, you should monitor your query’sperformance, viewing metrics on database resource consumption. The following are some areas tomonitor:\\nUsage of key resources such as disk, memory, and network.\\nData loading time versus processing time.\\nQuery execution time, number of records, the size of the data scanned, and the quantity of datashuffled.\\nCompeting queries that might cause resource contention in your database.\\nNumber of concurrent connections used versus connections available. Oversubscribed concurrentconnections can have negative effects on your users who may not be able to connect to thedatabase.\\nAvoid full table scans\\nAll queries scan data, but not all scans are created equal. As a rule of thumb, you should query only thedata you need. When you run SELECT * with no predicates, you’re scanning the entire table andretrieving every row and column. This is very inefficient performance-wise and expensive, especially ifyou’re using a pay-as-you-go database that charges you either for bytes scanned or compute resourcesutilized while a query is running.\\nWhenever possible, use pruning to reduce the quantity of data scanned in a query. Columnar and row-oriented databases require different pruning strategies. In a column-oriented database, you shouldselect only the columns you need. Most column-oriented OLAP databases also provide additional toolsfor optimizing your tables for better query performance. For instance, if you have a very large table(several terabytes in size or greater) Snowflake and BigQuery give you the option to define a cluster keyon a table, which orders the table’s data in a way that allows queries to more efficiently access portionsof very large datasets. BigQuery also allows you to partition a table into smaller segments, allowing youto query only specific partitions instead of the entire table. (Be aware that inappropriate clustering andkey distribution strategies can degrade performance.)\\nIn row-oriented databases, pruning usually centers around table indexes, which you learned inChapter 6. The general strategy is to create table indexes that will improve performance for your mostperformance-sensitive queries while not overloading the table with so many indexes such that youdegrade performance.\\nKnow how your database handles commits', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7f7cff4e-60ee-4ff4-b027-7d166b3e0f2d', embedding=None, metadata={'page_label': '419', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A database commit is a change within a database, such as creating, updating, or deleting a record,table, or other database objects. Many databases support transactions—i.e., a notion of committingseveral operations simultaneously in a way that maintains a consistent state. Please note that the termtransaction is somewhat overloaded; see Chapter 5. The purpose of a transaction is to keep aconsistent state of a database both while it’s active and in the event of a failure. Transactions alsohandle isolation when multiple concurrent events might be reading, writing, and deleting from the samedatabase objects. Without transactions, users would get potentially conflicting information whenquerying a database.\\nYou should be intimately familiar with how your database handles commits and transactions, anddetermine the expected consistency of query results. Does your database handle writes and updates inan ACID-compliant manner? Without ACID compliance, your query might return unexpected results.This could result from a dirty read, which happens when a row is read and an uncommitted transactionhas altered the row. Are dirty reads an expected behavior of your database? If so, how do you handlethis? Also, be aware that during update and delete transactions, some databases create new files torepresent the new state of the database and retain the old files for failure checkpoint references. Inthese databases, running a large number of small commits can lead to clutter and consume significantstorage space that might need to be vacuumed periodically.\\nLet’s briefly consider three databases to understand the impact of commits (note these examples arecurrent as of the time of this writing). First, suppose we’re looking at a PostgreSQL RDBMS andapplying ACID transactions. Each transaction consists of a package of operations that will either fail orsucceed as a group. We can also run analytics queries across many rows; these queries will present aconsistent picture of the database at a point in time.\\nThe disadvantage of the PostgreSQL approach is that it requires row locking (blocking reads and writesto certain rows), which can degrade performance in various ways. PostgreSQL is not optimized for largescans or the massive amounts of data appropriate for large-scale analytics applications.\\nNext, consider Google BigQuery. It utilizes a point-in-time full table commit model. When a read query isissued, BigQuery will read from the latest committed snapshot of the table. Whether the query runs forone second or two hours, it will read only from that snapshot and will not see any subsequent changes.BigQuery does not lock the table while I read from it. Instead, subsequent write operations will createnew commits and new snapshots while the query continues to run on the snapshot where it started.\\nTo prevent the inconsistent state, BigQuery allows only one write operation at a time. In this sense,BigQuery provides no write concurrency whatsoever. (In the sense that it can write massive amounts ofdata in parallel inside a single write query, it is highly concurrent.) If more than one client attempts towrite simultaneously, write queries are queued in order of arrival. BigQuery’s commit model is similar tothe commit models used by Snowflake, Spark, and others.\\nLast, let’s consider MongoDB. We refer to MongoDB as a variable consistency database. Engineershave various configurable consistency options, both for the database and at the level of individualqueries. MongoDB is celebrated for its extraordinary scalability and write concurrency but is somewhatnotorious for issues that arise when engineers abuse it.\\nFor instance, in certain modes, MongoDB supports ultra-high write performance. However, this comes ata cost: the database will unceremoniously and silently discard writes if it gets overwhelmed with traffic.This is perfectly suitable for applications that can stand to lose some data—for example, IoTapplications where we simply want many measurements but don’t care about capturing allmeasurements. It is not a great fit for applications that need to capture exact data and statistics.\\nNone of this is to say these are bad databases. They’re all fantastic databases when they are chosenfor appropriate applications and configured correctly. The same goes for virtually any databasetechnology.\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc461519-0004-4ea5-ae68-5571a11153c9', embedding=None, metadata={'page_label': '420', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Companies don’t hire engineers simply to hack on code in isolation. To be worthy of their title, engineersshould develop a deep understanding of the problems they’re tasked with solving and the technologytools. This applies to commit and consistency models and every other aspect of technologyperformance. Appropriate technology choices and configuration can ultimately differentiate extraordinarysuccess and massive failure. Refer to Chapter 6 for a deeper discussion of consistency.\\nVacuum dead records\\nAs we just discussed, transactions incur the overhead of creating new records during certain operations,such as updates, deletes, and index operations, while retaining the old records as pointers to the laststate of the database. As these old records accumulate in the database filesystem, they eventually nolonger need to be referenced. You should remove these dead records in a process called vacuuming.\\nYou can vacuum a single table, multiple tables, or all tables in a database. No matter how you choose tovacuum, deleting dead database records is important for a few reasons. First, it frees up space for newrecords, leading to less table bloat and faster queries. Second, new and relevant records mean queryplans are more accurate; outdated records can lead the query optimizer to generate suboptimal andinaccurate plans. Finally, vacuuming cleans up poor indexes, allowing for better index performance.\\nVacuum operations are handled differently depending on the type of database. For example, indatabases backed by object storage (BigQuery, Snowflake, Databricks), the only downside of old dataretention is that it uses storage space, potentially costing money depending on the storage pricingmodel for the database. In Snowflake, users cannot directly vacuum. Instead, they control a “time-travel”interval that determines how long table snapshots are retained before they are auto vacuumed.BigQuery utilizes a fixed seven-day history window. Databricks generally retains data indefinitely until itis manually vacuumed; vacuuming is important to control direct S3 storage costs.\\nAmazon Redshift handles its cluster disks in many configurations, and vacuuming can impactperformance and available storage. VACUUM runs automatically behind the scenes, but users maysometimes want to run it manually for tuning purposes.\\nVacuuming becomes even more critical for relational databases such as PostgreSQL and MySQL.Large numbers of transactional operations can cause a rapid accumulation of dead records, andengineers working in these systems need to familiarize themselves with the details and impact ofvacuuming.\\nLeverage cached query results\\nLet’s say you have an intensive query that you often run on a database that charges you for the amountof data you query. Each time a query is run, this costs you money. Instead of rerunning the same queryon the database repeatedly and incurring massive charges, wouldn’t it be nice if the results of the querywere stored and available for instant retrieval? Thankfully, many cloud OLAP databases cache queryresults.\\nWhen a query is initially run, it will retrieve data from various sources, filter and join it, and output aresult. This initial query—a cold query—is similar to the notion of cold data we explored in Chapter 6.For argument’s sake, let’s say this query took 40 seconds to run. Assuming your database caches queryresults, rerunning the same query might return results in 1 second or less. The results were cached, andthe query didn’t need to run cold. Whenever possible, leverage query cache results to help reducepressure on your database while providing a better user experience for frequently run queries. Note alsothat materialized views provide another form of query caching (see “Materialized Views, Federation, andQuery Virtualization”).\\nQueries on Streaming Data\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c7cd2b8-aca5-46a2-a9c5-ca423cdd0fdc', embedding=None, metadata={'page_label': '421', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Streaming data is constantly in flight. As you might imagine, querying streaming data is different frombatch data. To fully take advantage of a data stream, we must adapt query patterns that reflect its real-time nature. For example, systems such as Kafka and Pulsar make it easier to query streaming datasources. Let’s look at some common ways to do this.\\nBasic query patterns on streams\\nRecall continuous CDC, discussed in Chapter 7. CDC, in this form, essentially sets up an analyticsdatabase as a fast follower to a production database. One of the longest-standing streaming querypatterns simply entails querying the analytics database, retrieving statistical results and aggregationswith a slight lag behind the production database.\\nThe fast-follower approach\\nHow is this a streaming query pattern? Couldn’t we accomplish the same thing simply by running ourqueries on the production database? In principle, yes; in practice, no. Production databases generallyaren’t equipped to handle production workloads and simultaneously run large analytics scans acrosssignificant quantities of data. Running such queries can slow the production application or even cause itto crash. The basic CDC query pattern allows us to serve real-time analytics with a minimal impact onthe production system.\\nThe fast-follower pattern can utilize a conventional transactional database as the follower, but there aresignificant advantages to using a proper OLAP-oriented system (Figure 8-3). Both Druid and BigQuerycombine a streaming buffer with long-term columnar storage in a setup somewhat similar to the Lambdaarchitecture (see Chapter 3). This works extremely well for computing trailing statistics on vast historicaldata with near real-time updates.\\nFigure 8-3. CDC with a fast-follower analytics database\\nThe fast-follower CDC approach has critical limitations. It doesn’t fundamentally rethink batch querypatterns. You’re still running SELECT queries against the current table state, and missing the opportunityto dynamically trigger events off changes in the stream.\\nThe Kappa architecture\\nNext, recall the Kappa architecture we discussed in Chapter 3. The principal idea of this architecture isto handle all data like events and store these events as a stream rather than a table (Figure 8-4). Whenproduction application databases are the source, Kappa architecture stores events from CDC. Eventstreams can also flow directly from an application backend, from a swarm of IoT devices, or any systemthat generates events and can push them over a network. Instead of simply treating a streaming storagesystem as a buffer, Kappa architecture retains events in storage during a more extended retentionperiod, and data can be directly queried from this storage. The retention period can be pretty long(months or years). Note that this is much longer than the retention period used in purely real-timeoriented systems, usually a week at most.\\nFigure 8-4. The Kappa architecture is built around streaming storage and ingest systems\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='042a654f-a161-4f51-916f-c0bc2f267b74', embedding=None, metadata={'page_label': '422', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The “big idea” in Kappa architecture is to treat streaming storage as a real-time transport layer and adatabase for retrieving and querying historical data. This happens either through the direct querycapabilities of the streaming storage system or with the help of external tools. For example, Kafka KSQLsupports aggregation, statistical calculations, and even sessionization. If query requirements are morecomplex or data needs to be combined with other data sources, an external tool such as Spark reads atime range of data from Kafka and computes the query results. The streaming storage system can alsofeed other applications or a stream processor such as Flink or Beam.\\nWindows, triggers, emitted statistics, and late-arriving data\\nOne fundamental limitation of traditional batch queries is that this paradigm generally treats the queryengine as an external observer. An actor external to the data causes the query to run—perhaps anhourly cron job or a product manager opening a dashboard.\\nMost widely used streaming systems, on the other hand, support the notion of computations triggereddirectly from the data itself. They might emit mean and median statistics every time a certain number ofrecords are collected in the buffer or output a summary when a user session closes.\\nWindows are an essential feature in streaming queries and processing. Windows are small batches thatare processed based on dynamic triggers. Windows are generated dynamically over time in some ways.Let’s look at some common types of windows: session, fixed-time, and sliding. We’ll also look atwatermarks.\\nSession window\\nA session window groups events that occur close together, and filters out periods of inactivity when noevents occur. We might say that a user session is any time interval with no inactivity gap of five minutesor more. Our batch system collects data by a user ID key, orders events, determines the gaps andsession boundaries, and calculates statistics for each session. Data engineers often sessionize dataretrospectively by applying time conditions to user activity on web and desktop apps.\\nIn a streaming session, this process can happen dynamically. Note that session windows are per key; inthe preceding example, each user gets their own set of windows. The system accumulates data peruser. If a five-minute gap with no activity occurs, the system closes the window, sends its calculations,and flushes the data. If new events arrive for the use, the system starts a new session window.\\nSession windows may also make a provision for late-arriving data. Allowing data to arrive up to fiveminutes late to account for network conditions and system latency, the system will open the window if alate-arriving event indicates activity less than five minutes after the last event. We will have more to sayabout late-arriving data throughout this chapter. Figure 8-5 shows three session windows, eachseparated by five minutes of inactivity.\\nFigure 8-5. Session window with a five-minute timeout for inactivity\\nMaking sessionization dynamic and near real-time fundamentally changes its utility. With retrospectivesessionization, we could automate specific actions a day or an hour after a user session closed (e.g., afollow-up email with a coupon for a product viewed by the user). With dynamic sessionization, the user', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4ee44994-f5a8-412e-865e-d8ea3df6d1a3', embedding=None, metadata={'page_label': '423', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='could get an alert in a mobile app that is immediately useful based on their activity in the last 15minutes.\\nFixed-time windows\\nA fixed-time (aka tumbling) window features fixed time periods that run on a fixed schedule andprocesses all data since the previous window is closed. For example, we might close a window every 20seconds and process all data arriving from the previous window to give a mean and median statistic(Figure 8-6). Statistics would be emitted as soon as they could be calculated after the window closed.\\nFigure 8-6. Tumbling/fixed window\\nThis is similar to traditional batch ETL processing, where we might run a data update job every day orevery hour. The streaming system allows us to generate windows more frequently and deliver resultswith lower latency. As we’ll repeatedly emphasize, batch is a special case of streaming.\\nSliding windows\\nEvents in a sliding window are bucketed into windows of fixed time length. For example, we couldgenerate a new 60-second window every 30 seconds (Figure 8-7). Just as we did before, we can emitmean and median statistics. Sliding windows are also known as hopping windows.\\nFigure 8-7. Sliding windows\\nThe sliding can vary. For example, we might think of the window as truly sliding continuously butemitting statistics only when certain conditions (triggers) are met. Suppose we used a 30-secondcontinuously sliding window but calculated a statistic only when a user clicked a particular banner. Thiswould lead to an extremely high rate of output when many users click the banner, and no calculationsduring a lull.\\nWatermarks\\nWe’ve covered various types of windows and their uses. As discussed in Chapter 7, data is sometimesingested out of the order from which it originated. A watermark (Figure 8-8) is a threshold used by awindow to determine whether data in a window is within the established time interval or whether it’sconsidered late. If data arrives that is new to the window but older than the timestamp of the watermark,it is considered to be late-arriving data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cec2900d-32e1-4fdf-a481-ae5df8607b11', embedding=None, metadata={'page_label': '424', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 8-8. Watermark acting as a threshold for late-arriving data\\nCombining streams with other data\\nAs we’ve mentioned before, we often derive value from data by combining it with other data. Streamingdata is no different. For instance, multiple streams can be combined, or a stream can be combined withbatch historical data.\\nConventional table joins\\nSome tables may be fed by streams (Figure 8-9). The most basic approach to this problem is simplyjoining these two tables in a database. A stream can feed one or both of these tables.\\nFigure 8-9. Joining two tables fed by streams\\nEnrichment\\nEnrichment means that we join a stream to other data (Figure 8-10). Typically, this is done to provideenhanced data into another stream. For example, suppose that an online retailer receives an eventstream from a partner business containing product and user IDs. The retailer wishes to enhance theseevents with product details and demographic information on the users. The retailer feeds these eventsto a serverless function that looks up the product and user in an in-memory database (say, a cache),adds the required information to the event, and outputs the enhanced events to another stream.\\nFigure 8-10. In this example, a stream is enriched with data residing in object storage, resulting in a new enriched dataset\\nIn practice, the enrichment source could originate almost anywhere—a table in a cloud data warehouseor RDBMS, or a file in object storage. It’s simply a question of reading from the source and storing therequisite enrichment data in an appropriate place for retrieval by the stream.\\nStream-to-stream joining', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='17c43733-612f-40a4-9eeb-d98005da264e', embedding=None, metadata={'page_label': '425', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Increasingly, streaming systems support direct stream-to-stream joining. Suppose that an online retailerwishes to join its web event data with streaming data from an ad platform. The company can feed bothstreams into Spark, but a variety of complications arise. For instance, the streams may have significantlydifferent latencies for arrival at the point where the join is handled in the streaming system. The adplatform may provide its data with a five-minute delay. In addition, certain events may be significantlydelayed—for example, a session close event for a user, or an event that happens on the phone offlineand shows up in the stream only after the user is back in mobile network range.\\nAs such, typical streaming join architectures rely on streaming buffers. The buffer retention interval isconfigurable; a longer retention interval requires more storage and other resources. Events get joinedwith data in the buffer and are eventually evicted after the retention interval has passed (Figure 8-11).\\nFigure 8-11. An architecture to join streams buffers each stream and joins events if related events are found during the bufferretention interval\\nNow that we’ve covered how queries work for batch and streaming data, let’s discuss making your datauseful by modeling it.\\nData Modeling\\nData modeling is something that we see overlooked disturbingly often. We often see data teams jumpinto building data systems without a game plan to organize their data in a way that’s useful for thebusiness. This is a mistake. Well-constructed data architectures must reflect the goals and businesslogic of the organization that relies on this data. Data modeling involves deliberately choosing acoherent structure for data, and is a critical step to make data useful for the business.\\nData modeling has been a practice for decades in one form or another. For example, various types ofnormalization techniques (discussed in “Normalization”) have been used to model data since the earlydays of RDBMSs; data warehousing modeling techniques have been around since at least the early1990s, and arguably longer. As pendulums in technology often go, data modeling became somewhatunfashionable in the early to mid-2010s. The rise of data lake 1.0, NoSQL, and big data systemsallowed engineers to bypass traditional data modeling, sometimes for legitimate performance gains.Other times, the lack of rigorous data modeling created data swamps, along with lots of redundant,mismatched, or simply wrong data.\\nNowadays, the pendulum seems to be swinging back toward data modeling. The growing popularity ofdata management (in particular, data governance and data quality) is pushing the need for coherentbusiness logic. The meteoric rise of data’s prominence in companies creates a growing recognition thatmodeling is critical for realizing value at the higher levels of the Data Science Hierarchy of Needspyramid. On the other hand, we believe that new paradigms are required to truly embrace the needs ofstreaming data and ML. In this section, we survey current data modeling techniques and briefly muse onthe future of data modeling.\\nWhat Is a Data Model?\\nA data model represents the way data relates to the real world. It reflects how the data must bestructured and standardized to best reflect your organization’s processes, definitions, workflows, and\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d8741f1f-c263-4ee6-b3b3-63b8ac31e3e1', embedding=None, metadata={'page_label': '426', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='logic. A good data model captures how communication and work naturally flow within your organization.In contrast, a poor data model (or nonexistent one) is haphazard, confusing, and incoherent.\\nSome data professionals view data modeling as tedious and reserved for “big enterprises.” Like mostgood hygiene practices—such as flossing your teeth and getting a good night’s sleep—data modeling isacknowledged as a good thing to do but is often ignored in practice. Ideally, every organization shouldmodel its data if only to ensure that business logic and rules are translated at the data layer.\\nWhen modeling data, it’s critical to focus on translating the model to business outcomes. A good datamodel should correlate with impactful business decisions. For example, a customer might mean differentthings to different departments in a company. Is someone who’s bought from you over the last 30 days acustomer? What if they haven’t bought from you in the previous six months or a year? Carefully definingand modeling this customer data can have a massive impact on downstream reports on customerbehavior or the creation of customer churn models whereby the time since the last purchase is a criticalvariable.\\nTIP\\nCan you think of concepts or terms in your company that might mean different things to different people?\\nOur discussion focuses mainly on batch data modeling since that’s where most data modelingtechniques arose. We will also look at some approaches to modeling streaming data and generalconsiderations for modeling.\\nConceptual, Logical, and Physical Data Models\\nWhen modeling data, the idea is to move from abstract modeling concepts to concrete implementation.Along this continuum (Figure 8-12), three main data models are conceptual, logical, and physical. Thesemodels form the basis for the various modeling techniques we describe in this chapter:\\nConceptual\\nContains business logic and rules and describes the system’s data, such as schemas, tables, and\\nfields (names and types). When creating a conceptual model, it’s often helpful to visualize it in an\\nentity-relationship (ER) diagram, which is a standard tool for visualizing the relationships among\\nvarious entities in your data (orders, customers, products, etc.). For example, an ER diagram might\\nencode the connections among customer ID, customer name, customer address, and customer\\norders. Visualizing entity relationships is highly recommended for designing a coherent conceptual\\ndata model.\\nLogical\\nDetails how the conceptual model will be implemented in practice by adding significantly more detail.\\nFor example, we would add information on the types of customer ID, customer names, and custom\\naddresses. In addition, we would map out primary and foreign keys.\\nPhysical\\nDefines how the logical model will be implemented in a database system. We would add specific\\ndatabases, schemas, and tables to our logical model, including configuration details.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2375c790-6c77-41d2-9547-7fb43afd997d', embedding=None, metadata={'page_label': '427', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 8-12. The continuum of data models: conceptual, logical, and physical\\nSuccessful data modeling involves business stakeholders at the inception of the process. Engineersneed to obtain definitions and business goals for the data. Modeling data should be a full-contact sportwhose goal is to provide the business with quality data for actionable insights and automation. This is apractice that everyone must continuously participate in.\\nAnother important consideration for data modeling is the grain of the data, which is the resolution atwhich data is stored and queried. The grain is typically at the level of a primary key in a table, such ascustomer ID, order ID, and product ID; it’s often accompanied by a date or timestamp for increasedfidelity.\\nFor example, suppose that a company has just begun to deploy BI reporting. The company is smallenough that the same person is filling the role of data engineer and analyst. A request comes in for areport that summarizes daily customer orders. Specifically, the report should list all customers whoordered, the number of orders they placed that day, and the total amount they spent.\\nThis report is inherently coarse-grained. It contains no details on spending per order or the items in eachorder. It is tempting for the data engineer/analyst to ingest data from the production orders database andboil it down to a reporting table with only the basic aggregated data required for the report. However,this would entail starting over when a request comes in for a report with finer-grained data aggregation.\\nSince the data engineer is actually quite experienced, they elect to create tables with detailed data oncustomer orders, including each order, item, item cost, item IDs, etc. Essentially, their tables contain alldetails on customer orders. The data’s grain is at the customer-order level. This customer-order datacan be analyzed as is, or aggregated for summary statistics on customer order activity.\\nIn general, you should strive to model your data at the lowest level of grain possible. From here, it’seasy to aggregate this highly granular dataset. The reverse isn’t true, and it’s generally impossible torestore details that have been aggregated away.\\nNormalization\\nNormalization is a database data modeling practice that enforces strict control over the relationships oftables and columns within a database. The goal of normalization is to remove the redundancy of datawithin a database and ensure referential integrity. Basically, it’s don’t repeat yourself (DRY) applied todata in a database.\\nNormalization is typically applied to relational databases containing tables with rows and columns (weuse the terms column and field interchangeably in this section). It was first introduced by relationaldatabase pioneer Edgar Codd in the early 1970s.\\nCodd outlined four main objectives of normalization:\\nTo free the collection of relations from undesirable insertion, update, and deletion dependencies\\nTo reduce the need for restructuring the collection of relations, as new types of data are introduced,and thus increase the lifespan of application programs\\nTo make the relational model more informative to users\\nTo make the collection of relations neutral to the query statistics, where these statistics are liable tochange as time goes by\\nCodd introduced the idea of normal forms. The normal forms are sequential, with each formincorporating the conditions of prior forms. We describe Codd’s first three normal forms here:\\n6\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='840a3a78-7f4a-44b7-9cc1-0ce5e4130268', embedding=None, metadata={'page_label': '428', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Denormalized\\nNo normalization. Nested and redundant data is allowed.\\nFirst normal form (1NF)\\nEach column is unique and has a single value. The table has a unique primary key.\\nSecond normal form (2NF)\\nThe requirements of 1NF, plus partial dependencies are removed.\\nThird normal form (3NF)\\nThe requirements of 2NF, plus each table contains only relevant fields related to its primary key and\\nhas no transitive dependencies.\\nIt’s worth spending a moment to unpack a couple of terms we just threw at you. A unique primary key isa single field or set of multiple fields that uniquely determines rows in the table. Each key value occursat most once; otherwise, a value would map to multiple rows in the table. Thus, every other value in arow is dependent on (can be determined from) the key. A partial dependency occurs when a subset offields in a composite key can be used to determine a nonkey column of the table. A transitivedependency occurs when a nonkey field depends on another nonkey field.\\nLet’s look at stages of normalization—from denormalized to 3NF—using an ecommerce example ofcustomer orders (Table 8-1). We’ll provide concrete explanations of each of the concepts introduced inthe previous paragraph.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1a8f4b54-380e-4157-a44f-f8ccfe78540f', embedding=None, metadata={'page_label': '429', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-1. OrderDetail\\nOrderID OrderItems CustomerID CustomerNameOrderDate\\n100\\n[{              \"sku\": 1,              \"price\": 50,\"quantity\": 1,              \"name:\": \"Thingamajig\"}, {              \"sku\": 2,              \"price\": 25,\"quantity\": 2,              \"name:\": \"Whatchamacallit\"}]\\n5 Joe Reis 2022-03-01\\nFirst, this denormalized OrderDetail table contains five fields. The primary key is OrderID. Noticethat the OrderItems field contains a nested object with two SKUs along with their price, quantity, andname.\\nTo convert this data to 1NF, let’s move OrderItems into four fields (Table 8-2). Now we have anOrderDetail table in which fields do not contain repeats or nested data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b5bb03fe-c187-45f9-b929-42efd7743f39', embedding=None, metadata={'page_label': '430', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-2. OrderDetail without repeats or nested data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c7b95548-2131-4f50-a0ad-686377e7821b', embedding=None, metadata={'page_label': '431', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='OrderID Sku Price Quantity ProductName CustomerID CustomerName OrOrderID Sku Price Quantity ProductName CustomerID CustomerName Or\\n100 1 50 1 Thingamajig 5 Joe Reis 20\\n100 2 25 2 Whatchamacallit 5 Joe Reis 20\\nThe problem is that now we don’t have a unique primary key. That is, 100 occurs in the OrderIDcolumn in two different rows. To get a better grasp of the situation, let’s look at a larger sample from ourtable (Table 8-3).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a99d3c9f-af0a-45ea-97b0-7e6bf0e929c9', embedding=None, metadata={'page_label': '432', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-3. OrderDetail with a larger sample\\nOrderID Sku Price Quantity ProductName CustomerID CustomerName Or\\n100 1 50 1 Thingamajig 5 Joe Reis 20\\n100 2 25 2 Whatchamacallit 5 Joe Reis 20\\n101 3 75 1 Whozeewhatzit 7 Matt Housley 20\\n102 1 50 1 Thingamajig 7 Matt Housley 20\\nTo create a unique primary (composite) key, let’s number the lines in each order by adding a columncalled LineItemNumber (Table 8-4).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f633ade2-82f3-435e-b445-c31de9946d81', embedding=None, metadata={'page_label': '433', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-4. OrderDetail with LineItemNumber column\\nOrderID LineItemNumber Sku Price Quantity ProductName CustomerID Cue\\n100 1 1 50 1 Thingamajig 5 Jo', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f61fd063-9ad5-452f-8278-7919aececc57', embedding=None, metadata={'page_label': '434', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='100 2 2 25 2 Whatchamacallit 5 Jo\\n101 1 3 75 1 Whozeewhatzit 7 M\\n102 1 1 50 1 Thingamajig 7 M\\nThe composite key (OrderID, LineItemNumber) is now a unique primary key.\\nTo reach 2NF, we need to ensure that no partial dependencies exist. A partial dependency is a nonkeycolumn that is fully determined by a subset of the columns in the unique primary (composite) key; partialdependencies can occur only when the primary key is composite. In our case, the last three columns aredetermined by order number. To fix this problem, let’s split OrderDetail into two tables: Orders andOrderLineItem (Tables 8-5 and 8-6).\\nTable 8-5. Orders\\nOrderID CustomerID CustomerNameOrderDate\\n100 5 Joe Reis 2022-03-01\\n101 7 Matt Housley 2022-03-01\\n102 7 Matt Housley 2022-03-01', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bf4324e5-d7b8-44e9-b6f8-676aa92ce2c8', embedding=None, metadata={'page_label': '435', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-6. OrderLineItem\\nOrderID LineItemNumberSku Price Quantity ProductName\\n100 1 1 50 1 Thingamajig\\n100 2 2 25 2 Whatchamacallit\\n101 1 3 75 1 Whozeewhatzit\\n102 1 1 50 1 Thingamajig\\nThe composite key (OrderID, LineItemNumber) is a unique primary key for OrderLineItem, whileOrderID is a primary key for Orders.\\nNotice that Sku determines ProductName in OrderLineItem. That is, Sku depends on the compositekey, and ProductName depends on Sku. This is a transitive dependency. Let’s break OrderLineIteminto OrderLineItem and Skus (Tables 8-7 and 8-8).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='87fcf5a8-668b-4efd-975e-79808c6f5dae', embedding=None, metadata={'page_label': '436', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-7. OrderLineItem\\nOrderID LineItemNumberSku Price Quantity\\n100 1 1 50 1\\n100 2 2 25 2\\n101 1 3 75 1\\n102 1 1 50 1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a951458-2620-4a36-9441-ffe17304c338', embedding=None, metadata={'page_label': '437', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-8. Skus\\nSku ProductName\\n1 Thingamajig\\n2 Whatchamacallit\\n3 Whozeewhatzit\\nNow, both OrderLineItem and Skus are in 3NF. Notice that Orders does not satisfy 3NF. Whattransitive dependencies are present? How would you fix this?\\nAdditional normal forms exist (up to 6NF in the Boyce-Codd system), but these are much less commonthan the first three. A database is usually considered normalized if it’s in third normal form, and that’s theconvention we use in this book.\\nThe degree of normalization that you should apply to your data depends on your use case. No one-size-fits-all solution exists, especially in databases where some denormalization presents performanceadvantages. Although denormalization may seem like an antipattern, it’s common in many OLAPsystems that store semistructured data. Study normalization conventions and database best practices tochoose an appropriate strategy.\\nTechniques for Modeling Batch Analytical Data\\nWhen describing data modeling for data lakes or data warehouses, you should assume that the rawdata takes many forms (e.g., structured and semistructured), but the output is a structured data model ofrows and columns. However, several approaches to data modeling can be used in these environments.The big approaches you’ll likely encounter are Kimball, Inmon, and data vault.\\nIn practice, some of these techniques can be combined. For example, we see some data teams startwith data vault and then add a Kimball star schema alongside it. We’ll also look at wide anddenormalized data models and other batch data-modeling techniques you should have in your arsenal.As we discuss each of these techniques, we will use the example of modeling transactions occurring inan ecommerce order system.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b5401960-bd0e-4a8c-81e4-f115f8780b61', embedding=None, metadata={'page_label': '438', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='NOTE\\nOur coverage of the first three approaches—Inmon, Kimball, and data vault—is cursory and hardly does justice totheir respective complexity and nuance. At the end of each section, we list the canonical books from their creators.For a data engineer, these books are must-reads, and we highly encourage you to read them, if only to understandhow and why data modeling is central to batch analytical data.\\nInmon\\nThe father of the data warehouse, Bill Inmon, created his approach to data modeling in 1990. Before thedata warehouse, the analysis would often occur directly on the source system itself, with the obviousconsequence of bogging down production transactional databases with long-running queries. The goalof the data warehouse was to separate the source system from the analytical system.\\nInmon defines a data warehouse the following way:\\nA data warehouse is a subject-oriented, integrated, nonvolatile, and time-variant collection of data insupport of management’s decisions. The data warehouse contains granular corporate data. Data inthe data warehouse is able to be used for many different purposes, including sitting and waiting forfuture requirements which are unknown today.\\nThe four critical parts of a data warehouse mean the following:\\nSubject-oriented\\nThe data warehouse focuses on a specific subject area, such as sales or marketing.\\nNonvolatile\\nData remains unchanged after data is stored in a data warehouse.\\nIntegrated\\nData from disparate sources is consolidated and normalized.\\nTime-variant\\nVarying time ranges can be queried.\\nLet’s look at each of these parts to understand its influence on an Inmon data model. First, the logicalmodel must focus on a specific area. For instance, if the subject orientation is “sales,” then the logicalmodel contains all details related to sales—business keys, relationships, attributes, etc. Next, thesedetails are integrated into a consolidated and highly normalized data model. Finally, the data is storedunchanged in a nonvolatile and time-variant way, meaning you can (theoretically) query the original datafor as long as storage history allows. The Inmon data warehouse must strictly adhere to all four of thesecritical parts in support of management’s decisions. This is a subtle point, but it positions the datawarehouse for analytics, not OLTP.\\nHere is another key characteristic of Inmon’s data warehouse:\\nThe second salient characteristic of the data warehouse is that it is integrated. Of all the aspects of adata warehouse, integration is the most important. Data is fed from multiple, disparate sources intothe data warehouse. As the data is fed, it is converted, reformatted, resequenced, summarized, etc.The result is that data—once it resides in the data warehouse —has a single physical corporateimage.\\nWith Inmon’s data warehouse, data is integrated from across the organization in a granular, highlynormalized ER model, with a relentless emphasis on ETL. Because of the subject-oriented nature of the\\n8\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5af872f1-820c-4da3-852e-506dcd30d939', embedding=None, metadata={'page_label': '439', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data warehouse, the Inmon data warehouse consists of key source databases and information systemsused in an organization. Data from key business source systems is ingested and integrated into a highlynormalized (3NF) data warehouse that often closely resembles the normalization structure of the sourcesystem itself; data is brought in incrementally, starting with the highest-priority business areas. The strictnormalization requirement ensures as little data duplication as possible, which leads to fewerdownstream analytical errors because data won’t diverge or suffer from redundancies. The datawarehouse represents a “single source of truth,” which supports the overall business’s informationrequirements. The data is presented for downstream reports and analysis via business and department-specific data marts, which may also be denormalized.\\nLet’s look at how an Inmon data warehouse is used for ecommerce (Figure 8-13). The business sourcesystems are orders, inventory, and marketing. The data from these source systems are ETLed to thedata warehouse and stored in 3NF. Ideally, the data warehouse holistically encompasses the business’sinformation. To serve data for department-specific information requests, ETL processes take data fromthe data warehouse, transform the data, and place it in downstream data marts to be viewed in reports.\\nFigure 8-13. An ecommerce data warehouse\\nA popular option for modeling data in a data mart is a star schema (discussed in the following section onKimball), though any data model that provides easily accessible information is also suitable. In thepreceding example, sales, marketing, and purchasing have their own star schema, fed upstream fromthe granular data in the data warehouse. This allows each department to have its own data structurethat’s unique and optimized to its specific needs.\\nInmon continues to innovate in the data warehouse space, currently focusing on textual ETL in the datawarehouse. He’s also a prolific writer and thinker, writing over 60 books and countless articles. Forfurther reading about Inmon’s data warehouse, please refer to his books listed in “AdditionalResources”.\\nKimball\\nIf there are spectrums to data modeling, Kimball is very much on the opposite end of Inmon. Created byRalph Kimball in the early 1990s, this approach to data modeling focuses less on normalization, and insome cases accepting denormalization. As Inmon says about the difference between the datawarehouse and data mart, “A data mart is never a substitute for a data warehouse.”\\nWhereas Inmon integrates data from across the business in the data warehouse, and servesdepartment-specific analytics via data marts, the Kimball model is bottom-up, encouraging you to modeland serve department or business analytics in the data warehouse itself (Inmon argues this approachskews the definition of a data warehouse). This may enable faster iteration and modeling than Inmon,with the trade-off of potential looser data integration, data redundancy, and duplication.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f1cd511a-c6b0-4d30-9865-c9f2ec8f85fe', embedding=None, metadata={'page_label': '440', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In Kimball’s approach, data is modeled with two general types of tables: facts and dimensions. You canthink of a fact table as a table of numbers, and dimension tables as qualitative data referencing a fact.Dimension tables surround a single fact table in a relationship called a star schema (Figure 8-14).Let’s look at facts, dimensions, and star schemas.\\nFigure 8-14. A Kimball star schema, with facts and dimensions\\nFact tables\\nThe first type of table in a star schema is the fact table, which contains factual, quantitative, and event-related data. The data in a fact table is immutable because facts relate to events. Therefore, fact tablesdon’t change and are append-only. Fact tables are typically narrow and long, meaning they have not alot of columns but a lot of rows that represent events. Fact tables should be at the lowest grain possible.\\nQueries against a star schema start with the fact table. Each row of a fact table should represent thegrain of the data. Avoid aggregating or deriving data within a fact table. If you need to performaggregations or derivations, do so in a downstream query, data mart table, or view. Finally, fact tablesdon’t reference other fact tables; they reference only dimensions.\\nLet’s look at an example of an elementary fact table (Table 8-9). A common question in your companymight be, “Show me gross sales, by each customer order, by date.” Again, facts should be at the lowestgrain possible—in this case, the orderID of the sale, customer, date, and gross sale amount. Notice thatthe data types in the fact table are all numbers (integers and floats); there are no strings. Also, in thisexample, CustomerKey 7 has two orders on the same day, reflecting the grain of the table. Instead,the fact table has keys that reference dimension tables containing their respective attributes, such asthe customer and date information. The gross sales amount represents the total sale for the sales event.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='efbf0e41-42b8-4841-8ce6-8c7d61748ebe', embedding=None, metadata={'page_label': '441', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-9. A fact table\\nOrderID CustomerKey DateKey GrossSalesAmt\\n100 5 20220301 100.00\\n101 7 20220301 75.00\\n102 7 20220301 50.00\\nDimension tables\\nThe second primary type of table in a Kimball data model is called a dimension. Dimension tablesprovide the reference data, attributes, and relational context for the events stored in fact tables.Dimension tables are smaller than fact tables and take an opposite shape, typically wide and short.When joined to a fact table, dimensions can describe the events’ what, where, and when. Dimensionsare denormalized, with the possibility of duplicate data. This is OK in the Kimball data model. Let’s lookat the two dimensions referenced in the earlier fact table example.\\nIn a Kimball data model, dates are typically stored in a date dimension, allowing you to reference thedate key (DateKey) between the fact and date dimension table. With the date dimension table, you caneasily answer questions like, “What are my total sales in the first quarter of 2022?” or “How many morecustomers shop on Tuesday than Wednesday?” Notice we have five fields in addition to the date key(Table 8-10). The beauty of a date dimension is that you can add as many new fields as makes sense toanalyze your data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3fd00172-06d9-4996-8e62-943ae85e9193', embedding=None, metadata={'page_label': '442', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-10. A date dimension table\\nDateKey Date-ISO Year Quarter Month Day-of-week\\n20220301 2022-03-01 2022 1 3 Tuesday\\n20220302 2022-03-02 2022 1 3 Wednesday\\n20220303 2022-03-03 2022 1 3 Thursday\\nTable 8-11 also references another dimension—the customer dimension—by the CustomerKey field.The customer dimension contains several fields that describe the customer: first and last name, zipcode, and a couple of peculiar-looking date fields. Let’s look at these date fields, as they illustrateanother concept in the Kimball data model: a Type 2 slowly changing dimension, which we’ll describe ingreater detail next.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d653fae-3ccf-4cd5-8a82-d34fb6426f73', embedding=None, metadata={'page_label': '443', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-11. A Type 2 customer dimension table\\nCustomerKey FirstName LastName ZipCode EFF_StartDate EFF_EndDate\\n5 Joe Reis 84108 2019-01-04 9999-01-01\\n7 Matt Housley 84101 2020-05-04 2021-09-19\\n7 Matt Housley 84123 2021-09-19 9999-01-01\\n11 Lana Belle 90210 2022-02-04 9999-01-01\\nFor example, take a look at CustomerKey 5, with the EFF_StartDate (EFF_StartDate meanseffective start date) of 2019-01-04 and an EFF_EndDate of 9999-01-01. This means Joe Reis’s', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae0c004e-7405-4369-a1f3-234debcdaccc', embedding=None, metadata={'page_label': '444', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='customer record was created in the customer dimension table on 2019-01-04 and has an end date of9999-01-01. Interesting. What does this end date mean? It means the customer record is active andisn’t changed.\\nNow let’s look at Matt Housley’s customer record (CustomerKey = 7). Notice the two entries forHousley’s start date: 2020-05-04 and 2021-09-19. It looks like Housley changed his zip code on2021-09-19, resulting in a change to his customer record. When the data is queried for the most recentcustomer records, you will query where the end date is equal to 9999-01-01.\\nA slowly changing dimension (SCD) is necessary to track changes in dimensions. The precedingexample is a Type 2 SCD: a new record is inserted when an existing record changes. Though SCDs cango up to seven levels, let’s look at the three most common ones:\\nType 1\\nOverwrite existing dimension records. This is super simple and means you have no access to the\\ndeleted historical dimension records.\\nType 2\\nKeep a full history of dimension records. When a record changes, that specific record is flagged as\\nchanged, and a new dimension record is created that reflects the current status of the attributes. In\\nour example, Housley moved to a new zip code, which triggered his initial record to reflect an\\neffective end date, and a new record was created to show his new zip code.\\nType 3\\nA Type 3 SCD is similar to a Type 2 SCD, but instead of creating a new row, a change in a Type 3\\nSCD creates a new field. Using the preceding example, let’s see what this looks like as a Type 3\\nSCD in the following tables.\\nIn Table 8-12, Housley lives in the 84101 zip code. When Housley moves to a new zip code, the Type 3SCD creates two new fields, one for his new zip code and the date of the change (Table 8-13). Theoriginal zip code field is also renamed to reflect that this is the older record.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6b9b18f0-9f04-4681-bb25-eca992a8c9d0', embedding=None, metadata={'page_label': '445', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-12. Type 3 slowly changing dimension\\nCustomerKey FirstName LastName ZipCode\\n7 Matt Housley 84101', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='384855ce-3ecd-47c6-a35a-fe070a98fc75', embedding=None, metadata={'page_label': '446', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-13. Type 3 customer dimension table\\nCustomerKey FirstName LastName Original ZipCodeCurrent ZipCodeCurrentDate\\n7 Matt Housley 84101 84123 2021-09-19\\nOf the types of SCDs described, Type 1 is the default behavior of most data warehouses, and Type 2 isthe one we most commonly see used in practice. There’s a lot to know about dimensions, and wesuggest using this section as a starting point to get familiar with how dimensions work and how they’reused.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d7f018f-4b65-41ba-a5a8-dce829b38a2c', embedding=None, metadata={'page_label': '447', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Star schema\\nNow that you have a basic understanding of facts and dimensions, it’s time to integrate them into a starschema. The star schema represents the data model of the business. Unlike highly normalizedapproaches to data modeling, the star schema is a fact table surrounded by the necessary dimensions.This results in fewer joins than other data models, which speeds up query performance. Anotheradvantage of a star schema is it’s arguably easier for business users to understand and use.\\nNote that the star schema shouldn’t reflect a particular report, though you can model a report in adownstream data mart or directly in your BI tool. The star schema should capture the facts andattributes of your business logic and be flexible enough to answer the respective critical questions.\\nBecause a star schema has one fact table, sometimes you’ll have multiple star schemas that addressdifferent facts of the business. You should strive to reduce the number of dimensions whenever possiblesince this reference data can potentially be reused among different fact tables. A dimension that isreused across multiple star schemas, thus sharing the same fields, is called a conformed dimension. Aconformed dimension allows you to combine multiple fact tables across multiple star schemas.Remember, redundant data is OK with the Kimball method, but avoid replicating the same dimensiontables to avoid drifting business definitions and data integrity.\\nThe Kimball data model and star schema have a lot of nuance. You should be aware that this mode isappropriate only for batch data and not for streaming data. Because the Kimball data model is popular,there’s a good chance you’ll run into it\\nData vault\\nWhereas Kimball and Inmon focus on the structure of business logic in the data warehouse, the datavault offers a different approach to data modeling. Created in the 1990s by Dan Linstedt, the datavault model separates the structural aspects of a source system’s data from its attributes. Instead ofrepresenting business logic in facts, dimensions, or highly normalized tables, a data vault simply loadsdata from source systems directly into a handful of purpose-built tables in an insert-only manner. Unlikethe other data modeling approaches you’ve learned about, there’s no notion of good, bad, or conformeddata in a data vault.\\nData moves fast these days, and data models need to be agile, flexible, and scalable; the data vaultmodel aims to meet this need. The goal of this model is to keep the data as closely aligned to thebusiness as possible, even while the business’s data evolves.\\nA data vault consists of three main types of tables: hubs, links, and satellites (Figure 8-15). In short, ahub stores business keys, a link maintains relationships among business keys, and a satelliterepresents a business key’s attributes and context. A user will query a hub, which will link to a satellitetable containing the query’s relevant attributes. Let’s explore hubs, links, and satellites in more detail.\\nFigure 8-15. Data vault tables: hubs, links, and satellites connected together\\nHubs\\nQueries often involve searching by a business key, such as a customer ID or an order ID from ourecommerce example. A hub is the central entity of a data vault that retains a record of all business keysloaded into the data vault. The hub should reflect the state of the source system from which it loadsdata.\\nA hub always contains the following standard fields:\\nHash key\\nThe primary key used to join data between systems. This is a calculated hash field (MD5 or similar)\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='41621c4e-d5bb-4dbf-b040-89752b7db4c6', embedding=None, metadata={'page_label': '448', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Load date\\nThe date the data was loaded into the hub.\\nRecord source\\nThe source system.\\nBusiness key(s)\\nThe business key, or record ID, in the source system.\\nIt’s important to note that a hub is insert-only, and data from source systems is not altered in a hub.Once data is loaded into a hub, it’s permanent.\\nWhen designing a hub, identifying the business key is critical. Ask yourself: What is the identifiablebusiness element in a source system? Put another way, how do users of a source system commonlylook for data? Ideally, this is discovered as you build the conceptual data model of your organization andbefore you start building your data vault.\\nUsing our ecommerce scenario, let’s look at an example of a hub for products. First, let’s look at thephysical design of a product hub (Table 8-14).\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='973f07e1-b6ce-4ea3-bd6b-fa95ceaaa5dc', embedding=None, metadata={'page_label': '449', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-14. A physical design for a product hub\\nHubProduct\\nProductHashKey\\nLoadDate\\nRecordSource\\nProductID', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8497f035-6c39-4a22-bf68-a45e1d2f7417', embedding=None, metadata={'page_label': '450', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In practice, the product hub looks like this when populated with data (Table 8-15). In this example, threedifferent products are loaded into a hub from an ERP system on two separate dates.\\nTable 8-15. A product hub populated with data\\nProductHashKeyLoadDate RecordSource ProductID\\n4041fd80ab68e267490522b4a99048f52020-01-02 ERP 1\\nde8435530d6ca93caabc00cf88982dc12021-03-09 ERP 2\\ncf27369bd80f53d0e60d394817d77bab2021-03-09 ERP 3\\nWhile we’re at it, let’s create another hub for orders (Table 8-16) using the same schema asHubProduct, and populate it with some sample order data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7cf13746-b111-4538-a299-4f33046b4b38', embedding=None, metadata={'page_label': '451', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-16. An order hub populated with data\\nOrderHashKeyLoadDate RecordSource OrderID\\nf899139df5e1059396431415e770c6dd 2022-03-01 Website 100\\n38b3eff8baf56627478ec76a704e9b52 2022-03-01 Website 101\\nec8956637a99787bd197eacd77acce5e2022-03-01 Website 102\\nLinks', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3475da93-5a26-44d6-847e-cedfdb130e1a', embedding=None, metadata={'page_label': '452', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A link table tracks the relationships of business keys between hubs. Link tables must connect at leasttwo hubs, ideally at the lowest possible grain. Because link tables connect data from various hubs, theyare many to many. The data vault model’s relationships are straightforward and handled throughchanges to the links. This provides excellent flexibility in the inevitable event that the underlying sourcedata changes. You add a new hub, and this hub needs to connect to existing hubs. Simply add a newrelationship to that hub in your link table. That’s it! Now let’s look at ways to view data contextually usingsatellites.\\nBack to our ecommerce example, we’d like to associate orders with products. Let’s see what a link tablemight look like for orders and products (Table 8-17).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d07b059-a6c7-4bf4-903e-c035b33b7064', embedding=None, metadata={'page_label': '453', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-17. A link table for products and orders\\nLinkOrderProduct\\nOrderProductHashKey\\nLoadDate\\nRecordSource\\nProductHashKey\\nOrderHashKey', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='603b51e4-5991-44da-b89b-cc9754fd75a1', embedding=None, metadata={'page_label': '454', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='When the LinkOrderProduct table is populated, here’s what it looks like (Table 8-18). Note that we’reusing the order’s record source in this example.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8af0cde9-61f4-4e9d-9916-aaf2b9df6ee9', embedding=None, metadata={'page_label': '455', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-18. A link table connecting orders and products\\nOrderProductHashKeyLoadDate RecordSource ProductHashKeyOrderHashKey\\nff64ec193d7bacf05e8b97ea04b39066 2022-03-01 Website 4041fd80ab68e267490522b4a99048f5 f899139df5e1059396431415e770c6dd\\nff64ec193d7bacf05e8b97ea04 2022-03-01 Website de8435530d6ca93caabc0 f899139df5e10593964314', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='28684fc0-73d3-4385-8c64-0e51b6d4182c', embedding=None, metadata={'page_label': '456', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='b39066 0cf88982dc1 15e770c6dd\\ne232628c251e3cad0f53f7e6e13cdea7 2022-03-01 Website cf27369bd80f53d0e60d394817d77bab 38b3eff8baf56627478ec76a704e9b52\\n26166a5871b6d21ae12e9c464262be67 2022-03-01 Website 4041fd80ab68e267490522b4a99048f5 ec8956637a99787bd197eacd77acce5e\\nSatellites\\nWe’ve described relationships between hubs and links that involve keys, load dates, and recordsources. How do you get a sense of what these relationships mean? Satellites are descriptive attributesthat give meaning and context to hubs. Satellites can connect to either hubs or links. The only requiredfields in a satellite are a primary key consisting of the business key of the parent hub and a load date.Beyond that, a satellite can contain however many attributes that make sense.\\nLet’s look at an example of a satellite for the Product hub (Table 8-19). In this example, theSatelliteProduct table contains additional information about the product, such as product nameand price.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ab9d40d-0ac5-41dd-b1d3-d3cfd0b9985d', embedding=None, metadata={'page_label': '457', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-19. SatelliteProduct\\nSatelliteProduct\\nProductHashKey\\nLoadDate\\nRecordSource\\nProductName\\nPrice\\nAnd here’s the SatelliteProduct table with some sample data (Table 8-20).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eb02ed15-7622-424d-a9e5-5d438729977f', embedding=None, metadata={'page_label': '458', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-20. A product satellite table with sample data\\nProductHashKeyLoadDate RecordSource ProductName Price\\n4041fd80ab68e267490522b4a99048f52020-01-02 ERP Thingamajig 50\\nde8435530d6ca93caabc00cf88982dc12021-03-09 ERP Whatchamacallit 25\\ncf27369bd80f53d0e60d394817d77bab2021-03-09 ERP Whozeewhatzit 75', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a039a989-70b6-41c7-97e8-bf7b17e4a3fb', embedding=None, metadata={'page_label': '459', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Let’s tie this all together and join the hub, product, and link tables into a data vault (Figure 8-16).\\nFigure 8-16. The data vault for orders and products\\nOther types of data vault tables exist, including point in time (PIT) and bridge tables. We don’t coverthese here, but mention them because the data vault is quite comprehensive. Our goal is to simply giveyou an overview of the data vault’s power.\\nUnlike other data modeling techniques we’ve discussed, in a data vault, the business logic is createdand interpreted when the data from these tables is queried. Please be aware that the data vault modelcan be used with other data modeling techniques. It’s not unusual for a data vault to be the landing zonefor analytical data, after which it’s separately modeled in a data warehouse, commonly using a starschema. The data vault model also can be adapted for NoSQL and streaming data sources. The datavault is a huge topic, and this section is simply meant to make you aware of its existence.\\nWide denormalized tables\\nThe strict modeling approaches we’ve described, especially Kimball and Inmon, were developed whendata warehouses were expensive, on premises, and heavily resource-constrained with tightly coupledcompute and storage. While batch data modeling has traditionally been associated with these strictapproaches, more relaxed approaches are becoming more common.\\nThere are reasons for this. First, the popularity of the cloud means that storage is dirt cheap. It’scheaper to store data than agonize over the optimum way to represent the data in storage. Second, thepopularity of nested data (JSON and similar) means schemas are flexible in source and analyticalsystems.\\nYou have the option to rigidly model your data as we’ve described, or you can choose to throw all ofyour data into a single wide table. A wide table is just what it sounds like: a highly denormalized andvery wide collection of many fields, typically created in a columnar database. A field may be a singlevalue or contain nested data. The data is organized along with one or multiple keys; these keys areclosely tied to the grain of the data.\\nA wide table can potentially have thousands of columns, whereas fewer than 100 are typical in relationaldatabases. Wide tables are usually sparse; the vast majority of entries in a given field may be null. Thisis extremely expensive in a traditional relational database because the database allocates a fixedamount of space for each field entry; nulls take up virtually no space in a columnar database. A wideschema in a relational database dramatically slows reading because each row must allocate all thespace specified by the wide schema, and the database must read the contents of each row in itsentirety. On the other hand, a columnar database reads only columns selected in a query, and readingnulls is essentially free.\\nWide tables generally arise through schema evolution; engineers gradually add fields over time.Schema evolution in a relational database is a slow and resource-heavy process. In a columnardatabase, adding a field is initially just a change to metadata. As data is written into the new field, newfiles are added to the column.\\nAnalytics queries on wide tables often run faster than equivalent queries on highly normalized datarequiring many joins. Removing joins can have a huge impact on scan performance. The wide tablesimply contains all of the data you would have joined in a more rigorous modeling approach. Facts and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cc789eb2-1a08-4ca7-957b-4999bbdcb0dd', embedding=None, metadata={'page_label': '460', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='dimensions are represented in the same table. The lack of data model rigor also means not a lot ofthought is involved. Load your data into a wide table and start querying it. Especially with schemas insource systems becoming more adaptive and flexible, this data usually results from high-volumetransactions, meaning there’s a lot of data. Storing this as nested data in your analytical storage has alot of benefits.\\nThrowing all of your data into a single table might seem like heresy for a hardcore data modeler, andwe’ve seen plenty of criticism. What are some of these criticisms? The biggest criticism is as you blendyour data, you lose the business logic in your analytics. Another downside is the performance ofupdates to things like an element in an array, which can be very painful.\\nLet’s look at an example of a wide table (Table 8-21), using the original denormalized table from ourearlier normalization example. This table can have many more columns—hundreds or more!—and weinclude only a handful of columns for brevity and ease of understanding. As you can see, this tablecombines various data types, represented along a grain of orders for a customer on a date.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7f22dbe4-581c-47c7-ba78-371b1e7757d4', embedding=None, metadata={'page_label': '461', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8-21. An example of denormalized data\\nOrderID OrderItems CustomerID CustomerNameOrderDate Site SiteRegion\\n100\\n[{              \"sku\": 1,              \"price\": 50,\"quantity\": 1,              \"name:\": \"Thingamajig\"}, {              \"sku\": 2,              \\n5 Joe Reis 2022-03-01 abc.com US', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd2fa14d-4703-4bc7-82c3-3d460dd350e6', embedding=None, metadata={'page_label': '462', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\"price\": 25,\"quantity\": 2,              \"name:\": \"Whatchamacallit\"}]\\nWe suggest using a wide table when you don’t care about data modeling, or when you have a lot of datathat needs more flexibility than traditional data-modeling rigor provides. Wide tables also lendthemselves to streaming data, which we’ll discuss next. As data moves toward fast-moving schemasand streaming-first, we expect to see a new wave of data modeling, perhaps something along the linesof “relaxed normalization.”\\nWHAT IF YOU DON’T MODEL YOUR DATA?\\nYou also have the option of not modeling your data. In this case, just query data sources directly.This pattern is often used, especially when companies are just getting started and want to get quickinsights or share analytics with their users. While it allows you to get answers to various questions,you should consider the following:\\nIf I don’t model my data, how do I know the results of my queries are consistent?\\nDo I have proper definitions of business logic in the source system, and will my query producetruthful answers?\\nWhat query load am I putting on my source systems, and how does this impact users of thesesystems?\\nAt some point, you’ll probably gravitate toward a stricter batch data model paradigm and a dedicateddata architecture that doesn’t rely on the source systems for the heavy lifting.\\nModeling Streaming Data\\nWhereas many data-modeling techniques are well established for batch, this is not the case forstreaming data. Because of the unbounded and continuous nature of streaming data, translating batchtechniques like Kimball to a streaming paradigm is tricky, if not impossible. For example, given a streamof data, how would you continuously update a Type-2 slowly changing dimension without bringing yourdata warehouse to its knees?\\nThe world is evolving from batch to streaming and from on premises to the cloud. The constraints of theolder batch methods no longer apply. That said, big questions remain about how to model data tobalance the need for business logic against fluid schema changes, fast-moving data, and self-service.What is the streaming equivalent of the preceding batch data model approaches? There isn’t (yet) aconsensus approach on streaming data modeling. We spoke with many experts in streaming datasystems, many of whom told us that traditional batch-oriented data modeling doesn’t apply to streaming.A few suggested the data vault as an option for streaming data modeling.\\nAs you may recall, two main types of streams exist: event streams and CDC. Most of the time, theshape of the data in these streams is semistructured, such as JSON. The challenge with modelingstreaming data is that the payload’s schema might change on a whim. For example, suppose you havean IoT device that recently upgraded its firmware and introduced a new field. In that case, it’s possiblethat your downstream destination data warehouse or processing pipeline isn’t aware of this change andbreaks. That’s not great. As another example, a CDC system might recast a field as a different type—', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bf88841c-9469-4924-ba58-46895aee9df2', embedding=None, metadata={'page_label': '463', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='say, a string instead of an International Organization for Standardization (ISO) datetime format. Again,how does the destination handle this seemingly random change?\\nThe streaming data experts we’ve talked with overwhelmingly suggest you anticipate changes in thesource data and keep a flexible schema. This means there’s no rigid data model in the analyticaldatabase. Instead, assume the source systems are providing the correct data with the right businessdefinition and logic, as it exists today. And because storage is cheap, store the recent streaming andsaved historical data in a way they can be queried together. Optimize for comprehensive analyticsagainst a dataset with a flexible schema. Furthermore, instead of reacting to reports, why not createautomation that responds to anomalies and changes in the streaming data instead?\\nThe world of data modeling is changing, and we believe a sea change will soon occur in data modelparadigms. These new approaches will likely incorporate metrics and semantic layers, data pipelines,and traditional analytics workflows in a streaming layer that sits directly on top of the source system.Since data is being generated in real time, the notion of artificially separating source and analyticssystems into two distinct buckets may not make as much sense as when data moved more slowly andpredictably. Time will tell…\\nWe have more to say on the future of streaming data in Chapter 11.\\nTransformations\\nThe net result of transforming data is the ability to unify and integrate data. Once data is transformed,the data can be viewed as a single entity. But without transforming data, you cannot have a unifiedview of data across the organization.—Bill Inmon\\nNow that we’ve covered queries and data modeling, you might be wondering, if I can model data, queryit, and get results, why do I need to think about transformations? Transformations manipulate, enhance,and save data for downstream use, increasing its value in a scalable, reliable, and cost-effectivemanner.\\nImagine running a query every time you want to view results from a particular dataset. You’d run thesame query dozens or hundreds of times a day. Imagine that this query involves parsing, cleansing,joining, unioning, and aggregating across 20 datasets. To further exacerbate the pain, the query takes30 minutes to run, consumes significant resources, and incurs substantial cloud charges over severalrepetitions. You’d probably go insane. Thankfully, you can save the results of your query instead, or atleast run the most compute-intensive portions only once, so subsequent queries are simplified.\\nA transformation differs from a query. A query retrieves the data from various sources based on filteringand join logic. A transformation persists the results for consumption by additional transformations orqueries. These results may be stored ephemerally or permanently.\\nBesides persistence, a second aspect that differentiates transformations from queries is complexity.You’ll likely build complex pipelines that combine data from multiple sources and reuse intermediateresults for multiple final outputs. These complex pipelines might normalize, model, aggregate, orfeaturize data. While you can build complex dataflows in single queries using common tableexpressions, scripts, or DAGs, this quickly becomes unwieldy, inconsistent, and intractable. Entertransformations.\\nTransformations critically rely on one of the major undercurrents in this book: orchestration.Orchestration combines many discrete operations, such as intermediate transformations, that store datatemporarily or permanently for consumption by downstream transformations or serving. Increasingly,transformation pipelines span not only multiple tables and datasets, but also multiple systems.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f0442164-52fb-4d0c-bd9d-aa3ec731ab6e', embedding=None, metadata={'page_label': '464', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Batch Transformations\\nBatch transformations run on discrete chunks of data, in contrast to streaming transformations, wheredata is processed continuously as it arrives. Batch transformations can run on a fixed schedule (e.g.,daily, hourly, or every 15 minutes) to support ongoing reporting, analytics, and ML models. In thissection, you’ll learn various batch transformation patterns and technologies.\\nDistributed joins\\nThe basic idea behind distributed joins is that we need to break a logical join (the join defined by thequery logic) into much smaller node joins that run on individual servers in the cluster. The basicdistributed join patterns apply whether one is in MapReduce (discussed in “MapReduce”), BigQuery,Snowflake, or Spark, though the details of intermediate storage between processing steps vary (on diskor in memory). In the best case scenario, the data on one side of the join is small enough to fit on asingle node (broadcast join). Often, a more resource-intensive shuffle hash join is required.\\nBroadcast join\\nA broadcast join is generally asymmetric, with one large table distributed across nodes and one smalltable that can easily fit on a single node (Figure 8-17). The query engine “broadcasts” the small table(table A) out to all nodes, where it gets joined to the parts of the large table (table B). Broadcast joinsare far less compute intensive than shuffle hash joins.\\nFigure 8-17. In a broadcast join, the query engine sends table A out to all nodes in the cluster to be joined with the variousparts of table B\\nIn practice, table A is often a down-filtered larger table that the query engine collects and broadcasts.One of the top priorities in query optimizers is join reordering. With the early application of filters, andmovement of small tables to the left (for left joins) it is often possible to dramatically reduce the amountof data that is processed in each join. Pre-filtering data to create broadcast joins where possible candramatically improve performance and reduce resource consumption.\\nShuffle hash join\\nIf neither table is small enough to fit on a single node, the query engine will use a shuffle hash join. InFigure 8-18, the same nodes are represented above and below the dotted line. The area above thedotted line represents the initial partitioning of tables A and B across the nodes. In general, thispartitioning will have no relation to the join key. A hashing scheme is used to repartition data by join key.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7fe1ed5d-a38a-458e-a3e8-646160056728', embedding=None, metadata={'page_label': '465', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 8-18. Shuffle hash join\\nIn this example, the hashing scheme will partition the join key into three parts, with each part assignedto a node. The data is then reshuffled to the appropriate node, and the new partitions for tables A and Bon each node are joined. Shuffle hash joins are generally more resource intensive than broadcast joins.\\nETL, ELT and data pipelines\\nAs we discussed in Chapter 3, a widespread transformation pattern dating to the early days of relationaldatabases is a batch ETL. Traditional ETL relies on an external transformation system to pull, transform,and clean data while preparing it for a target schema, such as a data mart or a Kimball star schema.The transformed data would then be loaded into a target system, such as a data warehouse, wherebusiness analytics could be performed.\\nThe ETL pattern itself was driven by the limitations of both source and target systems. The extractphase tended to be a major bottleneck, with the constraints of the source RDBMS limiting the rate atwhich data could be pulled. And, the transformation was handled in a dedicated system because thetarget system was extremely resource-constrained in both storage and CPU capacity.\\nA now-popular evolution of ETL is ELT. As data warehouse systems have grown in performance andstorage capacity, it has become common to simply extract raw data from a source system, import it intoa data warehouse with minimal transformation, and then clean and transform it directly in the warehousesystem. (See our discussion of data warehouses in Chapter 3 for a more detailed discussion of thedifference between ETL and ELT.)\\nA second, slightly different notion of ELT was popularized with the emergence of data lakes. In thisversion, the data is not transformed at the time it’s loaded. Indeed, massive quantities of data may beloaded with no preparation and no plan whatsoever. The assumption is that the transformation step willhappen at some undetermined future time. Ingesting data without a plan is a great recipe for a dataswamp. As Inmon says:\\nI’ve always been a fan of ETL because of the fact that ETL forces you to transform data before youput it into a form where you can work with it. But some organizations want to simply take the data, putit into a database, then do the transformation… I’ve seen too many cases where the organizationsays, oh we’ll just put the data in and transform it later. And guess what? Six months later, that data[has] never been touched.\\nWe have also seen that the line between ETL and ELT can become somewhat blurry in a datalakehouse environment. With object storage as a base layer, it’s no longer clear what’s in the database\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea1a05db-694f-4f9f-9fba-f105bce78e1a', embedding=None, metadata={'page_label': '466', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and out of the database. The ambiguity is further exacerbated with the emergence of data federation,virtualization, and live tables. (We discuss these topics later in this section.)\\nIncreasingly, we feel that the terms ETL and ELT should be applied only at the micro level (withinindividual transformation pipelines) rather than at the macro level (to describe a transformation patternfor a whole organization). Organizations no longer need to standardize on ETL or ELT but can insteadfocus on applying the proper technique on a case-by-case basis as they build data pipelines.\\nSQL and general-purpose code-transformation tools\\nAt this juncture, the distinction between SQL-based and non-SQL-based transformation systems feelssomewhat synthetic. Since the introduction of Hive on the Hadoop platform, SQL has become a first-class citizen in the big data ecosystem. For example, Spark SQL was an early feature of Apache Spark.Streaming-first frameworks such as Kafka, Flink, and Beam also support SQL, with varying features andfunctionality.\\nIt is more appropriate to think about SQL-only tools versus those that support more powerful, general-purpose programming paradigms. SQL-only transformation tools span a wide variety of proprietary andopen source options.\\nSQL is declarative...but it can still build complex data workflows\\nWe often hear SQL dismissed because it is “not procedural.” This is technically correct. SQL is adeclarative language: instead of coding a data processing procedure, SQL writers stipulate thecharacteristics of their final data in set-theoretic language; the SQL compiler and optimizer determinethe steps required to put data in this state.\\nPeople sometimes imply that because SQL is not procedural, it cannot build out complex pipelines. Thisis false. SQL can effectively be used to build complex DAGs using common table expressions, SQLscripts, or an orchestration tool.\\nTo be clear, SQL has limits, but we often see engineers doing things in Python and Spark that could bemore easily and efficiently done in SQL. For a better idea of the trade-offs we’re talking about, let’s lookat a couple of examples of Spark and SQL.\\nExample: When to avoid SQL for batch transformations in Spark\\nWhen you’re determining whether to use native Spark or PySpark code instead of Spark SQL or anotherSQL engine, ask yourself the following questions:\\n1. How difficult is it to code the transformation in SQL?\\n2. How readable and maintainable will the resulting SQL code be?\\n3. Should some of the transformation code be pushed into a custom library for future reuse across theorganization?\\nRegarding question 1, many transformations coded in Spark could be realized in fairly simple SQLstatements. On the other hand, if the transformation is not realizable in SQL, or if it would be extremelyawkward to implement, native Spark is a better option. For example, we might be able to implementword stemming in SQL by placing word suffixes in a table, joining with that table, using a parsingfunction to find suffixes in words, and then reducing the word to its stem by using a substring function.However, this sounds like an extremely complex process with numerous edge cases to consider. A morepowerful procedural programming language is a better fit here.\\nQuestion 2 is closely related. The word-stemming query will be neither readable nor maintainable.\\nRegarding question 3, one of the major limitations of SQL is that it doesn’t include a natural notion oflibraries or reusable code. One exception is that some SQL engines allow you to maintain user-definedfunctions (UDFs) as objects inside a database. However, these aren’t committed to a Git repository15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0f1a56a0-1e59-4ecd-bc7f-4c80d39114d8', embedding=None, metadata={'page_label': '467', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='without an external CI/CD system to manage deployment. Furthermore, SQL doesn’t have a goodnotion of reusability for more complex query components. Of course, reusable libraries are easy tocreate in Spark and PySpark.\\nWe will add that it is possible to recycle SQL in two ways. First, we can easily reuse the results of a SQLquery by committing to a table or creating a view. This process is often best handled in an orchestrationtool such as Airflow so that downstream queries can start once the source query has finished. Second,Data Build Tool (dbt) facilitates the reuse of SQL statements and offers a templating language thatmakes customization easier.\\nExample: Optimizing Spark and other processing frameworks\\nSpark acolytes often complain that SQL doesn’t give them control over data processing. The SQLengine takes your statements, optimizes them, and compiles them into its processing steps. (In practice,optimization may happen before or after compilation, or both.)\\nThis is a fair complaint, but a corollary exists. With Spark and other code-heavy processing frameworks,the code writer becomes responsible for much of the optimization that is handled automatically in aSQL-based engine. The Spark API is powerful and complex, meaning it is not so easy to identifycandidates for reordering, combination, or decomposition. When embracing Spark, data engineeringteams need to actively engage with the problems of Spark optimization, especially for expensive, long-running jobs. This means building optimization expertise on the team and teaching individual engineershow to optimize.\\nA few top-level things to keep in mind when coding in native Spark:\\n1. Filter early and often.\\n2. Rely heavily on the core Spark API, and learn to understand the Spark native way of doing things.Try to rely on well-maintained public libraries if the native Spark API doesn’t support your use case.Good Spark code is substantially declarative.\\n3. Be careful with UDFs.\\n4. Consider intermixing SQL.\\nRecommendation 1 applies to SQL optimization as well, the difference being that Spark may not be ableto reorder something that SQL would handle for you automatically. Spark is a big data processingframework, but the less data you have to process, the less resource-heavy and more performant yourcode will be.\\nIf you find yourself writing extremely complex custom code, pause and determine whether there’s amore native way of doing whatever you’re trying to accomplish. Learn to understand idiomatic Spark byreading public examples and working through tutorials. Is there something in the Spark API that canaccomplish what you’re trying to do? Is there a well-maintained and optimized public library that canhelp?\\nThe third recommendation is crucial for PySpark. In general, PySpark is an API wrapper for ScalaSpark. Your code pushes work into native Scala code running in the JVM by calling the API. RunningPython UDFs forces data to be passed to Python, where processing is less efficient. If you find yourselfusing Python UDFs, look for a more Spark-native way to accomplish what you’re doing. Go back to therecommendation: is there a way to accomplish your task by using the core API or a well-maintainedlibrary? If you must use UDFs, consider rewriting them in Scala or Java to improve performance.\\nAs for recommendation 4, using SQL allows us to take advantage of the Spark Catalyst optimizer, whichmay be able to squeeze out more performance than we can with native Spark code. SQL is often easierto write and maintain for simple operations. Combining native Spark and SQL lets us realize the best ofboth worlds—powerful, general-purpose functionality combined with simplicity where applicable.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='462eb32a-5e86-4481-beb3-8e7d2717640b', embedding=None, metadata={'page_label': '468', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Much of the optimization advice in this section is fairly generic and would apply just as well to ApacheBeam, for example. The main point is that programmable data processing APIs require a bit moreoptimization finesse than SQL, which is perhaps less powerful and easier to use.\\nUpdate patterns\\nSince transformations persist data, we will often update persisted data in place. Updating data is amajor pain point for data engineering teams, especially as they transition between data engineeringtechnologies. We’re discussing DML in SQL, which we introduced earlier in the chapter.\\nWe’ve mentioned several times throughout the book that the original data lake concept didn’t reallyaccount for updating data. This now seems nonsensical for several reasons. Updating data has longbeen a key part of handling data transformation results, even though the big data community dismissedit. It is silly to rerun significant amounts of work because we have no update capabilities. Thus, the datalakehouse concept now builds in updates. Also, GDPR and other data deletion standards now requireorganizations to delete data in a targeted fashion, even in raw datasets.\\nLet’s consider several basic update patterns.\\nTruncate and reload\\nTruncate is an update pattern that doesn’t update anything. It simply wipes the old data. In a truncate-and-reload update pattern, a table is cleared of data, and transformations are rerun and loaded into thistable, effectively generating a new table version.\\nInsert only\\nInsert only inserts new records without changing or deleting old records. Insert-only patterns can beused to maintain a current view of data—for example, if new versions of records are inserted withoutdeleting old records. A query or view can present the current data state by finding the newest record byprimary key. Note that columnar databases don’t typically enforce primary keys. The primary key wouldbe a construct used by engineers to maintain a notion of the current state of the table. The downside tothis approach is that it can be extremely computationally expensive to find the latest record at querytime. Alternatively, we can use a materialized view (covered later in the chapter), an insert-only tablethat maintains all records, and a truncate-and-reload target table that holds the current state for servingdata.\\nWARNING\\nWhen inserting data into a column-oriented OLAP database, the common problem is that engineers transitioningfrom row-oriented systems attempt to use single-row inserts. This antipattern puts a massive load on the system. Italso causes data to be written in many separate files; this is extremely inefficient for subsequent reads, and thedata must be reclustered later. Instead, we recommend loading data in a periodic micro-batch or batch fashion.\\nWe’ll mention an exception to the advice not to insert frequently: the enhanced Lambda architectureused by BigQuery and Apache Druid, which hybridizes a streaming buffer with columnar storage.Deletes and in-place updates can still be expensive, as we’ll discuss next.\\nDelete\\nDeletion is critical when a source system deletes data and satisfies recent regulatory changes. Incolumnar systems and data lakes, deletes are more expensive than inserts.\\nWhen deleting data, consider whether you need to do a hard or soft delete. A hard delete permanentlyremoves a record from a database, while a soft delete marks the record as “deleted.” Hard deletes areuseful when you need to remove data for performance reasons (say, a table is too big), or if there’s a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='459faea8-1faa-4cf4-a6a7-bf3c64510e7a', embedding=None, metadata={'page_label': '469', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='legal or compliance reason to do so. Soft deletes might be used when you don’t want to delete a recordpermanently but also want to filter it out of query results.\\nA third approach to deletes is closely related to soft deletes: insert deletion inserts a new record with adeleted flag without modifying the previous version of the record. This allows us to follow an insert-only pattern but still account for deletions. Just note that our query to get the latest table state gets alittle more complicated. We must now deduplicate, find the latest version of each record by key, and notshow any record whose latest version shows deleted.\\nUpsert/merge\\nOf these update patterns, the upsert and merge patterns are the ones that consistently cause the mosttrouble for data engineering teams, especially for people transitioning from row-based data warehousesto column-based cloud systems.\\nUpserting takes a set of source records and looks for matches against a target table by using a primarykey or another logical condition. (Again, it’s the responsibility of the data engineering team to managethis primary key by running appropriate queries. Most columnar systems will not enforce uniqueness.)When a key match occurs, the target record gets updated (replaced by the new record). When no matchexists, the database inserts the new record. The merge pattern adds to this the ability to delete records.\\nSo, what’s the problem? The upsert/merge pattern was originally designed for row-based databases. Inrow-based databases, updates are a natural process: the database looks up the record in question andchanges it in place.\\nOn the other hand, file-based systems don’t actually support in-place file updates. All of these systemsutilize copy on write (COW). If one record in a file is changed or deleted, the whole file must be rewrittenwith the new changes.\\nThis is part of the reason that early adopters of big data and data lakes rejected updates: managing filesand updates seemed too complicated. So they simply used an insert-only pattern and assumed thatdata consumers would determine the current state of the data at query time or in downstreamtransformations. In reality, columnar databases such as Vertica have long supported in-place updates byhiding the complexity of COW from users. They scan files, change the relevant records, write new files,and change file pointers for the table. The major columnar cloud data warehouses support updates andmerges, although engineers should investigate update support if they consider adopting an exotictechnology.\\nThere are a few key things to understand here. Even though distributed columnar data systems supportnative update commands, merges come at a cost: the performance impact of updating or deleting asingle record can be quite high. On the other hand, merges can be extremely performant for largeupdate sets and may even outperform transactional databases.\\nIn addition, it is important to understand that COW seldom entails rewriting the whole table. Dependingon the database system in question, COW can operate at various resolutions (partition, cluster, block).To realize performant updates, focus on developing an appropriate partitioning and clustering strategybased on your needs and the innards of the database in question.\\nAs with inserts, be careful with your update or merge frequency. We’ve seen many engineering teamstransition between database systems and try to run near real-time merges from CDC just as they did ontheir old system. It simply doesn’t work. No matter how good your CDC system is, this approach willbring most columnar data warehouses to their knees. We’ve seen systems fall weeks behind onupdates, where an approach that simply merged every hour would make much more sense.\\nWe can use various approaches to bring columnar databases closer to real time. For example,BigQuery allows us to stream insert new records into a table, and then supports specialized materializedviews that present an efficient, near real-time deduplicated table view. Druid uses two-tier storage andSSDs to support ultrafast real-time queries.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='83707c1e-637c-48da-8616-d234cac24cca', embedding=None, metadata={'page_label': '470', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Schema updates\\nData has entropy, and may change without your control or consent. External data sources may changetheir schema, or application development teams may add new fields to the schema. One advantage ofcolumnar systems over row-based systems is that while updating the data is more difficult, updating theschema is easier. Columns can typically be added, deleted, and renamed.\\nIn spite of these technological improvements, practical organizational schema management is morechallenging. Will some schema updates be automated? (This is the approach that Fivetran uses whenreplicating from sources.) As convenient as this sounds, there’s a risk that downstream transformationswill break.\\nIs there a straightforward schema update request process? Suppose a data science team wants to adda column from a source that wasn’t previously ingested. What will the review process look like? Willdownstream processes break? (Are there queries that run SELECT * rather than using explicit columnselection? This is generally bad practice in columnar databases.) How long will it take to implement thechange? Is it possible to create a table fork—i.e., a new table version specific to this project?\\nA new interesting option has emerged for semistructured data. Borrowing an idea from document stores,many cloud data warehouses now support data types that encode arbitrary JSON data. One approachstores raw JSON in a field while storing frequently accessed data in adjacent flattened fields. This takesup additional storage space but allows for the convenience of flattened data, with the flexibility ofsemistructured data for advanced users. Frequently accessed data in the JSON field can be addeddirectly into the schema over time.\\nThis approach works extremely well when data engineers must ingest data from an applicationdocument store with a frequently changing schema. Semistructured data available as a first-class citizenin data warehouses is extremely flexible and opens new opportunities for data analysts and datascientists since data is no longer constrained to rows and columns.\\nData wrangling\\nData wrangling takes messy, malformed data and turns it into useful, clean data. This is generally abatch transformation process.\\nData wrangling has long been a major source of pain and job security for ETL developers. For example,suppose that developers receive EDI data (see Chapter 7) from a partner business regardingtransactions and invoices, potentially a mix of structured data and text. The typical process of wranglingthis data involves first trying to ingest it. Often, the data is so malformed that a good deal of textpreprocessing is involved. Developers may choose to ingest the data as a single text field table—anentire row ingested as a single field. Developers then begin writing queries to parse and break apart thedata. Over time, they discover data anomalies and edge cases. Eventually, they will get the data intorough shape. Only then can the process of downstream transformation begin.\\nData wrangling tools aim to simplify significant parts of this process. These tools often put off dataengineers because they claim to be no code, which sounds unsophisticated. We prefer to think of datawrangling tools as integrated development environments (IDEs) for malformed data. In practice, dataengineers spend way too much time parsing nasty data; automation tools allow data engineers to spendtime on more interesting tasks. Wrangling tools may also allow engineers to hand some parsing andingestion work off to analysts.\\nGraphical data-wrangling tools typically present a sample of data in a visual interface, with inferredtypes, statistics including distributions, anomalous data, outliers, and nulls. Users can then addprocessing steps to fix data issues. A step might provide instructions for dealing with mistyped data,splitting a text field into multiple parts, or joining with a lookup table.\\nUsers can run the steps on a full dataset when the full job is ready. The job typically gets pushed to ascalable data processing system such as Spark for large datasets. After the job runs, it will return errors', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5429288b-8aba-4d9e-823a-fd0ae9a4719a', embedding=None, metadata={'page_label': '471', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and unhandled exceptions. The user can further refine the recipe to deal with these outliers.\\nWe highly recommend that both aspiring and seasoned engineers experiment with wrangling tools;major cloud providers sell their version of data-wrangling tools, and many third-party options areavailable. Data engineers may find that these tools significantly streamline certain parts of their jobs.Organizationally, data engineering teams may want to consider training specialists in data wrangling ifthey frequently ingest from new, messy data sources.\\nExample: Data transformation in Spark\\nLet’s look at a practical, concrete example of data transformation. Suppose we build a pipeline thatingests data from three API sources in JSON format. This initial ingestion step is handled in Airflow.Each data source gets its prefix (filepath) in an S3 bucket.\\nAirflow then triggers a Spark job by calling an API. This Spark job ingests each of the three sources intoa dataframe, converting the data into a relational format, with nesting in certain columns. The Spark jobcombines the three sources into a single table, and then filters the results with a SQL statement. Theresults are finally written out to a Parquet-formatted Delta Lake table stored in S3.\\nIn practice, Spark creates a DAG of steps based on the code that we write for ingesting, joining, andwriting out the data. The basic ingestion of data happens in cluster memory, although one of the datasources is large enough that it must spill to disk during the ingestion process. (This data gets written tocluster storage; it will be reloaded into memory for subsequent processing steps.)\\nThe join requires a shuffle operation. A key is used to redistribute data across the cluster; once again, aspill to disk occurs as the data is written to each node. The SQL transformation filters through the rowsin memory and discards the unused rows. Finally, Spark converts the data into Parquet format,compresses it, and writes it back to S3. Airflow periodically calls back to Spark to see if the job iscompleted. Once it confirms that the job has finished, it marks the full Airflow DAG as completed. (Notethat we have two DAG constructs here, an Airflow DAG and a DAG specific to the Spark job.)\\nBusiness logic and derived data\\nOne of the most common use cases for transformation is to render business logic. We’ve placed thisdiscussion under batch transformations because this is where this type of transformation happens mostfrequently, but note that it could also happen in a streaming pipeline.\\nSuppose that a company uses multiple specialized internal profit calculations. One version might look atprofits before marketing costs, and another might look at a profit after subtracting marketing costs. Eventhough this appears to be a straightforward accounting exercise, each of these metrics is highly complexto render.\\nProfit before marketing costs might need to account for fraudulent orders; determining a reasonableprofit estimate for the previous business day entails estimating what percentage of revenue and profitwill ultimately be lost to orders canceled in the coming days as the fraud team investigates suspiciousorders. Is there a special flag in the database that indicates an order with a high probability of fraud, orone that has been automatically canceled? Does the business assume that a certain percentage oforders will be canceled because of fraud even before the fraud-risk evaluation process has beencompleted for specific orders?\\nFor profits after marketing costs, we must account for all the complexities of the previous metric, plusthe marketing costs attributed to the specific order. Does the company have a naive attribution model—e.g., marketing costs attributed to items weighted by price? Marketing costs might also be attributed perdepartment, or item category, or—in the most sophisticated organizations—per individual item based onuser ad clicks.\\nThe business logic transformation that generates this nuanced version of profit must integrate all thesubtleties of attribution—i.e., a model that links orders to specific ads and advertising costs. Is attribution', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='edb47978-ee42-4ec0-bda3-5ae38b7dfa87', embedding=None, metadata={'page_label': '472', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data stored in the guts of ETL scripts, or is it pulled from a table that is automatically generated from adplatform data?\\nThis type of reporting data is a quintessential example of derived data—data computed from other datastored in a data system. Derived data critics will point out that it is challenging for the ETL to maintainconsistency in the derived metrics. For example, if the company updates its attribution model, thischange may need to be merged into many ETL scripts for reporting. (ETL scripts are notorious forbreaking the DRY principle.) Updating these ETL scripts is a manual and labor-intensive process,involving domain expertise in processing logic and previous changes. Updated scripts must also bevalidated for consistency and accuracy.\\nFrom our perspective, these are legitimate criticisms but not necessarily very constructive because thealternative to derived data in this instance is equally distasteful. Analysts will need to run their reportingqueries if profit data is not stored in the data warehouse, including profit logic. Updating complex ETLscripts to represent changes to business logic accurately is an overwhelming, labor-intensive task, butgetting analysts to update their reporting queries consistently is well-nigh impossible.\\nOne interesting alternative is to push business logic into a metrics layer,  but still leverage the datawarehouse or other tool to do the computational heavy lifting. A metrics layer encodes business logicand allows analysts and dashboard users to build complex analytics from a library of defined metrics.The metrics layer generates queries from the metrics and sends these to the database. We discusssemantic and metrics layers in more detail in Chapter 9.\\nMapReduce\\nNo discussion of batch transformation can be complete without touching on MapReduce. This isn’tbecause MapReduce is widely used by data engineers these days. MapReduce was the defining batchdata transformation pattern of the big data era, it still influences many distributed systems dataengineers use today, and it’s useful for data engineers to understand at a basic level. MapReduce wasintroduced by Google in a follow-up to its paper on GFS. It was initially the de facto processing patternof Hadoop, the open source analogue technology of GFS that we introduced in Chapter 6.\\nA simple MapReduce job consists of a collection of map tasks that read individual data blocks scatteredacross the nodes, followed by a shuffle that redistributes result data across the cluster and a reducestep that aggregates data on each node. For example, suppose that we wanted to run the following SQLquery:\\nSELECT COUNT(*), user_idFROM user_eventsGROUP BY user_id;\\nThe table data is spread across nodes in data blocks; the MapReduce job generates one map task perblock. Each map task essentially runs the query on a single block—i.e., it generates a count for eachuser ID that appears in the block. While a block might contain hundreds of megabytes, the full tablecould be petabytes in size. However, the map portion of the job is a nearly perfect example ofembarrassing parallelism; the data scan rate across the full cluster essentially scales linearly with thenumber of nodes.\\nWe then need to aggregate (reduce) to gather results from the full cluster. We’re not gathering results toa single node; rather, we redistribute results by key so that each key ends up on one and only one node.This is the shuffle step, which is often executed using a hashing algorithm on keys. Once the mapresults have been shuffled, we sum the results for each key. The key/count pairs can be written to thelocal disk on the node where they are computed. We collect the results stored across nodes to view thefull query results.\\n16\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='683ebcbb-4241-4b47-b56a-ca471f5ad924', embedding=None, metadata={'page_label': '473', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Real-world MapReduce jobs can be far more complex than what we describe here. A complex querythat filters with a WHERE clause joins three tables and applies a window function that would consist ofmany map and reduce stages.\\nAfter MapReduce\\nGoogle’s original MapReduce model is extremely powerful but is now viewed as excessively rigid. Itutilizes numerous short-lived ephemeral tasks that read from and write to disk. In particular, nointermediate state is preserved in memory; all data is transferred between tasks by storing it to disk orpushing it over the network. This simplifies state and workflow management, and minimizes memoryconsumption, but it can also drive high-disk bandwidth utilization and increase processing time.\\nThe MapReduce paradigm was constructed around the idea that magnetic disk capacity and bandwidthwere so cheap that it made sense to simply throw a massive amount of disk at data to realize ultra-fastqueries. This worked to an extent; MapReduce repeatedly set data processing records during the earlydays of Hadoop.\\nHowever, we have lived in a post-MapReduce world for quite some time. Post-MapReduce processingdoes not truly discard MapReduce; it still includes the elements of map, shuffle, and reduce, but itrelaxes the constraints of MapReduce to allow for in-memory caching. Recall that RAM is much fasterthan SSD and HDDs in transfer speed and seek time. Persisting even a tiny amount of judiciouslychosen data in memory can dramatically speed up specific data processing tasks and utterly crush theperformance of MapReduce.\\nFor example, Spark, BigQuery, and various other data processing frameworks were designed around in-memory processing. These frameworks treat data as a distributed set that resides in memory. If dataoverflows available memory, this causes a spill to disk. The disk is treated as a second-class data-storage layer for processing, though it is still highly valuable.\\nThe cloud is one of the drivers for the broader adoption of memory caching; it is much more effective tolease memory during a specific processing job than to own it 24 hours a day. Advancements inleveraging memory for transformations will continue to yield gains for the foreseeable future.\\nMaterialized Views, Federation, and Query Virtualization\\nIn this section, we look at several techniques that virtualize query results by presenting them as table-like objects. These techniques can become part of a transformation pipeline or sit right before end-userdata consumption.\\nViews\\nFirst, let’s review views to set the stage for materialized views. A view is a database object that we canselect from just like any other table. In practice, a view is just a query that references other tables. Whenwe select from a view, that database creates a new query that combines the view subquery with ourquery. The query optimizer then optimizes and runs the full query.\\nViews play a variety of roles in a database. First, views can serve a security role. For example, viewscan select only specific columns and filter rows, thus providing restricted data access. Various views canbe created for job roles depending on user data access.\\nSecond, a view might be used to provide a current deduplicated picture of data. If we’re using an insert-only pattern, a view may be used to return a deduplicated version of a table showing only the latestversion of each record.\\nThird, views can be used to present common data access patterns. Suppose that marketing analystsmust frequently run a query that joins five tables. We could create a view that joins together these fivetables into a wide table. Analysts can then write queries that filter and aggregate on top of this view.\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='caacd58d-93bb-4dfc-b503-d09df1f348c2', embedding=None, metadata={'page_label': '474', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Materialized views\\nWe mentioned materialized views in our earlier discussion of query caching. A potential disadvantage of(nonmaterialized) views is that they don’t do any precomputation. In the example of a view that joins fivetables, this join must run every time a marketing analyst runs a query on this view, and the join could beextremely expensive.\\nA materialized view does some or all of the view computation in advance. In our example, a materializedview might save the five table join results every time a change occurs in the source tables. Then, whena user references the view, they’re querying from the prejoined data. A materialized view is a de factotransformation step, but the database manages execution for convenience.\\nMaterialized views may also serve a significant query optimization role depending on the database,even for queries that don’t directly reference them. Many query optimizers can identify queries that “looklike” a materialized view. An analyst may run a query that uses a filter that appears in a materializedview. The optimizer will rewrite the query to select from the precomputed results.\\nComposable materialized views\\nIn general, materialized views do not allow for composition—that is, a materialized view cannot selectfrom another materialized view. However, we’ve recently seen the emergence of tools that support thiscapability. For example, Databricks has introduced the notion of live tables. Each table is updated asdata arrives from sources. Data flows down to subsequent tables asynchronously.\\nFederated queries\\nFederated queries are a database feature that allows an OLAP database to select from an external datasource, such as object storage or RDBMS. For example, let’s say you need to combine data acrossobject storage and various tables in MySQL and PostgreSQL databases. Your data warehouse canissue a federated query to these sources and return the combined results (Figure 8-19).\\nFigure 8-19. An OLAP database issues a federated query that gets data from object storage, MySQL, and PostgreSQL andreturns a query result with the combined data\\nAs another example, Snowflake supports the notion of external tables defined on S3 buckets. Anexternal data location and a file format are defined when creating the table, but data is not yet ingestedinto the table. When the external table is queried, Snowflake reads from S3 and processes the data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ec7ebec3-c67a-4274-9f69-dec22c91f7c4', embedding=None, metadata={'page_label': '475', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='based on the parameters set at the time of the table’s creation. We can even join S3 data to internaldatabase tables. This makes Snowflake and similar databases more compatible with a data lakeenvironment.\\nSome OLAP systems can convert federated queries into materialized views. This gives us much of theperformance of a native table without the need to manually ingest data every time the external sourcechanges. The materialized view gets updated whenever the external data changes.\\nData virtualization\\nData virtualization is closely related to federated queries, but this typically entails a data processing andquery system that doesn’t store data internally. Right now, Trino (e.g., Starburst) and Presto areexamples par excellence. Any query/processing engine that supports external tables can serve as adata virtualization engine. The most significant considerations with data virtualization are supportedexternal sources and performance.\\nA closely related concept is the notion of query pushdown. Suppose I wanted to query data fromSnowflake, join data from a MySQL database, and filter the results. Query pushdown aims to move asmuch work as possible to the source databases. The engine might look for ways to push filteringpredicates into the queries on the source systems. This serves two purposes: first, it offloadscomputation from the virtualization layer, taking advantage of the query performance of the source.Second, it potentially reduces the quantity of data that must push across the network, a criticalbottleneck for virtualization performance.\\nData virtualization is a good solution for organizations with data stored across various data sources.However, data virtualization should not be used haphazardly. For example, virtualizing a productionMySQL database doesn’t solve the core problem of analytics queries adversely impacting theproduction system—because Trino does not store data internally, it will pull from MySQL every time itruns a query.\\nAlternatively, data virtualization can be used as a component of data ingestion and processing pipelines.For instance, Trino might be used to select from MySQL once a day at midnight when the load on theproduction system is low. Results could be saved into S3 for consumption by downstreamtransformations and daily queries, protecting MySQL from direct analytics queries.\\nData virtualization can be viewed as a tool that expands the data lake to many more sources byabstracting away barriers used to silo data between organizational units. An organization can storefrequently accessed, transformed data in S3 and virtualize access between various parts of thecompany. This fits closely with the notion of a data mesh (discussed in Chapter 3), wherein small teamsare responsible for preparing their data for analytics and sharing it with the rest of the company;virtualization can serve as a critical access layer for practical sharing.\\nStreaming Transformations and Processing\\nWe’ve already discussed stream processing in the context of queries. The difference between streamingtransformations and streaming queries is subtle and warrants more explanation.\\nBasics\\nStreaming queries run dynamically to present a current view of data, as discussed previously. Streamingtransformations aim to prepare data for downstream consumption.\\nFor instance, a data engineering team may have an incoming stream carrying events from an IoTsource. These IoT events carry a device ID and event data. We wish to dynamically enrich these eventswith other device metadata, which is stored in a separate database. The stream-processing enginequeries a separate database containing this metadata by device ID, generates new events with the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='275be18d-a1ec-48c8-869d-e917c9309bd1', embedding=None, metadata={'page_label': '476', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='added data, and passes it on to another stream. Live queries and triggered metrics run on this enrichedstream (see Figure 8-20).\\nFigure 8-20. An incoming stream is carried by a streaming event platform and passed into a stream processor\\nTransformations and queries are a continuum\\nThe line between transformations and queries is also blurry in batch processing, but the differencesbecome even more subtle in the domain of streaming. For example, if we dynamically compute roll-upstatistics on windows, and then send the output to a target stream, is this a transformation or a query?\\nMaybe we will eventually adopt new terminology for stream processing that better represents real-worlduse cases. For now, we will do our best with the terminology we have.\\nStreaming DAGs\\nOne interesting notion closely related to stream enrichment and joins is the streaming DAG.  We firsttalked about this idea in our discussion of orchestration in Chapter 2. Orchestration is inherently a batchconcept, but what if we wanted to enrich, merge, and split multiple streams in real time?\\nLet’s take a simple example where streaming DAG would be useful. Suppose that we want to combinewebsite clickstream data with IoT data. This will allow us to get a unified view of user activity bycombining IoT events with clicks. Furthermore, each data stream needs to be preprocessed into astandard format (see Figure 8-21).\\nFigure 8-21. A simple streaming DAG\\nThis has long been possible by combining a streaming store (e.g., Kafka) with a stream processor (e.g.,Flink). Creating the DAG amounted to building a complex Rube Goldberg machine, with numeroustopics and processing jobs connected.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cd764b2e-9edf-4681-a5ce-0587289546bd', embedding=None, metadata={'page_label': '477', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pulsar dramatically simplifies this process by treating DAGs as a core streaming abstraction. Ratherthan managing flows across several systems, engineers can define their streaming DAGs as codeinside a single system.\\nMicro-batch versus true streaming\\nA long-running battle has been ongoing between micro-batch and true streaming approaches.Fundamentally, it’s important to understand your use case, the performance requirements, and theperformance capabilities of the framework in question.\\nMicro-batching is a way to take a batch-oriented framework and apply it in a streaming situation. Amicro-batch might run anywhere from every two minutes to every second. Some micro-batchframeworks (e.g., Apache Spark Streaming) are designed for this use case and will perform well withappropriately allocated resources at a high batch frequency. (In truth, DBAs and engineers have longused micro-batching with more traditional databases; this often led to horrific performance and resourceconsumption.)\\nTrue streaming systems (e.g., Beam and Flink) are designed to process one event at a time. However,this comes with significant overhead. Also, it’s important to note that even in these true streamingsystems, many processes will still occur in batches. A basic enrichment process that adds data toindividual events can deliver one event at a time with low latency. However, a triggered metric onwindows may run every few seconds, every few minutes, etc.\\nWhen you’re using windows and triggers (hence, batch processing), what’s the window frequency?What’s the acceptable latency? If you are collecting Black Friday sales metrics published every fewminutes, micro-batches are probably just fine as long as you set an appropriate micro-batch frequency.On the other hand, if your ops team is computing metrics every second to detect DDoS attacks, truestreaming may be in order.\\nWhen should you use one over the other? Frankly, there is no universal answer. The term micro-batchhas often been used to dismiss competing technologies, but it may work just fine for your use case, andcan be superior in many respects depending on your needs. If your team already has expertise inSpark, you will be able to spin up a Spark (micro-batch) streaming solution extremely fast.\\nThere’s no substitute for domain expertise and real-world testing. Talk to experts who can present aneven-handed opinion. You can also easily test the alternatives by spinning up tests on cloudinfrastructure. Also, watch out for spurious benchmarks provided by vendors. Vendors are notorious forcherry-picking benchmarks and setting up artificial examples that don’t match reality (recall ourconversation on benchmarks in Chapter 4). Frequently, vendors will show massive advantages in theirbenchmark results but fail to deliver in the real world for your use case.\\nWhom You’ll Work With\\nQueries, transformations, and modeling impact all stakeholders up and down the data engineeringlifecycle. The data engineer is responsible for several things at this stage in the lifecycle. From atechnical angle, the data engineer designs, builds, and maintains the integrity of the systems that queryand transform data. The data engineer also implements data models within this system. This is the most“full-contact” stage where your focus is to add as much value as possible, both in terms of functioningsystems and reliable and trustworthy data.\\nUpstream Stakeholders\\nWhen it comes to transformations, upstream stakeholders can be broken into two broad categories:those who control the business definitions and those who control the systems generating data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e5031e8b-1101-4216-aa7c-9c58d6907e4f', embedding=None, metadata={'page_label': '478', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='When interfacing with upstream stakeholders about business definitions and logic, you’ll need to knowthe data sources—what they are, how they’re used, and the business logic and definitions involved.You’ll work with the engineers in charge of these source systems and the business stakeholders whooversee the complementary products and apps. A data engineer might work alongside “the business”and technical stakeholders on a data model.\\nThe data engineer needs to be involved in designing the data model and later updates because ofchanges in business logic or new processes. Transformations are easy enough to do; just write a queryand plop the results into a table or view. Creating them so they’re both performant and valuable to thebusiness is another matter. Always keep the requirements and expectations of the business top of mindwhen transforming data.\\nThe stakeholders of the upstream systems want to make sure your queries and transformationsminimally impact their systems. Ensure bidirectional communication about changes to the data models(column and index changes, for example) in source systems, as these can directly impact queries,transformations, and analytical data models. Data engineers should know about schema changes,including the addition or deletion of fields, data type changes, and anything else that might materiallyimpact the ability to query and transform data.\\nDownstream Stakeholders\\nTransformations are where data starts providing utility to downstream stakeholders. Your downstreamstakeholders include many people, including data analysts, data scientists, ML engineers, and “thebusiness.” Collaborate with them to ensure the data model and transformations you provide areperformant and useful. In terms of performance, queries should execute as quickly as possible in themost cost-effective way. What do we mean by useful? Analysts, data scientists, and ML engineersshould be able to query a data source with the confidence the data is of the highest quality andcompleteness, and can be integrated into their workflows and data products. The business should beable to trust that transformed data is accurate and actionable.\\nUndercurrents\\nThe transformation stage is where your data mutates and morphs into something useful for thebusiness. Because there are many moving parts, the undercurrents are especially critical at this stage.\\nSecurity\\nQueries and transformations combine disparate datasets into new datasets. Who has access to this newdataset? If someone does have access to a dataset, continue to control who has access to a dataset’scolumn, row, and cell-level access.\\nBe aware of attack vectors against your database at query time. Read/write privileges to the databasemust be tightly monitored and controlled. Query access to the database must be controlled in the sameway as you normally control access to your organization’s systems and environments.\\nKeep credentials hidden; avoid copying and pasting passwords, access tokens, or other credentials intocode or unencrypted files. It’s shockingly common to see code in GitHub repositories with databaseusernames and passwords pasted directly in the codebase! It goes without saying, don’t sharepasswords with other users. Finally, never allow unsecured or unencrypted data to traverse the publicinternet.\\nData Management', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4936ce6f-13af-4cbd-b43f-0097bb1da8bc', embedding=None, metadata={'page_label': '479', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Though data management is essential at the source system stage (and every other stage of the dataengineering lifecycle), it’s especially critical at the transformation stage. Transformation inherentlycreates new datasets that need to be managed. As with other stages of the data engineering lifecycle,it’s critical to involve all stakeholders in data models and transformations and manage theirexpectations. Also, make sure everyone agrees on naming conventions that align with the respectivebusiness definitions of the data. Proper naming conventions should be reflected in easy-to-understandfield names. Users can also check in a data catalog for more clarity on what the field means when it wascreated, who maintains the dataset, and other relevant information.\\nAccounting for definitional accuracy is key at the transformation stage. Does the transformation adhereto the expected business logic? Increasingly, the notion of a semantic or metrics layer that sitsindependent of transformations is becoming popular. Instead of enforcing business logic within thetransformation at runtime, why not keep these definitions as a standalone stage before yourtransformation layer? While it’s still early days, expect to see semantic and metrics layers becomingmore popular and commonplace in data engineering and data management.\\nBecause transformations involve mutating data, it’s critical to ensure that the data you’re using is free ofdefects and represents ground truth. If MDM is an option at your company, pursue its implementation.Conformed dimensions and other transformations rely on MDM to preserve data’s original integrity andground truth. If MDM isn’t possible, work with upstream stakeholders who control the data to ensure thatany data you’re transforming is correct and complies with the agreed-upon business logic.\\nData transformations make it potentially difficult to know how a dataset was derived along the samelines. In Chapter 6, we discussed data catalogs. As we transform data, data lineage tools becomeinvaluable. Data lineage tools help both data engineers, who must understand previous transformationsteps as they create new transformations, and analysts, who need to understand where data came fromas they run queries and build reports.\\nFinally, what impact does regulatory compliance have on your data model and transformations? Aresensitive fields data masked or obfuscated if necessary? Do you have the ability to delete data inresponse to deletion requests? Does your data lineage tracking allow you to see data derived fromdeleted data, and rerun transformations to remove data downstream of raw sources?\\nDataOps\\nWith queries and transformations, DataOps has two areas of concern: data and systems. You need tomonitor and be alerted for changes or anomalies in these areas. The field of data observability isexploding right now, with a big focus on data reliability. There’s even a recent job title called datareliability engineer. This section emphasizes data observability and data health, which focuses on thequery and transformation stage.\\nLet’s start with the data side of DataOps. When you query data, are the inputs and outputs correct? Howdo you know? If this query is saved to a table, is the schema correct? How about the shape of the dataand related statistics such as min/max values, null counts, and more? You should run data-quality testson the input datasets and the transformed dataset, which will ensure that the data meets theexpectations of upstream and downstream users. If there’s a data-quality issue in the transformation,you should have the ability to flag this issue, roll back the changes, and investigate the root cause.\\nNow let’s look at the Ops part of DataOps. How are the systems performing? Monitor metrics such asquery queue length, query concurrency, memory usage, storage utilization, network latency, and diskI/O. Use metric data to spot bottlenecks and poor-performing queries that might be candidates forrefactoring and tuning. If the query is perfectly fine, you’ll have a good idea of where to tune thedatabase itself (for instance, by clustering a table for faster lookup performance). Or, you may need toupgrade the database’s compute resources. Today’s cloud and SaaS databases give you a ton of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d185e5fc-ef30-4677-a951-829fd395ad21', embedding=None, metadata={'page_label': '480', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='flexibility for quickly upgrading (and downgrading) your system. Take a data-driven approach and useyour observability metrics to pinpoint whether you have a query or a systems-related issue.\\nThe shift toward SaaS-based analytical databases changes the cost profile of data consumption. In thedays of on-premises data warehouses, the system and licenses were purchased up front, with noadditional usage cost. Whereas traditional data engineers would focus on performance optimization tosqueeze the maximum utility out of their expensive purchases, data engineers working with cloud datawarehouses that charge on a consumption basis need to focus on cost management and costoptimization. This is the practice of FinOps (see Chapter 4).\\nData Architecture\\nThe general rules of good data architecture in Chapter 3 apply to the transformation stage. Build robustsystems that can process and transform data without imploding. Your choices for ingestion and storagewill directly impact your general architecture’s ability to perform reliable queries and transformations. Ifthe ingestion and storage are appropriate to your query and transformation patterns, you should be in agreat place. On the other hand, if your queries and transformations don’t work well with your upstreamsystems, you’re in for a world of pain.\\nFor example, we often see data teams using the wrong data pipelines and databases for the job. A datateam might connect a real-time data pipeline to an RDBMS or Elasticsearch and use this as their datawarehouse. These systems are not optimized for high-volume aggregated OLAP queries and willimplode under this workload. This data team clearly didn’t understand how their architectural choiceswould impact query performance. Take the time to understand the trade-offs inherent in yourarchitecture choices; be clear about how your data model will work with ingestion and storage systems,and how queries will perform.\\nOrchestration\\nData teams often manage their transformation pipelines using simple time-based schedules—e.g., cronjobs. This works reasonably well at first but turns into a nightmare as workflows grow more complicated.Use orchestration to manage complex pipelines using a dependency-based approach. Orchestration isalso the glue that allows us to assemble pipelines that span multiple systems.\\nSoftware Engineering\\nWhen writing transformation code, you can use many languages such as SQL, Python, and JVM-basedlanguages, and platforms ranging from data warehouses to distributed computing clusters, andeverything in between. Each language and platform has its strengths and quirks, so you should knowthe best practices of your tools. For example, you might write data transformations in Python, poweredby a distributed system such as Spark or Dask. When running a data transformation, are you using aUDF when a native function might work much better? We’ve seen cases where poorly written, sluggishUDFs were replaced by a built-in SQL command, with instant and dramatic improvement inperformance.\\nThe rise of analytics engineering brings software engineering practices to end users, with the notion ofanalytics as code. Analytics engineering transformation tools like dbt have exploded in popularity, givinganalysts and data scientists the ability to write in-database transformations using SQL, without the directintervention of a DBA or a data engineer. In this case, the data engineer is responsible for setting up thecode repository and CI/CD pipeline used by the analysts and data scientists. This is a big change in therole of a data engineer, who would historically build and manage the underlying infrastructure and createthe data transformations. As data tools lower the barriers to entry and become more democratizedacross data teams, it will be interesting to see how the workflows of data teams change.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='793a4fd4-4b7b-43a4-b2a6-b29635b787af', embedding=None, metadata={'page_label': '481', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Using a GUI-based low-code tool, you’ll get useful visualizations of the transformation workflow. You stillneed to understand what’s going on under the hood. These GUI-based transformation tools will oftengenerate SQL or some other language behind the scenes. While the point of a low-code tool is toalleviate the need to be involved in low-level details, understanding the code behind the scenes will helpwith debugging and performance optimization. Blindly assuming that the tool is generating performantcode is a mistake.\\nWe suggest that data engineers pay particular attention to software engineering best practices at thequery and transformation stage. While it’s tempting to simply throw more processing resources at adataset, knowing how to write clean, performant code is a much better approach.\\nConclusion\\nTransformations sit at the heart of data pipelines. Frankly, transformations are the “sexy” part of dataengineering. It’s critical to keep in mind the purpose of transformations. Ultimately, engineers are nothired to play with the latest technological toys but to serve their customers.\\nOur opinion is that it is possible to adopt exciting transformation technologies and serve stakeholders.Chapter 11 talks about the live data stack, essentially reconfiguring the data stack around streamingdata ingestion and bringing transformation workflows closer to the source system applicationsthemselves. Engineering teams that think about real-time data as the technology for the sake oftechnology will repeat the mistakes of the big data era. But in reality, the majority of organizations thatwe work with have a business use case that would benefit from streaming data. Identifying these usecases and focusing on the value before choosing technologies and complex systems is key.\\nAs we head into the serving stage of the data engineering lifecycle in Chapter 9, reflect on technologyas a tool for realizing organizational goals. If you’re a working data engineer, think about howimprovements in transformation systems could help you to serve your end customers better. If you’rejust embarking on a path toward data engineering, think about the kinds of business problems you’reinterested in solving with technology.\\nAdditional Resources\\n“How a SQL Database Engine Works,” by Dennis Pham\\n“A Detailed Guide on SQL Query Optimization” tutorial by Megha\\n“Modeling of Real-Time Streaming Data?” Stack Exchange thread\\n“Eventual vs. Strong Consistency in Distributed Databases” by Saurabh.v\\n“Caching in Snowflake Data Warehouse” Snowflake Community page\\nGoogle Cloud’s “Using Cached Query Results” documentation\\nHolistics’ “Cannot Combine Fields Due to Fan-Out Issues?” FAQ page\\n“A Simple Explanation of Symmetric Aggregates or ‘Why on Earth Does My SQL Look Like That?’”by Lloyd Tabb\\n“Building a Real-Time Data Vault in Snowflake” by Dmytro Yaroshenko and Kent Graziano\\n“Streaming Event Modeling” by Paul Stanton\\nOracle’s “Slowly Changing Dimensions” tutorial\\n“The New ‘Unified Star Schema’ Paradigm in Analytics Data Modeling Review” by AndriyZabavskyy', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8374d3eb-40b6-4f28-8d09-9afe9066e06d', embedding=None, metadata={'page_label': '482', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“Data Warehouse: The Choice of Inmon vs. Kimball” by Ian Abramson\\n“Inmon or Kimball: Which Approach Is Suitable for Your Data Warehouse?” by Sansu George\\nScienceDirect’s “Corporate Information Factory” web page\\nZentut’s “Bill Inmon Data Warehouse” web page\\n“The Evolution of the Corporate Information Factory” by Bill Inmon\\n“Types of Data Warehousing Architecture” by Amritha Fernando\\n“Difference Between Kimball and Inmon” by manmeetjuneja5\\n“Introduction to Data Warehousing”, “Introduction to Dimensional Modelling for Data Warehousing”,and “Introduction to Data Vault for Data Warehousing” by Simon Kitching\\nGavroshe USA’s “DW 2.0” web page\\nUS patent for “Method and Apparatus for Functional Integration of Metadata”\\nKimball Group’s “Four-Step Dimensional Design Process”, “Conformed Dimensions”, and“Dimensional Modeling Techniques” web pages\\n“Kimball vs. Inmon vs. Vault” Reddit thread\\n“How Should Organizations Structure Their Data?” by Michael Berk\\n“Data Vault—An Overview” by John Ryan\\n“Introduction to Data Vault Modeling” document compiled by Kent Graziano and Dan Linstedt\\n“Data Vault 2.0 Modeling Basics” by Kent Graziano\\nBuilding the Data Warehouse (Wiley), Corporate Information Factory, and The Unified Star Schema(Technics Publications) by W.H. (Bill) Inmon\\nThe Data Warehouse Toolkit by Ralph Kimball and Margy Ross (Wiley)\\nBuilding a Scalable Data Warehouse with Data Vault 2.0 (Morgan Kaufmann) by Daniel Linstedtand Michael Olschimke\\n1 “Rewriting Slow-Running Queries Which Have Explosive Equi-Join Conditions and Additional Join Conditions Basedon Like Operator, ” Snowflake Community website, May 1, 2020, https://oreil.ly/kUsO9.\\n2 See, for example, Emin Gün Sirer, “NoSQL Meets Bitcoin and Brings Down Two Exchanges: The Story of Flexcoinand Poloniex,” Hacking, Distributed, April 6, 2014, https://oreil.ly/RM3QX.\\n3 Some Redshift configurations rely on object storage instead.\\n4 The authors are aware of an incident involving a new analyst at a large grocery store chain running SELECT * on aproduction database and bringing down a critical inventory database for three days.\\n5 Figure 8-11 and the example it depicts are significantly based on “Introducing Stream—Stream Joins in Apache 2.3”by Tathagata Das and Joseph Torres, March 13, 2018.\\n6 For more details on the DRY principle, see The Pragmatic Programmer by David Thomas and Andrew Hunt (Addison-Wesley Professional, 2019).\\n7 E.F. Codd, “Further Normalization of the Data Base Relational Model,” IBM Research Laboratory (1971),https://oreil.ly/Muajm.\\n8 H.W. Inmon, Building the Data Warehouse (Hoboken: Wiley, 2005).\\n9 Inmon, Building the Data Warehouse.\\n10 Inmon, Building the Data Warehouse.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='23bfe4fb-0751-4593-a966-8c544ab39338', embedding=None, metadata={'page_label': '483', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11 Although dimensions and facts are often associated with Kimball, they were first used at General Mills and DartmouthUniversity in the 1960s, and had early adoption at Nielsen and IRI, among other companies.\\n12 The data vault has two versions, 1.0 and 2.0. This section focuses on data vault 2.0, but we’ll call it data vault for thesake of brevity.\\n13 Kent Graziano, “Data Vault 2.0 Modeling Basics,” Vertabelo, October 20, 2015, https://oreil.ly/iuW1U.\\n14 Alex Woodie, “Lakehouses Prevent Data Swamps, Bill Inmon Says,” Datanami, June 1, 2021, https://oreil.ly/XMwWc.\\n15 We remind you to use UDFs responsibly. SQL UDFs often perform reasonably well. We’ve seen JavaScript UDFsincrease query time from a few minutes to several hours.\\n16 Michael Blaha, “Be Careful with Derived Data,” Dataversity, December 5, 2016, https://oreil.ly/garoL.\\n17 Benn Stancil, “The Missing Piece of the Modern Data Stack,” benn.substack, April 22, 2021, https://oreil.ly/GYf3Z.\\n18 “What Is the Difference Between Apache Spark and Hadoop MapReduce?,” Knowledge Powerhouse YouTube video,May 20, 2017, https://oreil.ly/WN0eX.\\n19 For a detailed application of the concept of a streaming DAG, see “Why We Moved from Apache Kafka to ApachePulsar” by Simba Khadder, StreamNative blog, April 21, 2020, https://oreil.ly/Rxfko.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='baadf6fd-bcc2-4149-9469-c257f10d35d6', embedding=None, metadata={'page_label': '484', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 9. Serving Data for\\nAnalytics, Machine Learning,\\nand Reverse ETL\\nCongratulations! You’ve reached the final stage of the data\\nengineering lifecycle—serving data for downstream use cases (see\\nFigure 9-1). In this chapter, you’ll learn about various ways to serve\\ndata for three major use cases you’ll encounter as a data engineer.\\nFirst, you’ll serve data for analytics and BI. You’ll prepare data for\\nuse in statistical analysis, reporting, and dashboards. This is the\\nmost traditional area of data serving. Arguably, it predates IT and\\ndatabases, but it is as important as ever for stakeholders to have\\nvisibility into the business, organizational, and financial processes.\\nFigure 9-1. Serving delivers data for use cases\\nSecond, you’ll serve data for ML applications. ML is not possible\\nwithout high-quality data, appropriately prepared. Data engineers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21afc233-a964-424d-8a9c-dde9f393a50c', embedding=None, metadata={'page_label': '485', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='work with data scientists and ML engineers to acquire, transform,\\nand deliver the data necessary for model training.\\nThird, you’ll serve data through reverse ETL. Reverse ETL is the\\nprocess of sending data back to data sources. For example, we\\nmight acquire data from an ad tech platform, run a statistical process\\non this data to determine cost-per-click bids, and then feed this data\\nback into the ad tech platform. Reverse ETL is highly entangled with\\nBI and ML.\\nBefore we get into these three major ways of serving data, let’s look\\nat some general considerations.\\nGeneral Considerations for Serving Data\\nBefore we get further into serving data, we have a few big\\nconsiderations. First and foremost is trust. People need to trust the\\ndata you’re providing. Additionally, you need to understand your use\\ncases and users, the data products that will be produced, how you’ll\\nbe serving data (self-service or not), data definitions and logic, and\\ndata mesh. The considerations we’ll discuss here are general and\\napply to any of the three ways of serving data. Understanding these\\nconsiderations will help you be much more effective in serving your\\ndata customers.\\nTrust\\nIt takes 20 years to build a reputation and five minutes to ruin it. If\\nyou think about that, you’ll do things differently.\\n—Warren Buffett\\nAbove all else, trust is the root consideration in serving data; end\\nusers need to trust the data they’re receiving. The fanciest, most\\nsophisticated data architecture and serving layer are irrelevant if end\\nusers don’t believe the data is a reliable representation of their\\nbusiness. A loss of trust is often a silent death knell for a data', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a8f3e5c1-569d-42e8-8d35-f4c4a2e9a0bc', embedding=None, metadata={'page_label': '486', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='project, even if the project isn’t officially canceled until months or\\nyears later. The job of a data engineer is to serve the best data\\npossible, so you’ll want to make sure your data products always\\ncontain high-quality and trustworthy data.\\nAs you learn to serve data throughout this chapter, we’ll reinforce the\\nidea of baking trust into your data and discuss pragmatic ways to\\naccomplish this. We see too many cases in which data teams are\\nfixated on pushing out data without asking whether stakeholders\\ntrust it in the first place. Often, stakeholders lose trust in the data.\\nOnce trust is gone, earning it back is insanely difficult. This inevitably\\nleads to the business not performing to its fullest potential with data,\\nand data teams losing credibility (and possibly being dissolved).\\nTo realize data quality and build stakeholder trust, utilize data\\nvalidation and data observability processes, in conjunction with\\nvisually inspecting and confirming validity with stakeholders. Data\\nvalidation is analyzing data to ensure that it accurately represents\\nfinancial information, customer interactions, and sales. Data\\nobservability provides an ongoing view of data and data processes.\\nThese processes must be applied throughout the data engineering\\nlifecycle to realize a good result as we reach the end. We’ll discuss\\nthese further in “Undercurrents”.\\nIn addition to building trust in data quality, it is incumbent on\\nengineers to build trust in their SLAs and SLOs with their end users\\nand upstream stakeholders. Once users come to depend on data to\\naccomplish business processes, they will require that data is\\nconsistently available and up-to-date per the commitments made by\\ndata engineers. High-quality data is of little value if it’s not available\\nas expected when it’s time to make a critical business decision.\\nNote, the SLAs and SLOs may also take the form of data contracts\\n(see Chapter 5), formally or informally.\\nWe talked about SLAs in Chapter 5, but discussing them again here\\nis worthwhile. SLAs come in a variety of forms. Regardless of its', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='563e0147-5a33-4e65-928d-e9237915669a', embedding=None, metadata={'page_label': '487', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='form, an SLA tells users what to expect from your data product; it is\\na contract between you and your stakeholders. An example of an\\nSLA might be, “Data will be reliably available and of high quality.” An\\nSLO is a key part of an SLA, and describes the ways you’ll measure\\nperformance against what you’ve agreed to. For example, given the\\npreceding example SLA, an SLO might be, “Our data pipelines to\\nyour dashboard or ML workflow will have 99% uptime, with 95% of\\ndata free of defects.” Be sure expectations are clear and you have\\nthe ability to verify you’re operating within your agreed SLA and SLO\\nparameters.\\nIt’s not enough to simply agree on an SLA. Ongoing communication\\nis a central feature of a good SLA. Have you communicated possible\\nissues that might affect your SLA or SLO expectations? What’s your\\nprocess for remediation and improvement?\\nTrust is everything. It takes a long time to earn, and it’s easy to lose.\\nWhat’s the Use Case, and Who’s the User?\\nThe serving stage is about data in action. But what is a productive\\nuse of data? You need to consider two things to answer this\\nquestion: what’s the use case, and who’s the user?\\nThe use case for data goes well beyond viewing reports and\\ndashboards. Data is at its best when it leads to action. Will an\\nexecutive make a strategic decision from a report? Will a user of a\\nmobile food delivery app receive a coupon that entices them to\\npurchase in the next two minutes? The data is often used in more\\nthan one use case—e.g., to train an ML model that does lead\\nscoring and populates a CRM (reverse ETL). High-quality, high-\\nimpact data will inherently attract many interesting use cases. But in\\nseeking use cases, always ask, “What action will this data trigger,\\nand who will be performing this action?,” with the appropriate follow-\\nup question, “Can this action be automated?”', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5ae24f61-9ec8-4765-978b-2e656fede159', embedding=None, metadata={'page_label': '488', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whenever possible, prioritize use cases with the highest possible\\nROI. Data engineers love to obsess over the technical\\nimplementation details of the systems they build while ignoring the\\nbasic question of purpose. Engineers want to do what they do best:\\nengineer things. When engineers recognize the need to focus on\\nvalue and use cases, they become much more valuable and\\neffective in their roles.\\nWhen starting a new data project, working backward is helpful. While\\nit’s tempting to focus on tools, we encourage you to start with the\\nuse case and the users. Here are some questions to ask yourself as\\nyou get started:\\nWho will use the data, and how will they use it?\\nWhat do stakeholders expect?\\nHow can I collaborate with data stakeholders (e.g., data\\nscientists, analysts, business users) to understand how the data\\nI’m working with will be used?\\nAgain, always approach data engineering from the perspective of the\\nuser and their use case. By understanding their expectations and\\ngoals, you can work backward to create amazing data products more\\neasily. Let’s take a moment to expand on our discussion of a data\\nproduct.\\nData Products\\nA good definition of a data product is a product that facilitates an\\nend goal through the use of data.\\n—DJ Patil\\nData products aren’t created in a vacuum. Like so many other\\norganizational processes that we’ve discussed, making data\\nproducts is a full-contact sport, involving a mix of product and\\nbusiness alongside technology. It’s important to involve key', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eae9a073-302a-47b8-b13c-518a358b2c34', embedding=None, metadata={'page_label': '489', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='stakeholders in developing a data product. In most companies, a\\ndata engineer is a couple of steps removed from the end users of a\\ndata product; a good data engineer will seek to fully understand\\noutcomes for direct users such as data analysts and data scientists\\nor customers external to the company.\\nWhen creating a data product, it’s useful to think of the “jobs to be\\ndone.” A user “hires” a product for a “job to be done.” This means\\nyou need to know what the user wants—i.e., their motivation for\\n“hiring” your product. A classic engineering mistake is simply building\\nwithout understanding the requirements, needs of the end user, or\\nproduct/market fit. This disaster happens when you build data\\nproducts nobody wants to use.\\nA good data product has positive feedback loops. More usage of a\\ndata product generates more useful data, which is used to improve\\nthe data product. Rinse and repeat.\\nWhen building a data product, keep these considerations in mind:\\nWhen someone uses the data product, what do they hope to\\naccomplish? All too often, data products are made without a\\nclear understanding of the outcome expected by the user.\\nWill the data product serve internal or external users? In\\nChapter 2, we discussed internal- and external-facing data\\nengineering. When creating a data product, knowing whether\\nyour customer is internal or external facing will impact the way\\ndata is served.\\nWhat are the outcomes and ROI of the data product you’re\\nbuilding?\\nBuilding data products that people will use and love is critical.\\nNothing will ruin the adoption of a data product more than unwanted\\nutility and loss of trust in the data outputs. Pay attention to the\\nadoption and usage of data products, and be willing to adjust to\\nmake users happy.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c272265e-d5fa-45c5-8a38-af87f5f95119', embedding=None, metadata={'page_label': '490', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Self-Service or Not?\\nHow will users interface with your data product? Will a business\\ndirector request a report from the data team, or can this director\\nsimply build the report? Self-service data products—giving the user\\nthe ability to build data products on their own—have been a common\\naspiration of data users for many years. What’s better than just\\ngiving the end user the ability to directly create reports, analyses,\\nand ML models?\\nToday, self-service BI and data science is still mostly aspirational.\\nWhile we occasionally see companies successfully doing self-service\\nwith data, this is rare. Most of the time, attempts at self-service data\\nbegin with great intentions but ultimately fail; self-service data is\\ntough to implement in practice. Thus, the analyst or data scientist is\\nleft to perform the heavy lifting of providing ad hoc reports and\\nmaintaining dashboards.\\nWhy is self-service data so hard? The answer is nuanced, but it\\ngenerally involves understanding the end user. If the user is an\\nexecutive who needs to understand how the business is doing, that\\nperson probably just wants a predefined dashboard of clear and\\nactionable metrics. The executive will likely ignore any self-serve\\ntools for creating custom data views. If reports provoke further\\nquestions, they might have analysts at their disposal to pursue a\\ndeeper investigation. On the other hand, a user who is an analyst\\nmight already be pursuing self-service analytics via more powerful\\ntools such as SQL. Self-service analytics through a BI layer is not\\nuseful. The same considerations apply to data science. Although\\ngranting self-service ML to “citizen data scientists” has been a goal\\nof many automated ML vendors, adoption is still nascent for the\\nsame reasons as self-service analytics. In these two extreme cases,\\na self-service data product is a wrong tool for the job.\\nSuccessful self-service data projects boil down to having the right\\naudience. Identify the self-service users and the “job” they want to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b337fa64-687f-4d4f-982c-c29ae99657b6', embedding=None, metadata={'page_label': '491', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='do. What are they trying to accomplish by using a self-service data\\nproduct versus partnering with a data analyst to get the job done? A\\ngroup of executives with a background in data forms an ideal\\naudience for self-service; they likely want to slice and dice data\\nthemselves without needing to dust off their languishing SQL skills.\\nBusiness leaders willing to invest the time to learn data skills through\\na company initiative and training program could also realize\\nsignificant value from self-service.\\nDetermine how you will provide data to this group. What are their\\ntime requirements for new data? What happens if they inevitably\\nwant more data or change the scope of what’s required from self-\\nservice? More data often means more questions, which requires\\nmore data. You’ll need to anticipate the growing needs of your self-\\nservice users. You also need to understand the fine balance\\nbetween flexibility and guardrails that will help your audience find\\nvalue and insights without incorrect results and confusion.\\nData Definitions and Logic\\nAs we’ve emphatically discussed, the utility of data in an\\norganization is ultimately derived from its correctness and\\ntrustworthiness. Critically, the correctness of data goes beyond\\nfaithful reproduction of event values from source systems. Data\\ncorrectness also encompasses proper data definitions and logic;\\nthese must be baked into data through all lifecycle stages, from\\nsource systems to data pipelines to BI tools and much more.\\nData definition refers to the meaning of data as it is understood\\nthroughout the organization. For example, customer has a precise\\nmeaning within a company and across departments. When the\\ndefinition of a customer varies, these must be documented and\\nmade available to everyone who uses the data.\\nData logic stipulates formulas for deriving metrics from data—say,\\ngross sales or customer lifetime value. Proper data logic must', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4fcce68-498a-44cc-8364-1f515fccd385', embedding=None, metadata={'page_label': '492', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='encode data definitions and details of statistical calculations. To\\ncompute customer churn metrics, we would need a definition: who is\\na customer? To calculate net profits, we would need a set of logical\\nrules to determine which expenses to deduct from gross revenue.\\nFrequently, we see data definitions and logic taken for granted, often\\npassed around the organization in the form of tribal knowledge.\\nTribal knowledge takes on a life of its own, often at the expense of\\nanecdotes replacing data-driven insights, decisions, and actions.\\nInstead, formally declaring data definitions and logic both in a data\\ncatalog and within the systems of the data engineering lifecycle goes\\na long way to ensuring data correctness, consistency, and\\ntrustworthiness.\\nData definitions can be served in many ways, sometimes explicitly,\\nbut mostly implicitly. By implicit, we mean that anytime you serve\\ndata for a query, a dashboard, or an ML model, the data and derived\\nmetrics are presented consistently and correctly. When you write a\\nSQL query, you’re implicitly assuming that the inputs to this query\\nare correct, including upstream pipeline logic and definitions. This is\\nwhere data modeling (described in Chapter 8) is incredibly useful to\\ncapture data definitions and logic in a way that’s understandable and\\nusable by multiple end users.\\nUsing a semantic layer, you consolidate business definitions and\\nlogic in a reusable fashion. Write once, use anywhere. This\\nparadigm is an object-oriented approach to metrics, calculations, and\\nlogic. We’ll have more to say in “Semantic and Metrics Layers”.\\nData Mesh\\nData mesh will increasingly be a consideration when serving data.\\nData mesh fundamentally changes the way data is served within an\\norganization. Instead of siloed data teams serving their internal\\nconstituents, every domain team takes on two aspects of\\ndecentralized, peer-to-peer data serving.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='91c7617a-3e35-4abf-8f1c-37b2bc7eceeb', embedding=None, metadata={'page_label': '493', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='First, teams are responsible for serving data to other teams by\\npreparing it for consumption. Data must be good for use in data\\napps, dashboards, analytics, and BI tools across the organization.\\nSecond, each team potentially runs its dashboards and analytics for\\nself-service. Teams consume data from across the organization\\nbased on the particular needs in their domain. Data consumed from\\nother teams may also make its way into the software designed by a\\ndomain team through user-facing analytics or an ML feature.\\nThis dramatically changes the details and structure of serving. We\\nintroduced the concept of a data mesh in Chapter 3. Now that we’ve\\ncovered some general considerations for serving data, let’s look at\\nthe first major area: analytics.\\nAnalytics\\nThe first data serving use case you’ll likely encounter is analytics,\\nwhich is discovering, exploring, identifying, and making visible key\\ninsights and patterns within data. Analytics has many aspects. As a\\npractice, analytics is carried out using statistical methods, reporting,\\nBI tools, and more. As a data engineer, knowing the various types\\nand techniques of analytics is key to accomplishing your work. This\\nsection aims to show how you’ll serve data for analytics and\\npresents some points to think about to help your analysts succeed.\\nBefore you even serve data for analytics, the first thing you need to\\ndo (which should sound familiar after reading the preceding section)\\nis identify the end use case. Is the user looking at historical trends?\\nShould users be immediately and automatically notified of an\\nanomaly, such as a fraud alert? Is someone consuming a real-time\\ndashboard on a mobile application? These examples highlight the\\ndifferences between business analytics (usually BI), operational\\nanalytics, and user-facing analytics. Each of these analytics\\ncategories has different goals and unique serving requirements.\\nLet’s look at how you’ll serve data for these types of analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7596d57e-1fce-40b8-aa83-116c5ffe52e9', embedding=None, metadata={'page_label': '494', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Business Analytics\\nBusiness analytics uses historical and current data to make strategic\\nand actionable decisions. The types of decisions tend to factor in\\nlonger-term trends, and often involve a mix of statistical and trend\\nanalysis, alongside domain expertise and human judgment.\\nBusiness analysis is as much an art as it is a science.\\nBusiness analytics typically falls into a few big areas—dashboards,\\nreports, and ad hoc analysis. A business analyst might focus on one\\nor all of these categories. Let’s quickly look at the differences\\nbetween these practices and related tools. Understanding an\\nanalyst’s workflow will help you, the data engineer, understand how\\nto serve data.\\nA dashboard concisely shows decision makers how an organization\\nis performing against a handful of core metrics, such as sales and\\ncustomer retention. These core metrics are presented as\\nvisualizations (e.g., charts or heatmaps), summary statistics, or even\\na single number. This is similar to a car dashboard, which gives you\\na single readout of the critical things you need to know while driving\\na vehicle. An organization may have more than one dashboard, with\\nC-level executives using an overarching dashboard, and their direct\\nreports using dashboards with their particular metrics, KPIs, or\\nobjectives and key results (OKRs). Analysts help create and\\nmaintain these dashboards. Once business stakeholders embrace\\nand rely on a dashboard, the analyst usually responds to requests to\\nlook into a potential issue with a metric or add a new metric to the\\ndashboard. Currently, you might use BI platforms to create\\ndashboards, such as Tableau, Looker, Sisense, Power BI, or Apache\\nSuperset/Preset.\\nAnalysts are often tasked by business stakeholders with creating a\\nreport. The goal of a report is to use data to drive insights and action.\\nAn analyst working at an online retail company is asked to\\ninvestigate which factors are driving a higher-than-expected rate of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='688ac8fa-eb7b-4508-a9f7-1ee18ebcb605', embedding=None, metadata={'page_label': '495', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='returns for women’s running shorts. The analyst runs some SQL\\nqueries in the data warehouse, aggregates the return codes that\\ncustomers provide as the reason for their return, and discovers that\\nthe fabric in the running shorts is of inferior quality, often wearing out\\nwithin a few uses. Stakeholders such as manufacturing and quality\\ncontrol are notified of these findings. Furthermore, the findings are\\nsummarized in a report and distributed in the same BI tool where the\\ndashboard resides.\\nThe analyst was asked to dig into a potential issue and come back\\nwith insights. This represents an example of ad hoc analysis.\\nReports typically start as ad hoc requests. If the results of the ad hoc\\nanalysis are impactful, they often end up in a report or dashboard.\\nThe technologies used for reports and ad hoc analysis are similar to\\ndashboards but may include Excel, Python, R-based notebooks,\\nSQL queries, and much more.\\nGood analysts constantly engage with the business and dive into the\\ndata to answer questions and uncover hidden and counterintuitive\\ntrends and insights. They also work with data engineers to provide\\nfeedback on data quality, reliability issues, and requests for new\\ndatasets. The data engineer is responsible for addressing this\\nfeedback and providing new datasets for the analyst to use.\\nReturning to the running shorts example, suppose that after\\ncommunicating their findings, analysts learn that manufacturing can\\nprovide them with various supply-chain details regarding the\\nmaterials used in the running shorts. Data engineers undertake a\\nproject to ingest this data into the data warehouse. Once the supply-\\nchain data is present, analysts can correlate specific garment serial\\nnumbers with the supplier of the fabric used in the item. They\\ndiscover that most failures are tied to one of their three suppliers,\\nand the factory stops using fabric from this supplier.\\nThe data for business analytics is frequently served in batch mode\\nfrom a data warehouse or a data lake. This varies wildly across', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fdccc66b-f284-44ad-bbea-2d9ac4fc38f2', embedding=None, metadata={'page_label': '496', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='companies, departments, and even data teams within companies.\\nNew data might be available every second, minute, every 30\\nminutes, every day, or once a week. The frequency of the batches\\ncan vary for several reasons. One key thing to note is that engineers\\nworking on analytics problems should consider various potential\\napplications of data, current, and future. It is common to have mixed\\ndata update frequencies to serve use cases appropriately but\\nremember that the frequency of ingestion sets a ceiling on\\ndownstream frequency. If streaming applications exist for the data, it\\nshould be ingested as a stream even if some downstream\\nprocessing and serving steps are handled in batches.\\nOf course, data engineers must address various backend technical\\nconsiderations in serving business analytics. Some BI tools store\\ndata in an internal storage layer. Other tools run queries on your data\\nlake or data warehouse. This is advantageous because you can take\\nfull advantage of your OLAP database’s power. As we’ve discussed\\nin earlier chapters, the downside is cost, access control, and latency.\\nOperational Analytics\\nIf business analytics is about using data to discover actionable\\ninsights, then operational analytics uses data to take immediate\\naction:\\nOperational analytics versus business analytics = immediate\\naction versus actionable insights\\nThe big difference between operational and business analytics is\\ntime. Data used in business analytics takes a longer view of the\\nquestion under consideration. Up-to-the-second updates are nice to\\nknow but won’t materially impact the quality or outcome. Operational\\nanalytics is quite the opposite, as real-time updates can be impactful\\nin addressing a problem when it occurs.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a79fc99-0f3a-4489-aacd-f41524ee7086', embedding=None, metadata={'page_label': '497', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='An example of operational analytics is real-time application\\nmonitoring. Many software engineering teams want to know how\\ntheir application is performing; if issues arise, they want to be notified\\nimmediately. The engineering team might have a dashboard (see,\\ne.g., Figure 9-2) that shows the key metrics such as requests per\\nsecond, database I/O, or whatever metrics are important. Certain\\nconditions can trigger scaling events, adding more capacity if servers\\nare overloaded. If certain thresholds are breached, the monitoring\\nsystem might also send alerts via text message, group chat, and\\nemail.\\nFigure 9-2. An operational analytics dashboard showing some key metrics from\\nGoogle Compute Engine', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='08d06813-eb74-46b5-a236-6cfce04f97e9', embedding=None, metadata={'page_label': '498', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='BUSINESS AND OPERATIONAL ANALYTICS\\nThe line between business and operational analytics has begun\\nto blur. As streaming and low-latency data become more\\npervasive, it is only natural to apply operational approaches to\\nbusiness analytics problems; in addition to monitoring website\\nperformance on Black Friday, an online retailer could also\\nanalyze and present sales, revenue and the impact of advertising\\ncampaigns in real time.\\nThe data architectures will change to fit into a world where you\\ncan have both your red hot and warm data in one place. The\\ncentral question you should always ask yourself, and your\\nstakeholders, is this: if you have streaming data, what are you\\ngoing to do with it? What action should you take? Correct action\\ncreates impact and value. Real-time data without action is an\\nunrelenting distraction.\\nIn the long term, we predict that streaming will supplant batch.\\nData products over the next 10 years will likely be streaming-first,\\nwith the ability to seamlessly blend historical data. After real-time\\ncollection, data can still be consumed and processed in batches\\nas required.\\nLet’s return once again to our running shorts example. Using\\nanalytics to discover bad fabric in the supply chain was a huge\\nsuccess; business leaders and data engineers want to find more\\nopportunities to utilize data to improve product quality. The data\\nengineers suggest deploying real-time analytics at the factory. The\\nplant already uses a variety of machines capable of streaming real-\\ntime data. In addition, the plant has cameras recording video on the\\nmanufacturing line. Right now, technicians watch the footage in real\\ntime, look for defective items, and alert those running the line when\\nthey see a high rate of snags appearing in items.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ff4f8c7a-b2a6-4230-86f5-741d061aa831', embedding=None, metadata={'page_label': '499', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data engineers realize that they can use an off-the-shelf cloud\\nmachine vision tool to identify defects in real time automatically.\\nDefect data is tied to specific item serial numbers and streamed.\\nFrom here, a real-time analytics process can tie defective items to\\nstreaming events from machines further up the assembly line.\\nUsing this approach, factory floor analysts discover that the quality of\\nraw fabric stock varies significantly from box to box. When the\\nmonitoring system shows a high rate of snag defects, line workers\\ncan remove the defective box and charge it back to the supplier.\\nSeeing the success of this quality improvement project, the supplier\\ndecides to adopt similar quality-control processes. Data engineers\\nfrom the retailer work with the supplier to deploy their real-time data\\nanalytics, dramatically improving the quality of their fabric stock.\\nEmbedded Analytics\\nWhereas business and operational analytics are internally focused, a\\nrecent trend is external- or user-facing analytics. With so much data\\npowering applications, companies increasingly provide analytics to\\nend users. These are typically referred to as data applications, often\\nwith analytics dashboards embedded within the application itself.\\nAlso known as embedded analytics, these end-user-facing\\ndashboards give users key metrics about their relationship with the\\napplication.\\nA smart thermostat has a mobile application that shows the\\ntemperature in real time and up-to-date power consumption metrics,\\nallowing the user to create a better energy-efficient heating or\\ncooling schedule. In another example, a third-party ecommerce\\nplatform provides its sellers a real-time dashboard on sales,\\ninventory, and returns. The seller has the option to use this\\ninformation to offer deals to customers in near real time. In both\\ncases, an application allows users to make real-time decisions\\n(manually or automatically) based on data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97e5aab9-d7a8-49d0-a04d-ec52eb3f2092', embedding=None, metadata={'page_label': '500', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The landscape of embedded analytics is snowballing, and we expect\\nthat such data applications will become increasingly pervasive within\\nthe next few years. As a data engineer, you’re probably not creating\\nthe embedded analytics frontend, as the application developers\\nhandle that. Since you’re responsible for the databases serving the\\nembedded analytics, you’ll need to understand the speed and\\nlatency requirements for user-facing analytics.\\nPerformance for embedded analytics encompasses three problems.\\nFirst, app users are not as tolerant of infrequent batch processing as\\ninternal company analysts; users of a recruiting SaaS platform may\\nexpect to see a change in their statistics as soon as they upload a\\nnew resume. Users want low data latency. Second, users of data\\napps expect fast query performance. When they adjust parameters\\nin an analytics dashboard, they want to see refreshed results appear\\nin seconds. Third, data apps must often support extremely high\\nquery rates across many dashboards and numerous customers.\\nHigh concurrency is critical.\\nGoogle and other early major players in the data apps space\\ndeveloped exotic technologies to cope with these challenges. For\\nnew startups, the default is to use conventional transactional\\ndatabases for data applications. As their customer bases expand,\\nthey outgrow their initial architecture. They have access to a new\\ngeneration of databases that combine high performance—fast\\nqueries, high concurrency, and near real-time updates—with relative\\nease of use (e.g., SQL-based analytics).\\nMachine Learning\\nThe second major area for serving data is machine learning. ML is\\nincreasingly common, so we’ll assume you’re at least familiar with\\nthe concept. With the rise of ML engineering (itself almost a parallel\\nuniverse to data engineering), you might ask yourself where a data\\nengineer fits into the picture.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae161d7e-8724-45a9-b31b-0938bc540c8d', embedding=None, metadata={'page_label': '501', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Admittedly, the boundary between ML, data science, data\\nengineering, and ML engineering is increasingly fuzzy, and this\\nboundary varies dramatically between organizations. In some\\norganizations, ML engineers take over data processing for ML\\napplications right after data collection or may even form an entirely\\nseparate and parallel data organization that handles the entire\\nlifecycle for all ML applications. Data engineers handle all data\\nprocessing in other settings and then hand off data to ML engineers\\nfor model training. Data engineers may even handle some extremely\\nML-specific tasks, such featurization of data.\\nLet’s return to our example of quality for control of running shorts\\nproduced by an online retailer. Suppose that streaming data has\\nbeen implemented in the factory that makes the raw fabric stock for\\nthe shorts. Data scientists discovered that the quality of the\\nmanufactured fabric is susceptible to characteristics of the input raw\\npolyester, temperature, humidity, and various tunable parameters of\\nthe loom that weaves the fabric. Data scientists develop a basic\\nmodel to optimize loom parameters. ML engineers automate model\\ntraining and set up a process to automatically tune the loom based\\non input parameters. Data and ML engineers work together to design\\na featurization pipeline, and data engineers implement and maintain\\nthe pipeline.\\nWhat a Data Engineer Should Know About ML\\nBefore we discuss serving data for ML, you may ask yourself how\\nmuch ML you need to know as a data engineer. ML is an incredibly\\nvast topic, and we won’t attempt to teach you the field; countless\\nbooks and courses are available to learn ML.\\nWhile a data engineer doesn’t need to have a deep understanding of\\nML, it helps tremendously to know the basics of how classical ML\\nworks and the fundamentals of deep learning. Knowing the basics of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='37d11991-b587-4848-8323-b344d494641a', embedding=None, metadata={'page_label': '502', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ML will go a long way in helping you work alongside data scientists\\nin building data products.\\nHere are some areas of ML that we think a data engineer should be\\nfamiliar with:\\nThe difference between supervised, unsupervised, and\\nsemisupervised learning.\\nThe difference between classification and regression\\ntechniques.\\nThe various techniques for handling time-series data. This\\nincludes time-series analysis, as well as time-series forecasting.\\nWhen to use the “classical” techniques (logistic regression, tree-\\nbased learning, support vector machines) versus deep learning.\\nWe constantly see data scientists immediately jump to deep\\nlearning when it’s overkill. As a data engineer, your basic\\nknowledge of ML can help you spot whether an ML technique is\\nappropriate and scales the data you’ll need to provide.\\nWhen would you use automated machine learning (AutoML)\\nversus handcrafting an ML model? What are the trade-offs with\\neach approach regarding the data being used?\\nWhat are data-wrangling techniques used for structured and\\nunstructured data?\\nAll data that is used for ML is converted to numbers. If you’re\\nserving structured or semistructured data, ensure that the data\\ncan be properly converted during the feature-engineering\\nprocess.\\nHow to encode categorical data and the embeddings for various\\ntypes of data.\\nThe difference between batch and online learning. Which\\napproach is appropriate for your use case?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5cb3cea1-80e8-4bb7-a267-53071a1d0021', embedding=None, metadata={'page_label': '503', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='How does the data engineering lifecycle intersect with the ML\\nlifecycle at your company? Will you be responsible for\\ninterfacing with or supporting ML technologies such as feature\\nstores or ML observability?\\nKnow when it’s appropriate to train locally, on a cluster, or at the\\nedge. When would you use a GPU over a CPU? The type of\\nhardware you use largely depends on the type of ML problem\\nyou’re solving, the technique you’re using, and the size of your\\ndataset.\\nKnow the difference between the applications of batch and\\nstreaming data in training ML models. For example, batch data\\noften fits well with offline model training, while streaming data\\nworks with online training.\\nWhat are data cascades, and how might they impact ML\\nmodels?\\nAre results returned in real time or in batch? For example, a\\nbatch speech transcription model might process speech\\nsamples and return text in batch after an API call. A product\\nrecommendation model might need to operate in real time as\\nthe customer interacts with an online retail site.\\nThe use of structured versus unstructured data. We might\\ncluster tabular (structured) customer data or recognize images\\n(unstructured) by using a neural net.\\nML is a vast subject area, and this book won’t teach you these\\ntopics, or even ML generalities. If you’d like to learn more about ML,\\nwe suggest reading Hands on Machine Learning with Scikit-Learn,\\nKeras, and TensorFlow by Aurélien Géron (O’Reilly); countless other\\nML courses and books are available online. Because the books and\\nonline courses evolve so rapidly, do your research on what seems\\nlike a good fit for you.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c060024d-f1ed-4de8-b138-b6a716ec8969', embedding=None, metadata={'page_label': '504', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ways to Serve Data for Analytics and ML\\nAs with analytics, data engineers provide data scientists and ML\\nengineers with the data they need to do their jobs. We have placed\\nserving for ML alongside analytics because the pipelines and\\nprocesses are extremely similar. There are many ways to serve data\\nfor analytics and ML. Some common ways to serve this data include\\nfiles, databases, query engines, and data sharing. Let’s briefly look\\nat each.\\nFile Exchange\\nFile exchange is ubiquitous in data serving. We process data and\\ngenerate files to pass to data consumers.\\nKeep in mind that a file might be used for many purposes. A data\\nscientist might load a text file (unstructured data) of customer\\nmessages to analyze the sentiments of customer complaints. A\\nbusiness unit might receive invoice data from a partner company as\\na collection of CSVs (structured data), and an analyst must perform\\nsome statistical analysis on these files. Or, a data vendor might\\nprovide an online retailer with images of products on a competitor’s\\nwebsite (unstructured data) for automated classification using\\ncomputer vision.\\nThe way you serve files depends on several factors, such as these:\\nUse case—business analytics, operational analytics, user-facing\\nanalytics\\nThe data consumer’s data-handling processes\\nThe size and number of individual files in storage\\nWho is accessing this file\\nData type—structured, semistructured, or unstructured', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1f48d12b-87bb-424a-b4df-56b6ebe3f714', embedding=None, metadata={'page_label': '505', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The second bullet point is one of the main considerations. It is often\\nnecessary to serve data through files rather than data sharing\\nbecause the data consumer cannot use a sharing platform.\\nThe simplest file to serve is something along the lines of emailing a\\nsingle Excel file. This is still a common workflow even in an era when\\nfiles can be collaboratively shared. The problem with emailing files is\\neach recipient gets their version of the file. If a recipient edits the file,\\nthese edits are specific to that user’s file. Deviations among files\\ninevitably result. If you need a coherent, consistent version of a file,\\nwe suggest using a collaboration platform such as Microsoft 365 or\\nGoogle Docs.\\nOf course, serving single files is hard to scale, and your needs will\\neventually outgrow simple cloud file storage. You’ll likely grow into an\\nobject storage bucket if you have a handful of large files, or a data\\nlake if you have a steady supply of files. Object storage can store\\nany type of blob file, and is especially useful for semistructured or\\nunstructured files.\\nWe’ll note that we generally consider file exchange through object\\nstorage (data lake) to land under “data sharing” rather than file\\nexchange since the process can be significantly more scalable and\\nstreamlined than ad hoc file exchange.\\nDatabases\\nDatabases are a critical layer in serving data for analytics and ML.\\nFor this discussion, we’ll implicitly keep our focus on serving data\\nfrom OLAP databases (e.g., data warehouses and data lakes). In the\\nprevious chapter, you learned about querying databases. Serving\\ndata involves querying a database, and then consuming those\\nresults for a use case. An analyst or data scientist might query a\\ndatabase by using a SQL editor and export those results to a CSV\\nfile for consumption by a downstream application, or analyze the\\nresults in a notebook (described in “Serving Data in Notebooks”).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3cb96eb-29bb-4fd5-a3a9-9fc05c64c749', embedding=None, metadata={'page_label': '506', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Serving data from a database carries a variety of benefits. A\\ndatabase imposes order and structure on the data through schema;\\ndatabases can offer fine-grained permission controls at the table,\\ncolumn, and row level, allowing database administrators to craft\\ncomplex access policies for various roles; and, databases can offer\\nhigh serving performance for large, computationally intensive\\nqueries, high query concurrency, or both.\\nBI systems usually share the data processing workload with a source\\ndatabase, but the boundary between processing in the two systems\\nvaries. For example, a Tableau server runs an initial query to pull\\ndata from a database and stores it locally. Basic OLAP/BI slicing and\\ndicing (interactive filtering and aggregation) runs directly on the\\nserver from the local data copy. On the other hand, Looker relies on\\na computational model called query pushdown; Looker encodes data\\nprocessing logic in a specialized language (LookML), combines this\\nwith dynamic user input to generate SQL queries, runs these against\\nthe source database, and presents the output. (See “Semantic and\\nMetrics Layers”.) Both Tableau and Looker have various\\nconfiguration options for caching results to reduce the processing\\nburden for frequently run queries.\\nA data scientist might connect to a database, extract data, and\\nperform feature engineering and selection. This converted dataset is\\nthen fed into an ML model; the offline model is trained and produces\\npredictive results.\\nData engineers are quite often tasked with managing the database\\nserving layer. This includes management of performance and costs.\\nIn databases that separate compute and storage, this is a somewhat\\nmore subtle optimization problem than in the days of fixed on-\\npremises infrastructure. For example, it is now possible to spin up a\\nnew Spark cluster or Snowflake warehouse for each analytical or ML\\nworkload. It is generally recommended to at least split out clusters by\\nmajor use cases, such as ETL and serving for analytics and data\\nscience. Often data teams choose to slice more finely, assigning one', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='09e575ab-e7d5-4fc3-9a8a-3440b9005c47', embedding=None, metadata={'page_label': '507', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='warehouse per major area. This makes it possible for different teams\\nto budget for their query costs under the supervision of a data\\nengineering team.\\nAlso, recall the three performance considerations that we discussed\\nin “Embedded Analytics”. These are data latency, query\\nperformance, and concurrency. A system that can ingest directly\\nfrom a stream can lower data latency. And many database\\narchitectures rely on SSD or memory caching to enhance query\\nperformance and concurrency to serve the challenging use cases\\ninherent in user-facing analytics.\\nIncreasingly, data platforms like Snowflake and Databricks allow\\nanalysts and data scientists to operate under a single environment,\\nproviding SQL editors and data science notebooks under one roof.\\nBecause compute and storage are separated, the analysts and data\\nscientists can consume the underlying data in various ways without\\ninterfering with each other. This will allow high throughput and faster\\ndelivery of data products to stakeholders.\\nStreaming Systems\\nStreaming analytics are increasingly important in the realm of\\nserving. At a high level, understand that this type of serving may\\ninvolve emitted metrics, which are different from traditional queries.\\nAlso, we see operational analytics databases playing a growing role\\nin this area (see “Operational Analytics”). These databases allow\\nqueries to run across a large range of historical data, encompassing\\nup-to-the-second current data. Essentially, they combine aspects of\\nOLAP databases with stream-processing systems. Increasingly,\\nyou’ll work with streaming systems to serve data for analytics and\\nML, so get familiar with this paradigm.\\nYou’ve learned about streaming systems throughout the book. For\\nan idea of where it’s going, read about the live data stack in\\nChapter 11.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bff4d5c5-d891-4b59-9a20-47ff18e31db8', embedding=None, metadata={'page_label': '508', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Query Federation\\nAs you learned in Chapter 8, query federation pulls data from\\nmultiple sources, such as data lakes, RDBMSs, and data\\nwarehouses. Federation is becoming more popular as distributed\\nquery virtualization engines gain recognition as ways to serve\\nqueries without going through the trouble of centralizing data in an\\nOLAP system. Today, you can find OSS options like Trino and Presto\\nand managed services such as Starburst. Some of these offerings\\ndescribe themselves as ways to enable the data mesh; time will tell\\nhow that unfolds.\\nWhen serving data for federated queries, you should be aware that\\nthe end user might be querying several systems—OLTP, OLAP,\\nAPIs, filesystems, etc. (Figure 9-3). Instead of serving data from a\\nsingle system, you’re now serving data from multiple systems, each\\nwith its usage patterns, quirks, and nuances. This poses challenges\\nfor serving data. If federated queries touch live production source\\nsystems, you must ensure that the federated query won’t consume\\nexcessive resources in the source.\\nFigure 9-3. A federated query with three data sources', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a1748c35-76d2-48cd-8edc-0183d732febb', embedding=None, metadata={'page_label': '509', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In our experience, federated queries are ideally suited when you\\nwant flexibility in analyzing data or the source data needs to be\\ntightly controlled. Federation allows ad hoc queries for performing\\nexploratory analysis, blending data from various systems without the\\ncomplexity of setting up data pipelines or ETL. This will allow you to\\ndetermine whether the performance of a federated query is sufficient\\nfor ongoing purposes or you need to set up ingestion on some or all\\ndata sources and centralize the data in an OLAP database or data\\nlake.\\nFederated queries also provide read-only access to source systems,\\nwhich is great when you don’t want to serve files, database access,\\nor data dumps. The end user reads only the version of the data\\nthey’re supposed to access and nothing more. Query federation is a\\ngreat option to explore for situations where access and compliance\\nare critical.\\nData Sharing\\nChapter 5 includes an extensive discussion of data sharing. Any\\ndata exchange between organizations or units within a larger\\norganization can be viewed as data sharing. Still, we mean\\nspecifically sharing through massively multitenant storage systems in\\na cloud environment. Data sharing generally turns data serving into a\\nsecurity and access control problem.\\nThe actual queries are now handled by the data consumers\\n(analysts and data scientists) rather than the engineers sourcing the\\ndata. Whether serving data in a data mesh within an organization,\\nproviding data to the public, or serving to partner businesses, data\\nsharing is a compelling serving model. Data sharing is increasingly a\\ncore feature of major data platforms like Snowflake, Redshift, and\\nBigQuery allowing companies to share data safely and securely with\\neach other.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='17a1ac24-f85c-45c4-95bf-ec93702a12d4', embedding=None, metadata={'page_label': '510', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Semantic and Metrics Layers\\nWhen data engineers think about serving, they naturally tend to\\ngravitate toward the data processing and storage technologies—i.e.,\\nwill you use Spark or a cloud data warehouse? Is your data stored in\\nobject storage or cached in a fleet of SSDs? But powerful processing\\nengines that deliver quick query results across vast datasets don’t\\ninherently make for quality business analytics. When fed poor-quality\\ndata or poor-quality queries, powerful query engines quickly return\\nbad results.\\nWhere data quality focuses on characteristics of the data itself and\\nvarious techniques to filter or improve bad data, query quality is a\\nquestion of building a query with appropriate logic that returns\\naccurate answers to business questions. Writing high-quality ETL\\nqueries and reporting is time-intensive, detailed work. Various tools\\ncan help automate this process while facilitating consistency,\\nmaintenance, and continuous improvement.\\nFundamentally, a metrics layer is a tool for maintaining and\\ncomputing business logic. (A semantic layer is extremely similar\\nconceptually,  and headless BI is another closely related term.) This\\nlayer can live in a BI tool or in software that builds transformation\\nqueries. Two concrete examples are Looker and Data Build Tool\\n(dbt).\\nFor instance, Looker’s LookML allows users to define virtual,\\ncomplex business logic. Reports and dashboards point to specific\\nLookML for computing metrics. Looker allows users to define\\nstandard metrics and reference them in many downstream queries;\\nthis is meant to solve the traditional problem of repetition and\\ninconsistency in traditional ETL scripts. Looker uses LookML to\\ngenerate SQL queries, which are pushed down to the database.\\nResults can be persisted in the Looker server or in the database\\nitself for large result sets.\\n2\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='07b1e702-8dcf-4330-9f2f-11ffabda5228', embedding=None, metadata={'page_label': '511', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='dbt allows users to define complex SQL data flows encompassing\\nmany queries and standard definitions of business metrics, much like\\nLooker. Unlike Looker, dbt runs exclusively in the transform layer,\\nalthough this can include pushing queries into views that are\\ncomputed at query time. Whereas Looker focuses on serving queries\\nand reporting, dbt can serve as a robust data pipeline orchestration\\ntool for analytics engineers.\\nWe believe that metrics layer tools will grow more popular with wider\\nadoption and more entrants, as well as move upstream toward the\\napplication. Metrics layer tools help solve a central question in\\nanalytics that has plagued organizations since people have analyzed\\ndata: “Are these numbers correct?” Many new entrants are in the\\nspace beside the ones we’ve mentioned.\\nServing Data in Notebooks\\nData scientists often use notebooks in their day-to-day work.\\nWhether it’s exploring data, engineering features, or training a\\nmodel, the data scientist will likely use a notebook. At this writing, the\\nmost popular notebook platform is Jupyter Notebook, along with its\\nnext-generation iteration, JupyterLab. Jupyter is open source and\\ncan be hosted locally on a laptop, on a server, or through various\\ncloud-managed services. Jupyter stands for Julia, Python, and R —\\nthe latter two are popular for data science applications, especially\\nnotebooks. Regardless of the language used, the first thing you’ll\\nneed to consider is how data can be accessed from a notebook.\\nData scientists will programmatically connect to a data source, such\\nas an API, a database, a data warehouse, or a data lake (Figure 9-\\n4). In a notebook, all connections are created using the appropriate\\nbuilt-in or imported libraries to load a file from a filepath, connect to\\nan API endpoint, or make an ODBC connection to a database. A\\nremote connection may require the correct credentials and privileges\\nto establish a connection. Once connected, a user may need the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='93ac7dbd-f35d-4b55-9d2f-cd0a712b79b4', embedding=None, metadata={'page_label': '512', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='correct access to tables (and rows/columns) or files stored in object\\nstorage. The data engineer will often assist the data scientist in\\nfinding the right data, and then ensure that they have the right\\npermissions to access the rows and columns required.\\nFigure 9-4. A notebook can be served data from many sources, such as object\\nstorage, a database, data warehouse, or data lake', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='403bebd0-402e-4a68-ade1-71ac1c31429e', embedding=None, metadata={'page_label': '513', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CREDENTIAL HANDLING\\nIncorrectly handled credentials in notebooks and data science\\ncode are a major security risk; we constantly see credentials\\nmishandled in this domain. It is common to embed credentials\\ndirectly in code, where they often leak into version control repos.\\nCredentials are also frequently passed around through\\nmessages and email.\\nWe encourage data engineers to audit data science security\\npractices and work collaboratively on improvements. Data\\nscientists are highly receptive to these conversations if they are\\ngiven alternatives. Data engineers should set standards for\\nhandling credentials. Credentials should never be embedded in\\ncode; ideally, data scientists use credential managers or CLI\\ntools to manage access.\\nLet’s look at an incredibly common workflow for data scientists:\\nrunning a local notebook and loading data into a pandas dataframe.\\nPandas is a prevalent Python library used for data manipulation and\\nanalysis and is commonly used to load data (say, a CSV file) into a\\nJupyter notebook. When pandas loads a dataset, it stores this\\ndataset in memory.\\nWhat happens when the dataset size exceeds the local machine’s\\navailable memory? This inevitably happens given the limited memory\\nof laptops and workstations: it stops a data science project dead in\\nits tracks. It’s time to consider more scalable options. First, move to\\na cloud-based notebook where the underlying storage and memory\\nfor the notebook can be flexibly scaled. Upon outgrowing this option,\\nlook at distributed execution systems; popular Python-based options\\ninclude Dask, Ray, and Spark. If a full-fledged cloud-managed\\noffering seems appealing, consider setting up a data science\\nworkflow using Amazon SageMaker, Google Cloud Vertex AI, or\\nMicrosoft Azure Machine Learning. Finally, open source end-to-end', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d13c5eae-d128-4c0e-9f59-643bb03e8a85', embedding=None, metadata={'page_label': '514', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ML workflow options such as Kubeflow and MLflow make it easy to\\nscale ML workloads in Kubernetes and Spark, respectively. The\\npoint is to get data scientists off their laptops and take advantage of\\nthe cloud’s power and scalability.\\nData engineers and ML engineers play a key role in facilitating the\\nmove to scalable cloud infrastructure. The exact division of labor\\ndepends a great deal on the details of your organization. They\\nshould take the lead in setting up cloud infrastructure, overseeing the\\nmanagement of environments, and training data scientists on cloud-\\nbased tools.\\nCloud environments require significant operational work, such as\\nmanaging versions and updates, controlling access, and maintaining\\nSLAs. As with other operational work, a significant payoff can result\\nwhen “data science ops” are done well.\\nNotebooks may even become a part of production data science;\\nnotebooks are widely deployed at Netflix. This is an interesting\\napproach with advantages and trade-offs. Productionized notebooks\\nallow data scientists to get their work into production much faster, but\\nthey are also inherently a substandard form of production. The\\nalternative is to have ML and data engineers convert notebooks for\\nproduction use, placing a significant burden on these teams. A\\nhybrid of these approaches may be ideal, with notebooks used for\\n“light” production and a full productionization process for high-value\\nprojects.\\nReverse ETL\\nToday, reverse ETL is a buzzword that describes serving data by\\nloading it from an OLAP database back into a source system. That\\nsaid, any data engineer who’s worked in the field for more than a few\\nyears has probably done some variation of reverse ETL. Reverse\\nETL grew in popularity in the late 2010s/early 2020s and is\\nincreasingly recognized as a formal data engineering responsibility.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='de143752-d242-4ee1-a4ee-afaf23b9fa1a', embedding=None, metadata={'page_label': '515', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A data engineer might pull customers and order data from a CRM\\nand store it in a data warehouse. This data is used to train a lead\\nscoring model, whose results are returned to the data warehouse.\\nYour company’s sales team wants access to these scored leads to\\ntry to generate more sales. You have a few options to get the results\\nof this lead scoring model into the hands of the sales team. You can\\nput the results in a dashboard for them to view. Or you might email\\nthe results to them as an Excel file.\\nThe challenge with these approaches is that they are not connected\\nto the CRM, where a salesperson does their work. Why not just put\\nthe scored leads back into the CRM? As we mentioned, successful\\ndata products reduce friction with the end user. In this case, the end\\nuser is the sales team.\\nUsing reverse ETL and loading the scored leads back into the CRM\\nis the easiest and best approach for this data product. Reverse ETL\\ntakes processed data from the output side of the data engineering\\nlifecycle and feeds it back into source systems (Figure 9-5).\\nFigure 9-5. Reverse ETL', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e7e3827d-170f-4408-8553-2c6b420b8d2b', embedding=None, metadata={'page_label': '516', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='NOTE\\nInstead of reverse ETL, we, the authors, half-jokingly call it bidirectional\\nload and transform (BLT). The term reverse ETL doesn’t quite\\naccurately describe what’s happening in this process. Regardless, the\\nterm has stuck in the popular imagination and press, so we’ll use it\\nthroughout the book. More broadly, whether the term reverse ETL sticks\\naround is anyone’s guess, but the practice of loading data from OLAP\\nsystems back into source systems will remain important.\\nWays to Serve Data with Reverse ETL\\nHow do you begin serving data with reverse ETL? While you can roll\\nyour reverse ETL solution, many off-the-shelf reverse ETL options\\nare available. We suggest using open source, or a commercial\\nmanaged service. That said, the reverse ETL space is changing\\nextremely quickly. No clear winners have emerged, and many\\nreverse ETL products will be absorbed by major clouds or other data\\nproduct vendors. Choose carefully.\\nWe do have a few words of warning regarding reverse ETL. Reverse\\nETL inherently creates feedback loops. For example, imagine that\\nwe download Google Ads data, use a model to compute new bids,\\nload the bids back into Google Ads, and start the process again.\\nSuppose that because of an error in your bid model, the bids trend\\never higher, and your ads get more and more clicks. You can quickly\\nwaste massive amounts of money! Be careful, and build in\\nmonitoring and guardrails.\\nWhom You’ll Work With\\nAs we’ve discussed, in the serving stage, a data engineer will\\ninterface with a lot of stakeholders. These include (but aren’t limited\\nto) the following:\\nData analysts', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b45fb5c5-ee7a-4f4d-b06f-88e8cc9dc6d3', embedding=None, metadata={'page_label': '517', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data scientists\\nMLOps/ML engineers\\nThe business—nondata or nontechnical stakeholders,\\nmanagers, and executives\\nAs a reminder, the data engineer operates in a support role for these\\nstakeholders and is not necessarily responsible for the end uses of\\ndata. For example, a data engineer supplies the data for a report that\\nanalysts interpret, but the data engineer isn’t responsible for these\\ninterpretations. Instead, the data engineer is responsible for\\nproducing the highest-quality data products possible.\\nA data engineer should be aware of feedback loops between the\\ndata engineering lifecycle and the broader use of data once it’s in the\\nhands of stakeholders. Data is rarely static, and the outside world\\nwill influence the data that is ingested and served and reingested\\nand re-served.\\nA big consideration for data engineers in the serving stage of the\\nlifecycle is the separation of duties and concerns. If you’re at an\\nearly-stage company, the data engineer may also be an ML engineer\\nor data scientist; this is not sustainable. As the company grows, you\\nneed to establish a clear division of duties with other data team\\nmembers.\\nAdopting a data mesh dramatically reorganizes team responsibilities,\\nand every domain team takes on aspects of serving. For a data\\nmesh to be successful, each team must work effectively on its data\\nserving responsibilities, and teams must also effectively collaborate\\nto ensure organizational success.\\nUndercurrents\\nThe undercurrents come to finality with serving. Remember that the\\ndata engineering lifecycle is just that—a lifecycle. What goes around', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7439b09-7aac-45b6-a686-0a1d0dc926e0', embedding=None, metadata={'page_label': '518', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='comes around. We see many instances where serving data\\nhighlights something missed earlier in the lifecycle. Always be on the\\nlookout for how the undercurrents can help you spot ways to improve\\ndata products.\\nWe’re fond of saying, “Data is a silent killer,” and the undercurrents\\ncome to a head in the serving stage. Serving is your final chance to\\nmake sure your data is in great shape before it gets into the hands of\\nend users.\\nSecurity\\nThe same security principles apply whether sharing data with people\\nor systems. We often see data shared indiscriminately, with little to\\nno access controls or thought as to what the data will be used for.\\nThis is a huge mistake that can have catastrophic results, such as a\\ndata breach and the resulting fines, bad press, and lost jobs. Take\\nsecurity seriously, especially in this stage of the lifecycle. Of all the\\nlifecycle stages, serving presents the largest security surface.\\nAs always, exercise the principle of least privilege both for people\\nand systems, and provide only the access required for the purpose\\nat hand, and the job to be done. What data does an executive need\\nversus an analyst or data scientist? What about an ML pipeline or\\nreverse ETL process? These users and destinations all have\\ndifferent data needs, and access should be provided accordingly.\\nAvoid giving carte blanche permissions to everyone and everything.\\nServing data is often read-only unless a person or process needs to\\nupdate data in the system from which it is queried. People should be\\ngiven read-only access to specific databases and datasets unless\\ntheir role requires something more advanced like write or update\\naccess. This can be accomplished by combining groups of users\\nwith certain IAM roles (i.e., analysts group, data scientist group) or\\ncustom IAM roles if this makes sense. For systems, provide service\\naccounts and roles in a similar fashion. For both users and systems,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fc1afe15-14ae-488f-9647-c2e6ffabf54b', embedding=None, metadata={'page_label': '519', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='narrow access to a dataset’s fields, rows, columns, and cells if this is\\nwarranted. Access controls should be as fine-grained as possible\\nand revoked when access is no longer required.\\nAccess controls are critical when serving data in a multitenant\\nenvironment. Make sure users can access only their data and\\nnothing more. A good approach is to mediate access through filtered\\nviews, thus alleviating the security risks inherent in sharing access to\\na common table. Another suggestion is to use data sharing in your\\nworkflows, which allows for read-only granular controls between you\\nand people consuming your data.\\nCheck how often data products are used, and whether it makes\\nsense to stop sharing certain data products. It’s extremely common\\nfor an executive to urgently request an analyst to create a report,\\nonly to have this report very quickly go unused. If data products\\naren’t used, ask the users if they’re still needed. If not, kill off the\\ndata product. This means one less security vulnerability floating\\naround.\\nFinally, you should view access control and security not as\\nimpediments to serving but as key enablers. We’re aware of many\\ninstances where complex, advanced data systems were built,\\npotentially having a significant impact on a company. Because\\nsecurity was not implemented correctly, few people were allowed to\\naccess the data, so it languished. Fine-grained, robust access\\ncontrol means that more interesting data analytics and ML can be\\ndone while still protecting the business and its customers.\\nData Management\\nYou’ve been incorporating data management along the data\\nengineering lifecycle, and the impact of your efforts will soon become\\napparent as people use your data products. At the serving stage,\\nyou’re mainly concerned with ensuring that people can access high-\\nquality and trustworthy data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='de684b6f-0a63-4e32-8e62-df1d55a5c7a0', embedding=None, metadata={'page_label': '520', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='As we mentioned at the beginning of this chapter, trust is perhaps\\nthe most critical variable in data serving. If people trust their data,\\nthey will use it; untrusted data will go unused. Be sure to make data\\ntrust and data improvement an active process by providing feedback\\nloops. As users interact with data, they can report problems and\\nrequest improvements. Actively communicate back to your users as\\nchanges are made.\\nWhat data do people need to do their jobs? Especially with\\nregulatory and compliance concerns weighing on data teams, giving\\npeople access to the raw data—even with limited fields and rows—\\nposes a problem of tracing data back to an entity, such as a person\\nor a group of people. Thankfully, advancements in data obfuscation\\nallow you to serve synthetic, scrambled, or anonymized data to end\\nusers. These “fake” datasets should sufficiently allow an analyst or\\ndata scientist to get the necessary signal from the data, but in a way\\nthat makes identifying protected information difficult. Though this\\nisn’t a perfect process—with enough effort, many datasets can be\\nde-anonymized or reverse-engineered—it at least reduces the risk of\\ndata leakage.\\nAlso, incorporate semantic and metrics layers into your serving layer,\\nalongside rigorous data modeling that properly expresses business\\nlogic and definitions. This provides a single source of truth, whether\\nfor analytics, ML, reverse ETL, or other serving uses.\\nDataOps\\nThe steps you take in data management—data quality, governance,\\nand security—are monitored in DataOps. Essentially, DataOps\\noperationalizes data management. The following are some things to\\nmonitor:\\nData health and data downtime\\nLatency of systems serving data—dashboards, databases, etc.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='62827fe3-e3bc-4949-8bb7-6e63e921d348', embedding=None, metadata={'page_label': '521', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data quality\\nData and system security and access\\nData and model versions being served\\nUptime to achieve an SLO\\nA variety of new tools have sprung up to address various monitoring\\naspects. For example, many popular data observability tools aim to\\nminimize data downtime and maximize data quality. Observability\\ntools may cross over from data to ML, supporting monitoring of\\nmodels and model performance. More conventional DevOps\\nmonitoring is also critical to DataOps—e.g., you need to monitor\\nwhether connections are stable among storage, transformation, and\\nserving.\\nAs in every stage of the data engineering lifecycle, version-control\\ncode and operationalize deployment. This applies to analytical code,\\ndata logic code, ML scripts, and orchestration jobs. Use multiple\\nstages of deployment (dev, test, prod) for reports and models.\\nData Architecture\\nServing data should have the same architectural considerations as\\nother data engineering lifecycle stages. At the serving stage,\\nfeedback loops must be fast and tight. Users should be able to\\naccess the data they need as quickly as possible when they need it.\\nData scientists are notorious for doing most development on their\\nlocal machines. As discussed earlier, encourage them to migrate\\nthese workflows to common systems in a cloud environment, where\\ndata teams can collaborate in dev, test, and production environments\\nand create proper production architectures. Facilitate your analysts\\nand data scientists by supporting tools for publishing data insights\\nwith little encumbrance.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c6869506-dcd1-4266-9c66-e3abd1346ec0', embedding=None, metadata={'page_label': '522', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Orchestration\\nData serving is the last stage of the data engineering lifecycle.\\nBecause serving is downstream of so many processes, it’s an area\\nof extremely complex overlap. Orchestration is not simply a way of\\norganizing and automating complex work but a means of\\ncoordinating data flow across teams so that data is made available\\nto consumers at the promised time.\\nOwnership of orchestration is a key organizational decision. Will\\norchestration be centralized or decentralized? A decentralized\\napproach allows small teams to manage their data flows, but it can\\nincrease the burden of cross-team coordination. Instead of simply\\nmanaging flows within a single system, directly triggering the\\ncompletion of DAGs or tasks belonging to other teams, teams must\\npass messages or queries between systems.\\nA centralized approach means that work is easier to coordinate, but\\nsignificant gatekeeping must also exist to protect a single production\\nasset. For example, a poorly written DAG can bring Airflow to a halt.\\nThe centralized approach would mean bringing down data processes\\nand serving across the whole organization. Centralized orchestration\\nmanagement requires high standards, automated testing of DAGs,\\nand gatekeeping.\\nIf orchestration is centralized, who will own it? When a company has\\na DataOps team, orchestration usually lands here. Often, a team\\ninvolved in serving is a natural fit because it has a fairly holistic view\\nof all data engineering lifecycle stages. This could be the DBAs,\\nanalytics engineers, data engineers, or ML engineers. ML engineers\\ncoordinate complex model-training processes but may or may not\\nwant to add the operational complexity of managing orchestration to\\nan already crowded docket of responsibilities.\\nSoftware Engineering', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='75342dc3-7096-4599-a85a-57243c0cec2d', embedding=None, metadata={'page_label': '523', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Compared to a few years ago, serving data has become simpler.\\nThe need to write code has been drastically simplified. Data has also\\nbecome more code-first, with the proliferation of open source\\nframeworks focused on simplifying the serving of data. Many ways\\nexist to serve data to end users, and a data engineer’s focus should\\nbe on knowing how these systems work and how data is delivered.\\nDespite the simplicity of serving data, if code is involved, a data\\nengineer should still understand how the main serving interfaces\\nwork. For example, a data engineer may need to translate the code\\na data scientist is running locally on a notebook and convert it into a\\nreport or a basic ML model to operate.\\nAnother area where data engineers will be useful is understanding\\nthe impact of how code and queries will perform against the storage\\nsystems. Analysts can generate SQL in various programmatic ways,\\nincluding LookML, Jinja via dbt, various object-relational mapping\\n(ORM) tools, and metrics layers. When these programmatic layers\\ncompile to SQL, how will this SQL perform? A data engineer can\\nsuggest optimizations where the SQL code might not perform as well\\nas handwritten SQL.\\nThe rise of analytics and ML IaC means the role of writing code is\\nmoving toward building the systems that support data scientists and\\nanalysts. Data engineers might be responsible for setting up the\\nCI/CD pipelines and building processes for their data team. They\\nwould also do well to train and support their data team in using the\\nData/MLOps infrastructure they’ve built so that these data teams can\\nbe as self-sufficient as possible.\\nFor embedded analytics, data engineers may need to work with\\napplication developers to ensure that queries are returned quickly\\nand cost-effectively. The application developer will control the\\nfrontend code that users deal with. The data engineer is there to\\nensure that developers receive the correct payloads as they’re\\nrequested.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3cbde02-9377-43c3-b19a-5f90be5a5412', embedding=None, metadata={'page_label': '524', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Conclusion\\nThe data engineering lifecycle has a logical ending at the serving\\nstage. As with all lifecycles, a feedback loop occurs (Figure 9-6). You\\nshould view the serving stage as a chance to learn what’s working\\nand what can be improved. Listen to your stakeholders. If they bring\\nup issues—and they inevitably will—try not to take offense. Instead,\\nuse this as an opportunity to improve what you’ve built.\\nFigure 9-6. Build, learn, improve\\nA good data engineer is always open to new feedback and\\nconstantly finds ways to improve their craft. Now that we’ve taken a\\njourney through the data engineering lifecycle, you know how to\\ndesign, architect, build, maintain, and improve your data engineering\\nsystems and products. Let’s turn our attention to Part III of the book,\\nwhere we’ll cover some aspects of data engineering we’re constantly\\nasked about and, frankly, deserve more attention.\\nAdditional Resources\\n“Designing Data Products” by Seth O’Regan\\n“Data Jujitsu: The Art of Turning Data into Product” by DJ Patil\\n“What Is User-Facing Analytics?” by Chinmon Soman', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='136d0711-9c25-43ea-acee-d8e1f36dac06', embedding=None, metadata={'page_label': '525', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“Data as a Product vs. Data Products: What Are the\\nDifferences?” by Xavier Gumara Rigol\\n“How to Build Great Data Products” by Emily Glassberg Sands\\n“The Evolution of Data Products” and “What Is Data Science” by\\nMike Loukides\\n“Know Your Customers’ ‘Jobs to Be Done’” by Clayton M.\\nChristensen et al.\\n“Data Mesh Principles and Logical Architecture” by Martin\\nFowler\\n“Understanding the Superset Semantic Layer” by Srini Kadamati\\n“The Future of BI Is Headless” by ZD\\n“How to Structure a Data Analytics Team” by Niall Napier\\n“What Is Operational Analytics (and How Is It Changing How We\\nWork with Data)?” by Sylvain Giuliani\\n“Fundamentals of Self-Service Machine Learning” by Paramita\\n(Guha) Ghosh\\n“What Do Modern Self-Service BI and Data Analytics Really\\nMean?” by Harry Dix\\n“Self-Service Analytics” in the Gartner Glossary\\n“The Missing Piece of the Modern Data Stack” and “Why Is Self-\\nServe Still a Problem?” by Benn Stancil\\nForrester’s “Self-Service Business Intelligence: Dissolving the\\nBarriers to Creative Decision-Support Solutions” blog article\\nData Mesh by Zhamak Dehghani (O’Reilly)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='db274f53-6684-4db0-b74b-37abe83dca3b', embedding=None, metadata={'page_label': '526', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1  “Know Your Customers’ ‘Jobs to Be Done’” by Clayton M. Christensen et al.,\\nHarvard Business Review, September 2016, https://oreil.ly/3uU4j.\\n2  Benn Stancil, “The Missing Piece of the Modern Data Stack,”\\nbenn.substack, April 22, 2021, https://oreil.ly/wQyPb.\\n3  Srini Kadamati, “Understanding the Superset Semantic Layer,” Preset blog,\\nDecember 21, 2021, https://oreil.ly/6smWC.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ce646374-62bb-40be-9b34-cb63ef29e627', embedding=None, metadata={'page_label': '527', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Part III. Security, Privacy, and\\nthe Future of Data Engineering', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c521dbe1-6b5c-433d-bce0-583b0141c220', embedding=None, metadata={'page_label': '528', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 10. Security and\\nPrivacy\\nSecurity is vital to the practice of data engineering. This should be\\nblindingly obvious, but we’re constantly amazed at how often data\\nengineers view security as an afterthought. We believe that security\\nis the first thing a data engineer needs to think about in every aspect\\nof their job and every stage of the data engineering lifecycle. You\\ndeal with sensitive data, information, and access daily. Your\\norganization, customers, and business partners expect these\\nvaluable assets to be handled with the utmost care and concern.\\nOne security breach or a data leak can leave your business dead in\\nthe water; your career and reputation are ruined if it’s your fault.\\nSecurity is a key ingredient for privacy. Privacy has long been critical\\nto trust in the corporate information technology space; engineers\\ndirectly or indirectly handle data related to people’s private lives. This\\nincludes financial information, data on private communications\\n(emails, texts, phone calls), medical history, educational records, and\\njob history. A company that leaked this information or misused it\\ncould find itself a pariah when the breach came to light.\\nIncreasingly, privacy is a matter of significant legal importance. For\\nexample, the Family Educational Rights and Privacy Act (FERPA)\\nwent into effect in the US in the 1970s; the Health Insurance\\nPortability and Accountability Act (HIPAA) followed in the 1990s;\\nGDPR was passed in Europe in the mid-2010s. Several US-based\\nprivacy bills have passed or will soon. This is just a tiny sampling of\\nprivacy-related statutes (and we believe just the beginning). Still, the\\npenalties for violation of any of these laws can be significant, even\\ndevastating, to a business. And because data systems are woven', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='60116a4c-6b0f-4f18-92b6-35230d8dedcc', embedding=None, metadata={'page_label': '529', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='into the fabric of education, health care, and business, data\\nengineers handle sensitive data related to each of these laws.\\nA data engineer’s exact security and privacy responsibilities will vary\\nsignificantly between organizations. At a small startup, a data\\nengineer may do double duty as a data security engineer. A large\\ntech company will have armies of security engineers and security\\nresearchers. Even in this situation, data engineers will often be able\\nto identify security practices and technology vulnerabilities within\\ntheir own teams and systems that they can report and mitigate in\\ncollaboration with dedicated security personnel.\\nBecause security and privacy are critical to data engineering\\n(security being an undercurrent), we want to spend some more time\\ncovering security and privacy. In this chapter, we lay out some things\\ndata engineers should consider around security, particularly in\\npeople, processes, and technology (in that order). This isn’t a\\ncomplete list, but lays out the major things we’d wish would improve\\nbased on our experience.\\nPeople\\nThe weakest link in security and privacy is you. Security is often\\ncompromised at the human level, so conduct yourself as if you’re\\nalways a target. A bot or human actor is trying to infiltrate your\\nsensitive credentials and information at any given time. This is our\\nreality, and it’s not going away. Take a defensive posture with\\neverything you do online and offline. Exercise the power of negative\\nthinking and always be paranoid.\\nThe Power of Negative Thinking\\nIn a world obsessed with positive thinking, negative thinking is\\ndistasteful. However, American surgeon Atul Gawande wrote a 2007\\nop-ed in the New York Times on precisely this subject. His central', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='582f77de-c194-4e94-99b0-524e0a3d64a9', embedding=None, metadata={'page_label': '530', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='thesis is that positive thinking can blind us to the possibility of\\nterrorist attacks or medical emergencies and deter preparation.\\nNegative thinking allows us to consider disastrous scenarios and act\\nto prevent them.\\nData engineers should actively think through the scenarios for data\\nutilization and collect sensitive data only if there is an actual need\\ndownstream. The best way to protect private and sensitive data is to\\navoid ingesting this data in the first place.\\nData engineers should think about the attack and leak scenarios with\\nany data pipeline or storage system they utilize. When deciding on\\nsecurity strategies, ensure that your approach delivers proper\\nsecurity and not just the illusion of safety.\\nAlways Be Paranoid\\nAlways exercise caution when someone asks you for your\\ncredentials. When in doubt—and you should always be in extreme\\ndoubt when asked for credentials—hold off and get second opinions\\nfrom your coworkers and friends. Confirm with other people that the\\nrequest is indeed legitimate. A quick chat or phone call is cheaper\\nthan a ransomware attack triggered through an email click. Trust\\nnobody at face value when asked for credentials, sensitive data, or\\nconfidential information, including from your coworkers.\\nYou are also the first line of defense in respecting privacy and ethics.\\nAre you uncomfortable with sensitive data you’ve been tasked to\\ncollect? Do you have ethical questions about the way data is being\\nhandled in a project? Raise your concerns with colleagues and\\nleadership. Ensure that your work is both legally compliant and\\nethical.\\nProcesses', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6b0d3a19-b6c6-4c26-88c9-9a388df7d430', embedding=None, metadata={'page_label': '531', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='When people follow regular security processes, security becomes\\npart of the job. Make security a habit, regularly practice real security,\\nexercise the principle of least privilege, and understand the shared\\nresponsibility model in the cloud.\\nSecurity Theater Versus Security Habit\\nWith our corporate clients, we see a pervasive focus on compliance\\n(with internal rules, laws, recommendations from standards bodies),\\nbut not enough attention to potentially bad scenarios. Unfortunately,\\nthis creates an illusion of security but often leaves gaping holes that\\nwould be evident with a few minutes of reflection.\\nSecurity needs to be simple and effective enough to become\\nhabitual throughout an organization. We’re amazed at the number of\\ncompanies with security policies in the hundreds of pages that\\nnobody reads, the annual security policy review that people\\nimmediately forget, all in checking a box for a security audit. This is\\nsecurity theater, where security is done in the letter of compliance\\n(SOC-2, ISO 27001, and related) without real commitment.\\nInstead, pursue the spirit of genuine and habitual security; bake a\\nsecurity mindset into your culture. Security doesn’t need to be\\ncomplicated. For example, at our company, we run security training\\nand policy review at least once a month to ingrain this into our\\nteam’s DNA and update each other on security practices we can\\nimprove. Security must not be an afterthought for your data team.\\nEveryone is responsible and has a role to play. It must be the priority\\nfor you and everyone else you work with.\\nActive Security\\nReturning to the idea of negative thinking, active security entails\\nthinking about and researching security threats in a dynamic and\\nchanging world. Rather than simply deploying scheduled simulated\\nphishing attacks, you can take an active security posture by', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7974a8a-2f91-4632-89a8-4b138e2428b5', embedding=None, metadata={'page_label': '532', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='researching successful phishing attacks and thinking through your\\norganizational security vulnerabilities. Rather than simply adopting a\\nstandard compliance checklist, you can think about internal\\nvulnerabilities specific to your organization and incentives employees\\nmight have to leak or misuse private information.\\nWe have more to say about active security in “Technology”.\\nThe Principle of Least Privilege\\nThe principle of least privilege means that a person or system should\\nbe given only the privileges and data they need to complete the task\\nat hand and nothing more. Often, we see an antipattern in the cloud:\\na regular user is given administrative access to everything, when\\nthat person may need just a handful of IAM roles to do their work.\\nGiving someone carte blanche administrative access is a huge\\nmistake and should never happen under the principle of least\\nprivilege.\\nInstead, provide the user (or group they belong to) the IAM roles\\nthey need when they need them. When these roles are no longer\\nneeded, take them away. The same rule applies to service accounts.\\nTreat humans and machines the same way: give them only the\\nprivileges and data they need to do their jobs, and only for the\\ntimespan when needed.\\nOf course, the principle of least privilege is also critical to privacy.\\nYour users and customers expect that people will look at their\\nsensitive data only when necessary. Make sure that this is the case.\\nImplement column, row, and cell-level access controls around\\nsensitive data; consider masking PII and other sensitive data and\\ncreate views that contain only the information the viewer needs to\\naccess. Some data must be retained, but should be accessed only in\\nan emergency. Put this data behind a broken glass process: users\\ncan access it only after going through an emergency approval', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='136cdfc9-c2b5-4da1-99f2-cc2a8f99540b', embedding=None, metadata={'page_label': '533', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='process to fix a problem, query critical historical information, etc.\\nAccess is revoked immediately once the work is done.\\nShared Responsibility in the Cloud\\nSecurity is a shared responsibility in the cloud. The cloud vendor is\\nresponsible for ensuring the physical security of its data center and\\nhardware. At the same time, you are responsible for the security of\\nthe applications and systems you build and maintain in the cloud.\\nMost cloud security breaches continue to be caused by end users,\\nnot the cloud. Breaches occur because of unintended\\nmisconfigurations, mistakes, oversights, and sloppiness.\\nAlways Back Up Your Data\\nData disappears. Sometimes it’s a dead hard drive or server; in other\\ncases, someone might accidentally delete a database or an object\\nstorage bucket. A bad actor can also lock away data. Ransomware\\nattacks are widespread these days. Some insurance companies are\\nreducing payouts in the event of an attack, leaving you on the hook\\nboth to recover your data and pay the bad actor who’s holding it\\nhostage. You need to back up your data regularly, both for disaster\\nrecovery and continuity of business operations, if a version of your\\ndata is compromised in a ransomware attack. Additionally, test the\\nrestoration of your data backups on a regular basis.\\nData backup doesn’t strictly fit under security and privacy practices;\\nit goes under the larger heading of disaster prevention, but it’s\\nadjacent to security, especially in the era of ransomware attacks.\\nAn Example Security Policy\\nThis section presents a sample security policy regarding credentials,\\ndevices, and sensitive information. Notice that we don’t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b095592e-15af-40b8-b1a9-26240d41d8e4', embedding=None, metadata={'page_label': '534', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='overcomplicate things; instead, we give people a short list of\\npractical actions they can take immediately.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9d55341c-5748-437c-844e-9e0a5f5c87b4', embedding=None, metadata={'page_label': '535', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='EXAMPLE SECURITY POLICY\\nProtect Your Credentials\\nProtect your credentials at all costs. Here are some ground rules\\nfor credentials:\\nUse a single-sign-on (SSO) for everything. Avoid passwords\\nwhenever possible, and use SSO as the default.\\nUse multifactor authentication with SSO.\\nDon’t share passwords or credentials. This includes client\\npasswords and credentials. If in doubt, see the person you\\nreport to. If that person is in doubt, keep digging until you\\nfind an answer.\\nBeware of phishing and scam calls. Don’t ever give your\\npasswords out. (Again, prioritize SSO.)\\nDisable or delete old credentials. Preferably the latter.\\nDon’t put your credentials in code. Handle secrets as\\nconfiguration and never commit them to version control. Use\\na secrets manager where possible.\\nAlways exercise the principle of least privilege. Never give\\nmore access than is required to do the job. This applies to all\\ncredentials and privileges in the cloud and on premises.\\nProtect Your Devices\\nUse device management for all devices used by employees.\\nIf an employee leaves the company or your device gets lost,\\nthe device can be remotely wiped.\\nUse multifactor authentication for all devices.\\nSign in to your device using your company email credentials.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d87b0a0-eba4-470c-9ac1-f382c1c2dd64', embedding=None, metadata={'page_label': '536', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='All policies covering credentials and behavior apply to your\\ndevice(s).\\nTreat your device as an extension of yourself. Don’t let your\\nassigned device(s) out of your sight.\\nWhen screen sharing, be aware of exactly what you’re\\nsharing to protect sensitive information and communications.\\nShare only single documents, browser tabs, or windows, and\\navoid sharing your full desktop. Share only what’s required\\nto convey your point.\\nUse “do not disturb” mode when on video calls; this prevents\\nmessages from appearing during calls or recordings.\\nSoftware Update Policy\\nRestart your web browser when you see an update alert.\\nRun minor OS updates on company and personal devices.\\nThe company will identify critical major OS updates and\\nprovide guidance.\\nDon’t use the beta version of an OS.\\nWait a week or two for new major OS version releases.\\nThese are some basic examples of how security can be simple and\\neffective. Based on your company’s security profile, you may need to\\nadd more requirements for people to follow. And again, always\\nremember that people are your weakest link in security.\\nTechnology\\nAfter you’ve addressed security with people and processes, it’s time\\nto look at how you leverage technology to secure your systems and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='862098b9-7112-488b-bfc3-50c5a35ae29b', embedding=None, metadata={'page_label': '537', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='data assets. The following are some significant areas you should\\nprioritize.\\nPatch and Update Systems\\nSoftware gets stale, and security vulnerabilities are constantly\\ndiscovered. To avoid exposing a security flaw in an older version of\\nthe tools you’re using, always patch and update operating systems\\nand software as new updates become available. Thankfully, many\\nSaaS and cloud-managed services automatically perform upgrades\\nand other maintenance without your intervention. To update your\\nown code and dependencies, either automate builds or set alerts on\\nreleases and vulnerabilities so you can be prompted to perform the\\nupdates manually.\\nEncryption\\nEncryption is not a magic bullet. It will do little to protect you in the\\nevent of a human security breach that grants access to credentials.\\nEncryption is a baseline requirement for any organization that\\nrespects security and privacy. It will protect you from basic attacks,\\nsuch as network traffic interception.\\nLet’s look separately at encryption at rest and in transit.\\nEncryption at rest\\nBe sure your data is encrypted when it is at rest (on a storage\\ndevice). Your company laptops should have full-disk encryption\\nenabled to protect data if a device is stolen. Implement server-side\\nencryption for all data stored in servers, filesystems, databases, and\\nobject storage in the cloud. All data backups for archival purposes\\nshould also be encrypted. Finally, incorporate application-level\\nencryption where applicable.\\nEncryption over the wire', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2acc7999-eaf2-43b1-ba5b-22d39c35643b', embedding=None, metadata={'page_label': '538', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Encryption over the wire is now the default for current protocols. For\\ninstance, HTTPS is generally required for modern cloud APIs. Data\\nengineers should always be aware of how keys are handled; bad key\\nhandling is a significant source of data leaks. In addition, HTTPS\\ndoes nothing to protect data if bucket permissions are left open to\\nthe public, another cause of several data scandals over the last\\ndecade.\\nEngineers should also be aware of the security limitations of older\\nprotocols. For example, FTP is simply not secure on a public\\nnetwork. While this may not appear to be a problem when data is\\nalready public, FTP is vulnerable to man-in-the-middle attacks,\\nwhereby an attacker intercepts downloaded data and changes it\\nbefore it arrives at the client. It is best to simply avoid FTP.\\nMake sure everything is encrypted over the wire, even with legacy\\nprotocols. When in doubt, use robust technology with encryption\\nbaked in.\\nLogging, Monitoring, and Alerting\\nHackers and bad actors typically don’t announce that they’re\\ninfiltrating your systems. Most companies don’t find out about\\nsecurity incidents until well after the fact. Part of DataOps is to\\nobserve, detect, and alert on incidents. As a data engineer, you\\nshould set up automated monitoring, logging, and alerting to be\\naware of peculiar events when they happen in your systems. If\\npossible, set up automatic anomaly detection.\\nHere are some areas you should monitor:\\nAccess\\nWho’s accessing what, when, and from where? What new\\naccesses were granted? Are there strange patterns with your\\ncurrent users that might indicate their account is compromised,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d40b595-aa14-4a22-bf02-6d8ab5f39487', embedding=None, metadata={'page_label': '539', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='such as trying to access systems they don’t usually access or\\nshouldn’t have access to? Do you see new unrecognized users\\naccessing your system? Be sure to regularly comb through\\naccess logs, users, and their roles to ensure that everything looks\\nOK.\\nResources\\nMonitor your disk, CPU, memory, and I/O for patterns that seem\\nout of the ordinary. Did your resources suddenly change? If so,\\nthis might indicate a security breach.\\nBilling\\nEspecially with SaaS and cloud-managed services, you need to\\noversee costs. Set up budget alerts to make sure your spending\\nis within expectations. If an unexpected spike occurs in your\\nbilling, this might indicate someone or something is utilizing your\\nresources for malicious purposes.\\nExcess permissions\\nIncreasingly, vendors are providing tools that monitor for\\npermissions that are not utilized by a user or service account\\nover some time. These tools can often be configured to\\nautomatically alert an administrator or remove permissions after a\\nspecified elapsed time.\\nFor example, suppose that a particular analyst hasn’t accessed\\nRedshift for six months. These permissions can be removed,\\nclosing a potential security hole. If the analyst needs to access\\nRedshift in the future, they can put in a ticket to restore\\npermissions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='57155313-fb40-45f1-8897-ea2c4e786429', embedding=None, metadata={'page_label': '540', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='It’s best to combine these areas in your monitoring to get a cross-\\nsectional view of your resource, access, and billing profile. We\\nsuggest setting up a dashboard for everyone on the data team to\\nview monitoring and receive alerts when something seems out of the\\nordinary. Couple this with an effective incident response plan to\\nmanage security breaches when they occur, and run through the\\nplan on a regular basis so you are prepared.\\nNetwork Access\\nWe often see data engineers doing pretty wild things regarding\\nnetwork access. In several instances, we’ve seen publicly available\\nAmazon S3 buckets housing lots of sensitive data. We’ve also\\nwitnessed Amazon EC2 instances with inbound SSH access open to\\nthe whole world for 0.0.0.0/0 (all IPs) or databases with open access\\nto all inbound requests over the public internet. These are just a few\\nexamples of terrible network security practices.\\nIn principle, network security should be left to security experts at\\nyour company. (In practice, you may need to assume significant\\nresponsibility for network security in a small company.) As a data\\nengineer, you will encounter databases, object storage, and servers\\nso often that you should at least be aware of simple measures you\\ncan take to make sure you’re in line with good network access\\npractices. Understand what IPs and ports are open, to whom, and\\nwhy. Allow the incoming IP addresses of the systems and users that\\nwill access these ports and avoid broadly opening connections for\\nany reason. When accessing the cloud or a SaaS tool, use an\\nencrypted connection. For example, don’t use an unencrypted\\nwebsite from a coffee shop.\\nAlso, while this book has focused almost entirely on running\\nworkloads in the cloud, we add a brief note here about hosting on-\\npremises servers. Recall that in Chapter 3, we discussed the\\ndifference between a hardened perimeter and zero-trust security.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c659181-aec3-4f86-92c2-16ae91d02214', embedding=None, metadata={'page_label': '541', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The cloud is generally closer to zero-trust security—every action\\nrequires authentication. We believe that the cloud is a more secure\\noption for most organizations because it imposes zero-trust practices\\nand allows companies to leverage the army of security engineers\\nemployed by the public clouds.\\nHowever, sometimes hardened perimeter security still makes sense;\\nwe find some solace in the knowledge that nuclear missile silos are\\nair gapped (not connected to any networks). Air-gapped servers are\\nthe ultimate example of a hardened security perimeter. Just keep in\\nmind that even on premises, air-gapped servers are vulnerable to\\nhuman security failings.\\nSecurity for Low-Level Data Engineering\\nFor engineers who work in the guts of data storage and processing\\nsystems, it is critical to consider the security implications of every\\nelement. Any software library, storage system, or compute node is a\\npotential security vulnerability. A flaw in an obscure logging library\\nmight allow attackers to bypass access controls or encryption. Even\\nCPU architectures and microcode represent potential vulnerabilities;\\nsensitive data can be vulnerable when it’s at rest in memory or a\\nCPU cache. No link in the chain can be taken for granted.\\nOf course, this book is principally about high-level data engineering\\n—stitching together tools to handle the entire lifecycle. Thus, we’ll\\nleave it to you to dig into the gory technical details.\\nInternal security research\\nWe discussed the idea of active security in “Processes”. We also\\nhighly recommend adopting an active security approach to\\ntechnology. Specifically, this means that every technology employee\\nshould think about security problems.\\nWhy is this important? Every technology contributor develops a\\ndomain of technical expertise. Even if your company employs an', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='36e7344a-2cff-4edb-9258-31da10cd6219', embedding=None, metadata={'page_label': '542', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='army of security researchers, data engineers will become intimately\\nfamiliar with specific data systems and cloud services in their\\npurview. Experts in a particular technology are well positioned to\\nidentify security holes in this technology.\\nEncourage every data engineer to be actively involved in security.\\nWhen they identify potential security risks in their systems, they\\nshould think through mitigations and take an active role in deploying\\nthese.\\nConclusion\\nSecurity needs to be a habit of mind and action; treat data like your\\nwallet or smartphone. Although you won’t likely be in charge of\\nsecurity for your company, knowing basic security practices and\\nkeeping security top of mind will help reduce the risk of data security\\nbreaches at your organization.\\nAdditional Resources\\nOpen Web Application Security Project (OWASP) publications\\nBuilding Secure and Reliable Systems by Heather Adkins et al.\\n(O’Reilly)\\nPractical Cloud Security by Chris Dotson (O’Reilly)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d6a4ea85-7f5f-4f8d-b6aa-bbdb6b74de59', embedding=None, metadata={'page_label': '543', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chapter 11. The Future of Data\\nEngineering\\nThis book grew out of the authors’ recognition that warp speed\\nchanges in the field have created a significant knowledge gap for\\nexisting data engineers, people interested in moving into a career in\\ndata engineering, technology managers, and executives who want to\\nbetter understand how data engineering fits into their companies.\\nWhen we started thinking about how to organize this book, we got\\nquite a bit of pushback from friends who’d ask, “How dare you write\\nabout a field that is changing so quickly?!” In many ways, they’re\\nright. It certainly feels like the field of data engineering—and, really,\\nall things data—is changing daily. Sifting through the noise and\\nfinding the signal of what’s unlikely to change was among the most\\nchallenging parts of organizing and writing this book.\\nIn this book, we focus on big ideas that we feel will be useful for the\\nnext several years—hence the continuum of the data engineering\\nlifecycle and its undercurrents. The order of operations and names of\\nbest practices and technologies might change, but the primary\\nstages of the lifecycle will likely remain intact for many years to\\ncome. We’re keenly aware that technology continues to change at\\nan exhausting pace; working in the technology sector in our present\\nera can feel like a rollercoaster ride or perhaps a hall of mirrors.\\nSeveral years ago, data engineering didn’t even exist as a field or\\njob title. Now you’re reading a book called Fundamentals of Data\\nEngineering! You’ve learned all about the fundamentals of data\\nengineering—its lifecycle, undercurrents, technologies, and best\\npractices. You might be asking yourself, what’s next in data\\nengineering? While nobody can predict the future, we have a good\\nperspective on the past, the present, and current trends. We’ve been', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6d1ac8cd-b606-4f3b-9714-645fc6dc9b16', embedding=None, metadata={'page_label': '544', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='fortunate to watch the genesis and evolution of data engineering\\nfrom a front-row seat. This final chapter presents our thoughts on the\\nfuture, including observations of ongoing developments and wild\\nfuture speculation.\\nThe Data Engineering Lifecycle Isn’t Going Away\\nWhile data science has received the bulk of the attention in recent\\nyears, data engineering is rapidly maturing into a distinct and visible\\nfield. It’s one of the fastest-growing careers in technology, with no\\nsigns of losing momentum. As companies realize they first need to\\nbuild a data foundation before moving to “sexier” things like AI and\\nML, data engineering will continue growing in popularity and\\nimportance. This progress centers around the data engineering\\nlifecycle.\\nSome question whether increasingly simple tools and practices will\\nlead to the disappearance of data engineers. This thinking is shallow,\\nlazy, and shortsighted. As organizations leverage data in new ways,\\nnew foundations, systems, and workflows will be needed to address\\nthese needs. Data engineers sit at the center of designing,\\narchitecting, building, and maintaining these systems. If tooling\\nbecomes easier to use, data engineers will move up the value chain\\nto focus on higher-level work. The data engineering lifecycle isn’t\\ngoing away anytime soon.\\nThe Decline of Complexity and the Rise of Easy-\\nto-Use Data Tools\\nSimplified, easy-to-use tools continue to lower the barrier to entry for\\ndata engineering. This is a great thing, especially given the shortage\\nof data engineers we’ve discussed. The trend toward simplicity will\\ncontinue. Data engineering isn’t dependent on a particular\\ntechnology or data size. It’s also not just for large companies. In the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0db4c9da-7e5c-417f-aea0-6c3eed2bdfd4', embedding=None, metadata={'page_label': '545', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2000s, deploying “big data” technologies required a large team and\\ndeep pockets. The ascendance of SaaS-managed services has\\nlargely removed the complexity of understanding the guts of various\\n“big data” systems. Data engineering is now something that all\\ncompanies can do.\\nBig data is a victim of its extraordinary success. For example,\\nGoogle BigQuery, a descendant of GFS and MapReduce, can query\\npetabytes of data. Once reserved for internal use at Google, this\\ninsanely powerful technology is now available to anybody with a\\nGCP account. Users simply pay for the data they store and query\\nrather than having to build a massive infrastructure stack. Snowflake,\\nAmazon EMR, and many other hyper-scalable cloud data solutions\\ncompete in the space and offer similar capabilities.\\nThe cloud is responsible for a significant shift in the usage of open\\nsource tools. Even in the early 2010s, using open source typically\\nentailed downloading the code and configuring it yourself.\\nNowadays, many open source data tools are available as managed\\ncloud services that compete directly with proprietary services. Linux\\nis available preconfigured and installed on server instances on all\\nmajor clouds. Serverless platforms like AWS Lambda and Google\\nCloud Functions allow you to deploy event-driven applications in\\nminutes, using mainstream languages such as Python, Java, and Go\\nrunning atop Linux behind the scenes. Engineers wishing to use\\nApache Airflow can adopt Google’s Cloud Composer or AWS’s\\nmanaged Airflow service. Managed Kubernetes allows us to build\\nhighly scalable microservice architectures. And so on.\\nThis fundamentally changes the conversation around open source\\ncode. In many cases, managed open source is just as easy to use\\nas its proprietary service competitors. Companies with highly\\nspecialized needs can also deploy managed open source, then\\nmove to self-managed open source later if they need to customize\\nthe underlying code.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fb7747aa-ca4b-4db0-bdff-d8d0dcbcd75d', embedding=None, metadata={'page_label': '546', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Another significant trend is the growth in popularity of off-the-shelf\\ndata connectors (at the time of this writing, popular ones include\\nFivetran and Airbyte). Data engineers have traditionally spent a lot of\\ntime and resources building and maintaining plumbing to connect to\\nexternal data sources. The new generation of managed connectors\\nis highly compelling, even for highly technical engineers, as they\\nbegin to recognize the value of recapturing time and mental\\nbandwidth for other projects. API connectors will be an outsourced\\nproblem so that data engineers can focus on the unique issues that\\ndrive their businesses.\\nThe intersection of red-hot competition in the data-tooling space with\\na growing number of data engineers means data tools will continue\\ndecreasing in complexity while adding even more functionality and\\nfeatures. This simplification will only grow the practice of data\\nengineering, as more and more companies find opportunities to\\ndiscover value in data.\\nThe Cloud-Scale Data OS and Improved\\nInteroperability\\nLet’s briefly review some of the inner workings of (single-device)\\noperating systems, then tie this back to data and the cloud. Whether\\nyou’re utilizing a smartphone, a laptop, an application server, or a\\nsmart thermostat, these devices rely on an operating system to\\nprovide essential services and orchestrate tasks and processes. For\\nexample, I can see roughly 300 processes running on the MacBook\\nPro that I’m typing on. Among other things, I see services such as\\nWindowServer (responsible for providing windows in a graphical\\ninterface) and CoreAudio (tasked with providing low-level audio\\ncapabilities).\\nWhen I run an application on this machine, it doesn’t directly access\\nsound and graphics hardware. Instead, it sends commands to\\noperating system services to draw windows and play sound. These', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='370d94cc-22bf-45f7-b12a-44b24ed38d48', embedding=None, metadata={'page_label': '547', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='commands are issued to standard APIs; a specification tells software\\ndevelopers how to communicate with operating system services. The\\noperating system orchestrates a boot process to provide these\\nservices, starting each service in the correct order based on\\ndependencies among them; it also maintains services by monitoring\\nthem and restarting them in the correct order in case of a failure.\\nNow let’s return to data in the cloud. The simplified data services that\\nwe’ve mentioned throughout this book (e.g., Google Cloud BigQuery,\\nAzure Blob Storage, Snowflake, and AWS Lambda) resemble\\noperating system services, but at a much larger scale, running\\nacross many machines rather than a single server.\\nNow that these simplified services are available, the next frontier of\\nevolution for this notion of a cloud data operating system will happen\\nat a higher level of abstraction. Benn Stancil called for the\\nemergence of standardized data APIs for building data pipelines and\\ndata applications. We predict that data engineering will gradually\\ncoalesce around a handful of data interoperability standards. Object\\nstorage in the cloud will grow in importance as a batch interface\\nlayer between various data services. New generation file formats\\n(such as Parquet and Avro) are already taking over for the purposes\\nof cloud data interchange, significantly improving on the dreadful\\ninteroperability of CSV, and the poor performance of raw JSON.\\nAnother critical ingredient of a data API ecosystem is a metadata\\ncatalog that describes schemas and data hierarchies. Currently, this\\nrole is largely filled by the legacy Hive Metastore. We expect that\\nnew entrants will emerge to take its place. Metadata will play a\\ncrucial role in data interoperability, both across applications and\\nsystems, and across clouds and networks, driving automation and\\nsimplification.\\nWe will also see significant improvements in the scaffolding that\\nmanages cloud data services. Apache Airflow has emerged as the\\nfirst truly cloud-oriented data orchestration platform, but we are on\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a399eae3-29e2-48fa-afab-f60671309054', embedding=None, metadata={'page_label': '548', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the cusp of significant enhancement. Airflow will grow in capabilities,\\nbuilding on its massive mindshare. New entrants such as Dagster\\nand Prefect will compete by rebuilding orchestration architecture\\nfrom the ground up.\\nThis next generation of data orchestration platforms will feature\\nenhanced data integration and data awareness. Orchestration\\nplatforms will integrate with data cataloging and lineage, becoming\\nsignificantly more data-aware in the process. In addition,\\norchestration platforms will build IaC capabilities (similar to\\nTerraform) and code deployment features (like GitHub Actions and\\nJenkins). This will allow engineers to code a pipeline, and then pass\\nit to the orchestration platform to automatically build, test, deploy and\\nmonitor. Engineers will be able to write infrastructure specifications\\ndirectly into their pipelines; missing infrastructure and services (e.g.,\\nSnowflake databases, Databricks clusters, and Amazon Kinesis\\nstreams) will be deployed the first time the pipeline runs.\\nWe will also see significant enhancements in the domain of live data\\n—e.g., streaming pipelines and databases capable of ingesting and\\nquerying streaming data. In the past, building a streaming DAG was\\nan extremely complex process with a high ongoing operational\\nburden (see Chapter 8). Tools like Apache Pulsar point the way\\ntoward a future in which streaming DAGs can be deployed with\\ncomplex transformations using relatively simple code. We have\\nalready seen the emergence of managed stream processors (such\\nas Amazon Kinesis Data Analytics and Google Cloud Dataflow), but\\nwe will see a new generation of orchestration tools for managing\\nthese services, stitching them together, and monitoring them. We\\ndiscuss live data in “The Live Data Stack”.\\nWhat does this enhanced abstraction mean for data engineers? As\\nwe’ve already argued in this chapter, the role of the data engineer\\nwon’t go away, but it will evolve significantly. By comparison, more\\nsophisticated mobile operating systems and frameworks have not\\neliminated mobile app developers. Instead, mobile app developers', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae681d7d-150e-4e14-9e09-24674bdcf24c', embedding=None, metadata={'page_label': '549', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='can now focus on building better-quality, more sophisticated\\napplications. We expect similar developments for data engineering\\nas the cloud-scale data OS paradigm increases interoperability and\\nsimplicity across various applications and systems.\\n“Enterprisey” Data Engineering\\nThe increasing simplification of data tools and the emergence and\\ndocumentation of best practices means data engineering will\\nbecome more “enterprisey.”  This will make many readers violently\\ncringe. The term enterprise, for some, conjures Kafkaesque\\nnightmares of faceless committees dressed in overly starched blue\\nshirts and khakis, endless red tape, and waterfall-managed\\ndevelopment projects with constantly slipping schedules and\\nballooning budgets. In short, some of you read “enterprise” and\\nimagine a soulless place where innovation goes to die.\\nFortunately, this is not what we’re talking about; we’re referring to\\nsome of the good things that larger companies do with data—\\nmanagement, operations, governance, and other “boring” stuff.\\nWe’re presently living through the golden age of “enterprisey” data\\nmanagement tools. Technologies and practices once reserved for\\ngiant organizations are trickling downstream. The once hard parts of\\nbig data and streaming data have now largely been abstracted away,\\nwith the focus shifting to ease of use, interoperability, and other\\nrefinements.\\nThis allows data engineers working on new tooling to find\\nopportunities in the abstractions of data management, DataOps, and\\nall the other undercurrents of data engineering. Data engineers will\\nbecome “enterprisey.” Speaking of which…\\nTitles and Responsibilities Will Morph...\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b3a0ee5e-6ae7-4657-a492-06860458b4fd', embedding=None, metadata={'page_label': '550', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='While the data engineering lifecycle isn’t going anywhere anytime\\nsoon, the boundaries between software engineering, data\\nengineering, data science, and ML engineering are increasingly\\nfuzzy. In fact, like the authors, many data scientists are transformed\\ninto data engineers through an organic process; tasked with doing\\n“data science” but lacking the tools to do their jobs, they take on the\\njob of designing and building systems to serve the data engineering\\nlifecycle.\\nAs simplicity moves up the stack, data scientists will spend a smaller\\nslice of their time gathering and munging data. But this trend will\\nextend beyond data scientists. Simplification also means data\\nengineers will spend less time on low-level tasks in the data\\nengineering lifecycle (managing servers, configuration, etc.), and\\n“enterprisey” data engineering will become more prevalent.\\nAs data becomes more tightly embedded in every business’s\\nprocesses, new roles will emerge in the realm of data and\\nalgorithms. One possibility is a role that sits between ML engineering\\nand data engineering. As ML toolsets become easier to use and\\nmanaged cloud ML services grow in capabilities, ML is shifting away\\nfrom ad hoc exploration and model development to become an\\noperational discipline.\\nThis new ML-focused engineer who straddles this divide will know\\nalgorithms, ML techniques, model optimization, model monitoring,\\nand data monitoring. However, their primary role will be to create or\\nutilize the systems that automatically train models, monitor\\nperformance, and operationalize the full ML process for model types\\nthat are well understood. They will also monitor data pipelines and\\nquality, overlapping into the current realm of data engineering. ML\\nengineers will become more specialized to work on model types that\\nare closer to research and less well understood.\\nAnother area in which titles may morph is at the intersection of\\nsoftware engineering and data engineering. Data applications, which', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f0eeb56d-cf99-42d6-a757-e438b0ad4eca', embedding=None, metadata={'page_label': '551', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='blend traditional software applications with analytics, will drive this\\ntrend. Software engineers will need to have a much deeper\\nunderstanding of data engineering. They will develop expertise in\\nthings like streaming, data pipelines, data modeling, and data quality.\\nWe will move beyond the “throw it over the wall” approach that is\\nnow pervasive. Data engineers will be integrated into application\\ndevelopment teams, and software developers will acquire data\\nengineering skills. The boundaries that exist between application\\nbackend systems and data engineering tools will be lowered as well,\\nwith deep integration through streaming and event-driven\\narchitectures.\\nMoving Beyond the Modern Data Stack, Toward\\nthe Live Data Stack\\nWe’ll be frank: the modern data stack (MDS) isn’t so modern. We\\napplaud the MDS for bringing a great selection of powerful data tools\\nto the masses, lowering prices, and empowering data analysts to\\ntake control of their data stack. The rise of ELT, cloud data\\nwarehouses, and the abstraction of SaaS data pipelines certainly\\nchanged the game for many companies, opening up new powers for\\nBI, analytics, and data science.\\nHaving said that, the MDS is basically a repackaging of old data\\nwarehouse practices using modern cloud and SaaS technologies;\\nbecause the MDS is built around the cloud data warehouse\\nparadigm, it has some serious limitations when compared to the\\npotential of next-generation real-time data applications. From our\\npoint of view, the world is moving beyond the use of data-\\nwarehouse-based internal-facing analytics and data science, toward\\npowering entire businesses and applications in real time with next\\ngeneration real-time databases.\\nWhat’s driving this evolution? In many cases, analytics (BI and\\noperational analytics) will be replaced by automation. Presently,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e1d8b0c8-a1f8-4153-a556-c39acaeca1d0', embedding=None, metadata={'page_label': '552', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='most dashboards and reports answer questions concerning what\\nand when. Ask yourself, “If I’m asking a what or when question, what\\naction do I take next?” If the action is repetitive, it is a candidate for\\nautomation. Why look at a report to determine whether to take action\\nwhen you can instead automate the action based on events as they\\noccur?\\nAnd it goes much further than this. Why does using a product like\\nTikTok, Uber, Google, or DoorDash feel like magic? While it seems\\nto you like a click of a button to watch a short video, order a ride or a\\nmeal, or find a search result, a lot is happening under the hood.\\nThese products are examples of true real-time data applications,\\ndelivering the actions you need at the click of a button while\\nperforming extremely sophisticated data processing and ML behind\\nthe scenes with miniscule latency. Presently, this level of\\nsophistication is locked away behind custom-built technologies at\\nlarge technology companies, but this sophistication and power are\\nbecoming democratized, similar to the way the MDS brought cloud-\\nscale data warehouses and pipelines to the masses. The data world\\nwill soon go “live.”\\nThe Live Data Stack\\nThis democratization of real-time technologies will lead us to the\\nsuccessor to the MDS: the live data stack will soon be accessible\\nand pervasive. The live data stack, depicted in Figure 11-1, will fuse\\nreal-time analytics and ML into applications by using streaming\\ntechnologies, covering the full data lifecycle from application source\\nsystems to data processing to ML, and back.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2cdb92cb-56c7-431b-b785-99cc11eac252', embedding=None, metadata={'page_label': '553', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 11-1. In the live data stack, data and intelligence moves in real time\\nbetween the application and supporting systems\\nJust as the MDS took advantage of the cloud and brought on-\\npremises data warehouse and pipeline technologies to the masses,\\nthe live data stack takes real-time data application technologies used\\nat elite tech companies and makes them available to companies of\\nall sizes as easy-to-use cloud-based offerings. This will open up a\\nnew world of possibilities for creating even better user experiences\\nand business value.\\nStreaming Pipelines and Real-Time Analytical\\nDatabases\\nThe MDS limits itself to batch techniques that treat data as bounded.\\nIn contrast, real-time data applications treat data as an unbounded,\\ncontinuous stream. Streaming pipelines and real-time analytical\\ndatabases are the two core technologies that will facilitate the move\\nfrom the MDS to the live data stack. While these technologies have\\nbeen around for some time, rapidly maturing managed cloud\\nservices will see them be deployed much more widely.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d504fae8-e2af-4219-9f69-cf43c72413d5', embedding=None, metadata={'page_label': '554', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Streaming technologies will continue to see extreme growth for the\\nforeseeable future. This will happen in conjunction with a clearer\\nfocus on the business utility of streaming data. Up to the present,\\nstreaming systems have frequently been treated like an expensive\\nnovelty or a dumb pipe for getting data from A to B. In the future,\\nstreaming will radically transform organizational technology and\\nbusiness processes; data architects and engineers will take the lead\\nin these fundamental changes.\\nReal-time analytical databases enable both fast ingestion and\\nsubsecond queries on this data. This data can be enriched or\\ncombined with historical datasets. When combined with a streaming\\npipeline and automation, or dashboard that is capable of real-time\\nanalytics, a whole new level of possibilities opens up. No longer are\\nyou constrained by slow-running ELT processes, 15-minute updates,\\nor other slow-moving parts. Data moves in a continuous flow. As\\nstreaming ingestion becomes more prevalent, batch ingestion will be\\nless and less common. Why create a batch bottleneck at the head of\\nyour data pipeline? We’ll eventually look at batch ingestion the same\\nway we now look at dial-up modems.\\nIn conjunction with the rise of streams, we expect a back-to-the-\\nfuture moment for data transformations. We’ll shift away from ELT—\\nin database transformations—to something that looks more like ETL.\\nWe provisionally refer to this as stream, transform, and load (STL).\\nIn a streaming context, extraction is an ongoing, continuous process.\\nOf course, batch transformations won’t entirely go away. Batch will\\nstill be very useful for model training, quarterly reporting, and more.\\nBut streaming transformation will become the norm.\\nWhile the data warehouse and data lake are great for housing large\\namounts of data and performing ad hoc queries, they are not so well\\noptimized for low-latency data ingestion or queries on rapidly moving\\ndata. The live data stack will be powered by OLAP databases that\\nare purpose-built for streaming. Today, databases like Druid,\\nClickHouse, Rockset, and Firebolt are leading the way in powering', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fb98f7af-97b6-441e-9b59-392eb8585781', embedding=None, metadata={'page_label': '555', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the backend of the next generation of data applications. We expect\\nthat streaming technologies will continue to evolve rapidly and that\\nnew technologies will proliferate.\\nAnother area we think is ripe for disruption is data modeling, where\\nthere hasn’t been serious innovation since the early 2000s. The\\ntraditional batch-oriented data modeling techniques you learned\\nabout in Chapter 8 aren’t suited for streaming data. New data-\\nmodeling techniques will occur not within the data warehouse, but in\\nthe systems that generate the data. We expect data modeling will\\ninvolve some notion of an upstream definitions layer—including\\nsemantics, metrics, lineage, and data definitions (see Chapter 9)—\\nbeginning where data is generated in the application. Modeling will\\nalso happen at every stage as data flows and evolves through the\\nfull lifecycle.\\nThe Fusion of Data with Applications\\nWe expect the next revolution will be the fusion of the application\\nand data layers. Right now, applications sit in one area, and the\\nMDS sits in another. To make matters worse, data is created with no\\nregard for how it will be used for analytics. Consequently, lots of duct\\ntape is needed to make systems talk with one another. This\\npatchwork, siloed setup is awkward and ungainly.\\nSoon, application stacks will be data stacks, and vice versa.\\nApplications will integrate real-time automation and decision making,\\npowered by the streaming pipelines and ML. The data engineering\\nlifecycle won’t necessarily change, but the time between stages of\\nthe lifecycle will drastically shorten. A lot of innovation will occur in\\nnew technologies and practices that will improve the experience of\\nengineering the live data stack. Pay attention to emerging database\\ntechnologies designed to address the mix of OLTP and OLAP use\\ncases; feature stores may also play a similar role for ML use cases.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='28423894-aee0-4da1-a364-060c7df73465', embedding=None, metadata={'page_label': '556', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The Tight Feedback Between Applications and\\nML\\nAnother area we’re excited about is the fusion of applications and\\nML. Today, applications and ML are disjointed systems, like\\napplications and analytics. Software engineers do their thing over\\nhere, data scientists and ML engineers do their thing over there.\\nML is well-suited for scenarios where data is generated at such a\\nhigh rate and volume that humans cannot feasibly process it by\\nhand. As data sizes and velocity grow, this applies to every scenario.\\nHigh volumes of fast-moving data, coupled with sophisticated\\nworkflows and actions, are candidates for ML. As data feedback\\nloops become shorter, we expect most applications to integrate ML.\\nAs data moves more quickly, the feedback loop between applications\\nand ML will tighten. The applications in the live data stack are\\nintelligent and able to adapt in real time to changes in the data. This\\ncreates a cycle of ever-smarter applications and increasing business\\nvalue.\\nDark Matter Data and the Rise\\nof...Spreadsheets?!\\nWe’ve talked about fast-moving data and how feedback loops will\\nshrink as applications, data, and ML work more closely together.\\nThis section might seem odd, but we need to address something\\nthat’s widely ignored in today’s data world, especially by engineers.\\nWhat’s the most widely used data platform? It’s the humble\\nspreadsheet. Depending on the estimates you read, the user base of\\nspreadsheets is between 700 million and 2 billion people.\\nSpreadsheets are the dark matter of the data world. A good deal of\\ndata analytics runs in spreadsheets and never makes its way into the\\nsophisticated data systems that we describe in this book. In many', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5b6b5e60-2925-46c8-b5a2-be72186d794a', embedding=None, metadata={'page_label': '557', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='organizations, spreadsheets handle financial reporting, supply-chain\\nanalytics, and even CRM.\\nAt heart, what is a spreadsheet? A spreadsheet is an interactive data\\napplication that supports complex analytics. Unlike purely code-\\nbased tools such as pandas (Python Data Analysis Library),\\nspreadsheets are accessible to a whole spectrum of users, ranging\\nfrom those who just know how to open files and look at reports to\\npower users who can script sophisticated procedural data\\nprocessing. So far, BI tools have failed to bring comparable\\ninteractivity to databases. Users who interact with the UI are typically\\nlimited to slicing and dicing data within certain guardrails, not\\ngeneral-purpose programmable analytics.\\nWe predict that a new class of tools will emerge that combines the\\ninteractive analytics capabilities of a spreadsheet with the backend\\npower of cloud OLAP systems. Indeed, some candidates are already\\nin the running. The ultimate winner in this product category may\\ncontinue to use spreadsheet paradigms, or may define entirely new\\ninterface idioms for interacting with data.\\nConclusion\\nThank you for joining us on this journey through data engineering!\\nWe traversed good architecture, the stages of the data engineering\\nlifecycle, and security best practices. We’ve discussed strategies for\\nchoosing technologies at a time when our field continues to change\\nat an extraordinary pace. In this chapter, we laid out our wild\\nspeculation about the near and intermediate future.\\nSome aspects of our prognostication sit on a relatively secure\\nfooting. The simplification of managed tooling and the rise of\\n“enterprisey” data engineering have proceeded day by day as we’ve\\nwritten this book. Other predictions are much more speculative in\\nnature; we see hints of an emerging live data stack, but this entails a\\nsignificant paradigm shift for both individual engineers and the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ed9a8879-c8e9-4cf2-9eee-743e4b0c1d72', embedding=None, metadata={'page_label': '558', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='organizations that employ them. Perhaps the trend toward real-time\\ndata will stall once again, with most companies continuing to focus\\non basic batch processing. Surely, other trends exist that we have\\ncompletely failed to identify. The evolution of technology involves\\ncomplex interactions of technology and culture. Both are\\nunpredictable.\\nData engineering is a vast topic; while we could not go into any\\ntechnical depth in individual areas, we hope that we have succeeded\\nin creating a kind of travel guide that will help current data engineers,\\nfuture data engineers, and those who work adjacent to the field to\\nfind their way in a domain that is in flux. We advise you to continue\\nexploration on your own. As you discover interesting topics and\\nideas in this book, continue the conversation as part of a community.\\nIdentify domain experts who can help you to uncover the strengths\\nand pitfalls of trendy technologies and practices. Read extensively\\nfrom the latest books, blog posts, and papers. Participate in meetups\\nand listen to talks. Ask questions and share your own expertise.\\nKeep an eye on vendor announcements to stay abreast of the latest\\ndevelopments, taking all claims with a healthy grain of salt.\\nThrough this process, you can choose technology. Next, you will\\nneed to adopt technology and develop expertise, perhaps as an\\nindividual contributor, perhaps within your team as a lead, perhaps\\nacross an entire technology organization. As you do this, don’t lose\\nsight of the larger goals of data engineering. Focus on the lifecycle,\\non serving your customers—internal and external—on your\\nbusiness, on serving and on your larger goals.\\nRegarding the future, many of you will play a role in determining\\nwhat comes next. Technology trends are defined not only by those\\nwho create the underlying technology, but by those who adopt it and\\nput it to good use. Successful tool use is as critical as tool creation.\\nFind opportunities to apply real-time technology that will improve the\\nuser experience, create value, and define entirely new types of\\napplications. It is this kind of practical application that will materialize', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='86f29f2d-c961-4322-b14f-6197fed81763', embedding=None, metadata={'page_label': '559', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='the live data stack as a new industry standard; or perhaps some\\nother new technology trend that we failed to identify will win the day.\\nFinally, we wish you an exciting career! We chose to work in data\\nengineering, to consult, and to write this book not simply because it\\nwas trendy, but because it was fascinating. We hope that we’ve\\nmanaged to convey to you a bit of the joy we’ve found working in this\\nfield.\\n1  Benn Stancil, “The Data OS,” benn.substack, September 3, 2021,\\nhttps://oreil.ly/HetE9.\\n2  Ben Rogojan, “Three Data Engineering Experts Share Their Thoughts on\\nWhere Data Is Headed,” Better Programming, May 27, 2021,\\nhttps://oreil.ly/IsY4W.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6317155c-e3f7-4910-b4c8-4139323885cd', embedding=None, metadata={'page_label': '560', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix A. Serialization and\\nCompression Technical Details\\nData engineers working in the cloud are generally freed from the\\ncomplexities of managing object storage systems. Still, they need to\\nunderstand details of serialization and deserialization formats. As we\\nmentioned in Chapter 6 about storage raw ingredients, serialization\\nand compression algorithms go hand in hand.\\nSerialization Formats\\nMany serialization algorithms and formats are available to data\\nengineers. While the abundance of options is a significant source of\\npain in data engineering, they are also a massive opportunity for\\nperformance improvements. We’ve sometimes seen job\\nperformance improve by a factor of 100 simply by switching from\\nCSV to Parquet serialization. As data moves through a pipeline,\\nengineers will also manage reserialization—conversion from one\\nformat to another. Sometimes data engineers have no choice but to\\naccept data in an ancient, nasty form; they must design processes to\\ndeserialize this format and handle exceptions, and then clean up and\\nconvert data for consistent, fast downstream processing and\\nconsumption.\\nRow-Based Serialization\\nAs its name suggests, row-based serialization organizes data by\\nrow. CSV format is an archetypal row-based format. For\\nsemistructured data (data objects that support nesting and schema\\nvariation), row-oriented serialization entails storing each object as a\\nunit.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d81f1ff-141e-4318-82ac-4481caae5384', embedding=None, metadata={'page_label': '561', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CSV: The nonstandard standard\\nWe discussed CSV in Chapter 7. CSV is a serialization format that\\ndata engineers love to hate. The term CSV is essentially a catchall\\nfor delimited text, but there is flexibility in conventions of escaping,\\nquote characters, delimiter, and more.\\nData engineers should avoid using CSV files in pipelines because\\nthey are highly error-prone and deliver poor performance. Engineers\\nare often required to use CSV format to exchange data with systems\\nand business processes outside their control. CSV is a common\\nformat for data archival. If you use CSV for archival, include a\\ncomplete technical description of the serialization configuration for\\nyour files so that future consumers can ingest the data.\\nXML\\nExtensible Markup Language (XML) was popular when HTML and\\nthe internet were new, but is now viewed as legacy; it is generally\\nslow to deserialize and serialize for data engineering applications.\\nXML is another standard that data engineers are often forced to\\ninteract with as they exchange data with legacy systems and\\nsoftware. JSON has largely replaced XML for plain-text object\\nserialization.\\nJSON and JSONL\\nJavaScript Object Notation (JSON) has emerged as the new\\nstandard for data exchange over APIs, and it has also become an\\nextremely popular format for data storage. In the context of\\ndatabases, the popularity of JSON has grown apace with the rise of\\nMongoDB and other document stores. Databases such as\\nSnowflake, BigQuery, and SQL Server also offer extensive native\\nsupport, facilitating easy data exchange between applications, APIs,\\nand database systems.\\nJSON Lines (JSONL) is a specialized version of JSON for storing\\nbulk semistructured data in files. JSONL stores a sequence of JSON', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d7002ec-170e-469d-a353-5e965618a712', embedding=None, metadata={'page_label': '562', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='objects, with objects delimited by line breaks. From our perspective,\\nJSONL is an extremely useful format for storing data right after it is\\ningested from API or applications. However, many columnar formats\\noffer significantly better performance. Consider moving to another\\nformat for intermediate pipeline stages and serving.\\nAvro\\nAvro is a row-oriented data format designed for RPCs and data\\nserialization. Avro encodes data into a binary format, with schema\\nmetadata specified in JSON. Avro is popular in the Hadoop\\necosystem and is also supported by various cloud data tools.\\nColumnar Serialization\\nThe serialization formats we’ve discussed so far are row-oriented.\\nData is encoded as complete relations (CSV) or documents (XML\\nand JSON), and these are written into files sequentially.\\nWith columnar serialization, data organization is essentially pivoted\\nby storing each column into its own set of files. One obvious\\nadvantage to columnar storage is that it allows us to read data from\\nonly a subset of fields rather than having to read full rows at once.\\nThis is a common scenario in analytics applications and can\\ndramatically reduce the amount of data that must be scanned to\\nexecute a query.\\nStoring data as columns also puts similar values next to each other,\\nallowing us to encode columnar data efficiently. One common\\ntechnique involves looking for repeated values and tokenizing these,\\na simple but highly efficient compression method for columns with\\nlarge numbers of repeats.\\nEven when columns don’t contain large numbers of repeated values,\\nthey may manifest high redundancy. Suppose that we organized\\ncustomer support messages into a single column of data. We likely\\nsee the same themes and verbiage again and again across these', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='751c9470-8857-4958-b208-51b5fb726770', embedding=None, metadata={'page_label': '563', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='messages, allowing data compression algorithms to realize a high\\nratio. For this reason, columnar storage is usually combined with\\ncompression, allowing us to maximize disk and network bandwidth\\nresources.\\nColumnar storage and compression come with some disadvantages\\ntoo. We cannot easily access individual data records; we must\\nreconstruct records by reading data from several column files.\\nRecord updates are also challenging. To change one field in one\\nrecord, we must decompress the column file, modify it, recompress\\nit, and write it back to storage. To avoid rewriting full columns on\\neach update, columns are broken into many files, typically using\\npartitioning and clustering strategies that organize data according to\\nquery and update patterns for the table. Even so, the overhead for\\nupdating a single row is horrendous. Columnar databases are a\\nterrible fit for transactional workloads, so transactional databases\\ngenerally utilize some form of row- or record-oriented storage.\\nParquet\\nParquet stores data in a columnar format and is designed to realize\\nexcellent read and write performance in a data lake environment.\\nParquet solves a few problems that frequently bedevil data\\nengineers. Parquet-encoded data builds in schema information and\\nnatively supports nested data, unlike CSV. Furthermore, Parquet is\\nportable; while databases such as BigQuery and Snowflake serialize\\ndata in proprietary columnar formats and offer excellent query\\nperformance on data stored internally, a huge performance hit\\noccurs when interoperating with external tools. Data must be\\ndeserialized, reserialized into an exchangeable format, and exported\\nto use data lake tools such as Spark and Presto. Parquet files in a\\ndata lake may be a superior option to proprietary cloud data\\nwarehouses in a polyglot tool environment.\\nParquet format is used with various compression algorithms; speed\\noptimized compression algorithms such as Snappy (discussed later', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e1c40147-ca16-48c1-b93f-65d6c159746c', embedding=None, metadata={'page_label': '564', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='in this appendix) are especially popular.\\nORC\\nOptimized Row Columnar (ORC) is a columnar storage format\\nsimilar to Parquet. ORC was very popular for use with Apache Hive;\\nwhile still widely used, we generally see it much less than Apache\\nParquet, and it enjoys somewhat less support in modern cloud\\necosystem tools. For example, Snowflake and BigQuery support\\nParquet file import and export; while they can read from ORC files,\\nneither tool can export to ORC.\\nApache Arrow or in-memory serialization\\nWhen we introduced serialization as a storage raw ingredient at the\\nbeginning of this chapter, we mentioned that software could store\\ndata in complex objects scattered in memory and connected by\\npointers, or more orderly, densely packed structures such as Fortran\\nand C arrays. Generally, densely packed in-memory data structures\\nwere limited to simple types (e.g., INT64) or fixed-width data\\nstructures (e.g., fixed-width strings). More complex structures (e.g.,\\nJSON documents) could not be densely stored in memory and\\nrequired serialization for storage and transfer between systems.\\nThe idea of Apache Arrow is to rethink serialization by utilizing a\\nbinary data format that is suitable for both in-memory processing and\\nexport. This allows us to avoid the overhead of serialization and\\ndeserialization; we simply use the same format for in-memory\\nprocessing, export over the network, and long-term storage. Arrow\\nrelies on columnar storage, where each column essentially gets its\\nown chunks of memory. For nested data, we use a technique called\\nshredding, which maps each location in the schema of JSON\\ndocuments into a separate column.\\nThis technique means that we can store a data file on disk, swap it\\ndirectly into program address space by using virtual memory, and\\nbegin running a query against the data without deserialization\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='701d4ce0-b5d7-4923-ae97-a1eabd4b3e05', embedding=None, metadata={'page_label': '565', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='overhead. In fact, we can swap chunks of the file into memory as we\\nscan it, and then swap them back out to avoid running out of\\nmemory for large datasets.\\nOne obvious headache with this approach is that different\\nprogramming languages serialize data in different ways. To address\\nthis issue, the Arrow Project has created software libraries for a\\nvariety of programming languages (including C, Go, Java,\\nJavaScript, MATLAB, Python, R, and Rust) that allow these\\nlanguages to interoperate with Arrow data in memory. In some\\ncases, these libraries use an interface between the chosen language\\nand low-level code in another language (e.g., C) to read and write\\nfrom Arrow. This allows high interoperability between languages\\nwithout extra serialization overhead. For example, a Scala program\\ncan use the Java library to write arrow data and then pass it as a\\nmessage to a Python program.\\nArrow is seeing rapid uptake with a variety of popular frameworks\\nsuch as Apache Spark. Arrow has also spanned a new data\\nwarehouse product; Dremio is a query engine and data warehouse\\nbuilt around Arrow serialization to support fast queries.\\nHybrid Serialization\\nWe use the term hybrid serialization to refer to technologies that\\ncombine multiple serialization techniques or integrate serialization\\nwith additional abstraction layers, such as schema management. We\\ncite as examples Apache Hudi and Apache Iceberg.\\nHudi\\nHudi stands for Hadoop Update Delete Incremental. This table\\nmanagement technology combines multiple serialization techniques\\nto allow columnar database performance for analytics queries while\\nalso supporting atomic, transactional updates. A typical Hudi\\napplication is a table that is updated from a CDC stream from a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a4fe24e3-9352-44b0-8bd1-779a9c87b8a6', embedding=None, metadata={'page_label': '566', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='transactional application database. The stream is captured into a\\nrow-oriented serialization format, while the bulk of the table is\\nretained in a columnar format. A query runs over both columnar and\\nrow-oriented files to return results for the current state of the table.\\nPeriodically, a repacking process runs that combines the row and\\ncolumnar files into updated columnar files to maximize query\\nefficiency.\\nIceberg\\nLike Hudi, Iceberg is a table management technology. Iceberg can\\ntrack all files that make up a table. It can also track files in each table\\nsnapshot over time, allowing table time travel in a data lake\\nenvironment. Iceberg supports schema evolution and can readily\\nmanage tables at a petabyte scale.\\nDatabase Storage Engines\\nTo round out the discussion of serialization, we briefly discuss\\ndatabase storage engines. All databases have an underlying storage\\nengine; many don’t expose their storage engines as a separate\\nabstraction (for example, BigQuery, Snowflake). Some (notably,\\nMySQL) support fully pluggable storage engines. Others (e.g., SQL\\nServer) offer major storage engine configuration options (columnar\\nversus row-based storage) that dramatically affect database\\nbehavior.\\nTypically, the storage engine is a separate software layer from the\\nquery engine. The storage engine manages all aspects of how data\\nis stored on a disk, including serialization, the physical arrangement\\nof data, and indexes.\\nStorage engines have seen significant innovation in the 2000s and\\n2010s. While storage engines in the past were optimized for direct\\naccess to spinning disks, modern storage engines are much better\\noptimized to support the performance characteristics of SSDs.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='38de1b2a-ad86-4d88-902d-9d8ab3ba61f4', embedding=None, metadata={'page_label': '567', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Storage engines also offer improved support for modern types and\\ndata structures, such as variable-length strings, arrays, and nested\\ndata.\\nAnother major change in storage engines is a shift toward columnar\\nstorage for analytics and data warehouse applications. SQL Server,\\nPostgreSQL, and MySQL offer robust columnar storage support.\\nCompression: gzip, bzip2, Snappy, etc.\\nThe math behind compression algorithms is complex, but the basic\\nidea is easy to understand: compression algorithms look for\\nredundancy and repetition in data, then re-encode data to reduce\\nredundancy. When we want to read the raw data, we decompress it\\nby reversing the algorithm and putting the redundancy back in.\\nFor example, you’ve noticed that certain words appear repeatedly in\\nreading this book. Running some quick analytics on the text, you\\ncould identify the words that occur most frequently and create\\nshortened tokens for these words. To compress, you would replace\\ncommon words with their tokens; to decompress, you would replace\\nthe tokens with their respective words.\\nPerhaps we could use this naive technique to realize a compression\\nratio of 2:1 or more. Compression algorithms utilize more\\nsophisticated mathematical techniques to identify and remove\\nredundancy; they can often realize compression ratios of 10: 1 on\\ntext data.\\nNote that we’re talking about lossless compression algorithms.\\nDecompressing data encoded with a lossless algorithm recovers a\\nbit-for-bit exact copy of the original data. Lossy compression\\nalgorithms for audio, images, and video aim for sensory fidelity;\\ndecompression recovers something that sounds like or looks like the\\noriginal but is not an exact copy. Data engineers might deal with', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e72c039f-b280-4d7f-af6b-d99825eea885', embedding=None, metadata={'page_label': '568', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='lossy compression algorithms in media processing pipelines but not\\nin serialization for analytics, where exact data fidelity is required.\\nTraditional compression engines such as gzip and bzip2 compress\\ntext data extremely well; they are frequently applied to JSON,\\nJSONL, XML, CSV, and other text-based data formats. Engineers\\nhave created a new generation of compression algorithms that\\nprioritize speed over compression ratio in recent years. Major\\nexamples are Snappy, Zstandard, LZFSE, and LZ4. These\\nalgorithms are frequently used to compress data in data lakes or\\ncolumnar databases to optimize fast query performance.\\n1  Dejan Simic, “Apache Arrow: Read DataFrame with Zero Memory,” Towards\\nData Science, June 25, 2020, https://oreil.ly/TDAdY.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f928df91-a933-4c8d-a39e-ccf061eb791b', embedding=None, metadata={'page_label': '569', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix B. Cloud Networking\\nThis appendix discusses some factors data engineers should\\nconsider about networking in the cloud. Data engineers frequently\\nencounter networking in their careers, and often ignore it despite its\\nimportance.\\nCloud Network Topology\\nA cloud network topology describes how various components in the\\ncloud are arranged and connected, such as cloud services,\\nnetworks, locations (zones, regions), and more. Data engineers\\nshould always know how cloud network topology will affect\\nconnectivity across the data systems they build. Microsoft Azure,\\nGoogle Cloud Platform (GCP), and Amazon Web Services (AWS) all\\nuse remarkably similar resource hierarchies of availability zones and\\nregions. At the time of this writing, GCP has added one additional\\nlayer, discussed in “GCP-Specific Networking and Multiregional\\nRedundancy”.\\nData Egress Charges\\nChapter 4 discusses cloud economics and how actual provider costs\\ndon’t necessarily drive cloud pricing. Regarding networking, clouds\\nallow inbound traffic for free but charge for outbound traffic to the\\ninternet. Outbound traffic is not inherently cheaper, but clouds use\\nthis method to create a moat around their services and increase the\\nstickiness of stored data, a practice that has been widely criticized.\\nNote that data egress charges can also apply to data passing\\nbetween availability zones and regions within a cloud.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ab8aa39-e5c7-48b1-abd1-e5c9f938f2a4', embedding=None, metadata={'page_label': '570', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Availability Zones\\nThe availability zone is the smallest unit of network topology that\\npublic clouds make visible to customers (Figure B-1). While a zone\\ncan potentially consist of multiple data centers, cloud customers\\ncannot control resource placement at this level.\\nFigure B-1. Availability zones in two separate regions\\nGenerally, clouds support their highest network bandwidth and\\nlowest latency between systems and services within a zone. High\\nthroughput data workloads should run on clusters located in a single\\nzone for performance and cost reasons. For example, an ephemeral\\nAmazon EMR cluster should generally sit in a single availability\\nzone.\\nIn addition, network traffic sent to VMs within a zone is free, but with\\na significant caveat: traffic must be sent to private IP addresses. The\\nmajor clouds utilize virtual networks known as virtual private clouds\\n(VPCs). Virtual machines have private IP addresses within the VPC.\\nThey may also be assigned public IP addresses to communicate\\nwith the outside world and receive traffic from the internet, but\\ncommunications using external IP addresses can incur data egress\\ncharges.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4ea974d5-b5fd-4e27-a555-cb6a39b7d831', embedding=None, metadata={'page_label': '571', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Regions\\nA region is a collection of two or more availability zones. Data\\ncenters require many resources to run (electrical power, water, etc.).\\nThe resources of separate availability zones are independent so that\\na local power outage doesn’t take down multiple availability zones.\\nEngineers can build highly resilient, separate infrastructure even\\nwithin a single region by running servers in multiple zones or\\ncreating automated cross-zone failover processes.\\nOffering multiple regions allows engineers to put resources close to\\nany of their users. Close means that users can realize good network\\nperformance in connecting to services, minimizing physical distance\\nalong the network path, and a minimal number of hops through\\nrouters. Both physical distance and network hops can increase\\nlatency and decrease performance. Major cloud providers continue\\nto add new regions.\\nIn general, regions support fast, low-latency networking between\\nzones; networking performance between zones will be worse than\\nwithin a single zone and incur nominal data egress charges between\\nVMs. Network data movement between regions is even slower and\\nmay incur higher egress fees.\\nIn general, object storage is a regional resource. Some data may\\npass between zones to reach a virtual machine, but this is mainly\\ninvisible to cloud customers, and there are no direct networking\\ncharges for this. (Of course, customers are still responsible for object\\naccess costs.)\\nDespite regions’ geo-redundant design, many major cloud service\\nfailures have affected entire regions, an example of correlated\\nfailure. Engineers often deploy code and configuration to entire\\nregions; the regional failures we’ve observed have generally resulted\\nfrom code or configuration problems occurring at the regional level.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7f8aa28f-cd5e-446f-bf98-fcdd5e403d20', embedding=None, metadata={'page_label': '572', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='GCP-Specific Networking and Multiregional\\nRedundancy\\nGCP offers a handful of unique abstractions that engineers should\\nbe aware of if they work in this cloud. The first is the multiregion, a\\nlayer in the resource hierarchy; a multiregion contains multiple\\nregions. Current multiregions are US (data centers in the United\\nStates), EU (data centers in European Union member states), and\\nASIA.\\nSeveral GCP resources support multiregions, including Cloud\\nStorage and BigQuery. Data is stored in multiple zones within the\\nmultiregion in a geo-redundant manner so that it should remain\\navailable in the event of a regional failure. Multiregional storage is\\nalso designed to deliver data efficiently to users within the\\nmultiregion without setting up complex replication processes\\nbetween regions. In addition, there are no data egress fees for VMs\\nin a multiregion to access Cloud Storage data in the same\\nmultiregion.\\nCloud customers can set up multiregional infrastructure on AWS or\\nAzure. In the case of databases or object storage, this involves\\nduplicating data between regions to increase redundancy and put\\ndata closer to users.\\nGoogle also essentially owns significantly more global scale\\nnetworking resources than other cloud providers, something which it\\noffers to its customers as premium tier networking. Premium tier\\nnetworking allows traffic between zones and regions to pass entirely\\nover Google-owned networks without traversing the public internet.\\nDirect Network Connections to the Clouds\\nEach major public cloud offers enhanced connectivity options,\\nallowing customers to integrate their networks with a cloud region or\\nVPC directly. For example, Amazon offers AWS Direct Connect. In', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4bbd549f-3537-428d-9641-2aef927a4918', embedding=None, metadata={'page_label': '573', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='addition to providing higher bandwidth and lower latency, these\\nconnection options often offer dramatic discounts on data egress\\ncharges. In a typical scenario in the US, AWS egress charges drop\\nfrom 9 cents per gigabyte over the public internet to 2 cents per\\ngigabyte over direct connect.\\nCDNs\\nContent delivery networks (CDNs) can offer dramatic performance\\nenhancements and discounts for delivering data assets to the public\\nor customers. Cloud providers offer CDN options and many other\\nproviders, such as Cloudflare. CDNs work best when delivering the\\nsame data repeatedly, but make sure that you read the fine print.\\nRemember that CDNs don’t work everywhere, and certain countries\\nmay block internet traffic and CDN delivery.\\nThe Future of Data Egress Fees\\nData egress fees are a significant impediment to interoperability,\\ndata sharing, and data movement to the cloud. Right now, data\\negress fees are a moat designed to prevent public cloud customers\\nfrom leaving or deploying across multiple clouds.\\nBut interesting signals indicate that change may be on the horizon.\\nIn particular, Zoom’s announcement in 2020 near the beginning of\\nthe COVID-19 pandemic that it chose Oracle as its cloud\\ninfrastructure provider caught the attention of many cloud watchers.\\nHow did Oracle win this significant cloud contract for critical remote\\nwork infrastructure against the cloud heavyweights? AWS expert\\nCorey Quinn offers a reasonably straightforward answer.  By his\\nback-of-the-envelope calculation, Zoom’s AWS monthly data egress\\nfees would run over $11 million dollars at list price; Oracle’s would\\ncost less than $2 million.\\n2\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2bb7e14c-b541-4fef-a87f-a4dbd4a578b5', embedding=None, metadata={'page_label': '574', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We suspect that one of GCP, AWS, or Azure will announce\\nsignificant cuts in egress fees in the next few years, leading to a sea\\nchange in the cloud business model.\\n1  Matthew Prince and Nitin Rao, “AWS’s Egregious Egress,” The Cloudflare\\nBlog, July 23, 2021, https://oreil.ly/NZqKa.\\n2  Mark Haranas and Steven Burke, “Oracle Bests Cloud Rivals to Win\\nBlockbuster Cloud Deal,” CRN, April 28, 2020, https://oreil.ly/LkqOi.\\n3  Corey Quinn, “Why Zoom Chose Oracle Cloud Over AWS and Maybe You\\nShould Too,” Last Week in AWS, April 28, 2020, https://oreil.ly/Lx5uu.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='023c2839-6917-4b3d-828a-6ff178a49757', embedding=None, metadata={'page_label': '575', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Index ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6f828160-94fd-4b94-9cf1-c46626f4fb59', embedding=None, metadata={'page_label': '576', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='About the Authors\\nJoe Reis is a business-minded data nerd who’s worked in the data\\nindustry for 20 years, with responsibilities ranging from statistical\\nmodeling, forecasting, machine learning, data engineering, data\\narchitecture, and almost everything else in between. Joe is the CEO\\nand Co-Founder of Ternary Data, a data engineering and\\narchitecture consulting firm based in Salt Lake City, Utah. In addition,\\nhe volunteers with several technology groups and teaches at the\\nUniversity of Utah. In his spare time, Joe likes to rock climb, produce\\nelectronic music, and take his kids on crazy adventures.\\nMatt Housley is a data engineering consultant and cloud specialist.\\nAfter some early programming experience with Logo, Basic and\\n6502 assembly, he completed a PhD in mathematics at the\\nUniversity of Utah. Matt then began working in data science,\\neventually specializing in cloud based data engineering. He co-\\nfounded Ternary Data with Joe Reis, where he leverages his\\nteaching experience to train future data engineers and advise teams\\non robust data architecture. Matt and Joe also pontificate on all\\nthings data on The Monday Morning Data Chat.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3a97e263-40b0-4bc1-a94f-79990dba934c', embedding=None, metadata={'page_label': '577', 'file_name': 'Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_path': 'c:\\\\Python_Testing\\\\rag-project\\\\data\\\\Fundamentals of Data Engineering -- Joe Reis & Matt Housley.pdf', 'file_type': 'application/pdf', 'file_size': 6735563, 'creation_date': '2025-06-01', 'last_modified_date': '2024-10-24'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Colophon \\nThe animal on the cover of Fundamentals of Data Engineering is the\\nwhite-eared puffbird (Nystalus chacuru).\\nSo named for the conspicuous patch of white at their ears, as well as\\nfor their fluffy plumage, these small, rotund birds are found across a\\nwide swath of central South America, where they inhabit forest\\nedges and savanna.\\nWhite-eared puffbirds are sit-and-wait hunters, perching in open\\nspaces for long periods and feeding opportunistically on insects,\\nlizards, and even small mammals that happen to come near. They\\nare most often found alone or in pairs and are relatively quiet birds,\\nvocalizing only rarely.\\nThe International Union for Conservation of Nature has listed the\\nwhite-eared puffbird as being of least concern, due, in part, to their\\nextensive range and stable population. Many of the animals on\\nO’Reilly covers are endangered; all of them are important to the\\nworld.\\nThe cover illustration is by Karen Montgomery, based on an antique\\nline engraving from Shaw’s General Zoology. The cover fonts are\\nGilroy Semibold and Guardian Sans. The text font is Adobe Minion\\nPro; the heading font is Adobe Myriad Condensed; and the code font\\nis Dalton Maag’s Ubuntu Mono.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e668da93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_Testing\\rag-project\\ragenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 854/854 [00:00<00:00, 3905.98it/s]\n",
      "Generating embeddings: 100%|██████████| 854/854 [00:12<00:00, 70.64it/s]\n"
     ]
    }
   ],
   "source": [
    "index= VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f22afb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x258b05db310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d6d0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e4a74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=query_engine.query(\"What is the Data Maturity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "923d3e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data Maturity refers to the level of development and sophistication an organization has achieved in managing, utilizing, and deriving value from its data. It is a way to assess an organization's data management capabilities and readiness for advanced analytics, artificial intelligence, and other data-driven initiatives. Each stage of Data Maturity represents a progression in leveraging data for business value and decision-making.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae82b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A relational data warehouse is a centralized repository where large volumes of structured data from various sources are stored and managed for historical and trend analysis reporting. It is based on the relational model, organizing data into tables consisting of rows and columns. The purpose of a relational data warehouse is to enable better business decision-making by providing a unified, consistent view of the organization's data, known as the single version of truth (SVOT). This ensures that all users have access to the same accurate information, eliminating discrepancies and data silos.\n"
     ]
    }
   ],
   "source": [
    "response=query_engine.query(\"Relational Data Warehouse?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2151d637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: A relational data warehouse is a centralized\n",
      "repository where large volumes of structured data from various sources\n",
      "are stored and managed for historical and trend analysis reporting. It\n",
      "is based on the relational model, organizing data into tables\n",
      "consisting of rows and columns. The purpose of a relational data\n",
      "warehouse is to enable better business decision-making by providing a\n",
      "unified, consistent view of the organization's data, known as the\n",
      "single version of truth (SVOT). This ensures that all users have\n",
      "access to the same accurate information, eliminating discrepancies and\n",
      "data silos.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 9cf63208-56c2-4a43-802f-d332672e9418\n",
      "Similarity: 0.8977204013314051\n",
      "Text: CHAPTER 4 The Relational Data Warehouse By the mid-2000s, I had\n",
      "used relational databases for years, but I had never been exposed to\n",
      "relational data warehouses. I was working as a database administrator\n",
      "(DBA) for a company that used an accounting software package to manage\n",
      "its finan‐ cial transactions. The reporting from the package was\n",
      "limited ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 4de3e9a5-e13e-4fc8-81a1-23403c9987f0\n",
      "Similarity: 0.8823345045639803\n",
      "Text: 2 A note on language: Y ou might see people refer to the\n",
      "relational data warehouse architecture as a traditional data\n",
      "warehouse. In this book, I mostly use relational data warehouse (RDW),\n",
      "sometimes shortening it to data warehouse. These terms all refer to\n",
      "the same thing. 3 RDW’s popularity increased thanks largely to both\n",
      "Barry Devlin and Bill ...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 6a2238ac-71a8-4b7b-a263-7c152309e11e\n",
      "Similarity: 0.8782176953635499\n",
      "Text: Not all data warehouses are based on the relational model. Non-\n",
      "relational data ware‐ houses include types like columnar, NoSQL, and\n",
      "graph data warehouses. However, relational data warehouses are much\n",
      "more popular and widely adopted, primarily because relational\n",
      "databases have been the dominant data management paradigm for decades.\n",
      "The relational...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 465bb94e-8cf1-451e-9169-cfcef02888ed\n",
      "Similarity: 0.8757133641055701\n",
      "Text: Summary This chapter covered the first widely used technology\n",
      "solution to centralize data from multiple sources and report on it:\n",
      "the relational data warehouse. The RDW revolu‐ tionized the way\n",
      "businesses and organizations manage their data by providing a cen‐\n",
      "tralized repository for data storage and retrieval, enabling more\n",
      "efficient data manag...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d016181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor ##if you want to use get response with similarity score more than perticular threshold\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4) ## You can change the number of top-k results to retrieve\n",
    "postprocessor = SimilarityPostprocessor(similarity_threshold=0.8)  # Optional: Set a similarity threshold for filtering results\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever,\n",
    "                                    node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51148e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 854/854 [00:00<00:00, 4054.50it/s]\n",
      "Generating embeddings: 100%|██████████| 854/854 [00:12<00:00, 67.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data Maturity refers to the level of development and sophistication an organization has achieved in managing, utilizing, and deriving value from its data. It is a model used to assess an organization's data management capabilities and readiness for advanced analytics, artificial intelligence, and other data-driven initiatives. Each stage of Data Maturity represents a progression in leveraging data for business value and decision-making.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    #load documents from the data directory and create a index\n",
    "    documents = SimpleDirectoryReader('data').load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "    #store it for later use\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    #load the index from storage\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "#either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the Data Maturity?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d1aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
